<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>X.X.Ren</title>
  
  <subtitle>个人博客</subtitle>
  <link href="https://xxren8218.github.io/atom.xml" rel="self"/>
  
  <link href="https://xxren8218.github.io/"/>
  <updated>2021-05-13T10:34:24.724Z</updated>
  <id>https://xxren8218.github.io/</id>
  
  <author>
    <name>任晓雄</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>聚类基础——KMEANS &amp; DBSCAN</title>
    <link href="https://xxren8218.github.io/20210513/%E8%81%9A%E7%B1%BB%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94KMEANS-DBSCAN.html"/>
    <id>https://xxren8218.github.io/20210513/%E8%81%9A%E7%B1%BB%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94KMEANS-DBSCAN.html</id>
    <published>2021-05-13T10:05:21.000Z</published>
    <updated>2021-05-13T10:34:24.724Z</updated>
    
    <content type="html"><![CDATA[<h1 id="聚类——啤酒分类"><a href="#聚类——啤酒分类" class="headerlink" title="聚类——啤酒分类"></a>聚类——啤酒分类</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># beer dataset</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">beer = pd.read_csv(<span class="string">&#x27;data.txt&#x27;</span>, sep=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">beer</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>name</th>      <th>calories</th>      <th>sodium</th>      <th>alcohol</th>      <th>cost</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>Budweiser</td>      <td>144</td>      <td>15</td>      <td>4.7</td>      <td>0.43</td>    </tr>    <tr>      <th>1</th>      <td>Schlitz</td>      <td>151</td>      <td>19</td>      <td>4.9</td>      <td>0.43</td>    </tr>    <tr>      <th>2</th>      <td>Lowenbrau</td>      <td>157</td>      <td>15</td>      <td>0.9</td>      <td>0.48</td>    </tr>    <tr>      <th>3</th>      <td>Kronenbourg</td>      <td>170</td>      <td>7</td>      <td>5.2</td>      <td>0.73</td>    </tr>    <tr>      <th>4</th>      <td>Heineken</td>      <td>152</td>      <td>11</td>      <td>5.0</td>      <td>0.77</td>    </tr>    <tr>      <th>5</th>      <td>Old_Milwaukee</td>      <td>145</td>      <td>23</td>      <td>4.6</td>      <td>0.28</td>    </tr>    <tr>      <th>6</th>      <td>Augsberger</td>      <td>175</td>      <td>24</td>      <td>5.5</td>      <td>0.40</td>    </tr>    <tr>      <th>7</th>      <td>Srohs_Bohemian_Style</td>      <td>149</td>      <td>27</td>      <td>4.7</td>      <td>0.42</td>    </tr>    <tr>      <th>8</th>      <td>Miller_Lite</td>      <td>99</td>      <td>10</td>      <td>4.3</td>      <td>0.43</td>    </tr>    <tr>      <th>9</th>      <td>Budweiser_Light</td>      <td>113</td>      <td>8</td>      <td>3.7</td>      <td>0.40</td>    </tr>    <tr>      <th>10</th>      <td>Coors</td>      <td>140</td>      <td>18</td>      <td>4.6</td>      <td>0.44</td>    </tr>    <tr>      <th>11</th>      <td>Coors_Light</td>      <td>102</td>      <td>15</td>      <td>4.1</td>      <td>0.46</td>    </tr>    <tr>      <th>12</th>      <td>Michelob_Light</td>      <td>135</td>      <td>11</td>      <td>4.2</td>      <td>0.50</td>    </tr>    <tr>      <th>13</th>      <td>Becks</td>      <td>150</td>      <td>19</td>      <td>4.7</td>      <td>0.76</td>    </tr>    <tr>      <th>14</th>      <td>Kirin</td>      <td>149</td>      <td>6</td>      <td>5.0</td>      <td>0.79</td>    </tr>    <tr>      <th>15</th>      <td>Pabst_Extra_Light</td>      <td>68</td>      <td>15</td>      <td>2.3</td>      <td>0.38</td>    </tr>    <tr>      <th>16</th>      <td>Hamms</td>      <td>139</td>      <td>19</td>      <td>4.4</td>      <td>0.43</td>    </tr>    <tr>      <th>17</th>      <td>Heilemans_Old_Style</td>      <td>144</td>      <td>24</td>      <td>4.9</td>      <td>0.43</td>    </tr>    <tr>      <th>18</th>      <td>Olympia_Goled_Light</td>      <td>72</td>      <td>6</td>      <td>2.9</td>      <td>0.46</td>    </tr>    <tr>      <th>19</th>      <td>Schlitz_Light</td>      <td>97</td>      <td>7</td>      <td>4.2</td>      <td>0.47</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = beer[[<span class="string">&quot;calories&quot;</span>,<span class="string">&quot;sodium&quot;</span>,<span class="string">&quot;alcohol&quot;</span>,<span class="string">&quot;cost&quot;</span>]]</span><br></pre></td></tr></table></figure><h2 id="K-means-clustering"><a href="#K-means-clustering" class="headerlink" title="K-means clustering"></a>K-means clustering</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line">km = KMeans(n_clusters=<span class="number">3</span>).fit(X) <span class="comment"># n_cluster就是聚集成几个簇</span></span><br><span class="line">km2 = KMeans(n_clusters=<span class="number">2</span>).fit(X)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">km.labels_  <span class="comment"># 调用函数直接返回结果了！</span></span><br></pre></td></tr></table></figure><pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 0, 0, 2, 1])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">beer[<span class="string">&#x27;cluster&#x27;</span>] = km.labels_</span><br><span class="line">beer[<span class="string">&#x27;cluster2&#x27;</span>] = km2.labels_</span><br><span class="line">beer.sort_values(<span class="string">&#x27;cluster&#x27;</span>)</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>name</th>      <th>calories</th>      <th>sodium</th>      <th>alcohol</th>      <th>cost</th>      <th>cluster</th>      <th>cluster2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>Budweiser</td>      <td>144</td>      <td>15</td>      <td>4.7</td>      <td>0.43</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>Schlitz</td>      <td>151</td>      <td>19</td>      <td>4.9</td>      <td>0.43</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>Lowenbrau</td>      <td>157</td>      <td>15</td>      <td>0.9</td>      <td>0.48</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>Kronenbourg</td>      <td>170</td>      <td>7</td>      <td>5.2</td>      <td>0.73</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>Heineken</td>      <td>152</td>      <td>11</td>      <td>5.0</td>      <td>0.77</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>5</th>      <td>Old_Milwaukee</td>      <td>145</td>      <td>23</td>      <td>4.6</td>      <td>0.28</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>6</th>      <td>Augsberger</td>      <td>175</td>      <td>24</td>      <td>5.5</td>      <td>0.40</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>7</th>      <td>Srohs_Bohemian_Style</td>      <td>149</td>      <td>27</td>      <td>4.7</td>      <td>0.42</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>17</th>      <td>Heilemans_Old_Style</td>      <td>144</td>      <td>24</td>      <td>4.9</td>      <td>0.43</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>16</th>      <td>Hamms</td>      <td>139</td>      <td>19</td>      <td>4.4</td>      <td>0.43</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>10</th>      <td>Coors</td>      <td>140</td>      <td>18</td>      <td>4.6</td>      <td>0.44</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>14</th>      <td>Kirin</td>      <td>149</td>      <td>6</td>      <td>5.0</td>      <td>0.79</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>12</th>      <td>Michelob_Light</td>      <td>135</td>      <td>11</td>      <td>4.2</td>      <td>0.50</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>13</th>      <td>Becks</td>      <td>150</td>      <td>19</td>      <td>4.7</td>      <td>0.76</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>9</th>      <td>Budweiser_Light</td>      <td>113</td>      <td>8</td>      <td>3.7</td>      <td>0.40</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>8</th>      <td>Miller_Lite</td>      <td>99</td>      <td>10</td>      <td>4.3</td>      <td>0.43</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>11</th>      <td>Coors_Light</td>      <td>102</td>      <td>15</td>      <td>4.1</td>      <td>0.46</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>19</th>      <td>Schlitz_Light</td>      <td>97</td>      <td>7</td>      <td>4.2</td>      <td>0.47</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>15</th>      <td>Pabst_Extra_Light</td>      <td>68</td>      <td>15</td>      <td>2.3</td>      <td>0.38</td>      <td>2</td>      <td>0</td>    </tr>    <tr>      <th>18</th>      <td>Olympia_Goled_Light</td>      <td>72</td>      <td>6</td>      <td>2.9</td>      <td>0.46</td>      <td>2</td>      <td>0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas.tools.plotting <span class="keyword">import</span> scatter_matrix  <span class="comment"># 导入scatter_matrix可以画多福图形。</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">cluster_centers = km.cluster_centers_</span><br><span class="line"></span><br><span class="line">cluster_centers_2 = km2.cluster_centers_</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beer.groupby(<span class="string">&quot;cluster&quot;</span>).mean()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>calories</th>      <th>sodium</th>      <th>alcohol</th>      <th>cost</th>      <th>cluster2</th>    </tr>    <tr>      <th>cluster</th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>150.00</td>      <td>17.0</td>      <td>4.521429</td>      <td>0.520714</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>102.75</td>      <td>10.0</td>      <td>4.075000</td>      <td>0.440000</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>70.00</td>      <td>10.5</td>      <td>2.600000</td>      <td>0.420000</td>      <td>0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beer.groupby(<span class="string">&quot;cluster2&quot;</span>).mean()</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>calories</th>      <th>sodium</th>      <th>alcohol</th>      <th>cost</th>      <th>cluster</th>    </tr>    <tr>      <th>cluster2</th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>91.833333</td>      <td>10.166667</td>      <td>3.583333</td>      <td>0.433333</td>      <td>1.333333</td>    </tr>    <tr>      <th>1</th>      <td>150.000000</td>      <td>17.000000</td>      <td>4.521429</td>      <td>0.520714</td>      <td>0.000000</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">centers = beer.groupby(<span class="string">&quot;cluster&quot;</span>).mean().reset_index() <span class="comment"># 中心点，为后面作图做准备！</span></span><br><span class="line">                                        <span class="comment"># reset_index(默认drop=False)，表示获取新的索引，并保留原来索引</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">14</span>  <span class="comment"># rcParams 可以设置图形整体的字体大小等。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">colors = np.array([<span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;yellow&#x27;</span>])</span><br></pre></td></tr></table></figure><p><strong> 先看其中两个特征的分布情况 </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(beer[<span class="string">&quot;calories&quot;</span>], beer[<span class="string">&quot;alcohol&quot;</span>],c=colors[beer[<span class="string">&quot;cluster&quot;</span>]])</span><br><span class="line"></span><br><span class="line">plt.scatter(centers.calories, centers.alcohol, linewidths=<span class="number">3</span>, marker=<span class="string">&#x27;+&#x27;</span>, s=<span class="number">300</span>, c=<span class="string">&#x27;black&#x27;</span>) <span class="comment"># centers.calories获取质心</span></span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&quot;Calories&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Alcohol&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.text.Text at 0x18a25af4ac8&gt;</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210513181033.png" alt=""></p><p><strong> 再看其中各个维度特征的分布 </strong> scatter_matrix</p><ul><li><p>数据是多维的，要么PCA，要么这种方式进行可视化</p></li><li><p>条形图代表自身的特征的分布情况</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scatter_matrix(beer[[<span class="string">&quot;calories&quot;</span>,<span class="string">&quot;sodium&quot;</span>,<span class="string">&quot;alcohol&quot;</span>,<span class="string">&quot;cost&quot;</span>]],s=<span class="number">100</span>, alpha=<span class="number">1</span>, c=colors[beer[<span class="string">&quot;cluster&quot;</span>]], figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.suptitle(<span class="string">&quot;With 3 centroids initialized&quot;</span>)</span><br></pre></td></tr></table></figure><div style="color: red">    C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: 'pandas.tools.plotting.scatter_matrix' is deprecated, import 'pandas.plotting.scatter_matrix' instead.      if __name__ == '__main__':</div><pre><code>&lt;matplotlib.text.Text at 0x18a25b67e80&gt;</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210513181104.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scatter_matrix(beer[[<span class="string">&quot;calories&quot;</span>,<span class="string">&quot;sodium&quot;</span>,<span class="string">&quot;alcohol&quot;</span>,<span class="string">&quot;cost&quot;</span>]],s=<span class="number">100</span>, alpha=<span class="number">1</span>, c=colors[beer[<span class="string">&quot;cluster2&quot;</span>]], figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.suptitle(<span class="string">&quot;With 2 centroids initialized&quot;</span>)</span><br></pre></td></tr></table></figure><div style="color: red">    C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: 'pandas.tools.plotting.scatter_matrix' is deprecated, import 'pandas.plotting.scatter_matrix' instead.      if __name__ == '__main__':</div><pre><code>&lt;matplotlib.text.Text at 0x18a2613c710&gt;</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210513181130.png" alt=""></p><h3 id="Scaled-data"><a href="#Scaled-data" class="headerlink" title="Scaled data"></a>Scaled data</h3><p><strong> sklearn进行数据标准化 </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line">X_scaled</span><br></pre></td></tr></table></figure><pre><code>array([[ 0.38791334,  0.00779468,  0.43380786, -0.45682969],       [ 0.6250656 ,  0.63136906,  0.62241997, -0.45682969],       [ 0.82833896,  0.00779468, -3.14982226, -0.10269815],       [ 1.26876459, -1.23935408,  0.90533814,  1.66795955],       [ 0.65894449, -0.6157797 ,  0.71672602,  1.95126478],       [ 0.42179223,  1.25494344,  0.3395018 , -1.5192243 ],       [ 1.43815906,  1.41083704,  1.1882563 , -0.66930861],       [ 0.55730781,  1.87851782,  0.43380786, -0.52765599],       [-1.1366369 , -0.7716733 ,  0.05658363, -0.45682969],       [-0.66233238, -1.08346049, -0.5092527 , -0.66930861],       [ 0.25239776,  0.47547547,  0.3395018 , -0.38600338],       [-1.03500022,  0.00779468, -0.13202848, -0.24435076],       [ 0.08300329, -0.6157797 , -0.03772242,  0.03895447],       [ 0.59118671,  0.63136906,  0.43380786,  1.88043848],       [ 0.55730781, -1.39524768,  0.71672602,  2.0929174 ],       [-2.18688263,  0.00779468, -1.82953748, -0.81096123],       [ 0.21851887,  0.63136906,  0.15088969, -0.45682969],       [ 0.38791334,  1.41083704,  0.62241997, -0.45682969],       [-2.05136705, -1.39524768, -1.26370115, -0.24435076],       [-1.20439469, -1.23935408, -0.03772242, -0.17352445]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">km = KMeans(n_clusters=<span class="number">3</span>).fit(X_scaled)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">beer[<span class="string">&quot;scaled_cluster&quot;</span>] = km.labels_</span><br><span class="line">beer.sort_values(<span class="string">&quot;scaled_cluster&quot;</span>)</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>name</th>      <th>calories</th>      <th>sodium</th>      <th>alcohol</th>      <th>cost</th>      <th>cluster</th>      <th>cluster2</th>      <th>scaled_cluster</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>Budweiser</td>      <td>144</td>      <td>15</td>      <td>4.7</td>      <td>0.43</td>      <td>0</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>Schlitz</td>      <td>151</td>      <td>19</td>      <td>4.9</td>      <td>0.43</td>      <td>0</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>17</th>      <td>Heilemans_Old_Style</td>      <td>144</td>      <td>24</td>      <td>4.9</td>      <td>0.43</td>      <td>0</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>16</th>      <td>Hamms</td>      <td>139</td>      <td>19</td>      <td>4.4</td>      <td>0.43</td>      <td>0</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>5</th>      <td>Old_Milwaukee</td>      <td>145</td>      <td>23</td>      <td>4.6</td>      <td>0.28</td>      <td>0</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>6</th>      <td>Augsberger</td>      <td>175</td>      <td>24</td>      <td>5.5</td>      <td>0.40</td>      <td>0</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>7</th>      <td>Srohs_Bohemian_Style</td>      <td>149</td>      <td>27</td>      <td>4.7</td>      <td>0.42</td>      <td>0</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>10</th>      <td>Coors</td>      <td>140</td>      <td>18</td>      <td>4.6</td>      <td>0.44</td>      <td>0</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>15</th>      <td>Pabst_Extra_Light</td>      <td>68</td>      <td>15</td>      <td>2.3</td>      <td>0.38</td>      <td>2</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>12</th>      <td>Michelob_Light</td>      <td>135</td>      <td>11</td>      <td>4.2</td>      <td>0.50</td>      <td>0</td>      <td>1</td>      <td>1</td>    </tr>    <tr>      <th>11</th>      <td>Coors_Light</td>      <td>102</td>      <td>15</td>      <td>4.1</td>      <td>0.46</td>      <td>1</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>9</th>      <td>Budweiser_Light</td>      <td>113</td>      <td>8</td>      <td>3.7</td>      <td>0.40</td>      <td>1</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>8</th>      <td>Miller_Lite</td>      <td>99</td>      <td>10</td>      <td>4.3</td>      <td>0.43</td>      <td>1</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>Lowenbrau</td>      <td>157</td>      <td>15</td>      <td>0.9</td>      <td>0.48</td>      <td>0</td>      <td>1</td>      <td>1</td>    </tr>    <tr>      <th>18</th>      <td>Olympia_Goled_Light</td>      <td>72</td>      <td>6</td>      <td>2.9</td>      <td>0.46</td>      <td>2</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>19</th>      <td>Schlitz_Light</td>      <td>97</td>      <td>7</td>      <td>4.2</td>      <td>0.47</td>      <td>1</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>13</th>      <td>Becks</td>      <td>150</td>      <td>19</td>      <td>4.7</td>      <td>0.76</td>      <td>0</td>      <td>1</td>      <td>2</td>    </tr>    <tr>      <th>14</th>      <td>Kirin</td>      <td>149</td>      <td>6</td>      <td>5.0</td>      <td>0.79</td>      <td>0</td>      <td>1</td>      <td>2</td>    </tr>    <tr>      <th>4</th>      <td>Heineken</td>      <td>152</td>      <td>11</td>      <td>5.0</td>      <td>0.77</td>      <td>0</td>      <td>1</td>      <td>2</td>    </tr>    <tr>      <th>3</th>      <td>Kronenbourg</td>      <td>170</td>      <td>7</td>      <td>5.2</td>      <td>0.73</td>      <td>0</td>      <td>1</td>      <td>2</td>    </tr>  </tbody></table></div><p>What are the “characteristics” of each cluster?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beer.groupby(<span class="string">&quot;scaled_cluster&quot;</span>).mean()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>calories</th>      <th>sodium</th>      <th>alcohol</th>      <th>cost</th>      <th>cluster</th>      <th>cluster2</th>    </tr>    <tr>      <th>scaled_cluster</th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>148.375</td>      <td>21.125</td>      <td>4.7875</td>      <td>0.4075</td>      <td>0.0</td>      <td>1.00</td>    </tr>    <tr>      <th>1</th>      <td>105.375</td>      <td>10.875</td>      <td>3.3250</td>      <td>0.4475</td>      <td>1.0</td>      <td>0.25</td>    </tr>    <tr>      <th>2</th>      <td>155.250</td>      <td>10.750</td>      <td>4.9750</td>      <td>0.7625</td>      <td>0.0</td>      <td>1.00</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.scatter_matrix(X, c=colors[beer.scaled_cluster], alpha=<span class="number">1</span>, figsize=(<span class="number">10</span>,<span class="number">10</span>), s=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><div style="color: red">    C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: pandas.scatter_matrix is deprecated. Use pandas.plotting.scatter_matrix instead      if __name__ == '__main__':</div>    <pre><code>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A279F8F28&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A282989B0&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27B5E2E8&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27B94F60&gt;],       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27BE41D0&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27C19F28&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27C61F60&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27C71C88&gt;],       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27CF1860&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27D3B7B8&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27D7C5C0&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27DC6F98&gt;],       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27E02748&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27E4FEB8&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27E8D588&gt;,        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27ED47B8&gt;]], dtype=object)</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210513181223.png" alt=""></p><h2 id="聚类评估：轮廓系数（Silhouette-Coefficient-）（常用）"><a href="#聚类评估：轮廓系数（Silhouette-Coefficient-）（常用）" class="headerlink" title="聚类评估：轮廓系数（Silhouette Coefficient ）（常用）"></a>聚类评估：轮廓系数（Silhouette Coefficient ）（常用）</h2><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210513183410.png" alt=""></p><ul><li>计算样本i到同簇其他样本的平均距离ai。ai 越小，说明样本i越应该被聚类到该簇。将ai 称为样本i的簇内不相似度。越小越好。</li><li>计算样本i到其他某簇Cj 的所有样本的平均距离bij，称为样本i与簇Cj 的不相似度。定义为样本i的簇间不相似度：bi =min{bi1, bi2, …, bik}。越大越好。</li></ul><ul><li>si接近1，则说明样本i聚类合理</li><li>si接近-1，则说明样本i更应该分类到另外的簇</li><li>若si 近似为0，则说明样本i在两个簇的边界上。</li></ul><p><strong> 使用sklearn模块的metrics进行聚类评估 </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">score_scaled = metrics.silhouette_score(X,beer.scaled_cluster) <span class="comment"># 做归一化的结果</span></span><br><span class="line">score = metrics.silhouette_score(X,beer.cluster) <span class="comment"># 不做归一化的结果</span></span><br><span class="line">print(score_scaled, score)</span><br></pre></td></tr></table></figure><pre><code>0.179780680894 0.673177504646</code></pre><p><strong> 做归一化的结果反而低了。</strong> 做归一化不一定会得到好结果。</p><h2 id="尝试计算不同的k值对结果的影响。"><a href="#尝试计算不同的k值对结果的影响。" class="headerlink" title="尝试计算不同的k值对结果的影响。"></a>尝试计算不同的k值对结果的影响。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scores = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,<span class="number">20</span>):</span><br><span class="line">    labels = KMeans(n_clusters=k).fit(X).labels_</span><br><span class="line">    score = metrics.silhouette_score(X, labels)</span><br><span class="line">    scores.append(score)</span><br><span class="line"></span><br><span class="line">scores</span><br></pre></td></tr></table></figure><pre><code>[0.69176560340794857, 0.67317750464557957, 0.58570407211277953, 0.42254873351720201, 0.4559182167013377, 0.43776116697963124, 0.38946337473125997, 0.39746405172426014, 0.33061511213823314, 0.34131096180393328, 0.34597752371272478, 0.31221439248428434, 0.30707782144770296, 0.31834561839139497, 0.28495140011748982, 0.23498077333071996, 0.15880910174962809, 0.084230513801511767]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">2</span>,<span class="number">20</span>)), scores)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Number of Clusters Initialized&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Sihouette Score&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.text.Text at 0x18a288239e8&gt;</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210513181248.png" alt=""></p><p><strong> 可以看出K=2的时候比较合适 </strong></p><h2 id="DBSCAN-clustering"><a href="#DBSCAN-clustering" class="headerlink" title="DBSCAN clustering"></a>DBSCAN clustering</h2><ul><li>在不规则的数据集上比较强大，简单数据集可能还不如kmeans.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line">db = DBSCAN(eps=<span class="number">10</span>, min_samples=<span class="number">2</span>).fit(X)  <span class="comment"># eps半径，min_samples指密度</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels = db.labels_</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">beer[<span class="string">&#x27;cluster_db&#x27;</span>] = labels</span><br><span class="line">beer.sort_values(<span class="string">&#x27;cluster_db&#x27;</span>)</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>name</th>      <th>calories</th>      <th>sodium</th>      <th>alcohol</th>      <th>cost</th>      <th>cluster</th>      <th>cluster2</th>      <th>scaled_cluster</th>      <th>cluster_db</th>    </tr>  </thead>  <tbody>    <tr>      <th>9</th>      <td>Budweiser_Light</td>      <td>113</td>      <td>8</td>      <td>3.7</td>      <td>0.40</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>-1</td>    </tr>    <tr>      <th>3</th>      <td>Kronenbourg</td>      <td>170</td>      <td>7</td>      <td>5.2</td>      <td>0.73</td>      <td>0</td>      <td>1</td>      <td>2</td>      <td>-1</td>    </tr>    <tr>      <th>6</th>      <td>Augsberger</td>      <td>175</td>      <td>24</td>      <td>5.5</td>      <td>0.40</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>-1</td>    </tr>    <tr>      <th>17</th>      <td>Heilemans_Old_Style</td>      <td>144</td>      <td>24</td>      <td>4.9</td>      <td>0.43</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>16</th>      <td>Hamms</td>      <td>139</td>      <td>19</td>      <td>4.4</td>      <td>0.43</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>14</th>      <td>Kirin</td>      <td>149</td>      <td>6</td>      <td>5.0</td>      <td>0.79</td>      <td>0</td>      <td>1</td>      <td>2</td>      <td>0</td>    </tr>    <tr>      <th>13</th>      <td>Becks</td>      <td>150</td>      <td>19</td>      <td>4.7</td>      <td>0.76</td>      <td>0</td>      <td>1</td>      <td>2</td>      <td>0</td>    </tr>    <tr>      <th>12</th>      <td>Michelob_Light</td>      <td>135</td>      <td>11</td>      <td>4.2</td>      <td>0.50</td>      <td>0</td>      <td>1</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>10</th>      <td>Coors</td>      <td>140</td>      <td>18</td>      <td>4.6</td>      <td>0.44</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>0</th>      <td>Budweiser</td>      <td>144</td>      <td>15</td>      <td>4.7</td>      <td>0.43</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>7</th>      <td>Srohs_Bohemian_Style</td>      <td>149</td>      <td>27</td>      <td>4.7</td>      <td>0.42</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>5</th>      <td>Old_Milwaukee</td>      <td>145</td>      <td>23</td>      <td>4.6</td>      <td>0.28</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>Heineken</td>      <td>152</td>      <td>11</td>      <td>5.0</td>      <td>0.77</td>      <td>0</td>      <td>1</td>      <td>2</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>Lowenbrau</td>      <td>157</td>      <td>15</td>      <td>0.9</td>      <td>0.48</td>      <td>0</td>      <td>1</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>Schlitz</td>      <td>151</td>      <td>19</td>      <td>4.9</td>      <td>0.43</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>8</th>      <td>Miller_Lite</td>      <td>99</td>      <td>10</td>      <td>4.3</td>      <td>0.43</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>1</td>    </tr>    <tr>      <th>11</th>      <td>Coors_Light</td>      <td>102</td>      <td>15</td>      <td>4.1</td>      <td>0.46</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>1</td>    </tr>    <tr>      <th>19</th>      <td>Schlitz_Light</td>      <td>97</td>      <td>7</td>      <td>4.2</td>      <td>0.47</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>1</td>    </tr>    <tr>      <th>15</th>      <td>Pabst_Extra_Light</td>      <td>68</td>      <td>15</td>      <td>2.3</td>      <td>0.38</td>      <td>2</td>      <td>0</td>      <td>1</td>      <td>2</td>    </tr>    <tr>      <th>18</th>      <td>Olympia_Goled_Light</td>      <td>72</td>      <td>6</td>      <td>2.9</td>      <td>0.46</td>      <td>2</td>      <td>0</td>      <td>1</td>      <td>2</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beer.groupby(<span class="string">&#x27;cluster_db&#x27;</span>).mean()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>calories</th>      <th>sodium</th>      <th>alcohol</th>      <th>cost</th>      <th>cluster</th>      <th>cluster2</th>      <th>scaled_cluster</th>    </tr>    <tr>      <th>cluster_db</th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>-1</th>      <td>152.666667</td>      <td>13.000000</td>      <td>4.800000</td>      <td>0.510000</td>      <td>0.333333</td>      <td>0.666667</td>      <td>1.000000</td>    </tr>    <tr>      <th>0</th>      <td>146.250000</td>      <td>17.250000</td>      <td>4.383333</td>      <td>0.513333</td>      <td>0.000000</td>      <td>1.000000</td>      <td>0.666667</td>    </tr>    <tr>      <th>1</th>      <td>99.333333</td>      <td>10.666667</td>      <td>4.200000</td>      <td>0.453333</td>      <td>1.000000</td>      <td>0.000000</td>      <td>1.000000</td>    </tr>    <tr>      <th>2</th>      <td>70.000000</td>      <td>10.500000</td>      <td>2.600000</td>      <td>0.420000</td>      <td>2.000000</td>      <td>0.000000</td>      <td>1.000000</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.scatter_matrix(X, c=colors[beer.cluster_db], figsize=(<span class="number">10</span>,<span class="number">10</span>), s=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><div style="color: red">    C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: pandas.scatter_matrix is deprecated. Use pandas.plotting.scatter_matrix instead      if __name__ == '__main__':</div>    <div style="overflow: scroll;">    array([[<matplotlib.axes._subplots.AxesSubplot object at 0x0000018A278A3940>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A284C56D8>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A28501CF8>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A28550080>],           [<matplotlib.axes._subplots.AxesSubplot object at 0x0000018A2856C588>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A285D1F60>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A286211D0>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A2865AF98>],           [<matplotlib.axes._subplots.AxesSubplot object at 0x0000018A286AABA8>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A286E7278>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A2872E390>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A287396A0>],           [<matplotlib.axes._subplots.AxesSubplot object at 0x0000018A287BC358>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A28B356A0>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A28B71240>,            <matplotlib.axes._subplots.AxesSubplot object at 0x0000018A28BBC470>]], dtype=object)</div><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210513181312.png" alt=""></p><p><strong> DBSCAN的后续过程一样，可以用轮廓系数进行评估。for循环eps和min_samples。也可以做数据增强。（看哪个数据的维度对结果有比较好的影响） </strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;聚类——啤酒分类&quot;&gt;&lt;a href=&quot;#聚类——啤酒分类&quot; class=&quot;headerlink&quot; title=&quot;聚类——啤酒分类&quot;&gt;&lt;/a&gt;聚类——啤酒分类&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td c</summary>
      
    
    
    
    <category term="机器学习" scheme="https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习基础" scheme="https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（六十七）：剪绳子</title>
    <link href="https://xxren8218.github.io/20210512/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AD%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9A%E5%89%AA%E7%BB%B3%E5%AD%90.html"/>
    <id>https://xxren8218.github.io/20210512/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AD%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9A%E5%89%AA%E7%BB%B3%E5%AD%90.html</id>
    <published>2021-05-12T08:49:55.000Z</published>
    <updated>2021-05-12T09:07:26.535Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><div style="color: red;">    给你一根长度为n的绳子，请把绳子剪成整数长的m段（m、n都是整数，n>1并且m>1，m<=n），每段绳子的长度记为k[1],...,k[m]。请问k[1]x...xk[m]可能的最大乘积是多少？例如，当绳子的长度是8时，我们把它剪成长度分别为2、3、3的三段，此时得到的最大乘积是18。</div><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><p>这道题可以用数学的方法来解决——基本不等式。如下图<br><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210512165604.JPG" alt=""><br>即将 x1+x2+…+xm=n 分成 m 份，然后求导得到极大值<br><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210512170104.JPG" alt=""><br><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210512170218.JPG" alt=""><br><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210512170240.JPG" alt=""><br><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210512170257.JPG" alt=""><br>得到应该均分成3份，又分三种情况，能整除。余数为1时，（4=2+2=1+3），明显2x2&gt;1x3，余数为2时正常写（5的话2x3是最大的。）<br><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210512170344.JPG" alt=""></p><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cutRope</span>(<span class="params">self, number</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> number &lt;= <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">return</span> number-<span class="number">1</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        a, b = number//<span class="number">3</span>, number%<span class="number">3</span></span><br><span class="line">        <span class="keyword">if</span> b == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">3</span>**a</span><br><span class="line">        <span class="keyword">if</span> b == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">3</span>**(a-<span class="number">1</span>)*<span class="number">4</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">3</span>**a*<span class="number">2</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;div style=&quot;color: red;&quot;&gt;
    给你一根长度为n的绳子，请把绳子剪成整数长的m段（m、n都是整数，n&gt;</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="其他" scheme="https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（六十三）：数据流中的中位数</title>
    <link href="https://xxren8218.github.io/20210512/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AD%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0.html"/>
    <id>https://xxren8218.github.io/20210512/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AD%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0.html</id>
    <published>2021-05-12T08:38:58.000Z</published>
    <updated>2021-05-12T08:45:00.618Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><div style="color: red">    如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。</div><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>可以建立一个列表来append所给的数据然后将列表排序，计算列表的长度。只需要区分奇数和偶数情况即可。</code></pre><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.s = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Insert</span>(<span class="params">self, num</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        self.s.append(num)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetMedian</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        self.s = <span class="built_in">sorted</span>(self.s)</span><br><span class="line">        n = <span class="built_in">len</span>(self.s)</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> n % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 奇数</span></span><br><span class="line">            <span class="keyword">return</span> self.s[n/<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 偶数</span></span><br><span class="line">            <span class="keyword">return</span> (self.s[n/<span class="number">2</span>]+self.s[n/<span class="number">2</span>-<span class="number">1</span>])/<span class="number">2.0</span> <span class="comment"># 注意如果这里写2的话，得到的是向下取整的整数，所以需要写成浮点型。</span></span><br><span class="line">            </span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;div style=&quot;color: red&quot;&gt;
    如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="其他" scheme="https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（六十四）：滑动窗口的最大值</title>
    <link href="https://xxren8218.github.io/20210512/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AD%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9A%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC.html"/>
    <id>https://xxren8218.github.io/20210512/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AD%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9A%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC.html</id>
    <published>2021-05-12T08:25:59.000Z</published>
    <updated>2021-05-12T08:37:50.788Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><div style="color: red;">    给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}。</div><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>创建一个列表res来存储最终结果创建一个临时列表temp来存储目前窗口内的数字python有内置的max()函数可以找出列表中的最大数字。将其放入结果列表中即可。while循环，直到窗口的右边界到达给定列表长度时，停止。</code></pre><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxInWindows</span>(<span class="params">self, num, size</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> num <span class="keyword">or</span> size &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        res = []</span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        end = start + size</span><br><span class="line">        <span class="keyword">while</span> end &lt;= <span class="built_in">len</span>(num):</span><br><span class="line">            temp_list = num[start:end]</span><br><span class="line">            res.append(<span class="built_in">max</span>(temp_list))</span><br><span class="line">            start += <span class="number">1</span></span><br><span class="line">            end += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;div style=&quot;color: red;&quot;&gt;
    给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="其他" scheme="https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯实战-新闻分类</title>
    <link href="https://xxren8218.github.io/20210511/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9E%E6%88%98-%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB.html"/>
    <id>https://xxren8218.github.io/20210511/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9E%E6%88%98-%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB.html</id>
    <published>2021-05-11T06:25:35.000Z</published>
    <updated>2021-05-11T10:39:40.681Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文本分析——新闻分类"><a href="#文本分析——新闻分类" class="headerlink" title="文本分析——新闻分类"></a>文本分析——新闻分类</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="comment"># pip install jieba</span></span><br></pre></td></tr></table></figure><h3 id="数据来源：http-www-sogou-com-labs-resource-ca-php"><a href="#数据来源：http-www-sogou-com-labs-resource-ca-php" class="headerlink" title="数据来源：http://www.sogou.com/labs/resource/ca.php"></a>数据来源：<a href="http://www.sogou.com/labs/resource/ca.php">http://www.sogou.com/labs/resource/ca.php</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_news = pd.read_table(<span class="string">&#x27;./data/val.txt&#x27;</span>,names=[<span class="string">&#x27;category&#x27;</span>,<span class="string">&#x27;theme&#x27;</span>,<span class="string">&#x27;URL&#x27;</span>,<span class="string">&#x27;content&#x27;</span>],encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="comment"># 涉及中文，用encoding</span></span><br><span class="line">df_news = df_news.dropna() <span class="comment"># 缺失值直接drop掉</span></span><br><span class="line">df_news.head()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>category</th>      <th>theme</th>      <th>URL</th>      <th>content</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>汽车</td>      <td>新辉腾　４．２　Ｖ８　４座加长Ｉｎｄｉｖｉｄｕａｌ版２０１１款　最新报价</td>      <td>http://auto.data.people.com.cn/model_15782/</td>      <td>经销商　电话　试驾／订车Ｕ憬杭州滨江区江陵路１７８０号４００８－１１２２３３转５８６４＃保常...</td>    </tr>    <tr>      <th>1</th>      <td>汽车</td>      <td>９１８　Ｓｐｙｄｅｒ概念车</td>      <td>http://auto.data.people.com.cn/prdview_165423....</td>      <td>呼叫热线　４００８－１００－３００　服务邮箱　ｋｆ＠ｐｅｏｐｌｅｄａｉｌｙ．ｃｏｍ．ｃｎ</td>    </tr>    <tr>      <th>2</th>      <td>汽车</td>      <td>日内瓦亮相　ＭＩＮＩ性能版／概念车－１．６Ｔ引擎</td>      <td>http://auto.data.people.com.cn/news/story_5249...</td>      <td>ＭＩＮＩ品牌在二月曾经公布了最新的ＭＩＮＩ新概念车Ｃｌｕｂｖａｎ效果图，不过现在在日内瓦车展...</td>    </tr>    <tr>      <th>3</th>      <td>汽车</td>      <td>清仓大甩卖一汽夏利Ｎ５威志Ｖ２低至３．３９万</td>      <td>http://auto.data.people.com.cn/news/story_6144...</td>      <td>清仓大甩卖！一汽夏利Ｎ５、威志Ｖ２低至３．３９万＝日，启新中国一汽强势推出一汽夏利Ｎ５、威志...</td>    </tr>    <tr>      <th>4</th>      <td>汽车</td>      <td>大众敞篷家族新成员　高尔夫敞篷版实拍</td>      <td>http://auto.data.people.com.cn/news/story_5686...</td>      <td>在今年３月的日内瓦车展上，我们见到了高尔夫家族的新成员，高尔夫敞篷版，这款全新敞篷车受到了众...</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_news.shape</span><br></pre></td></tr></table></figure><pre><code>(5000, 4)</code></pre><h3 id="分词：使用结吧分词器"><a href="#分词：使用结吧分词器" class="headerlink" title="分词：使用结吧分词器"></a>分词：使用结吧分词器</h3><p><strong> 结巴分词器需要转换成list的格式 </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">content = df_news.content.values.tolist()</span><br><span class="line"><span class="built_in">print</span> (content[<span class="number">1000</span>])</span><br></pre></td></tr></table></figure><div style="overflow: scroll;">      阿里巴巴集团昨日宣布，将在集团管理层面设立首席数据官岗位（Ｃｈｉｅｆ　Ｄａｔａ　Ｏｆｆｉｃｅｒ），阿里巴巴Ｂ２Ｂ公司ＣＥＯ陆兆禧将会出任上述职务，向集团ＣＥＯ马云直接汇报。＞菹ぃ和６月初的首席风险官职务任命相同，首席数据官亦为阿里巴巴集团在完成与雅虎股权谈判，推进“ｏｎｅ　ｃｏｍｐａｎｙ”目标后，在集团决策层面新增的管理岗位。０⒗锛团昨日表示，“变成一家真正意义上的数据公司”已是战略共识。记者刘夏</div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">content_S = []</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> content:</span><br><span class="line">    current_segment = jieba.lcut(line)  <span class="comment"># lcut可以进行分词</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(current_segment) &gt; <span class="number">1</span> <span class="keyword">and</span> current_segment != <span class="string">&#x27;\r\n&#x27;</span>: <span class="comment"># \n换行符,\r回车符号</span></span><br><span class="line">        content_S.append(current_segment)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">content_S[<span class="number">1000</span>]</span><br></pre></td></tr></table></figure><pre><code>[&#39;阿里巴巴&#39;, &#39;集团&#39;, &#39;昨日&#39;, &#39;宣布&#39;, &#39;，&#39;, &#39;将&#39;, &#39;在&#39;, &#39;集团&#39;, &#39;管理&#39;, &#39;层面&#39;, &#39;设立&#39;, &#39;首席&#39;, &#39;数据&#39;, &#39;官&#39;, &#39;岗位&#39;, &#39;（&#39;, &#39;Ｃ&#39;, &#39;ｈ&#39;, &#39;ｉ&#39;, &#39;ｅ&#39;, &#39;ｆ&#39;, &#39;\u3000&#39;, &#39;Ｄ&#39;, &#39;ａ&#39;, &#39;ｔ&#39;, &#39;ａ&#39;, &#39;\u3000&#39;, &#39;Ｏ&#39;, &#39;ｆ&#39;, &#39;ｆ&#39;, &#39;ｉ&#39;, &#39;ｃ&#39;, &#39;ｅ&#39;, &#39;ｒ&#39;, &#39;）&#39;, &#39;，&#39;, &#39;阿里巴巴&#39;, &#39;Ｂ&#39;, &#39;２&#39;, &#39;Ｂ&#39;, &#39;公司&#39;, &#39;Ｃ&#39;, &#39;Ｅ&#39;, &#39;Ｏ&#39;, &#39;陆兆禧&#39;, &#39;将&#39;, &#39;会&#39;, &#39;出任&#39;, &#39;上述&#39;, &#39;职务&#39;, &#39;，&#39;, &#39;向&#39;, &#39;集团&#39;, &#39;Ｃ&#39;, &#39;Ｅ&#39;, &#39;Ｏ&#39;, &#39;马云&#39;, &#39;直接&#39;, &#39;汇报&#39;, &#39;。&#39;, &#39;＞&#39;, &#39;菹&#39;, &#39;ぃ&#39;, &#39;和&#39;, &#39;６&#39;, &#39;月初&#39;, &#39;的&#39;, &#39;首席&#39;, &#39;风险&#39;, &#39;官&#39;, &#39;职务&#39;, &#39;任命&#39;, &#39;相同&#39;, &#39;，&#39;, &#39;首席&#39;, &#39;数据&#39;, &#39;官亦为&#39;, &#39;阿里巴巴&#39;, &#39;集团&#39;, &#39;在&#39;, &#39;完成&#39;, &#39;与&#39;, &#39;雅虎&#39;, &#39;股权&#39;, &#39;谈判&#39;, &#39;，&#39;, &#39;推进&#39;, &#39;“&#39;, &#39;ｏ&#39;, &#39;ｎ&#39;, &#39;ｅ&#39;, &#39;\u3000&#39;, &#39;ｃ&#39;, &#39;ｏ&#39;, &#39;ｍ&#39;, &#39;ｐ&#39;, &#39;ａ&#39;, &#39;ｎ&#39;, &#39;ｙ&#39;, &#39;”&#39;, &#39;目标&#39;, &#39;后&#39;, &#39;，&#39;, &#39;在&#39;, &#39;集团&#39;, &#39;决策&#39;, &#39;层面&#39;, &#39;新增&#39;, &#39;的&#39;, &#39;管理&#39;, &#39;岗位&#39;, &#39;。&#39;, &#39;０&#39;, &#39;⒗&#39;, &#39;锛&#39;, &#39;团&#39;, &#39;昨日&#39;, &#39;表示&#39;, &#39;，&#39;, &#39;“&#39;, &#39;变成&#39;, &#39;一家&#39;, &#39;真正&#39;, &#39;意义&#39;, &#39;上&#39;, &#39;的&#39;, &#39;数据&#39;, &#39;公司&#39;, &#39;”&#39;, &#39;已&#39;, &#39;是&#39;, &#39;战略&#39;, &#39;共识&#39;, &#39;。&#39;, &#39;记者&#39;, &#39;刘夏&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_content=pd.DataFrame(&#123;<span class="string">&#x27;content_S&#x27;</span>:content_S&#125;) <span class="comment"># DataFrame可以使用键值对的方式来获得下面的数据形式。</span></span><br><span class="line">df_content.head()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"> <style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>content_S</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>[经销商, 　, 电话, 　, 试驾, ／, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, ...</td>    </tr>    <tr>      <th>1</th>      <td>[呼叫, 热线, 　, ４, ０, ０, ８, －, １, ０, ０, －, ３, ０, ０...</td>    </tr>    <tr>      <th>2</th>      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 在, 二月, 曾经, 公布, 了, 最新, 的, Ｍ, Ｉ...</td>    </tr>    <tr>      <th>3</th>      <td>[清仓, 大, 甩卖, ！, 一汽, 夏利, Ｎ, ５, 、, 威志, Ｖ, ２, 低至, ...</td>    </tr>    <tr>      <th>4</th>      <td>[在, 今年, ３, 月, 的, 日内瓦, 车展, 上, ，, 我们, 见到, 了, 高尔夫...</td>    </tr>  </tbody></table></div><h3 id="进行数据的清洗，去掉停用词。—停用词表，可以网上下载。"><a href="#进行数据的清洗，去掉停用词。—停用词表，可以网上下载。" class="headerlink" title="进行数据的清洗，去掉停用词。—停用词表，可以网上下载。"></a>进行数据的清洗，去掉停用词。—停用词表，可以网上下载。</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stopwords=pd.read_csv(<span class="string">&quot;stopwords.txt&quot;</span>,index_col=<span class="literal">False</span>,sep=<span class="string">&quot;\t&quot;</span>,quoting=<span class="number">3</span>,names=[<span class="string">&#x27;stopword&#x27;</span>], encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">stopwords.head(<span class="number">20</span>)</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>stopword</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>!</td>    </tr>    <tr>      <th>1</th>      <td>"</td>    </tr>    <tr>      <th>2</th>      <td>#</td>    </tr>    <tr>      <th>3</th>      <td>$</td>    </tr>    <tr>      <th>4</th>      <td>%</td>    </tr>    <tr>      <th>5</th>      <td>&amp;</td>    </tr>    <tr>      <th>6</th>      <td>'</td>    </tr>    <tr>      <th>7</th>      <td>(</td>    </tr>    <tr>      <th>8</th>      <td>)</td>    </tr>    <tr>      <th>9</th>      <td>*</td>    </tr>    <tr>      <th>10</th>      <td>+</td>    </tr>    <tr>      <th>11</th>      <td>,</td>    </tr>    <tr>      <th>12</th>      <td>-</td>    </tr>    <tr>      <th>13</th>      <td>--</td>    </tr>    <tr>      <th>14</th>      <td>.</td>    </tr>    <tr>      <th>15</th>      <td>..</td>    </tr>    <tr>      <th>16</th>      <td>...</td>    </tr>    <tr>      <th>17</th>      <td>......</td>    </tr>    <tr>      <th>18</th>      <td>...................</td>    </tr>    <tr>      <th>19</th>      <td>./</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop_stopwords</span>(<span class="params">contents,stopwords</span>):</span></span><br><span class="line">    contents_clean = []</span><br><span class="line">    all_words = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> contents:</span><br><span class="line">        line_clean = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> line:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> stopwords:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            line_clean.append(word)</span><br><span class="line">            all_words.append(<span class="built_in">str</span>(word)) <span class="comment"># 一会做一个词云。有现成的库，可以轻松实现出来。</span></span><br><span class="line">        contents_clean.append(line_clean)</span><br><span class="line">    <span class="keyword">return</span> contents_clean,all_words</span><br><span class="line">    <span class="comment"># print (contents_clean)</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">contents = df_content.content_S.values.tolist()    </span><br><span class="line">stopwords = stopwords.stopword.values.tolist()</span><br><span class="line">contents_clean,all_words = drop_stopwords(contents,stopwords)</span><br><span class="line"></span><br><span class="line"><span class="comment"># df_content.content_S.isin(stopwords.stopword)</span></span><br><span class="line"><span class="comment"># df_content=df_content[~df_content.content_S.isin(stopwords.stopword)]</span></span><br><span class="line"><span class="comment"># df_content.head()</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_content=pd.DataFrame(&#123;<span class="string">&#x27;contents_clean&#x27;</span>:contents_clean&#125;)</span><br><span class="line">df_content.head() <span class="comment"># 也可以在停用词里面增加字母，将字母去掉。</span></span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>contents_clean</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>[经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...</td>    </tr>    <tr>      <th>1</th>      <td>[呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...</td>    </tr>    <tr>      <th>2</th>      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...</td>    </tr>    <tr>      <th>3</th>      <td>[清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...</td>    </tr>    <tr>      <th>4</th>      <td>[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_all_words=pd.DataFrame(&#123;<span class="string">&#x27;all_words&#x27;</span>:all_words&#125;)</span><br><span class="line">df_all_words.head()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>all_words</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>经销商</td>    </tr>    <tr>      <th>1</th>      <td>电话</td>    </tr>    <tr>      <th>2</th>      <td>试驾</td>    </tr>    <tr>      <th>3</th>      <td>订车</td>    </tr>    <tr>      <th>4</th>      <td>Ｕ</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">words_count=df_all_words.groupby(by=[<span class="string">&#x27;all_words&#x27;</span>])[<span class="string">&#x27;all_words&#x27;</span>].agg(&#123;<span class="string">&quot;count&quot;</span>:numpy.size&#125;) <span class="comment"># 先分组再agg求和。</span></span><br><span class="line">words_count=words_count.reset_index().sort_values(by=[<span class="string">&quot;count&quot;</span>],ascending=<span class="literal">False</span>) <span class="comment"># 按值进行排序</span></span><br><span class="line">words_count.head()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>all_words</th>      <th>count</th>    </tr>  </thead>  <tbody>    <tr>      <th>4077</th>      <td>中</td>      <td>5199</td>    </tr>    <tr>      <th>4209</th>      <td>中国</td>      <td>3115</td>    </tr>    <tr>      <th>88255</th>      <td>说</td>      <td>3055</td>    </tr>    <tr>      <th>104747</th>      <td>Ｓ</td>      <td>2646</td>    </tr>    <tr>      <th>1373</th>      <td>万</td>      <td>2390</td>    </tr>  </tbody></table></div><p><strong> 词云的展示 </strong> ——在github上面直接搜wordcloud就行！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud <span class="comment"># 导入词云</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">10.0</span>, <span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line">wordcloud=WordCloud(font_path=<span class="string">&quot;./data/simhei.ttf&quot;</span>,background_color=<span class="string">&quot;white&quot;</span>,max_font_size=<span class="number">80</span>)</span><br><span class="line">word_frequence = &#123;x[<span class="number">0</span>]:x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> words_count.head(<span class="number">100</span>).values&#125; <span class="comment"># 画前100个词</span></span><br><span class="line">wordcloud=wordcloud.fit_words(word_frequence)</span><br><span class="line">plt.imshow(wordcloud)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x186064c64e0&gt;</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210511142559.png" alt=""></p><h3 id="TF-IDF-：提取关键词"><a href="#TF-IDF-：提取关键词" class="headerlink" title="TF-IDF ：提取关键词"></a>TF-IDF ：提取关键词</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse <span class="comment"># jieba和sklearn都可以进行词频的提取</span></span><br><span class="line">index = <span class="number">2400</span></span><br><span class="line"><span class="built_in">print</span> (df_news[<span class="string">&#x27;content&#x27;</span>][index])</span><br><span class="line">content_S_str = <span class="string">&quot;&quot;</span>.join(content_S[index])  <span class="comment"># 将分词完的数据拿出来！</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;  &quot;</span>.join(jieba.analyse.extract_tags(content_S_str, topK=<span class="number">5</span>, withWeight=<span class="literal">False</span>))) <span class="comment"># jieba里面有提取关键词的模块</span></span><br></pre></td></tr></table></figure><div style="overflow: scroll;">    法国ＶＳ西班牙、里贝里ＶＳ哈维，北京时间６月２４日凌晨一场的大战举世瞩目，而这场胜利不仅仅关乎两支顶级强队的命运，同时也是他们背后的球衣赞助商耐克和阿迪达斯之间的一次角逐。Ｔ谌胙”窘炫分薇的１６支球队之中，阿迪达斯和耐克的势力范围也是几乎旗鼓相当：其中有５家球衣由耐克提供，而阿迪达斯则赞助了６家，此外茵宝有３家，而剩下的两家则由彪马赞助。而当比赛进行到现在，率先挺进四强的两支球队分别被耐克支持的葡萄牙和阿迪达斯支持的德国占据，而由于最后一场１／４决赛是茵宝（英格兰）和彪马（意大利）的对决，这也意味着明天凌晨西班牙同法国这场阿迪达斯和耐克在１／４决赛的唯一一次直接交手将直接决定两家体育巨头在此次欧洲杯上的胜负。８据评估，在２０１２年足球商品的销售额能总共超过４０亿欧元，而单单是不足一个月的欧洲杯就有高达５亿的销售额，也就是说在欧洲杯期间将有７００万件球衣被抢购一空。根据市场评估，两大巨头阿迪达斯和耐克的市场占有率也是并驾齐驱，其中前者占据３８％，而后者占据３６％。体育权利顾问奥利弗－米歇尔在接受《队报》采访时说：“欧洲杯是耐克通过法国翻身的一个绝佳机会！”Ｃ仔尔接着谈到两大赞助商的经营策略：“竞技体育的成功会燃起球衣购买的热情，不过即便是水平相当，不同国家之间的欧洲杯效应却存在不同。在德国就很出色，大约１／４的德国人通过电视观看了比赛，而在西班牙效果则差很多，由于民族主义高涨的加泰罗尼亚地区只关注巴萨和巴萨的球衣，他们对西班牙国家队根本没什么兴趣。”因此尽管西班牙接连拿下欧洲杯和世界杯，但是阿迪达斯只为西班牙足协支付每年２６００万的赞助费＃相比之下尽管最近两届大赛表现糟糕法国足协将从耐克手中每年可以得到４０００万欧元。米歇尔解释道：“法国创纪录的４０００万欧元赞助费得益于阿迪达斯和耐克竞逐未来１５年欧洲市场的竞争。耐克需要笼络一个大国来打赢这场欧洲大陆的战争，而尽管德国拿到的赞助费并不太高，但是他们却显然牢牢掌握在民族品牌阿迪达斯手中。从长期投资来看，耐克给法国的赞助并不算过高。”    耐克  阿迪达斯  欧洲杯  球衣  西班牙</div><h3 id="LDA-：主题模型"><a href="#LDA-：主题模型" class="headerlink" title="LDA ：主题模型"></a>LDA ：主题模型</h3><p>格式要求（很重要）：list（分好词的语料） of list（不同的文章）形式，分词好的的整个语料</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim <span class="keyword">import</span> corpora, models, similarities</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="comment"># http://radimrehurek.com/gensim/</span></span><br></pre></td></tr></table></figure><pre><code>C:\Anaconda3\lib\site-packages\gensim\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial  warnings.warn(&quot;detected Windows; aliasing chunkize to chunkize_serial&quot;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 做映射，相当于词袋</span></span><br><span class="line">dictionary = corpora.Dictionary(contents_clean)</span><br><span class="line">corpus = [dictionary.doc2bow(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> contents_clean]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=<span class="number">20</span>) <span class="comment"># 类似Kmeans自己指定K值。无监督</span></span><br><span class="line">                                                                                        <span class="comment">#    自定义主题数目</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一号分类结果</span></span><br><span class="line"><span class="built_in">print</span> (lda.print_topic(<span class="number">1</span>, topn=<span class="number">5</span>))</span><br></pre></td></tr></table></figure><pre><code>0.007*&quot;中&quot; + 0.006*&quot;说&quot; + 0.004*&quot;观众&quot; + 0.002*&quot;赛区&quot; + 0.002*&quot;岁&quot;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> topic <span class="keyword">in</span> lda.print_topics(num_topics=<span class="number">20</span>, num_words=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span> (topic[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>0.007*&quot;女人&quot; + 0.006*&quot;男人&quot; + 0.006*&quot;Ｍ&quot; + 0.004*&quot;Ｓ&quot; + 0.004*&quot;说&quot;0.004*&quot;中&quot; + 0.004*&quot;训练&quot; + 0.003*&quot;说&quot; + 0.003*&quot;学校&quot; + 0.002*&quot;研究生&quot;0.006*&quot;戏&quot; + 0.006*&quot;导演&quot; + 0.005*&quot;该剧&quot; + 0.004*&quot;中&quot; + 0.004*&quot;演员&quot;0.007*&quot;中&quot; + 0.006*&quot;说&quot; + 0.004*&quot;观众&quot; + 0.002*&quot;赛区&quot; + 0.002*&quot;岁&quot;0.004*&quot;万&quot; + 0.003*&quot;号&quot; + 0.003*&quot;中&quot; + 0.002*&quot;Ｓ&quot; + 0.002*&quot;Ｒ&quot;0.014*&quot;电影&quot; + 0.009*&quot;导演&quot; + 0.007*&quot;影片&quot; + 0.006*&quot;中国&quot; + 0.005*&quot;中&quot;0.006*&quot;中&quot; + 0.005*&quot;比赛&quot; + 0.004*&quot;说&quot; + 0.003*&quot;撒&quot; + 0.002*&quot;时间&quot;0.006*&quot;赛季&quot; + 0.005*&quot;中&quot; + 0.003*&quot;联赛&quot; + 0.003*&quot;中国&quot; + 0.002*&quot;航母&quot;0.005*&quot;李小璐&quot; + 0.004*&quot;中&quot; + 0.002*&quot;贾乃亮&quot; + 0.002*&quot;Ｗ&quot; + 0.002*&quot;皮肤&quot;0.004*&quot;万&quot; + 0.003*&quot;号&quot; + 0.003*&quot;Ｖ&quot; + 0.003*&quot;Ｔ&quot; + 0.003*&quot;刘涛&quot;0.021*&quot;男人&quot; + 0.008*&quot;女人&quot; + 0.007*&quot;考生&quot; + 0.004*&quot;说&quot; + 0.003*&quot;中&quot;0.005*&quot;中&quot; + 0.005*&quot;食物&quot; + 0.004*&quot;ｉ&quot; + 0.004*&quot;ａ&quot; + 0.004*&quot;吃&quot;0.006*&quot;中&quot; + 0.004*&quot;电影&quot; + 0.004*&quot;说&quot; + 0.002*&quot;中国&quot; + 0.002*&quot;高考&quot;0.007*&quot;中&quot; + 0.006*&quot;孩子&quot; + 0.004*&quot;说&quot; + 0.003*&quot;教育&quot; + 0.003*&quot;中国&quot;0.005*&quot;中&quot; + 0.005*&quot;节目&quot; + 0.004*&quot;说&quot; + 0.004*&quot;表演&quot; + 0.003*&quot;岁&quot;0.007*&quot;电视剧&quot; + 0.004*&quot;中&quot; + 0.003*&quot;说&quot; + 0.003*&quot;飞行&quot; + 0.002*&quot;飞机&quot;0.007*&quot;中&quot; + 0.006*&quot;球队&quot; + 0.005*&quot;选手&quot; + 0.004*&quot;观众&quot; + 0.004*&quot;ｉ&quot;0.005*&quot;中&quot; + 0.005*&quot;天籁&quot; + 0.004*&quot;产品&quot; + 0.004*&quot;肌肤&quot; + 0.003*&quot;职场&quot;0.008*&quot;中国&quot; + 0.008*&quot;饰演&quot; + 0.007*&quot;中&quot; + 0.004*&quot;说&quot; + 0.004*&quot;节目&quot;0.021*&quot;ｅ&quot; + 0.021*&quot;ａ&quot; + 0.016*&quot;ｏ&quot; + 0.013*&quot;ｉ&quot; + 0.013*&quot;ｎ&quot;</code></pre><h3 id="贝叶斯算法进行新闻的分类"><a href="#贝叶斯算法进行新闻的分类" class="headerlink" title="贝叶斯算法进行新闻的分类"></a>贝叶斯算法进行新闻的分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_train=pd.DataFrame(&#123;<span class="string">&#x27;contents_clean&#x27;</span>:contents_clean,<span class="string">&#x27;label&#x27;</span>:df_news[<span class="string">&#x27;category&#x27;</span>]&#125;)</span><br><span class="line">df_train.tail()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>contents_clean</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>4995</th>      <td>[天气, 炎热, 补水, 变得, 美国, 跑步, 世界, 杂志, 报道, 喝水, 身体, 补...</td>      <td>时尚</td>    </tr>    <tr>      <th>4996</th>      <td>[不想, 说, 话, 刺激, 说, 做, 只能, 走, 离开, 伤心地, 想起, 一句, 话...</td>      <td>时尚</td>    </tr>    <tr>      <th>4997</th>      <td>[岁, 刘晓庆, 最新, 嫩照, Ｏ, 衷, 诘, 牧跸, 庆, 看不出, 岁, 秒杀, 刘...</td>      <td>时尚</td>    </tr>    <tr>      <th>4998</th>      <td>[导语, 做, 爸爸, 一种, 幸福, 无论是, 领养, 亲生, 更何况, 影视剧, 中, ...</td>      <td>时尚</td>    </tr>    <tr>      <th>4999</th>      <td>[全球, 最美, 女人, 合成图, 国, 整形外科, 教授, 李承哲, 国际, 学术, 杂志...</td>      <td>时尚</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_train.label.unique()</span><br></pre></td></tr></table></figure><pre><code>array([&#39;汽车&#39;, &#39;财经&#39;, &#39;科技&#39;, &#39;健康&#39;, &#39;体育&#39;, &#39;教育&#39;, &#39;文化&#39;, &#39;军事&#39;, &#39;娱乐&#39;, &#39;时尚&#39;], dtype=object)</code></pre><p><strong> pandas 容易可以很容易进行打标签 </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">label_mapping = &#123;<span class="string">&quot;汽车&quot;</span>: <span class="number">1</span>, <span class="string">&quot;财经&quot;</span>: <span class="number">2</span>, <span class="string">&quot;科技&quot;</span>: <span class="number">3</span>, <span class="string">&quot;健康&quot;</span>: <span class="number">4</span>, <span class="string">&quot;体育&quot;</span>:<span class="number">5</span>, <span class="string">&quot;教育&quot;</span>: <span class="number">6</span>,<span class="string">&quot;文化&quot;</span>: <span class="number">7</span>,<span class="string">&quot;军事&quot;</span>: <span class="number">8</span>,<span class="string">&quot;娱乐&quot;</span>: <span class="number">9</span>,<span class="string">&quot;时尚&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line">df_train[<span class="string">&#x27;label&#x27;</span>] = df_train[<span class="string">&#x27;label&#x27;</span>].<span class="built_in">map</span>(label_mapping) <span class="comment"># label的替换</span></span><br><span class="line">df_train.head()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>contents_clean</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>[经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>[呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>[清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...</td>      <td>1</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(df_train[<span class="string">&#x27;contents_clean&#x27;</span>].values, df_train[<span class="string">&#x27;label&#x27;</span>].values, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x_train = x_train.flatten()</span></span><br><span class="line">x_train[<span class="number">0</span>][<span class="number">1</span>]</span><br></pre></td></tr></table></figure><pre><code>&#39;上海&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">words = []  </span><br><span class="line"><span class="keyword">for</span> line_index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_train)):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># x_train[line_index][word_index] = str(x_train[line_index][word_index])</span></span><br><span class="line">        words.append(<span class="string">&#x27; &#x27;</span>.join(x_train[line_index])) <span class="comment"># python的list往字符串转化</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span> (line_index,word_index)</span><br><span class="line">words[<span class="number">0</span>]  <span class="comment"># 格式转换，不是list of list格式（下文的CoountVector的格式）      </span></span><br></pre></td></tr></table></figure><div style="overflow: scroll;">    '中新网 上海 日电 于俊 父亲节 网络 吃 一顿 电影 快餐 微 电影 爸 对不起 我爱你 定于 本月 父亲节 当天 各大 视频 网站 首映 葜 谱 鞣 剑 保慈 障蚣 钦 呓 樯 埽 ⒌ 缬 埃 ǎ 停 椋 悖 颍 铩 妫 椋 恚 称 微型 电影 新 媒体 平台 播放 状态 短时 休闲 状态 观看 完整 策划 系统 制作 体系 支持 显示 较完整 故事情节 电影 微 超短 放映 微 周期 制作 天 数周 微 规模 投资 人民币 几千 数万元 每部 内容 融合 幽默 搞怪 时尚 潮流 人文 言情 公益 教育 商业 定制 主题 单独 成篇 系列 成剧 唇 开播 微 电影 爸 对不起 我爱你 讲述 一对 父子 观念 缺少 沟通 导致 关系 父亲 传统 固执 钟情 传统 生活 方式 儿子 新派 音乐 达 习惯 晚出 早 生活 性格 张扬 叛逆 两种 截然不同 生活 方式 理念 差异 一场 父子 间 拉开序幕 子 失手 打破 父亲 心爱 物品 父亲 赶出 家门 剧情 演绎 父亲节 妹妹 哥哥 化解 父亲 这场 矛盾 映逋坏 嚼 斫 狻 ⒍ 粤 ⒌ 桨容 争执 退让 传统 尴尬 父子 尴尬 情 男人 表达 心中 那份 感恩 一杯 滤挂 咖啡 父亲节 变得 温馨 镁 缬 缮 虾 Ｎ 逄 煳 幕 传播 迪欧 咖啡 联合 出品 出品人 希望 观摩 扪心自问 父亲节 父亲 记得 父亲 生日 哪一天 父亲 爱喝 跨出 家门 那一刻 感觉 一颗 颤动 心 操劳 天下 儿女 父亲节 大声 喊出 父亲 家人 爱 完'</div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="built_in">len</span>(words))</span><br></pre></td></tr></table></figure><pre><code>3750</code></pre><p><strong>CountVectorizer的简要介绍</strong>↓——将词转换成向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">texts=[<span class="string">&quot;dog cat fish&quot;</span>,<span class="string">&quot;dog cat cat&quot;</span>,<span class="string">&quot;fish bird&quot;</span>, <span class="string">&#x27;bird&#x27;</span>] <span class="comment"># 四篇文章，注意其格式，不是list of list格式</span></span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">cv_fit=cv.fit_transform(texts)</span><br><span class="line"></span><br><span class="line">print(cv.get_feature_names()) <span class="comment"># 获得语料库中不重复的词</span></span><br><span class="line">print(cv_fit.toarray())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(cv_fit.toarray().<span class="built_in">sum</span>(axis=<span class="number">0</span>))</span><br></pre></td></tr></table></figure><pre><code>[&#39;bird&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;fish&#39;][[0 1 1 1] [0 2 1 0] [1 0 0 1] [1 0 0 0]][2 3 2 2]</code></pre><p>ngram_range=(1,4)可以将特征值进行组合，让向量更复杂，一般2就可以</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">texts=[<span class="string">&quot;dog cat fish&quot;</span>,<span class="string">&quot;dog cat cat&quot;</span>,<span class="string">&quot;fish bird&quot;</span>, <span class="string">&#x27;bird&#x27;</span>]</span><br><span class="line">cv = CountVectorizer(ngram_range=(<span class="number">1</span>,<span class="number">4</span>))  <span class="comment"># 词可以组合。可以让向量更复杂！</span></span><br><span class="line">cv_fit=cv.fit_transform(texts)</span><br><span class="line"></span><br><span class="line">print(cv.get_feature_names())</span><br><span class="line">print(cv_fit.toarray())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(cv_fit.toarray().<span class="built_in">sum</span>(axis=<span class="number">0</span>))</span><br></pre></td></tr></table></figure><pre><code>[&#39;bird&#39;, &#39;cat&#39;, &#39;cat cat&#39;, &#39;cat fish&#39;, &#39;dog&#39;, &#39;dog cat&#39;, &#39;dog cat cat&#39;, &#39;dog cat fish&#39;, &#39;fish&#39;, &#39;fish bird&#39;][[0 1 0 1 1 1 0 1 1 0] [0 2 1 0 1 1 1 0 0 0] [1 0 0 0 0 0 0 0 1 1] [1 0 0 0 0 0 0 0 0 0]][2 3 1 1 2 2 1 1 2 1]</code></pre><p><strong> 数据准备好后可以开始进行操作了 </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">vec = CountVectorizer(analyzer=<span class="string">&#x27;word&#x27;</span>, max_features=<span class="number">4000</span>,  lowercase = <span class="literal">False</span>)</span><br><span class="line">vec.fit(words) <span class="comment"># fit,以某种规则传化</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong> 进行贝叶斯的计算 </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">classifier = MultinomialNB()</span><br><span class="line">classifier.fit(vec.transform(words), y_train) <span class="comment"># transform,规则训练好，进行转换为向量。为特征</span></span><br></pre></td></tr></table></figure><pre><code>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test_words = []</span><br><span class="line"><span class="keyword">for</span> line_index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_test)):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment">#x_train[line_index][word_index] = str(x_train[line_index][word_index])</span></span><br><span class="line">        test_words.append(<span class="string">&#x27; &#x27;</span>.join(x_test[line_index]))</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">         <span class="built_in">print</span> (line_index,word_index)</span><br><span class="line">test_words[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><div style="overflow: scroll;">     '国家 公务员 考试 申论 应用文 类 试题 实质 一道 集 概括 分析 提出 解决问题 一体 综合性 试题 说 一道 客观 凝练 申发 论述 文章 题目 分析 历年 国考 申论 真题 公文 类 试题 类型 多样 包括 公文 类 事务性 文书 类 题材 从题 干 作答 材料 内容 整合 分析 无需 太 创造性 发挥 纵观 历年 申论 真题 作答 应用文 类 试题 文种 格式 作出 特别 重在 内容 考查 行文 格式 考生 平常心 面对 应用文 类 试题 准确 把握 作答 领会 内在 含义 把握 题材 主旨 材料 结构 轻松 应对 应用文 类 试题 Ｒ 弧 ⒆ 钒 盐 展文 写作 原则 Ｔ 材料 中来 应用文 类 试题 材料 总体 把握 客观 考生 材料 中来 材料 中 把握 材料 准确 理解 题材 主旨 Ｔ 政府 角度 作答 应用文 类 试题 更应 注重 政府 角度 观点 政府 角度 出发 原则 表述 观点 提出 解决 之策 考生 作答 站 政府 人员 角度 看待 提出 解决问题 Ｔ 文体 结构 形式 考查 重点 文体 结构 大部分 评分 关键点 解答 方法 薄 ⒆ ス 丶 词 明 方向 作答 题目 题干 作答 作答 方向 作答 角度 关键 向导 考生 仔细阅读 题干 作答 抓住 关键词 作答 方向 相关 要点 整理 作答 思路 年国考 地市级 真 题为 例 潦惺姓 府 宣传 推进 近海 水域 污染 整治 工作 请 给定 资料 市政府 工作人员 身份 草拟 一份 宣传 纲要 Ｒ 求 保对 宣传 内容 要点 提纲挈领 陈述 玻 体现 政府 精神 全市 各界 关心 支持 污染 整治 工作 通俗易懂 超过 字 肮 丶 词 近海 水域 污染 整治 工作 市政府 工作人员 身份 宣传 纲要 提纲挈领 陈述 体现 政府 精神 全市 各界 关心 支持 污染 整治 工作 通俗易懂 提示 归结 作答 要点 包括 污染 情况 原因 解决 对策 作答 思路 情况 原因 对策 意义 逻辑 顺序 安排 文章 结构 病 ⒋ 缶殖 龇 ⅲ 明 结构 解答 应用文 类 试题 考生 材料 整体 出发 大局 出发 高屋建瓴 把握 材料 主题 思想 事件 起因 解决 对策 阅读文章 构建 文章 结构 直至 快速 解答 场 ⒗ 硭 乘悸 罚明 逻辑 应用文 类 试题 严密 逻辑思维 情况 原因 对策 意义 考生 作答 先 弄清楚 解答 思路 统筹安排 脉络 清晰 逻辑 表达 内容 表述 础 把握 明 详略 考生 仔细阅读 分析 揣摩 应用文 类 试题 内容 答题 时要 详略 得当 主次 分明 安排 内容 增加 文章 层次感 阅卷 老师 阅卷 时能 明白 清晰 一目了然 玻埃 保蹦旯 考 考试 申论 试卷 分为 省级 地市级 两套 试卷 能力 大有 省级 申论 试题 考生 宏观 角度看 注重 深度 广度 考生 深谋远虑 地市级 试题 考生 微观 视角 观察 侧重 考查 解决 能力 考生 贯彻执行 作答 区别对待'</div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classifier.score(vec.transform(test_words), y_test)</span><br></pre></td></tr></table></figure><pre><code>0.80400000000000005</code></pre><h3 id="TF-IDF的到导入计算"><a href="#TF-IDF的到导入计算" class="headerlink" title="TF-IDF的到导入计算"></a>TF-IDF的到导入计算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = TfidfVectorizer(analyzer=<span class="string">&#x27;word&#x27;</span>, max_features=<span class="number">4000</span>,  lowercase = <span class="literal">False</span>)</span><br><span class="line">vectorizer.fit(words)</span><br></pre></td></tr></table></figure><pre><code>TfidfVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,        dtype=&lt;class &#39;numpy.int64&#39;&gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,        lowercase=False, max_df=1.0, max_features=4000, min_df=1,        ngram_range=(1, 1), norm=&#39;l2&#39;, preprocessor=None, smooth_idf=True,        stop_words=None, strip_accents=None, sublinear_tf=False,        token_pattern=&#39;(?u)\\b\\w\\w+\\b&#39;, tokenizer=None, use_idf=True,        vocabulary=None)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">classifier = MultinomialNB()</span><br><span class="line">classifier.fit(vectorizer.transform(words), y_train)</span><br></pre></td></tr></table></figure><pre><code>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classifier.score(vectorizer.transform(test_words), y_test)</span><br></pre></td></tr></table></figure><pre><code>0.81520000000000004</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;文本分析——新闻分类&quot;&gt;&lt;a href=&quot;#文本分析——新闻分类&quot; class=&quot;headerlink&quot; title=&quot;文本分析——新闻分类&quot;&gt;&lt;/a&gt;文本分析——新闻分类&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;</summary>
      
    
    
    
    <category term="机器学习基础实战" scheme="https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="机器学习基础实战" scheme="https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（五十四）：字符流中第一个不重复的字符</title>
    <link href="https://xxren8218.github.io/20210511/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%94%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%AD%97%E7%AC%A6%E6%B5%81%E4%B8%AD%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%B8%8D%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AD%97%E7%AC%A6.html"/>
    <id>https://xxren8218.github.io/20210511/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%94%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%AD%97%E7%AC%A6%E6%B5%81%E4%B8%AD%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%B8%8D%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AD%97%E7%AC%A6.html</id>
    <published>2021-05-11T06:17:23.000Z</published>
    <updated>2021-05-11T06:21:39.150Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><pre><code>请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符&quot;go&quot;时，第一个只出现一次的字符是&quot;g&quot;。当从该字符流中读出前六个字符“google&quot;时，第一个只出现一次的字符是&quot;l&quot;。如果当前字符流没有存在出现一次的字符，返回#字符。</code></pre><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>可以创建一个列表进行字符的记录。然后按顺序进行遍历列表有count方法可以进行计数。</code></pre><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 返回对应char</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span>       </span><br><span class="line">        self.s = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Insert</span>(<span class="params">self, char</span>):</span></span><br><span class="line">        self.s.append(char)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">FirstAppearingOnce</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.s:</span><br><span class="line">            <span class="keyword">if</span> self.s.count(i) == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;#&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符&amp;quot;go&amp;q</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="其他" scheme="https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（四十八）：不用加减乘除的加法</title>
    <link href="https://xxren8218.github.io/20210511/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E7%94%A8%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E7%9A%84%E5%8A%A0%E6%B3%95.html"/>
    <id>https://xxren8218.github.io/20210511/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E7%94%A8%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E7%9A%84%E5%8A%A0%E6%B3%95.html</id>
    <published>2021-05-11T06:12:44.000Z</published>
    <updated>2021-05-11T06:16:07.727Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><pre><code>写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。</code></pre><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>首先看十进制是如何做的： 5+7=12，可以使用三步走：第一步：相加各位的值，不算进位，得到2。第二步：计算进位值，得到10. 如果这一步的进位值为0，那么第一步得到的值就是最终结果。第三步：重复上述两步，只是相加的值变成上述两步的得到的结果2和10，得到12。 同样我们可以三步走的方式计算二进制值相加： 5-101，7-111第一步：相加各位的值，不算进位，得到010，二进制每位相加就相当于各位做异或操作，101^111。第二步：计算进位值，得到1010，相当于各位做与操作得到101，再向左移一位得到1010，(101&amp;111)&lt;&lt;1。第三步：重复上述两步， 各位相加 010^1010=1000，进位值为100=(010&amp;1010)&lt;&lt;1。继续重复上述两步：1000^100 = 1100，进位值为0，跳出循环，1100为最终结果。</code></pre><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Add</span>(<span class="params">self, num1, num2</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="comment"># python2.7的int取值范围为：-2147483648至2147483647</span></span><br><span class="line">        <span class="comment"># 将数字&amp;0xffffffff，得到二进制。python的二进制不一样。其负数为 ~(num ^ mask)</span></span><br><span class="line">        MAX = <span class="number">0x7fffffff</span></span><br><span class="line">        mask = <span class="number">0xffffffff</span></span><br><span class="line">        <span class="keyword">while</span> num2 != <span class="number">0</span>:</span><br><span class="line">            num1, num2 = (num1 ^ num2), ((num1 &amp; num2) &lt;&lt; <span class="number">1</span>)</span><br><span class="line">            num1 = num1 &amp; mask</span><br><span class="line">            num2 = num2 &amp; mask</span><br><span class="line">        <span class="keyword">return</span> num1 <span class="keyword">if</span> num1 &lt;= MAX <span class="keyword">else</span> ~(num1 ^ mask)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。
&lt;/code&gt;&lt;/pre&gt;&lt;</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="位运算" scheme="https://xxren8218.github.io/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（四十七）：求1+2+3+…+n</title>
    <link href="https://xxren8218.github.io/20210511/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9A%E6%B1%821-2-3-%E2%80%A6-n.html"/>
    <id>https://xxren8218.github.io/20210511/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9A%E6%B1%821-2-3-%E2%80%A6-n.html</id>
    <published>2021-05-11T06:08:14.000Z</published>
    <updated>2021-05-11T06:11:26.586Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><pre><code>求1+2+3+...+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。</code></pre><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>这是一道超级无敌送分题，使用递归即可。</code></pre><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Sum_Solution</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        ans = n</span><br><span class="line">        <span class="keyword">if</span> ans:</span><br><span class="line">            ans += self.Sum_Solution(n-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;求1+2+3+...+n，要求不能使用乘除法、for、while、if、else、switch、case等关</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="递归" scheme="https://xxren8218.github.io/tags/%E9%80%92%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（四十六）：孩子们的游戏（圆圈中最后剩下的数）</title>
    <link href="https://xxren8218.github.io/20210510/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9A%E5%AD%A9%E5%AD%90%E4%BB%AC%E7%9A%84%E6%B8%B8%E6%88%8F%EF%BC%88%E5%9C%86%E5%9C%88%E4%B8%AD%E6%9C%80%E5%90%8E%E5%89%A9%E4%B8%8B%E7%9A%84%E6%95%B0%EF%BC%89.html"/>
    <id>https://xxren8218.github.io/20210510/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9A%E5%AD%A9%E5%AD%90%E4%BB%AC%E7%9A%84%E6%B8%B8%E6%88%8F%EF%BC%88%E5%9C%86%E5%9C%88%E4%B8%AD%E6%9C%80%E5%90%8E%E5%89%A9%E4%B8%8B%E7%9A%84%E6%95%B0%EF%BC%89.html</id>
    <published>2021-05-10T12:16:29.000Z</published>
    <updated>2021-05-11T06:07:11.635Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><pre><code>每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0...m-1报数....这样下去....直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1)</code></pre><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>这道题可以用数学归纳法解决。题目抽象：给定一个由[0...n-1]构成的数组，第一次从0开始数m个数，然后删除，以后每次都从删除的数下一个位置开始数m个数，然后删除，直到剩余一个数字，找出那个数字。比如：arr = [0 1 2 3 4]， m = 3第一次：删除2 ，变成 arr = [0 1 3 4]第二次，删除0，变成 arr = [1 3 4]第三次，删除4，变成 arr = [1 3]第四次，删除1，变成 arr = [3]f[1] = 0               f[2] = (f&#123;1] + m) % 2  f[3] = (f[2] + m) % 3  f[4] = (f[3] + m) % 4  f[5] = (f[4] + m) % 5  ...f[n] = (f[n-1] + m) % n</code></pre><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">LastRemaining_Solution</span>(<span class="params">self, n, m</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> n==<span class="number">0</span>:<span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n==<span class="number">1</span>:<span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        f=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,n+<span class="number">1</span>):</span><br><span class="line">            f=(f+m)%i</span><br><span class="line">        <span class="keyword">return</span> f</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="其他" scheme="https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（四十五）：扑克牌顺子</title>
    <link href="https://xxren8218.github.io/20210510/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9A%E6%89%91%E5%85%8B%E7%89%8C%E9%A1%BA%E5%AD%90.html"/>
    <id>https://xxren8218.github.io/20210510/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9A%E6%89%91%E5%85%8B%E7%89%8C%E9%A1%BA%E5%AD%90.html</id>
    <published>2021-05-10T12:01:26.000Z</published>
    <updated>2021-05-10T12:37:53.062Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><pre><code>LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张😊)...他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子.....LL不高兴了,他想了想,决定大\小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何。为了方便起见,你可以认为大小王是0。</code></pre><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>这道题可以用另外一种思路解决：无非两种特殊情况需要考虑：    1.因为有四个鬼，所以最多有四个0，遇到0直接跳过    2.若含有重复的数字，那肯定不能成为顺子。可以用python的set()得以解决。    3.要组成顺子，这几张牌的max(max-min)只能是5-1=4（1，2，3，4，5）        若有零，则肯定更小于4了。</code></pre><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">IsContinuous</span>(<span class="params">self, numbers</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        repeat = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> numbers:</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:<span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> repeat:</span><br><span class="line">                repeat.add(i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">max</span>(repeat) - <span class="built_in">min</span>(repeat) &lt;= <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张😊)...</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="其他" scheme="https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（四十二）：和为S的两个数字</title>
    <link href="https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%92%8C%E4%B8%BAS%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97.html"/>
    <id>https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%92%8C%E4%B8%BAS%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97.html</id>
    <published>2021-05-08T10:27:38.000Z</published>
    <updated>2021-05-08T10:32:19.450Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><pre><code>输入一个递增排序的数组和一个数字S，在数组中查找两个数，是的他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。输出描述：对应每个测试案例，输出两个数，小的先输出。</code></pre><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>对于一个数组，我们可以定义两个指针，一个从左往右遍历（left），另一个从右往左遍历（right）。首先，我们比较第一个数字和最后一个数字的和cursum与给定数字tsum，如果cursum &lt; tsum，那么我们就要加大输入值，所以，left向右移动一位，重复之前的计算；如果cursum &gt; tsum，那么我们就要减小输入值，所以，right向左移动一位，重复之前的计算；如果相等，那么这两个数字就是我们要找的数字，直接输出即可。这么做的好处是，也保证了乘积最小。</code></pre><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">FindNumbersWithSum</span>(<span class="params">self, array, tsum</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(array) &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(array)-<span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            <span class="keyword">if</span> array[left] + array[right] == tsum:</span><br><span class="line">                res.append(array[left])</span><br><span class="line">                res.append(array[right])</span><br><span class="line">                <span class="keyword">return</span> res</span><br><span class="line">            <span class="keyword">elif</span> array[left] + array[right] &lt; tsum:</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;输入一个递增排序的数组和一个数字S，在数组中查找两个数，是的他们的和正好是S，如果有多对数字的和等于S，输出</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="其他" scheme="https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（四十一）：和为S的连续正数序列</title>
    <link href="https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E5%92%8C%E4%B8%BAS%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97.html"/>
    <id>https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E5%92%8C%E4%B8%BAS%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97.html</id>
    <published>2021-05-08T10:23:27.000Z</published>
    <updated>2021-05-08T10:26:57.735Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><pre><code>小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Good Luck!输出描述：输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序。</code></pre><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>设定两个指针，一个指向第一个数，一个指向最后一个数，在此之前需要设定第一个数和最后一个数的值，由于是正数序列，所以可以把第一个数设为1，最后一个数为2（因为是要求是连续正数序列，最后不可能和第一个数重合）。下一步就是不断改变第一个数和最后一个数的值，如果从第一个数到最后一个数的和刚好是要求的和，那么把所有的数都添加到一个序列中；如果大于要求的和，则说明从第一个数到最后一个数之间的范围太大，因此减小范围，需要把第一个数的值加1，同时把当前和减去原来的第一个数的值；如果小于要求的和，说明范围太小，因此把最后一个数加1，同时把当前的和加上改变之后的最后一个数的值。这样，不断修改第一个数和最后一个数的值，就能确定所有连续正数序列的和等于S的序列了。注意：初中的求和公式应该记得吧，首项加尾项的和乘以个数除以2，即sum = (a + b) * n / 2。</code></pre><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">FindContinuousSequence</span>(<span class="params">self, tsum</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        res = []</span><br><span class="line">        left, right = <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            <span class="comment"># 求连续整数的和(a+b)*n/2</span></span><br><span class="line">            cursum = (left + right)*(right-left+<span class="number">1</span>)/<span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> cursum == tsum:</span><br><span class="line">                tmp = []</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(left, right+<span class="number">1</span>):</span><br><span class="line">                    tmp.append(i)</span><br><span class="line">                res.append(tmp)</span><br><span class="line">                <span class="comment"># 接着后移</span></span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> cursum &lt; tsum:</span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="其他" scheme="https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（三十三）：丑数</title>
    <link href="https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9A%E4%B8%91%E6%95%B0.html"/>
    <id>https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9A%E4%B8%91%E6%95%B0.html</id>
    <published>2021-05-08T10:02:57.000Z</published>
    <updated>2021-05-08T10:22:19.712Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><pre><code>把只包含因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。</code></pre><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>丑数其实可以看成是是下图的形式。即2、3、5的乘积，然后找出相乘后最小的数字放入列表(数组)a中即可。</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210508180923.JPG" alt=""></p><pre><code>可以设立三个指针。p2,p3,p5 = 0 如下图所示。</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210508181238.JPG" alt=""></p><pre><code>对于n=1 a[0],他们的值都是1.从a[2]开始(n=2)。如下图</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210508181529.JPG" alt=""></p><pre><code>最小的数是a[p2]x2,则将其放入数组a中，将p2指针后移一位。接下来 n=3 时候,以此类推。。。当 n = 6时，两个都是6，如下图，则需要移动两个指针。</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210508181951.JPG" alt="">    </p><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetUglyNumber_Solution</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> index == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        a = [<span class="number">1</span>]</span><br><span class="line">        p2 = p3 = p5 = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,index+<span class="number">1</span>):</span><br><span class="line">            newNum = <span class="built_in">min</span>(a[p2]*<span class="number">2</span>, a[p3]*<span class="number">3</span>, a[p5]*<span class="number">5</span>)</span><br><span class="line">            a.append(newNum)</span><br><span class="line">            <span class="comment"># 此处保证了，数值相等时都向后移动一位。</span></span><br><span class="line">            <span class="keyword">if</span> newNum == a[p2]*<span class="number">2</span>:p2 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> newNum == a[p3]*<span class="number">3</span>:p3 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> newNum == a[p5]*<span class="number">5</span>:p5 += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> a[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;把只包含因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含因子</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="其他" scheme="https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（三十一）：整数中1出现的次数（从1到n整数中1出现的次数）</title>
    <link href="https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%88%E4%BB%8E1%E5%88%B0n%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%89.html"/>
    <id>https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%88%E4%BB%8E1%E5%88%B0n%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%89.html</id>
    <published>2021-05-08T09:36:53.000Z</published>
    <updated>2021-05-08T10:02:11.605Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><pre><code>输入一个整数n，求从1到n这n个整数的十进制表示中1出现的次数。例如输入12，从1到12这些整数中包含1的数字有1，10，11和12，1一共出现了5次。</code></pre><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>“1”出现次数实际上就是各个位置上1出现的次数的求和，以3101592为例。我们以cur表示目前所在的位数,例如在百位  base = 100, cur = n//base%10a表示左侧的数字，b表示右侧的位数. a = n//base//10, b = n%base</code></pre><p><strong>如下图所示</strong><br><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210508174659.JPG" alt=""></p><p>接下来只需要考虑三种情况即可：</p><pre><code>1.cur &gt; 1: 共有(a+1)xbase2.cur == 1: 共有axbase+b+13.cur &lt; 1: 共有axbase</code></pre><p>最后求和即可。</p><p><strong>如下图所示</strong><br><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/20210508174706.JPG" alt=""></p><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">NumberOf1Between1AndN_Solution</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        base = <span class="number">1</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> base &lt;= n:</span><br><span class="line">            a = n//base</span><br><span class="line">            b = n%base</span><br><span class="line">            cur = a%<span class="number">10</span></span><br><span class="line">            a //= <span class="number">10</span></span><br><span class="line">            <span class="keyword">if</span> cur &gt; <span class="number">1</span>:res += (a+<span class="number">1</span>)*base</span><br><span class="line">            <span class="keyword">elif</span> cur == <span class="number">1</span>:res += (a*base+b+<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:res += a*base</span><br><span class="line">            base *= <span class="number">10</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;输入一个整数n，求从1到n这n个整数的十进制表示中1出现的次数。例如输入12，从1到12这些整数中包含1的数</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="其他" scheme="https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（二十九）：最小的K个数</title>
    <link href="https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B9%9D%EF%BC%89%EF%BC%9A%E6%9C%80%E5%B0%8F%E7%9A%84K%E4%B8%AA%E6%95%B0.html"/>
    <id>https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B9%9D%EF%BC%89%EF%BC%9A%E6%9C%80%E5%B0%8F%E7%9A%84K%E4%B8%AA%E6%95%B0.html</id>
    <published>2021-05-08T09:28:55.000Z</published>
    <updated>2021-05-08T09:36:02.471Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.题目</h2><pre><code>输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4。</code></pre><h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2.思路"></a>2.思路</h2><pre><code>可以通过python的排序对给定的数组进行排序，然后取值即可。</code></pre><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetLeastNumbers_Solution</span>(<span class="params">self, tinput, k</span>):</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> k==<span class="number">0</span> <span class="keyword">or</span> k&gt;<span class="built_in">len</span>(tinput):</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        tinput.sort()  <span class="comment"># list.sort()默认是升序排序。</span></span><br><span class="line">        <span class="keyword">return</span> tinput[:k]</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1.题目&quot;&gt;&lt;/a&gt;1.题目&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,</summary>
      
    
    
    
    <category term="传统算法" scheme="https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="其他" scheme="https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯基础</title>
    <link href="https://xxren8218.github.io/20210429/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9F%BA%E7%A1%80.html"/>
    <id>https://xxren8218.github.io/20210429/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9F%BA%E7%A1%80.html</id>
    <published>2021-04-29T15:52:24.000Z</published>
    <updated>2021-04-29T15:54:44.328Z</updated>
    
    <content type="html"><![CDATA[<h3 id="贝叶斯拼写检查器"><a href="#贝叶斯拼写检查器" class="headerlink" title="贝叶斯拼写检查器"></a>贝叶斯拼写检查器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">words</span>(<span class="params">text</span>):</span> <span class="keyword">return</span> re.findall(<span class="string">&#x27;[a-z]+&#x27;</span>, text.lower()) </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">features</span>):</span></span><br><span class="line">    model = collections.defaultdict(<span class="keyword">lambda</span>: <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> features:</span><br><span class="line">        model[f] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"> </span><br><span class="line">NWORDS = train(words(<span class="built_in">open</span>(<span class="string">&#x27;big.txt&#x27;</span>).read()))</span><br><span class="line"> </span><br><span class="line">alphabet = <span class="string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits1</span>(<span class="params">word</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(word)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set</span>([word[<span class="number">0</span>:i]+word[i+<span class="number">1</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)] +                     <span class="comment"># deletion</span></span><br><span class="line">               [word[<span class="number">0</span>:i]+word[i+<span class="number">1</span>]+word[i]+word[i+<span class="number">2</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>)] + <span class="comment"># transposition</span></span><br><span class="line">               [word[<span class="number">0</span>:i]+c+word[i+<span class="number">1</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n) <span class="keyword">for</span> c <span class="keyword">in</span> alphabet] + <span class="comment"># alteration</span></span><br><span class="line">               [word[<span class="number">0</span>:i]+c+word[i:] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n+<span class="number">1</span>) <span class="keyword">for</span> c <span class="keyword">in</span> alphabet])  <span class="comment"># insertion</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">known_edits2</span>(<span class="params">word</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set</span>(e2 <span class="keyword">for</span> e1 <span class="keyword">in</span> edits1(word) <span class="keyword">for</span> e2 <span class="keyword">in</span> edits1(e1) <span class="keyword">if</span> e2 <span class="keyword">in</span> NWORDS)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">known</span>(<span class="params">words</span>):</span> <span class="keyword">return</span> <span class="built_in">set</span>(w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> w <span class="keyword">in</span> NWORDS)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">correct</span>(<span class="params">word</span>):</span></span><br><span class="line">    candidates = known([word]) <span class="keyword">or</span> known(edits1(word)) <span class="keyword">or</span> known_edits2(word) <span class="keyword">or</span> [word]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(candidates, key=<span class="keyword">lambda</span> w: NWORDS[w])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># appl #appla #learw #tess #morw</span></span><br><span class="line">correct(<span class="string">&#x27;tess&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>&#39;less&#39;</code></pre><h3 id="求解：argmaxc-P-c-w-gt-argmaxc-P-w-c-P-c-P-w"><a href="#求解：argmaxc-P-c-w-gt-argmaxc-P-w-c-P-c-P-w" class="headerlink" title="求解：argmaxc P(c|w) -&gt; argmaxc P(w|c) P(c) / P(w)"></a>求解：argmaxc P(c|w) -&gt; argmaxc P(w|c) P(c) / P(w)</h3><ul><li>P(c), <strong>先验概率</strong> 文章中出现一个正确拼写词 c 的概率, 也就是说, 在英语文章中, c 出现的概率有多大</li><li>P(w|c), 在用户想键入 c 的情况下敲成 w 的概率. 因为这个是代表用户会以多大的概率把 c 敲错成 w</li><li>argmaxc, 用来枚举所有可能的 c 并且选取概率最大的</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把语料中的单词全部抽取出来, 转成小写, 并且去除单词中间的特殊符号</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">words</span>(<span class="params">text</span>):</span> <span class="keyword">return</span> re.findall(<span class="string">&#x27;[a-z]+&#x27;</span>, text.lower()) </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">features</span>):</span></span><br><span class="line">    <span class="comment"># 匿名函数：新出现的词，将其出现次数设置为1，符合实际，不然上面的公式可得，概率为0.太过绝对。另外出现一次就会加一</span></span><br><span class="line">    model = collections.defaultdict(<span class="keyword">lambda</span>: <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> features:</span><br><span class="line">        model[f] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"> </span><br><span class="line">NWORDS = train(words(<span class="built_in">open</span>(<span class="string">&#x27;big.txt&#x27;</span>).read()))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>要是遇到我们从来没有过见过的新词怎么办. 假如说一个词拼写完全正确, 但是语料库中没有包含这个词, 从而这个词也永远不会出现在训练集中. 于是, 我们就要返回出现这个词的概率是0. 这个情况不太妙, 因为概率为0这个代表了这个事件绝对不可能发生, 而在我们的概率模型中, 我们期望用一个很小的概率来代表这种情况. lambda: 1</p><h3 id="相当于先求先验概率"><a href="#相当于先求先验概率" class="headerlink" title="相当于先求先验概率"></a>相当于先求先验概率</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NWORDS</span><br></pre></td></tr></table></figure><pre><code>defaultdict(&lt;function __main__.train.&lt;locals&gt;.&lt;lambda&gt;&gt;,            &#123;&#39;the&#39;: 80031,             &#39;project&#39;: 289,             &#39;gutenberg&#39;: 264,             &#39;ebook&#39;: 88,             &#39;of&#39;: 40026,             &#39;adventures&#39;: 18,             &#39;sherlock&#39;: 102,             &#39;holmes&#39;: 468,             &#39;by&#39;: 6739,             &#39;sir&#39;: 178,             &#39;arthur&#39;: 35,             &#39;conan&#39;: 5,             &#39;doyle&#39;: 6,             &#39;in&#39;: 22048,             &#39;our&#39;: 1067,             &#39;series&#39;: 129,             &#39;copyright&#39;: 70,             &#39;laws&#39;: 234,             &#39;are&#39;: 3631,             &#39;changing&#39;: 45,             &#39;all&#39;: 4145,             &#39;over&#39;: 1283,             &#39;world&#39;: 363,             &#39;be&#39;: 6156,             &#39;sure&#39;: 124,             ...&#125;)</code></pre><h3 id="编辑距离"><a href="#编辑距离" class="headerlink" title="编辑距离:"></a>编辑距离:</h3><p>两个词之间的编辑距离定义为使用了几次插入(在词中插入一个单字母), 删除(删除一个单字母), 交换(交换相邻两个字母), 替换(把一个字母换成另一个)的操作从一个词变到另一个词.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回所有与单词 w 编辑距离为 1 的集合.一次操作，能从A到B</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits1</span>(<span class="params">word</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(word)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set</span>([word[<span class="number">0</span>:i]+word[i+<span class="number">1</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)] +                     <span class="comment"># deletion</span></span><br><span class="line">               [word[<span class="number">0</span>:i]+word[i+<span class="number">1</span>]+word[i]+word[i+<span class="number">2</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>)] + <span class="comment"># transposition</span></span><br><span class="line">               [word[<span class="number">0</span>:i]+c+word[i+<span class="number">1</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n) <span class="keyword">for</span> c <span class="keyword">in</span> alphabet] + <span class="comment"># alteration</span></span><br><span class="line">               [word[<span class="number">0</span>:i]+c+word[i:] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n+<span class="number">1</span>) <span class="keyword">for</span> c <span class="keyword">in</span> alphabet])  <span class="comment"># insertion</span></span><br></pre></td></tr></table></figure><p>与 something 编辑距离为2的单词居然达到了 114,324 个</p><p>优化:在这些编辑距离小于2的词中间, 只把那些正确的词作为候选词,只能返回 3 个单词: ‘smoothing’, ‘something’ 和 ‘soothing’</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回所有与单词 w 编辑距离为 2 的集合</span></span><br><span class="line"><span class="comment"># 在这些编辑距离小于2的词中间, 只把那些正确的词作为候选词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits2</span>(<span class="params">word</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set</span>(e2 <span class="keyword">for</span> e1 <span class="keyword">in</span> edits1(word) <span class="keyword">for</span> e2 <span class="keyword">in</span> edits1(e1))</span><br></pre></td></tr></table></figure><p><strong> P(w|c)的计算 </strong></p><p>正常来说把一个元音拼成另一个的概率要大于辅音 (因为人常常把 hello 打成 hallo 这样); 把单词的第一个字母拼错的概率会相对小, 等等.但是为了简单起见, 选择了一个简单的方法: 编辑距离为1的正确单词比编辑距离为2的优先级高, 而编辑距离为0的正确单词优先级比编辑距离为1的高. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">known</span>(<span class="params">words</span>):</span> <span class="keyword">return</span> <span class="built_in">set</span>(w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> w <span class="keyword">in</span> NWORDS)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果known(set)非空, candidate 就会选取这个集合, 而不继续计算后面的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">correct</span>(<span class="params">word</span>):</span></span><br><span class="line">    candidates = known([word]) <span class="keyword">or</span> known(edits1(word)) <span class="keyword">or</span> known_edits2(word) <span class="keyword">or</span> [word]  <span class="comment"># 优先级，前面的能得到结果，不会算后面的。</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(candidates, key=<span class="keyword">lambda</span> w: NWORDS[w])</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;贝叶斯拼写检查器&quot;&gt;&lt;a href=&quot;#贝叶斯拼写检查器&quot; class=&quot;headerlink&quot; title=&quot;贝叶斯拼写检查器&quot;&gt;&lt;/a&gt;贝叶斯拼写检查器&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td c</summary>
      
    
    
    
    <category term="机器学习" scheme="https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习基础" scheme="https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>决策树基础</title>
    <link href="https://xxren8218.github.io/20210429/%E5%86%B3%E7%AD%96%E6%A0%91.html"/>
    <id>https://xxren8218.github.io/20210429/%E5%86%B3%E7%AD%96%E6%A0%91.html</id>
    <published>2021-04-29T15:45:59.000Z</published>
    <updated>2021-04-29T15:50:48.301Z</updated>
    
    <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets.california_housing <span class="keyword">import</span> fetch_california_housing  <span class="comment"># 内置的数据集</span></span><br><span class="line">housing = fetch_california_housing()</span><br><span class="line">print(housing.DESCR)</span><br></pre></td></tr></table></figure><pre><code>downloading Cal. housing from http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz to C:\Users\user\scikit_learn_dataCalifornia housing dataset.The original database is available from StatLib    http://lib.stat.cmu.edu/The data contains 20,640 observations on 9 variables.This dataset contains the average house value as target variableand the following input variables (features): average income,housing average age, average rooms, average bedrooms, population,average occupation, latitude, and longitude in that order.References----------Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,Statistics and Probability Letters, 33 (1997) 291-297.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">housing.data.shape  <span class="comment"># 有两万多个数据，每个数据有8个特征</span></span><br></pre></td></tr></table></figure><pre><code>(20640, 8)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">housing.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>array([   8.3252    ,   41.        ,    6.98412698,    1.02380952,        322.        ,    2.55555556,   37.88      , -122.23      ])</code></pre><h2 id="sklearn运行决策树只需要两步"><a href="#sklearn运行决策树只需要两步" class="headerlink" title="sklearn运行决策树只需要两步"></a>sklearn运行决策树只需要两步</h2><ul><li><strong>1.实例化树模型，传参数(下面有部分API文档说明)——此处最大深度为2</strong></li><li><strong>2.用实例化的对象.fit（数据的X，数据的y）就可以了</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree  <span class="comment"># 导入需要指定的树模块</span></span><br><span class="line">dtr = tree.DecisionTreeRegressor(max_depth = <span class="number">2</span>)</span><br><span class="line">dtr.fit(housing.data[:, [<span class="number">6</span>, <span class="number">7</span>]], housing.target)   <span class="comment"># 只选择两个数据来用（试验而已）。——经度和纬度</span></span><br></pre></td></tr></table></figure><pre><code>DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=2, max_features=None,           max_leaf_nodes=None, min_impurity_split=1e-07,           min_samples_leaf=1, min_samples_split=2,           min_weight_fraction_leaf=0.0, presort=False, random_state=None,           splitter=&#39;best&#39;)</code></pre><h3 id="会自动输出一些默认的参数。"><a href="#会自动输出一些默认的参数。" class="headerlink" title="会自动输出一些默认的参数。"></a>会自动输出一些默认的参数。</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要可视化显示 首先需要安装 graphviz   http://www.graphviz.org/Download..php</span></span><br><span class="line">dot_data = \</span><br><span class="line">    tree.export_graphviz(</span><br><span class="line">        dtr,  <span class="comment"># 构造出来的决策树对象传入第一个参数</span></span><br><span class="line">        out_file = <span class="literal">None</span>,</span><br><span class="line">        feature_names = housing.feature_names[<span class="number">6</span>:<span class="number">8</span>], <span class="comment"># 用哪个特征，传哪个特征（含左不含右），其余不需要改了。</span></span><br><span class="line">        filled = <span class="literal">True</span>,</span><br><span class="line">        impurity = <span class="literal">False</span>,</span><br><span class="line">        rounded = <span class="literal">True</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p><strong>执行完以后会生成一个.dat的文件</strong></p><p><strong>需要一个库来对其进行显示</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install pydotplus</span></span><br><span class="line"><span class="keyword">import</span> pydotplus</span><br><span class="line">graph = pydotplus.graph_from_dot_data(dot_data)</span><br><span class="line">graph.get_nodes()[<span class="number">7</span>].set_fillcolor(<span class="string">&quot;#FFF2DD&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br><span class="line">Image(graph.create_png())</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/10_0.png" alt=""></p><p><strong>可以用下列的命令将构造好的图保存起来</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph.write_png(<span class="string">&quot;dtr_white_background.png&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><p><strong>数据的切分</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">data_train, data_test, target_train, target_test = \</span><br><span class="line">    train_test_split(housing.data, housing.target, test_size = <span class="number">0.1</span>, random_state = <span class="number">42</span>)</span><br><span class="line">dtr = tree.DecisionTreeRegressor(random_state = <span class="number">42</span>)  <span class="comment"># （切分时的随机性）为了方面复现，使得每一次随机的结果都是一致的。</span></span><br><span class="line">dtr.fit(data_train, target_train)</span><br><span class="line"></span><br><span class="line">dtr.score(data_test, target_test)</span><br></pre></td></tr></table></figure><pre><code>0.637318351331017</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line">rfr = RandomForestRegressor( random_state = <span class="number">42</span>)</span><br><span class="line">rfr.fit(data_train, target_train)</span><br><span class="line">rfr.score(data_test, target_test)</span><br></pre></td></tr></table></figure><pre><code>0.79086492280964926</code></pre><h2 id="树模型参数-（为了防止过拟合）"><a href="#树模型参数-（为了防止过拟合）" class="headerlink" title="树模型参数:（为了防止过拟合）"></a>树模型参数:（为了防止过拟合）</h2><ul><li><p>1.衡量标准  gini系数  or  entropy（熵）</p></li><li><p>2.splitter  best or random 前者是在所有特征中找最好的切分点 后者是在部分特征中（数据量大的时候）——特征多的时候，从头到尾遍历特征比较耗时（best默认），随机选择几个特征（random）,很少去改，因为很少遇到几百个特征。</p></li><li><p>3.max_features  None（所有），log2，sqrt，N  特征小于50的时候一般使用所有的</p></li><li><p><strong>4.max_depth</strong>  数据少或者特征少的时候可以不管这个值，如果模型样本量多，特征也多的情况下，可以尝试限制下（没有定值，可以交叉验证得到最优值）</p></li><li><p><strong>5.min_samples_split</strong>  如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p></li><li><p>6.min_samples_leaf  这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝，如果样本量不大，不需要管这个值，大些如10W可是尝试下5</p></li><li><p>7.min_weight_fraction_leaf 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</p></li><li><p>8.max_leaf_nodes 通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到。</p></li><li><p>9.class_weight 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。</p></li><li><p>10.min_impurity_split 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点 。</p></li><li>n_estimators:要建立树的个数</li></ul><h3 id="GridSearchCV-模块相当于循环遍历，方便对参数进行选择。"><a href="#GridSearchCV-模块相当于循环遍历，方便对参数进行选择。" class="headerlink" title="GridSearchCV 模块相当于循环遍历，方便对参数进行选择。"></a>GridSearchCV 模块相当于循环遍历，方便对参数进行选择。</h3><ul><li>实例化GridSearchCV后需要传的参数(算法，参数的候选值，VC交叉验证的次数)，一般候选的值写成字典的形式</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</span><br><span class="line">tree_param_grid = &#123; <span class="string">&#x27;min_samples_split&#x27;</span>: <span class="built_in">list</span>((<span class="number">3</span>,<span class="number">6</span>,<span class="number">9</span>)),<span class="string">&#x27;n_estimators&#x27;</span>:<span class="built_in">list</span>((<span class="number">10</span>,<span class="number">50</span>,<span class="number">100</span>))&#125;</span><br><span class="line">grid = GridSearchCV(RandomForestRegressor(),param_grid=tree_param_grid, cv=<span class="number">5</span>)</span><br><span class="line">grid.fit(data_train, target_train)</span><br><span class="line">grid.grid_scores_, grid.best_params_, grid.best_score_</span><br></pre></td></tr></table></figure><pre><code>([mean: 0.78405, std: 0.00505, params: &#123;&#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 10&#125;,  mean: 0.80529, std: 0.00448, params: &#123;&#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 50&#125;,  mean: 0.80673, std: 0.00433, params: &#123;&#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 100&#125;,  mean: 0.79016, std: 0.00124, params: &#123;&#39;min_samples_split&#39;: 6, &#39;n_estimators&#39;: 10&#125;,  mean: 0.80496, std: 0.00491, params: &#123;&#39;min_samples_split&#39;: 6, &#39;n_estimators&#39;: 50&#125;,  mean: 0.80671, std: 0.00408, params: &#123;&#39;min_samples_split&#39;: 6, &#39;n_estimators&#39;: 100&#125;,  mean: 0.78747, std: 0.00341, params: &#123;&#39;min_samples_split&#39;: 9, &#39;n_estimators&#39;: 10&#125;,  mean: 0.80481, std: 0.00322, params: &#123;&#39;min_samples_split&#39;: 9, &#39;n_estimators&#39;: 50&#125;,  mean: 0.80603, std: 0.00437, params: &#123;&#39;min_samples_split&#39;: 9, &#39;n_estimators&#39;: 100&#125;], &#123;&#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 100&#125;, 0.8067250881273065)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rfr = RandomForestRegressor( min_samples_split=<span class="number">3</span>,n_estimators = <span class="number">100</span>,random_state = <span class="number">42</span>)</span><br><span class="line">rfr.fit(data_train, target_train)</span><br><span class="line">rfr.score(data_test, target_test)</span><br></pre></td></tr></table></figure><pre><code>0.80908290496531576</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.Series(rfr.feature_importances_, index = housing.feature_names).sort_values(ascending = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><pre><code>MedInc        0.524257AveOccup      0.137947Latitude      0.090622Longitude     0.089414HouseAge      0.053970AveRooms      0.044443Population    0.030263AveBedrms     0.029084dtype: float64</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;决策树&quot;&gt;&lt;a href=&quot;#决策树&quot; class=&quot;headerlink&quot; title=&quot;决策树&quot;&gt;&lt;/a&gt;决策树&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;</summary>
      
    
    
    
    <category term="机器学习" scheme="https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习基础" scheme="https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归实战--信用卡诈骗检测</title>
    <link href="https://xxren8218.github.io/20210418/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98.html"/>
    <id>https://xxren8218.github.io/20210418/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98.html</id>
    <published>2021-04-17T17:26:48.000Z</published>
    <updated>2021-04-17T18:53:58.904Z</updated>
    
    <content type="html"><![CDATA[<h1 id="信用卡诈骗预测——二分类的问题"><a href="#信用卡诈骗预测——二分类的问题" class="headerlink" title="信用卡诈骗预测——二分类的问题"></a>信用卡诈骗预测——二分类的问题</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="先来看看数据长什么样子吧"><a href="#先来看看数据长什么样子吧" class="headerlink" title="先来看看数据长什么样子吧"></a>先来看看数据长什么样子吧</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">&quot;creditcard.csv&quot;</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Time</th>      <th>V1</th>      <th>V2</th>      <th>V3</th>      <th>V4</th>      <th>V5</th>      <th>V6</th>      <th>V7</th>      <th>V8</th>      <th>V9</th>      <th>...</th>      <th>V21</th>      <th>V22</th>      <th>V23</th>      <th>V24</th>      <th>V25</th>      <th>V26</th>      <th>V27</th>      <th>V28</th>      <th>Amount</th>      <th>Class</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.0</td>      <td>-1.359807</td>      <td>-0.072781</td>      <td>2.536347</td>      <td>1.378155</td>      <td>-0.338321</td>      <td>0.462388</td>      <td>0.239599</td>      <td>0.098698</td>      <td>0.363787</td>      <td>...</td>      <td>-0.018307</td>      <td>0.277838</td>      <td>-0.110474</td>      <td>0.066928</td>      <td>0.128539</td>      <td>-0.189115</td>      <td>0.133558</td>      <td>-0.021053</td>      <td>149.62</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>0.0</td>      <td>1.191857</td>      <td>0.266151</td>      <td>0.166480</td>      <td>0.448154</td>      <td>0.060018</td>      <td>-0.082361</td>      <td>-0.078803</td>      <td>0.085102</td>      <td>-0.255425</td>      <td>...</td>      <td>-0.225775</td>      <td>-0.638672</td>      <td>0.101288</td>      <td>-0.339846</td>      <td>0.167170</td>      <td>0.125895</td>      <td>-0.008983</td>      <td>0.014724</td>      <td>2.69</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>1.0</td>      <td>-1.358354</td>      <td>-1.340163</td>      <td>1.773209</td>      <td>0.379780</td>      <td>-0.503198</td>      <td>1.800499</td>      <td>0.791461</td>      <td>0.247676</td>      <td>-1.514654</td>      <td>...</td>      <td>0.247998</td>      <td>0.771679</td>      <td>0.909412</td>      <td>-0.689281</td>      <td>-0.327642</td>      <td>-0.139097</td>      <td>-0.055353</td>      <td>-0.059752</td>      <td>378.66</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>1.0</td>      <td>-0.966272</td>      <td>-0.185226</td>      <td>1.792993</td>      <td>-0.863291</td>      <td>-0.010309</td>      <td>1.247203</td>      <td>0.237609</td>      <td>0.377436</td>      <td>-1.387024</td>      <td>...</td>      <td>-0.108300</td>      <td>0.005274</td>      <td>-0.190321</td>      <td>-1.175575</td>      <td>0.647376</td>      <td>-0.221929</td>      <td>0.062723</td>      <td>0.061458</td>      <td>123.50</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>2.0</td>      <td>-1.158233</td>      <td>0.877737</td>      <td>1.548718</td>      <td>0.403034</td>      <td>-0.407193</td>      <td>0.095921</td>      <td>0.592941</td>      <td>-0.270533</td>      <td>0.817739</td>      <td>...</td>      <td>-0.009431</td>      <td>0.798278</td>      <td>-0.137458</td>      <td>0.141267</td>      <td>-0.206010</td>      <td>0.502292</td>      <td>0.219422</td>      <td>0.215153</td>      <td>69.99</td>      <td>0</td>    </tr>  </tbody></table><p>5 rows × 31 columns</p></div><h2 id="先来看看正负样本的分布情况吧！"><a href="#先来看看正负样本的分布情况吧！" class="headerlink" title="先来看看正负样本的分布情况吧！"></a>先来看看正负样本的分布情况吧！</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">count_classes = pd.value_counts(data[<span class="string">&#x27;Class&#x27;</span>], sort = <span class="literal">True</span>).sort_index()  <span class="comment"># values_counts可以根据值进行计数。sort_index()按行索引排序</span></span><br><span class="line">count_classes.plot(kind = <span class="string">&#x27;bar&#x27;</span>)    <span class="comment"># 除了plt，pd也可以做一些简单的图</span></span><br><span class="line">plt.title(<span class="string">&quot;Fraud class histogram&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Class&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Frequency&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.text.Text at 0x216366d8860&gt;</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/5_11.png" alt=""></p><h2 id="样本数据极度不均匀，应该怎么做？"><a href="#样本数据极度不均匀，应该怎么做？" class="headerlink" title="样本数据极度不均匀，应该怎么做？"></a>样本数据极度不均匀，应该怎么做？</h2><ul><li><strong>下采样</strong>——对于不均衡的数据，让 1 和 0 的数据一样少</li><li><strong>过采样</strong>——对于不均衡的数据，生成一些数据，让生成的数据与 0 样本一样多</li></ul><h3 id="Amount数据分布不均衡，为了保证特征之间的分布是差不多的。——即保证数据的重要性一样。"><a href="#Amount数据分布不均衡，为了保证特征之间的分布是差不多的。——即保证数据的重要性一样。" class="headerlink" title="Amount数据分布不均衡，为了保证特征之间的分布是差不多的。——即保证数据的重要性一样。"></a>Amount数据分布不均衡，为了保证特征之间的分布是差不多的。——即保证数据的重要性一样。</h3><ul><li><strong>标准化</strong> </li><li><strong>归一化</strong>  </li></ul><p>可以使用sklearn的预处理模块进行标准化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;normAmount&#x27;</span>] = StandardScaler().fit_transform(data[<span class="string">&#x27;Amount&#x27;</span>].reshape(-<span class="number">1</span>, <span class="number">1</span>)) <span class="comment"># reshape(-1,1)</span></span><br><span class="line">                                                                                   <span class="comment"># -1表示让python给它行数</span></span><br><span class="line">data = data.drop([<span class="string">&#x27;Time&#x27;</span>,<span class="string">&#x27;Amount&#x27;</span>],axis=<span class="number">1</span>)  <span class="comment"># 有了新特征后，将没用的特征去掉。</span></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>V1</th>      <th>V2</th>      <th>V3</th>      <th>V4</th>      <th>V5</th>      <th>V6</th>      <th>V7</th>      <th>V8</th>      <th>V9</th>      <th>V10</th>      <th>...</th>      <th>V21</th>      <th>V22</th>      <th>V23</th>      <th>V24</th>      <th>V25</th>      <th>V26</th>      <th>V27</th>      <th>V28</th>      <th>Class</th>      <th>normAmount</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>-1.359807</td>      <td>-0.072781</td>      <td>2.536347</td>      <td>1.378155</td>      <td>-0.338321</td>      <td>0.462388</td>      <td>0.239599</td>      <td>0.098698</td>      <td>0.363787</td>      <td>0.090794</td>      <td>...</td>      <td>-0.018307</td>      <td>0.277838</td>      <td>-0.110474</td>      <td>0.066928</td>      <td>0.128539</td>      <td>-0.189115</td>      <td>0.133558</td>      <td>-0.021053</td>      <td>0</td>      <td>0.244964</td>    </tr>    <tr>      <th>1</th>      <td>1.191857</td>      <td>0.266151</td>      <td>0.166480</td>      <td>0.448154</td>      <td>0.060018</td>      <td>-0.082361</td>      <td>-0.078803</td>      <td>0.085102</td>      <td>-0.255425</td>      <td>-0.166974</td>      <td>...</td>      <td>-0.225775</td>      <td>-0.638672</td>      <td>0.101288</td>      <td>-0.339846</td>      <td>0.167170</td>      <td>0.125895</td>      <td>-0.008983</td>      <td>0.014724</td>      <td>0</td>      <td>-0.342475</td>    </tr>    <tr>      <th>2</th>      <td>-1.358354</td>      <td>-1.340163</td>      <td>1.773209</td>      <td>0.379780</td>      <td>-0.503198</td>      <td>1.800499</td>      <td>0.791461</td>      <td>0.247676</td>      <td>-1.514654</td>      <td>0.207643</td>      <td>...</td>      <td>0.247998</td>      <td>0.771679</td>      <td>0.909412</td>      <td>-0.689281</td>      <td>-0.327642</td>      <td>-0.139097</td>      <td>-0.055353</td>      <td>-0.059752</td>      <td>0</td>      <td>1.160686</td>    </tr>    <tr>      <th>3</th>      <td>-0.966272</td>      <td>-0.185226</td>      <td>1.792993</td>      <td>-0.863291</td>      <td>-0.010309</td>      <td>1.247203</td>      <td>0.237609</td>      <td>0.377436</td>      <td>-1.387024</td>      <td>-0.054952</td>      <td>...</td>      <td>-0.108300</td>      <td>0.005274</td>      <td>-0.190321</td>      <td>-1.175575</td>      <td>0.647376</td>      <td>-0.221929</td>      <td>0.062723</td>      <td>0.061458</td>      <td>0</td>      <td>0.140534</td>    </tr>    <tr>      <th>4</th>      <td>-1.158233</td>      <td>0.877737</td>      <td>1.548718</td>      <td>0.403034</td>      <td>-0.407193</td>      <td>0.095921</td>      <td>0.592941</td>      <td>-0.270533</td>      <td>0.817739</td>      <td>0.753074</td>      <td>...</td>      <td>-0.009431</td>      <td>0.798278</td>      <td>-0.137458</td>      <td>0.141267</td>      <td>-0.206010</td>      <td>0.502292</td>      <td>0.219422</td>      <td>0.215153</td>      <td>0</td>      <td>-0.073403</td>    </tr>  </tbody></table><p>5 rows × 30 columns</p></div><h2 id="先来进行下采样吧！"><a href="#先来进行下采样吧！" class="headerlink" title="先来进行下采样吧！"></a>先来进行下采样吧！</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">X = data.ix[:, data.columns != <span class="string">&#x27;Class&#x27;</span>] <span class="comment"># loc[标签] iloc[索引数字] 取值 ，ix[都可以]  </span></span><br><span class="line">y = data.ix[:, data.columns == <span class="string">&#x27;Class&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of data points in the minority class</span></span><br><span class="line">number_records_fraud = <span class="built_in">len</span>(data[data.Class == <span class="number">1</span>])  <span class="comment"># 计算异常样本的数目——采用bool索引的方式进行</span></span><br><span class="line">fraud_indices = np.array(data[data.Class == <span class="number">1</span>].index) <span class="comment"># 通过.index函数拿出来异常样本的索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Picking the indices of the normal classes</span></span><br><span class="line">normal_indices = data[data.Class == <span class="number">0</span>].index  <span class="comment"># 拿出来所有正常样本的index，为了下面的随机选择。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Out of the indices we picked, randomly select &quot;x&quot; number (number_records_fraud)</span></span><br><span class="line">random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = <span class="literal">False</span>)<span class="comment">#  np.random.choice(样本，选择数目)进行随机选择</span></span><br><span class="line">random_normal_indices = np.array(random_normal_indices)  <span class="comment"># 将拿出来的索引转化为np.array的类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Appending the 2 indices  ##合并两个样本的index进行合并</span></span><br><span class="line">under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Under sample dataset  ## 经过下采样以后拿到的数据</span></span><br><span class="line">under_sample_data = data.iloc[under_sample_indices,:]</span><br><span class="line"></span><br><span class="line">X_undersample = under_sample_data.ix[:, under_sample_data.columns != <span class="string">&#x27;Class&#x27;</span>]  <span class="comment"># 下采样的数据分成两部分</span></span><br><span class="line">y_undersample = under_sample_data.ix[:, under_sample_data.columns == <span class="string">&#x27;Class&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Showing ratio</span></span><br><span class="line">print(<span class="string">&quot;Percentage of normal transactions: &quot;</span>, <span class="built_in">len</span>(under_sample_data[under_sample_data.Class == <span class="number">0</span>])/<span class="built_in">len</span>(under_sample_data))</span><br><span class="line">print(<span class="string">&quot;Percentage of fraud transactions: &quot;</span>, <span class="built_in">len</span>(under_sample_data[under_sample_data.Class == <span class="number">1</span>])/<span class="built_in">len</span>(under_sample_data))</span><br><span class="line">print(<span class="string">&quot;Total number of transactions in resampled data: &quot;</span>, <span class="built_in">len</span>(under_sample_data))</span><br></pre></td></tr></table></figure><pre><code>Percentage of normal transactions:  0.5Percentage of fraud transactions:  0.5Total number of transactions in resampled data:  984</code></pre><h3 id="下采样的数据少了，会出现什么问题呢？-后面说。"><a href="#下采样的数据少了，会出现什么问题呢？-后面说。" class="headerlink" title="下采样的数据少了，会出现什么问题呢？ 后面说。"></a>下采样的数据少了，会出现什么问题呢？ 后面说。</h3><h2 id="先来进行交叉验证的数据的切分。——交叉验证，说到底是为了选参！"><a href="#先来进行交叉验证的数据的切分。——交叉验证，说到底是为了选参！" class="headerlink" title="先来进行交叉验证的数据的切分。——交叉验证，说到底是为了选参！"></a>先来进行交叉验证的数据的切分。——交叉验证，说到底是为了选参！</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split  <span class="comment"># sklearn有交叉验证的工具，能镜像数据的划分。</span></span><br><span class="line"><span class="comment"># from sklearn.model_selection import train_test_split</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Whole dataset  #【1】对原始的数据进行切分——（为了使用它的测试集进行测试）</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = <span class="number">0.3</span>, random_state = <span class="number">0</span>) <span class="comment">## 注意顺序！ 数据洗牌</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Number transactions train dataset: &quot;</span>, <span class="built_in">len</span>(X_train))</span><br><span class="line">print(<span class="string">&quot;Number transactions test dataset: &quot;</span>, <span class="built_in">len</span>(X_test))</span><br><span class="line">print(<span class="string">&quot;Total number of transactions: &quot;</span>, <span class="built_in">len</span>(X_train)+<span class="built_in">len</span>(X_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Undersampled dataset  # 【2】 对下采样的数据进行切分。（下采样的测试集小，不具备原始数据的分布规则。）</span></span><br><span class="line">X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample</span><br><span class="line">                                                                                                   ,y_undersample</span><br><span class="line">                                                                                                   ,test_size = <span class="number">0.3</span></span><br><span class="line">                                                                                                   ,random_state = <span class="number">0</span>)</span><br><span class="line">print(<span class="string">&quot;&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;Number transactions train dataset: &quot;</span>, <span class="built_in">len</span>(X_train_undersample))</span><br><span class="line">print(<span class="string">&quot;Number transactions test dataset: &quot;</span>, <span class="built_in">len</span>(X_test_undersample))</span><br><span class="line">print(<span class="string">&quot;Total number of transactions: &quot;</span>, <span class="built_in">len</span>(X_train_undersample)+<span class="built_in">len</span>(X_test_undersample))</span><br></pre></td></tr></table></figure><pre><code>C:\Anaconda3\lib\site-packages\sklearn\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.  &quot;This module will be removed in 0.20.&quot;, DeprecationWarning)Number transactions train dataset:  199364Number transactions test dataset:  85443Total number of transactions:  284807Number transactions train dataset:  688Number transactions test dataset:  296Total number of transactions:  984</code></pre><h2 id="现在数据切分完了，已经有数据了，可以进行建模了！——逻辑回归"><a href="#现在数据切分完了，已经有数据了，可以进行建模了！——逻辑回归" class="headerlink" title="现在数据切分完了，已经有数据了，可以进行建模了！——逻辑回归"></a>现在数据切分完了，已经有数据了，可以进行建模了！——逻辑回归</h2><h4 id="模型可以容易建立（如用sklearn），但是模型评估标准咋样呢？——精度骗人"><a href="#模型可以容易建立（如用sklearn），但是模型评估标准咋样呢？——精度骗人" class="headerlink" title="模型可以容易建立（如用sklearn），但是模型评估标准咋样呢？——精度骗人!"></a>模型可以容易建立（如用sklearn），但是模型评估标准咋样呢？——精度骗人!</h4><ul><li>样本数目不均衡时，类偏移现象。100个人，99个正常(0)，1个得癌症(1)。那如果我的模型是y = 0,我的准确率是 99 %,但是检测不出来一个患有癌症的人。——所以希望我们的模型查的全一点。</li></ul><h3 id="查准率与查全率（召回率）"><a href="#查准率与查全率（召回率）" class="headerlink" title="查准率与查全率（召回率）"></a>查准率与查全率（召回率）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Recall = TP/(TP+FN)  # 我判断得癌症的人/实际得癌症的人</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> KFold, cross_val_score   <span class="comment"># KFold 做几倍的交叉验证， cross_val_score交叉验证的结果</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,recall_score,classification_report </span><br></pre></td></tr></table></figure><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><ul><li>正则化惩罚项——解决<strong>高偏差</strong>（<strong>欠拟合</strong>，数据误差大）&amp; <strong>高方差</strong>（<strong>过拟合</strong>，泛化能力差）<ul><li>L1 惩罚项</li><li>L2 惩罚项</li></ul></li></ul><ul><li><p>直接使用sklearn的逻辑回归库进行拟合 </p><ul><li>先实例化一个逻辑回归对象（传入正则化参数C，惩罚方式即可）</li><li>然后进行fit拟合</li><li>sklearn会返回预测结果</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printing_Kfold_scores</span>(<span class="params">x_train_data,y_train_data</span>):</span></span><br><span class="line">    fold = KFold(<span class="number">5</span>,shuffle=<span class="literal">False</span>)   <span class="comment"># 对（训练的）测试集的五倍交叉验证。</span></span><br><span class="line">                                                      <span class="comment">### 返回值是一个列表是[[train1,test1],[train2,test2],...]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Different C parameters   # C越大惩罚力度越大，即heta的权重就越小。可以用交叉验证来检测到底等于多少好。</span></span><br><span class="line">    c_param_range = [<span class="number">0.01</span>,<span class="number">0.1</span>,<span class="number">1</span>,<span class="number">10</span>,<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">    results_table = pd.DataFrame(index = <span class="built_in">range</span>(<span class="built_in">len</span>(c_param_range),<span class="number">2</span>), columns = [<span class="string">&#x27;C_parameter&#x27;</span>,<span class="string">&#x27;Mean recall score&#x27;</span>])</span><br><span class="line">    results_table[<span class="string">&#x27;C_parameter&#x27;</span>] = c_param_range</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]  </span></span><br><span class="line">    <span class="comment">## k-fold会分成两个索引的列表：train_indices = indices[0], test_indices = indices[1]  </span></span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">&quot;&quot;&quot;来看哪个C好&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> c_param <span class="keyword">in</span> c_param_range:</span><br><span class="line">        print(<span class="string">&#x27;-------------------------------------------&#x27;</span>)</span><br><span class="line">        print(<span class="string">&#x27;C parameter: &#x27;</span>, c_param)</span><br><span class="line">        print(<span class="string">&#x27;-------------------------------------------&#x27;</span>)</span><br><span class="line">        print(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        recall_accs = []</span><br><span class="line">        <span class="string">&quot;&quot;&quot;来进行交叉验证，1 3 训练 2验证，1 2 训练，3验证...&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> iteration, indices <span class="keyword">in</span> <span class="built_in">enumerate</span>(fold,start=<span class="number">1</span>):  <span class="comment">## 一般情况下，如果要对一个列表或者数组既要遍历索引又要遍历元素时，可以用enumerate</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Call the logistic regression model with a certain C parameter</span></span><br><span class="line">            lr = LogisticRegression(C = c_param, penalty = <span class="string">&#x27;l1&#x27;</span>)  <span class="comment">## C正则化，惩罚方式 L1惩罚。</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Use the training data to fit the model. In this case, we use the portion of the fold to train the model</span></span><br><span class="line">            <span class="comment"># with indices[0]. We then predict on the portion assigned as the &#x27;test cross validation&#x27; with indices[1]</span></span><br><span class="line">            <span class="comment"># 进行数据的拟合</span></span><br><span class="line">            lr.fit(x_train_data.iloc[indices[<span class="number">0</span>],:],y_train_data.iloc[indices[<span class="number">0</span>],:].values.ravel())</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Predict values using the test indices in the training data</span></span><br><span class="line">            <span class="comment">## 比如在C = 0.01情况下，效果咋样</span></span><br><span class="line">            y_pred_undersample = lr.predict(x_train_data.iloc[indices[<span class="number">1</span>],:].values)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Calculate the recall score and append it to a list for recall scores representing the current c_parameter</span></span><br><span class="line">            recall_acc = recall_score(y_train_data.iloc[indices[<span class="number">1</span>],:].values,y_pred_undersample)  <span class="comment">## 召回率库自己生成 recall_score(实际值，预测值)</span></span><br><span class="line">            recall_accs.append(recall_acc)</span><br><span class="line">            print(<span class="string">&#x27;Iteration &#x27;</span>, iteration,<span class="string">&#x27;: recall score = &#x27;</span>, recall_acc)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The mean value of those recall scores is the metric we want to save and get hold of.</span></span><br><span class="line">        results_table.ix[j,<span class="string">&#x27;Mean recall score&#x27;</span>] = np.mean(recall_accs)</span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">        print(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        print(<span class="string">&#x27;Mean recall score &#x27;</span>, np.mean(recall_accs))</span><br><span class="line">        print(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    best_c = results_table.loc[results_table[<span class="string">&#x27;Mean recall score&#x27;</span>].idxmax()][<span class="string">&#x27;C_parameter&#x27;</span>]  <span class="comment"># 定义能取到最大值得索引位置，</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Finally, we can check which C parameter is the best amongst the chosen.</span></span><br><span class="line">    print(<span class="string">&#x27;*********************************************************************************&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;Best model to choose from cross validation is with C parameter = &#x27;</span>, best_c)</span><br><span class="line">    print(<span class="string">&#x27;*********************************************************************************&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> best_c</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample)</span><br></pre></td></tr></table></figure><pre><code>-------------------------------------------C parameter:  0.01-------------------------------------------Iteration  1 : recall score =  0.958904109589Iteration  2 : recall score =  0.917808219178Iteration  3 : recall score =  1.0Iteration  4 : recall score =  0.972972972973Iteration  5 : recall score =  0.954545454545Mean recall score  0.960846151257-------------------------------------------C parameter:  0.1-------------------------------------------Iteration  1 : recall score =  0.835616438356Iteration  2 : recall score =  0.86301369863Iteration  3 : recall score =  0.915254237288Iteration  4 : recall score =  0.932432432432Iteration  5 : recall score =  0.878787878788Mean recall score  0.885020937099-------------------------------------------C parameter:  1-------------------------------------------Iteration  1 : recall score =  0.835616438356Iteration  2 : recall score =  0.86301369863Iteration  3 : recall score =  0.966101694915Iteration  4 : recall score =  0.945945945946Iteration  5 : recall score =  0.893939393939Mean recall score  0.900923434357-------------------------------------------C parameter:  10-------------------------------------------Iteration  1 : recall score =  0.849315068493Iteration  2 : recall score =  0.86301369863Iteration  3 : recall score =  0.966101694915Iteration  4 : recall score =  0.959459459459Iteration  5 : recall score =  0.893939393939Mean recall score  0.906365863087-------------------------------------------C parameter:  100-------------------------------------------Iteration  1 : recall score =  0.86301369863Iteration  2 : recall score =  0.86301369863Iteration  3 : recall score =  0.966101694915Iteration  4 : recall score =  0.959459459459Iteration  5 : recall score =  0.893939393939Mean recall score  0.909105589115*********************************************************************************Best model to choose from cross validation is with C parameter =  0.01*********************************************************************************</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_confusion_matrix</span>(<span class="params">cm, classes,</span></span></span><br><span class="line"><span class="function"><span class="params">                          title=<span class="string">&#x27;Confusion matrix&#x27;</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                          cmap=plt.cm.Blues</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function prints and plots the confusion matrix.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    plt.imshow(cm, interpolation=<span class="string">&#x27;nearest&#x27;</span>, cmap=cmap)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    tick_marks = np.arange(<span class="built_in">len</span>(classes))</span><br><span class="line">    plt.xticks(tick_marks, classes, rotation=<span class="number">0</span>)</span><br><span class="line">    plt.yticks(tick_marks, classes)</span><br><span class="line"></span><br><span class="line">    thresh = cm.<span class="built_in">max</span>() / <span class="number">2.</span></span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> itertools.product(<span class="built_in">range</span>(cm.shape[<span class="number">0</span>]), <span class="built_in">range</span>(cm.shape[<span class="number">1</span>])):</span><br><span class="line">        plt.text(j, i, cm[i, j],</span><br><span class="line">                 horizontalalignment=<span class="string">&quot;center&quot;</span>,</span><br><span class="line">                 color=<span class="string">&quot;white&quot;</span> <span class="keyword">if</span> cm[i, j] &gt; thresh <span class="keyword">else</span> <span class="string">&quot;black&quot;</span>)</span><br><span class="line"></span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;True label&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Predicted label&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line">lr = LogisticRegression(C = best_c, penalty = <span class="string">&#x27;l1&#x27;</span>)</span><br><span class="line">lr.fit(X_train_undersample,y_train_undersample.values.ravel())</span><br><span class="line">y_pred_undersample = lr.predict(X_test_undersample.values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute confusion matrix</span></span><br><span class="line">cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)</span><br><span class="line">np.set_printoptions(precision=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Recall metric in the testing dataset: &quot;</span>, cnf_matrix[<span class="number">1</span>,<span class="number">1</span>]/(cnf_matrix[<span class="number">1</span>,<span class="number">0</span>]+cnf_matrix[<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot non-normalized confusion matrix</span></span><br><span class="line">class_names = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">plt.figure()</span><br><span class="line">plot_confusion_matrix(cnf_matrix</span><br><span class="line">                      , classes=class_names</span><br><span class="line">                      , title=<span class="string">&#x27;Confusion matrix&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Recall metric in the testing dataset:  0.931972789116</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/22_1.png" alt=""></p><h3 id="从下采样的测试集，可以看到召回率约为-137-10-137≈90"><a href="#从下采样的测试集，可以看到召回率约为-137-10-137≈90" class="headerlink" title="从下采样的测试集，可以看到召回率约为(137+10)/137≈90%"></a>从下采样的测试集，可以看到召回率约为(137+10)/137≈90%</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression(C = best_c, penalty = <span class="string">&#x27;l1&#x27;</span>)</span><br><span class="line">lr.fit(X_train_undersample,y_train_undersample.values.ravel())</span><br><span class="line">y_pred = lr.predict(X_test.values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute confusion matrix</span></span><br><span class="line">cnf_matrix = confusion_matrix(y_test,y_pred)</span><br><span class="line">np.set_printoptions(precision=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Recall metric in the testing dataset: &quot;</span>, cnf_matrix[<span class="number">1</span>,<span class="number">1</span>]/(cnf_matrix[<span class="number">1</span>,<span class="number">0</span>]+cnf_matrix[<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot non-normalized confusion matrix</span></span><br><span class="line">class_names = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">plt.figure()</span><br><span class="line">plot_confusion_matrix(cnf_matrix</span><br><span class="line">                      , classes=class_names</span><br><span class="line">                      , title=<span class="string">&#x27;Confusion matrix&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Recall metric in the testing dataset:  0.918367346939</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/24_1.png" alt=""></p><h3 id="可以看到下采样的预测应用到整体样本的测试集，虽然召回率约为90-，但有8581个误杀值，不是我们所希望的。"><a href="#可以看到下采样的预测应用到整体样本的测试集，虽然召回率约为90-，但有8581个误杀值，不是我们所希望的。" class="headerlink" title="可以看到下采样的预测应用到整体样本的测试集，虽然召回率约为90%，但有8581个误杀值，不是我们所希望的。"></a>可以看到下采样的预测应用到整体样本的测试集，虽然召回率约为90%，但有8581个误杀值，不是我们所希望的。</h3><h2 id="对于原始数据集进行验证。会得到什么结果呢？——不进行上-下-采样"><a href="#对于原始数据集进行验证。会得到什么结果呢？——不进行上-下-采样" class="headerlink" title="对于原始数据集进行验证。会得到什么结果呢？——不进行上(下)采样"></a>对于原始数据集进行验证。会得到什么结果呢？——不进行上(下)采样</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best_c = printing_Kfold_scores(X_train,y_train)</span><br></pre></td></tr></table></figure><pre><code>-------------------------------------------C parameter:  0.01-------------------------------------------Iteration  1 : recall score =  0.492537313433Iteration  2 : recall score =  0.602739726027Iteration  3 : recall score =  0.683333333333Iteration  4 : recall score =  0.569230769231Iteration  5 : recall score =  0.45Mean recall score  0.559568228405-------------------------------------------C parameter:  0.1-------------------------------------------Iteration  1 : recall score =  0.567164179104Iteration  2 : recall score =  0.616438356164Iteration  3 : recall score =  0.683333333333Iteration  4 : recall score =  0.584615384615Iteration  5 : recall score =  0.525Mean recall score  0.595310250644-------------------------------------------C parameter:  1-------------------------------------------Iteration  1 : recall score =  0.55223880597Iteration  2 : recall score =  0.616438356164Iteration  3 : recall score =  0.716666666667Iteration  4 : recall score =  0.615384615385Iteration  5 : recall score =  0.5625Mean recall score  0.612645688837-------------------------------------------C parameter:  10-------------------------------------------Iteration  1 : recall score =  0.55223880597Iteration  2 : recall score =  0.616438356164Iteration  3 : recall score =  0.733333333333Iteration  4 : recall score =  0.615384615385Iteration  5 : recall score =  0.575Mean recall score  0.61847902217-------------------------------------------C parameter:  100-------------------------------------------Iteration  1 : recall score =  0.55223880597Iteration  2 : recall score =  0.616438356164Iteration  3 : recall score =  0.733333333333Iteration  4 : recall score =  0.615384615385Iteration  5 : recall score =  0.575Mean recall score  0.61847902217*********************************************************************************Best model to choose from cross validation is with C parameter =  10.0*********************************************************************************</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression(C = best_c, penalty = <span class="string">&#x27;l1&#x27;</span>)</span><br><span class="line">lr.fit(X_train,y_train.values.ravel())</span><br><span class="line">y_pred_undersample = lr.predict(X_test.values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute confusion matrix</span></span><br><span class="line">cnf_matrix = confusion_matrix(y_test,y_pred_undersample)</span><br><span class="line">np.set_printoptions(precision=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Recall metric in the testing dataset: &quot;</span>, cnf_matrix[<span class="number">1</span>,<span class="number">1</span>]/(cnf_matrix[<span class="number">1</span>,<span class="number">0</span>]+cnf_matrix[<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot non-normalized confusion matrix</span></span><br><span class="line">class_names = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">plt.figure()</span><br><span class="line">plot_confusion_matrix(cnf_matrix</span><br><span class="line">                      , classes=class_names</span><br><span class="line">                      , title=<span class="string">&#x27;Confusion matrix&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Recall metric in the testing dataset:  0.619047619048</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/28_1.png" alt=""></p><h3 id="可以看到拿原始数据进行预测得到的召回率是比较低的。"><a href="#可以看到拿原始数据进行预测得到的召回率是比较低的。" class="headerlink" title="可以看到拿原始数据进行预测得到的召回率是比较低的。"></a>可以看到拿原始数据进行预测得到的召回率是比较低的。</h3><h2 id="接下来看看不同的阈值对模型的影响——划分正负样本的标准不是默认的0-5了"><a href="#接下来看看不同的阈值对模型的影响——划分正负样本的标准不是默认的0-5了" class="headerlink" title="接下来看看不同的阈值对模型的影响——划分正负样本的标准不是默认的0.5了"></a>接下来看看不同的阈值对模型的影响——划分正负样本的标准不是默认的0.5了</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression(C = <span class="number">0.01</span>, penalty = <span class="string">&#x27;l1&#x27;</span>)</span><br><span class="line">lr.fit(X_train_undersample,y_train_undersample.values.ravel())</span><br><span class="line">y_pred_undersample_proba = lr.predict_proba(X_test_undersample.values) <span class="comment">## 之前拿的是predict()现在是另外一个函数了</span></span><br><span class="line">                                                                       <span class="comment"># 之前预测是类别的值，现在预测是概率值</span></span><br><span class="line"></span><br><span class="line">thresholds = [<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">j = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> thresholds:</span><br><span class="line">    y_test_predictions_high_recall = y_pred_undersample_proba[:,<span class="number">1</span>] &gt; i  <span class="comment">## 这是关键，拿到概率后直接拿它与设定阈值比较</span></span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">3</span>,j)</span><br><span class="line">    j += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute confusion matrix</span></span><br><span class="line">    cnf_matrix = confusion_matrix(y_test_undersample,y_test_predictions_high_recall)</span><br><span class="line">    np.set_printoptions(precision=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;Recall metric in the testing dataset: &quot;</span>, cnf_matrix[<span class="number">1</span>,<span class="number">1</span>]/(cnf_matrix[<span class="number">1</span>,<span class="number">0</span>]+cnf_matrix[<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot non-normalized confusion matrix</span></span><br><span class="line">    class_names = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">    plot_confusion_matrix(cnf_matrix</span><br><span class="line">                          , classes=class_names</span><br><span class="line">                          , title=<span class="string">&#x27;Threshold &gt;= %s&#x27;</span>%i) </span><br></pre></td></tr></table></figure><pre><code>Recall metric in the testing dataset:  1.0Recall metric in the testing dataset:  1.0Recall metric in the testing dataset:  1.0Recall metric in the testing dataset:  0.986394557823Recall metric in the testing dataset:  0.931972789116Recall metric in the testing dataset:  0.884353741497Recall metric in the testing dataset:  0.836734693878Recall metric in the testing dataset:  0.748299319728Recall metric in the testing dataset:  0.571428571429</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/31_1.png" alt=""></p><h3 id="可以看到随着阈值的上升，误杀值减小，但是召回率也是减小了。——实际建模时，应该根据实际情况来选择阈值！"><a href="#可以看到随着阈值的上升，误杀值减小，但是召回率也是减小了。——实际建模时，应该根据实际情况来选择阈值！" class="headerlink" title="可以看到随着阈值的上升，误杀值减小，但是召回率也是减小了。——实际建模时，应该根据实际情况来选择阈值！"></a>可以看到随着阈值的上升，误杀值减小，但是召回率也是减小了。——实际建模时，应该根据实际情况来选择阈值！</h3><h2 id="看完下采样的分析，我们来看看上采样的结果吧！"><a href="#看完下采样的分析，我们来看看上采样的结果吧！" class="headerlink" title="看完下采样的分析，我们来看看上采样的结果吧！"></a>看完下采样的分析，我们来看看上采样的结果吧！</h2><ul><li>上采样需要额外的数据，这里我们采用 <strong><code>SMOTE</code></strong> 方法来生成少数样本的数据。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE   <span class="comment"># 需要安装 imblearn 库</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">credit_cards=pd.read_csv(<span class="string">&#x27;creditcard.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">columns=credit_cards.columns</span><br><span class="line"><span class="comment"># The labels are in the last column (&#x27;Class&#x27;). Simply remove it to obtain features columns</span></span><br><span class="line">features_columns=columns.delete(<span class="built_in">len</span>(columns)-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">features=credit_cards[features_columns]</span><br><span class="line">labels=credit_cards[<span class="string">&#x27;Class&#x27;</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">features_train, features_test, labels_train, labels_test = train_test_split(features, </span><br><span class="line">                                                                            labels, </span><br><span class="line">                                                                            test_size=<span class="number">0.2</span>, </span><br><span class="line">                                                                            random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oversampler=SMOTE(random_state=<span class="number">0</span>)  <span class="comment"># 每次生辰的随机数一样。</span></span><br><span class="line">os_features,os_labels=oversampler.fit_sample(features_train,labels_train)  <span class="comment"># 注意传入的是训练的x和y的值。没有测试部分的</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(os_labels[os_labels==<span class="number">1</span>])  <span class="comment"># 自动会进行平衡。1:1平衡</span></span><br></pre></td></tr></table></figure><pre><code>227454</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">os_features = pd.DataFrame(os_features)</span><br><span class="line">os_labels = pd.DataFrame(os_labels)</span><br><span class="line">best_c = printing_Kfold_scores(os_features,os_labels)</span><br></pre></td></tr></table></figure><pre><code>-------------------------------------------C parameter:  0.01-------------------------------------------Iteration  1 : recall score =  0.890322580645Iteration  2 : recall score =  0.894736842105Iteration  3 : recall score =  0.968861347792Iteration  4 : recall score =  0.957595541926Iteration  5 : recall score =  0.958430881173Mean recall score  0.933989438728-------------------------------------------C parameter:  0.1-------------------------------------------Iteration  1 : recall score =  0.890322580645Iteration  2 : recall score =  0.894736842105Iteration  3 : recall score =  0.970410534469Iteration  4 : recall score =  0.959980655302Iteration  5 : recall score =  0.960178498807Mean recall score  0.935125822266-------------------------------------------C parameter:  1-------------------------------------------Iteration  1 : recall score =  0.890322580645Iteration  2 : recall score =  0.894736842105Iteration  3 : recall score =  0.970454796946Iteration  4 : recall score =  0.96014552489Iteration  5 : recall score =  0.960596168431Mean recall score  0.935251182603-------------------------------------------C parameter:  10-------------------------------------------Iteration  1 : recall score =  0.890322580645Iteration  2 : recall score =  0.894736842105Iteration  3 : recall score =  0.97065397809Iteration  4 : recall score =  0.960343368396Iteration  5 : recall score =  0.960530220596Mean recall score  0.935317397966-------------------------------------------C parameter:  100-------------------------------------------Iteration  1 : recall score =  0.890322580645Iteration  2 : recall score =  0.894736842105Iteration  3 : recall score =  0.970543321899Iteration  4 : recall score =  0.960211472725Iteration  5 : recall score =  0.960903924995Mean recall score  0.935343628474*********************************************************************************Best model to choose from cross validation is with C parameter =  100.0*********************************************************************************</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression(C = best_c, penalty = <span class="string">&#x27;l1&#x27;</span>)</span><br><span class="line">lr.fit(os_features,os_labels.values.ravel())</span><br><span class="line">y_pred = lr.predict(features_test.values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute confusion matrix</span></span><br><span class="line">cnf_matrix = confusion_matrix(labels_test,y_pred)</span><br><span class="line">np.set_printoptions(precision=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Recall metric in the testing dataset: &quot;</span>, cnf_matrix[<span class="number">1</span>,<span class="number">1</span>]/(cnf_matrix[<span class="number">1</span>,<span class="number">0</span>]+cnf_matrix[<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot non-normalized confusion matrix</span></span><br><span class="line">class_names = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">plt.figure()</span><br><span class="line">plot_confusion_matrix(cnf_matrix</span><br><span class="line">                      , classes=class_names</span><br><span class="line">                      , title=<span class="string">&#x27;Confusion matrix&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Recall metric in the testing dataset:  0.90099009901</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/40_1png" alt=""></p><h3 id="召回率还可以，误杀率降下来——模型的精度变高。-56344-91-569344-91-517-10"><a href="#召回率还可以，误杀率降下来——模型的精度变高。-56344-91-569344-91-517-10" class="headerlink" title="召回率还可以，误杀率降下来——模型的精度变高。(56344+91)/(569344+91+517+10)"></a>召回率还可以，误杀率降下来——模型的精度变高。(56344+91)/(569344+91+517+10)</h3><h3 id="总之，能用数据生成方式尽量用，上采样的结果更好！"><a href="#总之，能用数据生成方式尽量用，上采样的结果更好！" class="headerlink" title="总之，能用数据生成方式尽量用，上采样的结果更好！"></a>总之，能用数据生成方式尽量用，上采样的结果更好！</h3><h1 id="案例流程总结："><a href="#案例流程总结：" class="headerlink" title="案例流程总结："></a>案例流程总结：</h1><ul><li><p><strong>1. 数据的观察。</strong></p><ul><li>1.1 数据浮动情况：<ul><li>归一化</li><li>标准化<ul><li>1.2 数据分布均匀情况：</li></ul></li><li>下采样</li><li>上采样<ul><li>1.3 此处的案例的特征是处理过的，纯净的特征，不需要额外处理。很多时候需要特种工程处理特征数据—后面讲</li></ul></li></ul></li></ul></li><li><p><strong>2. 对于不同的模型有不同的参数，需要自己进行选择。</strong></p><ul><li>比如逻辑回归的正则化参数C的选择(解决过拟合【高方差】和欠拟合【高偏差】)。<ul><li>采用交叉验证的方式来确定参数C（交叉验证多次来确定C的合适大小）</li></ul></li></ul></li><li><p><strong>3. 混淆矩阵，召回率——解决类偏移问题</strong></p><ul><li>预测模型为 y=1 ,准确率达到90%这类问题。</li></ul></li><li><p><strong>4. 不同的阈值（评判分类的不概率标准）</strong></p><ul><li>对结果有一定的影响。如此题。阈值越大，误杀率越高，召回率降低。——实际建模的时候，根据需要来确定。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;信用卡诈骗预测——二分类的问题&quot;&gt;&lt;a href=&quot;#信用卡诈骗预测——二分类的问题&quot; class=&quot;headerlink&quot; title=&quot;信用卡诈骗预测——二分类的问题&quot;&gt;&lt;/a&gt;信用卡诈骗预测——二分类的问题&lt;/h1&gt;&lt;figure class=&quot;highli</summary>
      
    
    
    
    <category term="机器学习基础实战" scheme="https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="机器学习基础实战" scheme="https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归基础</title>
    <link href="https://xxren8218.github.io/20210418/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80.html"/>
    <id>https://xxren8218.github.io/20210418/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80.html</id>
    <published>2021-04-17T17:13:29.000Z</published>
    <updated>2021-04-29T15:50:02.753Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><h2 id="The-data"><a href="#The-data" class="headerlink" title="The data"></a>The data</h2><p>我们将建立一个逻辑回归模型来预测一个学生是否被大学录取。假设你是一个大学系的管理员，你想根据两次考试的结果来决定每个申请人的录取机会。你有以前的申请人的历史数据，你可以用它作为逻辑回归的训练集。对于每一个培训例子，你有两个考试的申请人的分数和录取决定。为了做到这一点，我们将建立一个分类模型，根据考试成绩估计入学概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 三大件 # 可以直接在你的python console里面生成图像。不需要plt.show()就可进行展示</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline  </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">path = <span class="string">&#x27;data&#x27;</span> + os.sep + <span class="string">&#x27;LogiReg_data.txt&#x27;</span>  <span class="comment"># 为了让代码在不同的平台上都能运行，路径应该写&#x27;\&#x27;还是&#x27;/&#x27;无所谓。</span></span><br><span class="line">pdData = pd.read_csv(path, header=<span class="literal">None</span>, names=[<span class="string">&#x27;Exam 1&#x27;</span>, <span class="string">&#x27;Exam 2&#x27;</span>, <span class="string">&#x27;Admitted&#x27;</span>])  <span class="comment"># header = None自己制定列名</span></span><br><span class="line">pdData.head()</span><br></pre></td></tr></table></figure><div style="overflow: scroll;"><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Exam 1</th>      <th>Exam 2</th>      <th>Admitted</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>34.623660</td>      <td>78.024693</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>30.286711</td>      <td>43.894998</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>35.847409</td>      <td>72.902198</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>60.182599</td>      <td>86.308552</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>79.032736</td>      <td>75.344376</td>      <td>1</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pdData.shape <span class="comment"># 看数据的维度。</span></span><br></pre></td></tr></table></figure><pre><code>(100, 3)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">positive = pdData[pdData[<span class="string">&#x27;Admitted&#x27;</span>] == <span class="number">1</span>] <span class="comment"># returns the subset of rows such Admitted = 1, i.e. the set of *positive* examples</span></span><br><span class="line">negative = pdData[pdData[<span class="string">&#x27;Admitted&#x27;</span>] == <span class="number">0</span>] <span class="comment"># returns the subset of rows such Admitted = 0, i.e. the set of *negative* examples</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">ax.scatter(positive[<span class="string">&#x27;Exam 1&#x27;</span>], positive[<span class="string">&#x27;Exam 2&#x27;</span>], s=<span class="number">30</span>, c=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Admitted&#x27;</span>)</span><br><span class="line">ax.scatter(negative[<span class="string">&#x27;Exam 1&#x27;</span>], negative[<span class="string">&#x27;Exam 2&#x27;</span>], s=<span class="number">30</span>, c=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>, label=<span class="string">&#x27;Not Admitted&#x27;</span>)</span><br><span class="line">ax.legend()  <span class="comment">#  legend（）有一个loc参数，用于控制图例的位置。 比如 plot.legend(loc=2) , 这个位置就是4象项中的第二象项，也就是左上角。 loc可以为1,2,3,4 这四个数字。</span></span><br><span class="line">            <span class="comment"># 如果把那句legend() 的语句去掉，那么图形上的图例也就会消失了。</span></span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Exam 1 Score&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Exam 2 Score&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0,0.5,&#39;Exam 2 Score&#39;)</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/61.png" alt=""></p><h2 id="The-logistic-regression"><a href="#The-logistic-regression" class="headerlink" title="The logistic regression"></a>The logistic regression</h2><p>目标：建立分类器—即决策边界（求解出三个参数 $\theta_0         \theta_1         \theta_2 $）</p><p>设定<strong>阈值</strong>，根据阈值判断录取结果—就是分类的概率判断，一般为 0.5</p><h3 id="要完成的模块"><a href="#要完成的模块" class="headerlink" title="要完成的模块"></a>要完成的模块</h3><ul><li><p><code>sigmoid</code> : 映射到概率的函数</p></li><li><p><code>model</code> : 返回预测结果值</p></li><li><p><code>cost</code> : 根据参数计算损失</p></li><li><p><code>gradient</code> : 计算每个参数的梯度方向</p></li><li><p><code>descent</code> : 进行参数更新</p></li><li><p><code>accuracy</code>: 计算精度</p></li></ul><h3 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a><code>sigmoid</code> 函数</h3><script type="math/tex; mode=display">g(z) = \frac{1}{1+e^{-z}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))  <span class="comment"># np.exp(-z)表示e的多少次幂</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums = np.arange(-<span class="number">10</span>, <span class="number">10</span>, step=<span class="number">1</span>) <span class="comment">#creates a vector containing 20 equally spaced values from -10 to 10</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">ax.plot(nums, sigmoid(nums), <span class="string">&#x27;r&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x244554b2b70&gt;]</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/11_1.png" alt=""></p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><ul><li>$g:\mathbb{R} \to [0,1]$</li><li>$g(0)=0.5$</li><li>$g(- \infty)=0$</li><li>$g(+ \infty)=1$</li></ul><h3 id="model-完成预测函数-h-theta-x"><a href="#model-完成预测函数-h-theta-x" class="headerlink" title="model 完成预测函数 $h_\theta(x)$"></a><code>model</code> <strong>完成预测函数 $h_\theta(x)$</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span>(<span class="params">X, theta</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> sigmoid(np.dot(X, theta.T))  <span class="comment"># np.dot是矩阵的乘法,也可以用 @ </span></span><br><span class="line">                                        <span class="comment"># 求出的 model是 m行1列.m--样本数目：</span></span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{array}{ccc}\begin{pmatrix}\theta_{0} & \theta_{1} & \theta_{2}\end{pmatrix} & \times & \begin{pmatrix}1\\x_{1}\\x_{2}\end{pmatrix}\end{array}=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pdData.insert(<span class="number">0</span>, <span class="string">&#x27;Ones&#x27;</span>, <span class="number">1</span>) <span class="comment"># 插入零的一列，列指标为 Ones</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set X (training data) and y (target variable)</span></span><br><span class="line">orig_data = pdData.as_matrix() <span class="comment"># 习惯性的操作。很多时候取得的数据是DataFrame的形式(直接转换csv格式的数据以后)，这个时候要记得转换成数组</span></span><br><span class="line">cols = orig_data.shape[<span class="number">1</span>]  <span class="comment"># 看数据有几列。</span></span><br><span class="line">X = orig_data[:,<span class="number">0</span>:cols-<span class="number">1</span>]</span><br><span class="line">y = orig_data[:,cols-<span class="number">1</span>:cols]</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to numpy arrays and initalize the parameter array theta</span></span><br><span class="line"><span class="comment">#X = np.matrix(X.values)</span></span><br><span class="line"><span class="comment">#y = np.matrix(data.iloc[:,3:4].values) #np.array(y.values)</span></span><br><span class="line">theta = np.zeros([<span class="number">1</span>, <span class="number">3</span>])  <span class="comment"># 参数theta 一般先构造出来，用zero来占位，构造1行3列的数据。即三个theta参数 （1,3）[1,4]都可以。</span></span><br></pre></td></tr></table></figure><p><strong>来看看数据的样子吧</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X[:<span class="number">5</span>]  <span class="comment"># 前 5 行的数据</span></span><br></pre></td></tr></table></figure><pre><code>array([[ 1.        , 34.62365962, 78.02469282],       [ 1.        , 30.28671077, 43.89499752],       [ 1.        , 35.84740877, 72.90219803],       [ 1.        , 60.18259939, 86.3085521 ],       [ 1.        , 79.03273605, 75.34437644]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><pre><code>array([[0.],       [0.],       [0.],       [1.],       [1.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta</span><br></pre></td></tr></table></figure><pre><code>array([[0., 0., 0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape, y.shape, theta.shape</span><br></pre></td></tr></table></figure><pre><code>((100, 3), (100, 1), (1, 3))</code></pre><h3 id="损失函数（代价函数）"><a href="#损失函数（代价函数）" class="headerlink" title="损失函数（代价函数）"></a>损失函数（代价函数）</h3><p>将对数似然函数去负号</p><script type="math/tex; mode=display">D(h_\theta(x), y) = -y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x))</script><p>求平均损失</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum_{i=1}^{m} D(h_\theta(x_i), y_i)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">X, y, theta</span>):</span></span><br><span class="line">    left = np.multiply(-y, np.log(model(X, theta)))  <span class="comment"># np.multiply对数据完成的乘的操作</span></span><br><span class="line">    right = np.multiply(<span class="number">1</span> - y, np.log(<span class="number">1</span> - model(X, theta)))</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(left - right) / (<span class="built_in">len</span>(X))   <span class="comment"># np.sum完成对数据的加和</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cost(X, y, theta)</span><br></pre></td></tr></table></figure><pre><code>0.6931471805599453</code></pre><h3 id="计算梯度-—-最难的部分"><a href="#计算梯度-—-最难的部分" class="headerlink" title="计算梯度 — 最难的部分"></a>计算梯度 — 最难的部分</h3><script type="math/tex; mode=display">\frac{\partial J}{\partial \theta_j}=-\frac{1}{m}\sum_{i=1}^n (y_i - h_\theta (x_i))x_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">X, y, theta</span>):</span></span><br><span class="line">    grad = np.zeros(theta.shape)  <span class="comment"># 梯度计算需要考虑 theta 的个数（维度）</span></span><br><span class="line">    error = (model(X, theta)- y).ravel()  <span class="comment"># 把负号提取到里面了，revel()将数据降为1维！(1,m)</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(theta.ravel())):   <span class="comment"># theta降低为 1 维度，[1,2,3,4]这样,就可以求theta的个数了，按列进行遍历</span></span><br><span class="line">        term = np.multiply(error, X[:,j]) <span class="comment"># 矩阵的乘法，取第j列。  (1,m)@(m,1)</span></span><br><span class="line">        grad[<span class="number">0</span>, j] = np.<span class="built_in">sum</span>(term) / <span class="built_in">len</span>(X) <span class="comment"># 每一个梯度j算一个值。取[0, j]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure><h3 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h3><p>比较3种不同梯度下降方法—<strong>批量、随机、小批量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">STOP_ITER = <span class="number">0</span>  <span class="comment"># 根据迭代次数停止</span></span><br><span class="line">STOP_COST = <span class="number">1</span>  <span class="comment"># 根据损失值目标函数的变化停止</span></span><br><span class="line">STOP_GRAD = <span class="number">2</span>  <span class="comment"># 根据梯度的变化（很小）停止</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stopCriterion</span>(<span class="params"><span class="built_in">type</span>, value, threshold</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;设定三种不同的停止策略&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span> == STOP_ITER:        <span class="keyword">return</span> value &gt; threshold</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span> == STOP_COST:      <span class="keyword">return</span> <span class="built_in">abs</span>(value[-<span class="number">1</span>]-value[-<span class="number">2</span>]) &lt; threshold <span class="comment"># abs()返回绝对值</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span> == STOP_GRAD:      <span class="keyword">return</span> np.linalg.norm(value) &lt; threshold  <span class="comment"># np.linalg.norm默认是 2 范数--平方和开根号。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy.random</span><br><span class="line"><span class="comment"># 洗牌，将数据随机化，泛化能力变强</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffleData</span>(<span class="params">data</span>):</span></span><br><span class="line">    np.random.shuffle(data)  <span class="comment"># np.random.shuffle()将数据进行洗牌。</span></span><br><span class="line">    cols = data.shape[<span class="number">1</span>]</span><br><span class="line">    X = data[:, <span class="number">0</span>:cols-<span class="number">1</span>]</span><br><span class="line">    y = data[:, cols-<span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> X, y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">descent</span>(<span class="params">data, theta, batchSize, stopType, thresh, alpha</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;梯度下降求解&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 参数的初始化，第一次计算各个值。</span></span><br><span class="line">    init_time = time.time()</span><br><span class="line">    i = <span class="number">0</span> <span class="comment"># 迭代次数</span></span><br><span class="line">    k = <span class="number">0</span> <span class="comment"># batch</span></span><br><span class="line">    X, y = shuffleData(data)</span><br><span class="line">    grad = np.zeros(theta.shape) <span class="comment"># 计算的梯度</span></span><br><span class="line">    costs = [cost(X, y, theta)] <span class="comment"># 损失值</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        grad = gradient(X[k:k+batchSize], y[k:k+batchSize], theta)</span><br><span class="line">        k += batchSize <span class="comment"># 取batch个数据，每次取batchSize个数据进行计算。</span></span><br><span class="line">        <span class="keyword">if</span> k &gt;= n: </span><br><span class="line">            k = <span class="number">0</span> </span><br><span class="line">            X, y = shuffleData(data) <span class="comment"># 重新洗牌</span></span><br><span class="line">        theta = theta - alpha*grad <span class="comment"># 参数更新</span></span><br><span class="line">        costs.append(cost(X, y, theta)) <span class="comment"># 计算新的损失</span></span><br><span class="line">        i += <span class="number">1</span> </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> stopType == STOP_ITER:       value = i</span><br><span class="line">        <span class="keyword">elif</span> stopType == STOP_COST:     value = costs</span><br><span class="line">        <span class="keyword">elif</span> stopType == STOP_GRAD:     value = grad</span><br><span class="line">        <span class="keyword">if</span> stopCriterion(stopType, value, thresh): <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> theta, i-<span class="number">1</span>, costs, grad, time.time() - init_time</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runExpe</span>(<span class="params">data, theta, batchSize, stopType, thresh, alpha</span>):</span></span><br><span class="line">    <span class="comment">#import pdb; pdb.set_trace();</span></span><br><span class="line">    theta, <span class="built_in">iter</span>, costs, grad, dur = descent(data, theta, batchSize, stopType, thresh, alpha)</span><br><span class="line">    name = <span class="string">&quot;Original&quot;</span> <span class="keyword">if</span> (data[:,<span class="number">1</span>]&gt;<span class="number">2</span>).<span class="built_in">sum</span>() &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="string">&quot;Scaled&quot;</span>  <span class="comment"># 归一化时的区分</span></span><br><span class="line">    name += <span class="string">&quot; data - learning rate: &#123;&#125; - &quot;</span>.<span class="built_in">format</span>(alpha)</span><br><span class="line">    <span class="keyword">if</span> batchSize==n: strDescType = <span class="string">&quot;Gradient&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> batchSize==<span class="number">1</span>:  strDescType = <span class="string">&quot;Stochastic&quot;</span></span><br><span class="line">    <span class="keyword">else</span>: strDescType = <span class="string">&quot;Mini-batch (&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(batchSize)</span><br><span class="line">    name += strDescType + <span class="string">&quot; descent - Stop: &quot;</span></span><br><span class="line">    <span class="keyword">if</span> stopType == STOP_ITER: strStop = <span class="string">&quot;&#123;&#125; iterations&quot;</span>.<span class="built_in">format</span>(thresh)</span><br><span class="line">    <span class="keyword">elif</span> stopType == STOP_COST: strStop = <span class="string">&quot;costs change &lt; &#123;&#125;&quot;</span>.<span class="built_in">format</span>(thresh)</span><br><span class="line">    <span class="keyword">else</span>: strStop = <span class="string">&quot;gradient norm &lt; &#123;&#125;&quot;</span>.<span class="built_in">format</span>(thresh)</span><br><span class="line">    name += strStop</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;***&#123;&#125;\nTheta: &#123;&#125; - Iter: &#123;&#125; - Last cost: &#123;:03.2f&#125; - Duration: &#123;:03.2f&#125;s&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">        name, theta, <span class="built_in">iter</span>, costs[-<span class="number">1</span>], dur))</span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">    ax.plot(np.arange(<span class="built_in">len</span>(costs)), costs, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    ax.set_xlabel(<span class="string">&#x27;Iterations&#x27;</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">    ax.set_title(name.upper() + <span class="string">&#x27; - Error vs. Iteration&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure><h3 id="不同的停止策略"><a href="#不同的停止策略" class="headerlink" title="不同的停止策略"></a>不同的停止策略</h3><h4 id="设定迭代次数"><a href="#设定迭代次数" class="headerlink" title="设定迭代次数"></a>设定迭代次数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选择的梯度下降方法是基于所有样本的</span></span><br><span class="line">n=<span class="number">100</span></span><br><span class="line">runExpe(orig_data, theta, n, STOP_ITER, thresh=<span class="number">5000</span>, alpha=<span class="number">0.000001</span>)</span><br></pre></td></tr></table></figure><pre><code>***Original data - learning rate: 1e-06 - Gradient descent - Stop: 5000 iterationsTheta: [[-0.00027127  0.00705232  0.00376711]] - Iter: 5000 - Last cost: 0.63 - Duration: 0.82sarray([[-0.00027127,  0.00705232,  0.00376711]])</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/35_2.png" alt=""></p><h4 id="根据损失值停止"><a href="#根据损失值停止" class="headerlink" title="根据损失值停止"></a>根据损失值停止</h4><p>设定阈值 1E-6, 差不多需要110 000次迭代 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runExpe(orig_data, theta, n, STOP_COST, thresh=<span class="number">0.000001</span>, alpha=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><pre><code>***Original data - learning rate: 0.001 - Gradient descent - Stop: costs change &lt; 1e-06Theta: [[-5.13364014  0.04771429  0.04072397]] - Iter: 109901 - Last cost: 0.38 - Duration: 17.97sarray([[-5.13364014,  0.04771429,  0.04072397]])</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/38_2.png" alt=""></p><h4 id="根据梯度变化停止"><a href="#根据梯度变化停止" class="headerlink" title="根据梯度变化停止"></a>根据梯度变化停止</h4><p>设定阈值 0.05,差不多需要40 000次迭代</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runExpe(orig_data, theta, n, STOP_GRAD, thresh=<span class="number">0.05</span>, alpha=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><pre><code>***Original data - learning rate: 0.001 - Gradient descent - Stop: gradient norm &lt; 0.05Theta: [[-2.37033409  0.02721692  0.01899456]] - Iter: 40045 - Last cost: 0.49 - Duration: 6.87sarray([[-2.37033409,  0.02721692,  0.01899456]])</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/41_2.png" alt=""></p><h3 id="对比不同的梯度下降方法"><a href="#对比不同的梯度下降方法" class="headerlink" title="对比不同的梯度下降方法"></a>对比不同的梯度下降方法</h3><h4 id="Stochastic-descent"><a href="#Stochastic-descent" class="headerlink" title="Stochastic descent"></a>Stochastic descent</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runExpe(orig_data, theta, <span class="number">1</span>, STOP_ITER, thresh=<span class="number">5000</span>, alpha=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><pre><code>***Original data - learning rate: 0.001 - Stochastic descent - Stop: 5000 iterationsTheta: [[-0.39253059  0.04095984 -0.07371051]] - Iter: 5000 - Last cost: 1.84 - Duration: 0.27sarray([[-0.39253059,  0.04095984, -0.07371051]])</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/44_2.png" alt=""></p><p>有点爆炸。。。很不稳定,再来试试把学习率调小一些</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runExpe(orig_data, theta, <span class="number">1</span>, STOP_ITER, thresh=<span class="number">15000</span>, alpha=<span class="number">0.000002</span>)</span><br></pre></td></tr></table></figure><pre><code>***Original data - learning rate: 2e-06 - Stochastic descent - Stop: 15000 iterationsTheta: [[-0.00202238  0.00995606  0.00088035]] - Iter: 15000 - Last cost: 0.63 - Duration: 0.77sarray([[-0.00202238,  0.00995606,  0.00088035]])</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/46_2.png" alt=""></p><p>速度快，但稳定性差，需要很小的学习率</p><h4 id="Mini-batch-descent"><a href="#Mini-batch-descent" class="headerlink" title="Mini-batch descent"></a>Mini-batch descent</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runExpe(orig_data, theta, <span class="number">16</span>, STOP_ITER, thresh=<span class="number">15000</span>, alpha=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><pre><code>***Original data - learning rate: 0.001 - Mini-batch (16) descent - Stop: 15000 iterationsTheta: [[-1.0364887   0.02542788  0.00549476]] - Iter: 15000 - Last cost: 0.57 - Duration: 1.04sarray([[-1.0364887 ,  0.02542788,  0.00549476]])</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/49_2.png" alt=""></p><p>浮动仍然比较大，我们来尝试下对数据进行标准化<br>将数据按其属性(按列进行)减去其均值，然后除以其方差。最后得到的结果是，对每个属性/每列来说所有数据都聚集在0附近，方差值为1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing <span class="keyword">as</span> pp</span><br><span class="line"></span><br><span class="line">scaled_data = orig_data.copy()</span><br><span class="line">scaled_data[:, <span class="number">1</span>:<span class="number">3</span>] = pp.scale(orig_data[:, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">runExpe(scaled_data, theta, n, STOP_ITER, thresh=<span class="number">5000</span>, alpha=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><pre><code>***Scaled data - learning rate: 0.001 - Gradient descent - Stop: 5000 iterationsTheta: [[0.3080807  0.86494967 0.77367651]] - Iter: 5000 - Last cost: 0.38 - Duration: 0.88sarray([[0.3080807 , 0.86494967, 0.77367651]])</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/51_2.png" alt=""></p><p>它好多了！原始数据，只能达到达到0.61，而我们得到了0.38个在这里！<br>所以对数据做预处理是非常重要的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runExpe(scaled_data, theta, n, STOP_GRAD, thresh=<span class="number">0.02</span>, alpha=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><pre><code>***Scaled data - learning rate: 0.001 - Gradient descent - Stop: gradient norm &lt; 0.02Theta: [[1.0707921  2.63030842 2.41079787]] - Iter: 59422 - Last cost: 0.22 - Duration: 10.67sarray([[1.0707921 , 2.63030842, 2.41079787]])</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/53_2.png" alt=""></p><p>更多的迭代次数会使得损失下降的更多！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta = runExpe(scaled_data, theta, <span class="number">1</span>, STOP_GRAD, thresh=<span class="number">0.002</span>/<span class="number">5</span>, alpha=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><pre><code>***Scaled data - learning rate: 0.001 - Stochastic descent - Stop: gradient norm &lt; 0.0004Theta: [[1.14814786 2.79253048 2.56596963]] - Iter: 72591 - Last cost: 0.22 - Duration: 4.82s</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/55_1.png" alt=""></p><p>随机梯度下降更快，但是我们需要迭代的次数也需要更多，所以还是用batch的比较合适！！！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runExpe(scaled_data, theta, <span class="number">16</span>, STOP_GRAD, thresh=<span class="number">0.002</span>*<span class="number">2</span>, alpha=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><pre><code>***Scaled data - learning rate: 0.001 - Mini-batch (16) descent - Stop: gradient norm &lt; 0.004Theta: [[1.14982001 2.79586036 2.56934533]] - Iter: 307 - Last cost: 0.22 - Duration: 0.03sarray([[1.14982001, 2.79586036, 2.56934533]])</code></pre><p><img src="https://cdn.jsdelivr.net/gh/xxren8218/blogimages/img/57_2.png" alt=""></p><h2 id="精度"><a href="#精度" class="headerlink" title="精度"></a>精度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设定阈值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">X, theta</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x &gt;= <span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> model(X, theta)]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scaled_X = scaled_data[:, :<span class="number">3</span>]</span><br><span class="line">y = scaled_data[:, <span class="number">3</span>]</span><br><span class="line">predictions = predict(scaled_X, theta)</span><br><span class="line">correct = [<span class="number">1</span> <span class="keyword">if</span> ((a == <span class="number">1</span> <span class="keyword">and</span> b == <span class="number">1</span>) <span class="keyword">or</span> (a == <span class="number">0</span> <span class="keyword">and</span> b == <span class="number">0</span>)) <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> (a, b) <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, y)] </span><br><span class="line"><span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">zip(itertion1, iteration2)</span></span><br><span class="line"><span class="string"> A[1,2,3]</span></span><br><span class="line"><span class="string"> B[4,5,6]</span></span><br><span class="line"><span class="string"> zip[A,B] = [(1,4),(2,5),(3,6)] 可用list()进行转换</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">accuracy = (<span class="built_in">sum</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, correct)) / <span class="built_in">len</span>(correct))  <span class="comment"># map (func, iterations)</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;accuracy = &#123;&#125;%&#x27;</span>.<span class="built_in">format</span>(accuracy*<span class="number">100</span>))</span><br></pre></td></tr></table></figure><pre><code>accuracy = 89.0%</code></pre><h2 id="逻辑回归到这里就结束了！"><a href="#逻辑回归到这里就结束了！" class="headerlink" title="逻辑回归到这里就结束了！"></a>逻辑回归到这里就结束了！</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Logistic-Regression&quot;&gt;&lt;a href=&quot;#Logistic-Regression&quot; class=&quot;headerlink&quot; title=&quot;Logistic Regression&quot;&gt;&lt;/a&gt;Logistic Regression&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    <category term="机器学习" scheme="https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习基础" scheme="https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>机器学习数学基础-线性代数</title>
    <link href="https://xxren8218.github.io/20210405/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0.html"/>
    <id>https://xxren8218.github.io/20210405/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0.html</id>
    <published>2021-04-04T17:11:14.000Z</published>
    <updated>2021-04-17T18:10:24.541Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性代数复习和参考"><a href="#线性代数复习和参考" class="headerlink" title="线性代数复习和参考"></a>线性代数复习和参考</h2><h3 id="1-基础概念和符号"><a href="#1-基础概念和符号" class="headerlink" title="1.  基础概念和符号"></a>1.  基础概念和符号</h3><p>线性代数提供了一种紧凑地表示和操作线性方程组的方法。 例如，以下方程组：</p><script type="math/tex; mode=display">4x_1 − 5x_2 = −13</script><script type="math/tex; mode=display">−2x_1 + 3x_2 = 9</script><p>这是两个方程和两个变量，正如你从高中代数中所知，你可以找到 $x_1$ 和 $x_2$ 的唯一解（除非方程以某种方式退化，例如，如果第二个方程只是第一个的倍数，但在上面的情况下，实际上只有一个唯一解）。 在矩阵表示法中，我们可以更紧凑地表达：</p><script type="math/tex; mode=display">Ax= b</script><script type="math/tex; mode=display">\text { with } A=\left[\begin{array}{cc}{4} & {-5} \\ {-2} & {3}\end{array}\right], b=\left[\begin{array}{c}{-13} \\ {9}\end{array}\right]</script><p>我们可以看到，这种形式的线性方程有许多优点（比如明显地节省空间）。</p><h4 id="1-1-基本符号"><a href="#1-1-基本符号" class="headerlink" title="1.1 基本符号"></a>1.1 基本符号</h4><p>我们使用以下符号：</p><ul><li><p>$A \in \mathbb{R}^{m \times n}$，表示 $A$ 为由实数组成具有$m$行和$n$列的矩阵。</p></li><li><p>$x \in \mathbb{R}^{ n}$，表示具有$n$个元素的向量。 通常，向量$x$将表示列向量: 即，具有$n$行和$1$列的矩阵。 如果我们想要明确地表示行向量: 具有 $1$ 行和$n$列的矩阵 - 我们通常写$x^T$（这里$x^T$$x$的转置）。</p></li><li><p>$x_i$表示向量$x$的第$i$个元素</p></li></ul><script type="math/tex; mode=display">x=\left[\begin{array}{c}{x_{1}} \\ {x_{2}} \\ {\vdots} \\ {x_{n}}\end{array}\right]</script><ul><li>我们使用符号 $a_{ij}$（或$A_{ij}$,$A_{i,j}$等）来表示第 $i$ 行和第$j$列中的 $A$ 的元素：</li></ul><script type="math/tex; mode=display">A=\left[\begin{array}{cccc}{a_{11}} & {a_{12}} & {\cdots} & {a_{1 n}} \\ {a_{21}} & {a_{22}} & {\cdots} & {a_{2 n}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {a_{m 1}} & {a_{m 2}} & {\cdots} & {a_{m n}}\end{array}\right]</script><ul><li>我们用$a^j$或者$A_{:,j}$表示矩阵$A$的第$j$列：</li></ul><script type="math/tex; mode=display">A=\left[\begin{array}{llll}{ |} & { |} & {} & { |} \\ {a^{1}} & {a^{2}} & {\cdots} & {a^{n}} \\ { |} & { |} & {} & { |}\end{array}\right]</script><ul><li>我们用$a^T_i$或者$A_{i,:}$表示矩阵$A$的第$i$行：<script type="math/tex; mode=display">A=\left[\begin{array}{c}{-a_{1}^{T}-} \\ {-a_{2}^{T}-} \\ {\vdots} \\ {-a_{m}^{T}-}\end{array}\right]</script></li></ul><ul><li>在许多情况下，将矩阵视为列向量或行向量的集合非常重要且方便。 通常，在向量而不是标量上操作在数学上（和概念上）更清晰。只要明确定义了符号，用于矩阵的列或行的表示方式并没有通用约定。</li></ul><h3 id="2-矩阵乘法"><a href="#2-矩阵乘法" class="headerlink" title="2.矩阵乘法"></a>2.矩阵乘法</h3><p>两个矩阵相乘，其中 $A \in \mathbb{R}^{m \times n}$  and $B \in \mathbb{R}^{n \times p}$ ，则：</p><script type="math/tex; mode=display">C = AB \in \mathbb{R}^{m \times p}</script><p>其中：</p><script type="math/tex; mode=display">C_{i j}=\sum_{k=1}^{n} A_{i k} B_{k j}</script><p>请注意，为了使矩阵乘积存在，$A$中的列数必须等于$B$中的行数。有很多方法可以查看矩阵乘法，我们将从检查一些特殊情况开始。</p><h4 id="2-1-向量-向量乘法"><a href="#2-1-向量-向量乘法" class="headerlink" title="2.1 向量-向量乘法"></a>2.1 向量-向量乘法</h4><p>给定两个向量$x, y \in \mathbb{R}^{n}$,$x^T y$通常称为<strong>向量内积</strong>或者<strong>点积</strong>，结果是个<strong>实数</strong>。</p><script type="math/tex; mode=display">x^{T} y \in \mathbb{R}=\left[\begin{array}{llll}{x_{1}} & {x_{2}} & {\cdots} & {x_{n}}\end{array}\right]\left[\begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{n}}\end{array}\right]=\sum_{i=1}^{n} x_{i} y_{i}</script><p>注意：$x^T y = y^Tx$ 始终成立。</p><p>给定向量 $x \in \mathbb{R}^{m}$, $y \in \mathbb{R}^{n}$ (他们的维度是否相同都没关系)，$xy^T \in \mathbb{R}^{m \times n}$叫做<strong>向量外积 </strong> , 当 $(xy^T)_{ij} = x_iy_j$ 的时候，它是一个矩阵。</p><script type="math/tex; mode=display">x y^{T} \in \mathbb{R}^{m \times n}=\left[\begin{array}{c}{x_{1}} \\ {x_{2}} \\ {\vdots} \\ {x_{m}}\end{array}\right]\left[\begin{array}{llll}{y_{1}} & {y_{2}} & {\cdots} & {y_{n}}\end{array}\right]=\left[\begin{array}{cccc}{x_{1} y_{1}} & {x_{1} y_{2}} & {\cdots} & {x_{1} y_{n}} \\ {x_{2} y_{1}} & {x_{2} y_{2}} & {\cdots} & {x_{2} y_{n}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {x_{m} y_{1}} & {x_{m} y_{2}} & {\cdots} & {x_{m} y_{n}}\end{array}\right]</script><p>举一个外积如何使用的一个例子：让$1\in R^{n}$表示一个$n$维向量，其元素都等于1，此外，考虑矩阵$A \in R^{m \times n}$，其列全部等于某个向量 $x \in R^{m}$。 我们可以使用外积紧凑地表示矩阵 $A$:</p><script type="math/tex; mode=display">A=\left[\begin{array}{llll}{ |} & { |} & {} & { |} \\ {x} & {x} & {\cdots} & {x} \\ { |} & { |} & {} & { |}\end{array}\right]=\left[\begin{array}{cccc}{x_{1}} & {x_{1}} & {\cdots} & {x_{1}} \\ {x_{2}} & {x_{2}} & {\cdots} & {x_{2}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {x_{m}} & {x_{m}} & {\cdots} & {x_{m}}\end{array}\right]=\left[\begin{array}{c}{x_{1}} \\ {x_{2}} \\ {\vdots} \\ {x_{m}}\end{array}\right]\left[\begin{array}{lll}{1} & {1} & {\cdots} & {1}\end{array}\right]=x \mathbf{1}^{T}</script><h4 id="2-2-矩阵-向量乘法"><a href="#2-2-矩阵-向量乘法" class="headerlink" title="2.2 矩阵-向量乘法"></a>2.2 矩阵-向量乘法</h4><p>给定矩阵 $A \in \mathbb{R}^{m \times n}$，向量 $x \in  \mathbb{R}^{n}$ , 它们的积是一个向量 $y = Ax \in R^{m}$。 有几种方法可以查看矩阵向量乘法，我们将依次查看它们中的每一种。</p><p>如果我们按行写$A$，那么我们可以表示$Ax$为：</p><script type="math/tex; mode=display">y=A x=\left[\begin{array}{ccc}{-} & {a_{1}^{T}} & {-} \\ {-} & {a_{2}^{T}} & {-} \\ {} & {\vdots} & {} \\ {-} & {a_{m}^{T}} & {-}\end{array}\right] x=\left[\begin{array}{c}{a_{1}^{T} x} \\ {a_{2}^{T} x} \\ {\vdots} \\ {a_{m}^{T} x}\end{array}\right]</script><p>换句话说，第$i$个$y$是$A$的第$i$行和$x$的内积，即：$y_i = y_{i}=a_{i}^{T} x$。</p><p>同样的， 可以把 $A$ 写成列的方式，则公式如下：</p><script type="math/tex; mode=display">y=A x=\left[\begin{array}{cccc}{ |} & { |} & {} & { |} \\ {a^{1}} & {a^{2}} & {\cdots} & {a^{n}} \\ { |} & { |} & {} & { |}\end{array}\right]\left[\begin{array}{c}{x_{1}} \\ {x_{2}} \\ {\vdots} \\ {x_{n}}\end{array}\right]=\left[\begin{array}{c}{ } \\ {a^{1}{ } \\ }\end{array}\right] x_{1}+\left[\begin{array}{c}{ } \\ {a^{2}{ } \\ }\end{array}\right] x_{2}+{\cdots} +\left[\begin{array}{c}{ } \\ {a^{n}{ } \\ }\end{array}\right] x_{n}</script><p>换句话说，$y$是$A$的列的线性组合，其中线性组合的系数由$x$的元素给出。</p><p>到目前为止，我们一直在右侧乘以列向量，但也可以在左侧乘以行向量。 这是写的，$y^T = x^TA$ 表示$A \in \mathbb{R}^{m \times n}$，$x \in \mathbb{R}^{m}$，$y \in \mathbb{R}^{n}$。 和以前一样，我们可以用两种可行的方式表达$y^T$，这取决于我们是否根据行或列表达$A$.</p><p>第一种情况，我们把$A$用列表示：</p><script type="math/tex; mode=display">y^{T}=x^{T} A=x^{T}\left[\begin{array}{cccc}{ |} & { |} & {} & { |} \\ {a^{1}} & {a^{2}} & {\cdots} & {a^{n}} \\ { |} & { |} & {} & { |}\end{array}\right]=\left[\begin{array}{cccc}{x^{T} a^{1}} & {x^{T} a^{2}} & {\dots} & {x^{T} a^{n}}\end{array}\right]</script><p>这表明$y^T$的第$i$个元素等于$x$和$A$的第$i$列的内积。</p><p>最后，根据行表示$A$，我们得到了向量-矩阵乘积的最终表示:</p><script type="math/tex; mode=display">y^T=x^TA=\left[\begin{array}{llll}{x_{1}} & {x_{2}} & {\cdots} & {x_{n}}\end{array}\right]\left[\begin{array}{c}{-a_{1}^{T}-} \\ {-a_{2}^{T}-} \\ {\vdots} \\ {-a_{m}^{T}-}\end{array}\right]=x_{1}\left[-a_{1}^{T}-\right]+x_{2}\left[-a_{2}^{T}-\right]+\ldots+x_{n}\left[-a_{n}^{T}-\right]</script><p>所以我们看到$y^T$是$A$的行的线性组合，其中线性组合的系数由$x$的元素给出。</p><h4 id="2-3-矩阵-矩阵乘法"><a href="#2-3-矩阵-矩阵乘法" class="headerlink" title="2.3 矩阵-矩阵乘法"></a>2.3 矩阵-矩阵乘法</h4><p>有了这些知识，我们现在可以看看四种不同的（形式不同，但结果是相同的）矩阵-矩阵乘法：也就是本节开头所定义的$C=AB$的乘法。</p><p>首先，我们可以将矩阵 - 矩阵乘法视为一组向量-向量乘积。 从定义中可以得出：最明显的观点是$C $的$( i，j )$元素等于$A$的第$i$行和$B$的的$j$列的内积。如下面的公式所示：</p><script type="math/tex; mode=display">C=A B=\left[\begin{array}{cc}{-} & {a_{1}^{T}} &{-} \\ {-} & {a_{2}^{T}} &{-}  \\ {} & {\vdots} \\ {-} & {a_{m}^{T}} &{-} \end{array}\right]\left[\begin{array}{cccc}{ |} & { |} & {} & { |} \\ {b_{1}} & {b_{2}} & {\cdots} & {b_{p}} \\ { |} & { |} & {} & { |}\end{array}\right]=\left[\begin{array}{cccc}{a_{1}^{T} b_{1}} & {a_{1}^{T} b_{2}} & {\cdots} & {a_{1}^{T} b_{p}} \\ {a_{2}^{T} b_{1}} & {a_{2}^{T} b_{2}} & {\cdots} & {a_{2}^{T} b_{p}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {a_{m}^{T} b_{1}} & {a_{m}^{T} b_{2}} & {\cdots} & {a_{m}^{T} b_{p}}\end{array}\right]</script><p>这里的$ A \in \mathbb{R}^{m\times n}$ ，$B \in \mathbb{R}^{n \times p}$， $a_i \in \mathbb{R}^n$ ，$b^j \in \mathbb{R}^{n \times p}$， 这里的$  A \in \mathbb{R}^ {m \times n}，$ $B \in \mathbb{R}^ {n \times p} $， $a_i \in \mathbb{R} ^ n $，$ b ^ j \in \mathbb{R} ^ {n \times p} $，所以它们可以计算内积。 我们用通常用行表示$ A $而用列表示$B$。<br>或者，我们可以用列表示$ A$，用行表示$B $，这时$AB$是求外积的和。公式如下：</p><script type="math/tex; mode=display">C=A B=\left[\begin{array}{cccc}{ |} & { |} & {} & { |} \\ {a_{1}} & {a_{2}} & {\cdots} & {a_{n}} \\ { |} & { |} & {} & { |}\end{array}\right]\left[\begin{array}{c}{-}& {b_{1}^{T}}&{-} \\ {-}& {b_{2}^{T}}&{-}  \\ {\vdots} \\{-}& {b_{n}^{T}}&{-}\end{array}\right]=\sum_{i=1}^{n} a_{i} b_{i}^{T}</script><p>换句话说，$AB$等于所有的$A$的第$i$列和$B$第$i$行的外积的和。因此，在这种情况下， $a_i \in \mathbb{R}^ m $和$b_i \in \mathbb{R}^p$， 外积$a^ib_i^T$的维度是$m×p$，与$C$的维度一致。</p><p>其次，我们还可以将矩阵 - 矩阵乘法视为一组矩阵向量积。如果我们把$B$用列表示，我们可以将$C$的列视为$A$和$B$的列的矩阵向量积。公式如下：</p><script type="math/tex; mode=display">C=A B=A\left[\begin{array}{cccc}{ |} & { |} & {} & { |} \\ {b_{1}} & {b_{2}} & {\cdots} & {b_{p}} \\ { |} & { |} & {} & { |}\end{array}\right]=\left[\begin{array}{cccc}{ |} & { |} & {} & { |} \\ {A b_{1}} & {A b_{2}} & {\cdots} & {A b_{p}} \\ { |} & { |} & {} & { |}\end{array}\right]</script><p>这里$C$的第$i$列由矩阵向量乘积给出，右边的向量为$c_i = Ab_i$。 这些矩阵向量乘积可以使用前一小节中给出的两个观点来解释。<br>最后，我们有类似的观点，我们用行表示$A$，$C$的行作为$A$和$C$行之间的矩阵向量积。公式如下：</p><script type="math/tex; mode=display">C=A B=\left[\begin{array}{ccc}{-} & {a_{1}^{T}} & {-} \\ {-} & {a_{2}^{T}} & {-} \\ {} & {\vdots} & {} \\ {-} & {a_{m}^{T}} & {-}\end{array}\right]  B=\left[\begin{array}{c} {-} & {a_{1}^{T} B} & {-}\\ {-} & {a_{2}^{T} B} & {-} \\ {\vdots} \\ {-} & {a_{m}^{T} B}& {-}\end{array}\right]</script><p>这里第$i$行的$C$由左边的向量的矩阵向量乘积给出：$c_i^T = a_i^T B$</p><p>将矩阵乘法剖析到如此大的程度似乎有点过分，特别是当所有这些观点都紧跟在我们在本节开头给出的初始定义（在一行数学中）之后。 </p><p>这些不同方法的直接优势在于它们允许您<strong>在向量的级别/单位而不是标量上进行操作</strong>。 为了完全理解线性代数而不会迷失在复杂的索引操作中，关键是要用尽可能多的概念进行操作。</p><p>实际上所有的线性代数都处理某种矩阵乘法，花一些时间对这里提出的观点进行直观的理解是非常必要的。 </p><p>除此之外，了解一些更高级别的矩阵乘法的基本属性是很有必要的：</p><ul><li><p>矩阵乘法结合律: $(AB)C = A(BC)$</p></li><li><p>矩阵乘法分配律: $A(B + C) = AB + AC$</p></li><li><p>矩阵乘法通常不是可交换的; 也就是说，通常$AB \ne BA$。 （例如，假设$  A \in \mathbb{R}^ {m \times n}，$ $B \in \mathbb{R}^ {n \times p} $，如果$m$和$q$不相等，矩阵乘积$BA$甚至不存在！）</p></li></ul><p>如果您不熟悉这些属性，请花点时间自己验证它们。 例如，为了检查矩阵乘法的相关性，假设$A \in \mathbb{R}^ {m \times n}，$ $B \in \mathbb{R}^ {n \times p} $，$C \in \mathbb{R}^ {p \times q}$。 注意$AB \in \mathbb{R}^ {m \times p}$，所以$(AB)C \in \mathbb{R}^ {m \times q}$。 类似地，$BC \in \mathbb{R}^ {n \times q}$，所以$A(BC) \in \mathbb{R}^ {m \times q}$。 因此，所得矩阵的维度一致。 为了表明矩阵乘法是相关的，足以检查$(AB)C $的第$(i,j)$个元素是否等于$A(BC)$的第$(i,j)$个元素。 我们可以使用矩阵乘法的定义直接验证这一点：</p><script type="math/tex; mode=display">\begin{aligned}((A B) C)_{i j} &=\sum_{k=1}^{p}(A B)_{i k} C_{k j}=\sum_{k=1}^{p}\left(\sum_{l=1}^{n} A_{i l} B_{l k}\right) C_{k j} \\ &=\sum_{k=1}^{p}\left(\sum_{l=1}^{n} A_{i l} B_{l k} C_{k j}\right)=\sum_{l=1}^{n}\left(\sum_{k=1}^{p} A_{i l} B_{l k} C_{k j}\right) \\ &=\sum_{l=1}^{n} A_{i l}\left(\sum_{k=1}^{p} B_{l k} C_{k j}\right)=\sum_{l=1}^{n} A_{i l}(B C)_{l j}=(A(B C))_{i j} \end{aligned}</script><h3 id="3-运算和属性"><a href="#3-运算和属性" class="headerlink" title="3 运算和属性"></a>3 运算和属性</h3><p>在本节中，我们介绍矩阵和向量的几种运算和属性。 希望能够为您复习大量此类内容，这些笔记可以作为这些主题的参考。</p><h4 id="3-1-单位矩阵和对角矩阵"><a href="#3-1-单位矩阵和对角矩阵" class="headerlink" title="3.1 单位矩阵和对角矩阵"></a>3.1 单位矩阵和对角矩阵</h4><p><strong>单位矩阵</strong>,$I \in \mathbb{R}^{n \times n} $，它是一个方阵，对角线的元素是1，其余元素都是0：</p><script type="math/tex; mode=display">I_{i j}=\left\{\begin{array}{ll}{1} & {i=j} \\ {0} & {i \neq j}\end{array}\right.</script><p>对于所有$A \in \mathbb{R}^ {m \times n}$，有：</p><script type="math/tex; mode=display">AI = A = IA</script><p>注意，在某种意义上，单位矩阵的表示法是不明确的，因为它没有指定$I$的维数。通常，$I$的维数是从上下文推断出来的，以便使矩阵乘法成为可能。 例如，在上面的等式中，$AI = A$中的I是$n\times n$矩阵，而$A = IA$中的$I$是$m\times m$矩阵。</p><p>对角矩阵是一种这样的矩阵：对角线之外的元素全为0。对角阵通常表示为：$D= diag(d_1, d_2, . . . , d_n)$，其中：</p><script type="math/tex; mode=display">D_{i j}=\left\{\begin{array}{ll}{d_{i}} & {i=j} \\ {0} & {i \neq j}\end{array}\right.</script><p>很明显：单位矩阵$ I = diag(1, 1, . . . , 1)$。</p><h4 id="3-2-转置"><a href="#3-2-转置" class="headerlink" title="3.2 转置"></a>3.2 转置</h4><p>矩阵的转置是指翻转矩阵的行和列。</p><p>给定一个矩阵：</p><p>$A \in \mathbb{R}^ {m \times n}$, 它的转置为$n \times m$的矩阵$A^T \in \mathbb{R}^ {n \times m}$ ，其中的元素为：</p><script type="math/tex; mode=display">(A^T)_{ij} = A_{ji}</script><p>事实上，我们在描述行向量时已经使用了转置，因为列向量的转置自然是行向量。</p><p>转置的以下属性很容易验证：</p><ul><li>$(A^T )^T = A$</li><li>$ (AB)^T = B^T A^T$</li><li>$(A + B)^T = A^T + B^T$</li></ul><h4 id="3-3-对称矩阵"><a href="#3-3-对称矩阵" class="headerlink" title="3.3 对称矩阵"></a>3.3 对称矩阵</h4><p>如果$A =  A^T$，则矩阵$A \in \mathbb{R}^ {n \times n}$是对称矩阵。 如果$ A =  -  A^T$，它是反对称的。 很容易证明，对于任何矩阵$A \in \mathbb{R}^ {n \times n}$，矩阵$A  +  A^ T$是对称的，矩阵$A -A^T$是反对称的。 由此得出，任何方矩阵$A \in \mathbb{R}^ {n \times n}$可以表示为对称矩阵和反对称矩阵的和，所以：</p><script type="math/tex; mode=display">A=\frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T)</script><p>上面公式的右边的第一个矩阵是对称矩阵，而第二个矩阵是反对称矩阵。 事实证明，对称矩阵在实践中用到很多，它们有很多很好的属性，我们很快就会看到它们。<br>通常将大小为$n$的所有对称矩阵的集合表示为$\mathbb{S}^n$，因此$A \in \mathbb{S}^n$意味着$A$是对称的$n\times n$矩阵;</p><h4 id="3-4-矩阵的迹"><a href="#3-4-矩阵的迹" class="headerlink" title="3.4 矩阵的迹"></a>3.4 矩阵的迹</h4><p>方矩阵$A \in \mathbb{R}^ {n \times n}$的迹，表示为$\operatorname{tr} (A)$（或者只是$\operatorname{tr} A$，如果括号显然是隐含的），是矩阵中对角元素的总和：</p><script type="math/tex; mode=display">\operatorname{tr} A=\sum_{i=1}^{n} A_{i i}</script><p>如<strong>CS229</strong>讲义中所述，迹具有以下属性（如下所示）：</p><ul><li><p>对于矩阵$A \in \mathbb{R}^ {n \times n}$，则：$\operatorname{tr}A =\operatorname{tr}A^T$</p></li><li><p>对于矩阵$A,B \in \mathbb{R}^ {n \times n}$，则：$\operatorname{tr}(A + B) = \operatorname{tr}A + \operatorname{tr}B$</p></li><li><p>对于矩阵$A \in \mathbb{R}^ {n \times n}$，$ t \in \mathbb{R}$，则：$\operatorname{tr}(tA) = t\operatorname{tr}A$.</p></li><li><p>对于矩阵 $A$, $B$，$AB$ 为方阵, 则：$\operatorname{tr}AB = \operatorname{tr}BA$</p></li><li><p>对于矩阵 $A$, $B$, $C$, $ABC$为方阵, 则：$\operatorname{tr}ABC = \operatorname{tr}BCA=\operatorname{tr}CAB$, 同理，更多矩阵的积也是有这个性质。</p></li></ul><p>作为如何证明这些属性的示例，我们将考虑上面给出的第四个属性。 假设$A \in \mathbb{R}^ {m \times n}$和$B \in \mathbb{R}^ {n \times m}$（因此$AB \in \mathbb{R}^ {m \times m}$是方阵）。 观察到$BA \in \mathbb{R}^ {n \times n}$也是一个方阵，因此对它们进行迹的运算是有意义的。 要证明$\operatorname{tr}AB = \operatorname{tr}BA$，请注意：</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{tr} A B &=\sum_{i=1}^{m}(A B)_{i i}=\sum_{i=1}^{m}\left(\sum_{j=1}^{n} A_{i j} B_{j i}\right) \\ &=\sum_{i=1}^{m} \sum_{j=1}^{n} A_{i j} B_{j i}=\sum_{j=1}^{n} \sum_{i=1}^{m} B_{j i} A_{i j} \\ &=\sum_{j=1}^{n}\left(\sum_{i=1}^{m} B_{j i} A_{i j}\right)=\sum_{j=1}^{n}(B A)_{j j}=\operatorname{tr} B A \end{aligned}</script><p>这里，第一个和最后两个等式使用迹运算符和矩阵乘法的定义，重点在第四个等式，使用标量乘法的可交换性来反转每个乘积中的项的顺序，以及标量加法的可交换性和相关性，以便重新排列求和的顺序。</p><h4 id="3-5-范数"><a href="#3-5-范数" class="headerlink" title="3.5 范数"></a>3.5 范数</h4><p>向量的范数$|x|$是非正式度量的向量的“长度” 。 例如，我们有常用的欧几里德或$\ell_{2}$范数，</p><script type="math/tex; mode=display">\|x\|_{2}=\sqrt{\sum_{i=1}^{n} x_{i}^{2}}</script><p>注意：$|x|_{2}^{2}=x^{T} x$</p><p>更正式地，范数是满足4个属性的函数（$f : \mathbb{R}^{n} \rightarrow \mathbb{R}$）：</p><ol><li>对于所有的 $x \in \mathbb{R}^ {n}$, $f(x) \geq 0 $(非负).</li><li>当且仅当$x = 0$ 时，$f(x) = 0$ (明确性).</li><li>对于所有$x \in \mathbb{R}^ {n}$,$t\in \mathbb{R}$，则 $f(tx) = \left| t \right|f(x)$ (正齐次性).</li><li>对于所有 $x,y \in \mathbb{R}^ {n}$, $f(x + y) \leq f(x) + f(y)$ (三角不等式)</li></ol><p>其他范数的例子是$\ell_1$范数:</p><script type="math/tex; mode=display">\|x\|_{1}=\sum_{i=1}^{n}\left|x_{i}\right|</script><p>和$\ell_{\infty }$范数：</p><script type="math/tex; mode=display">\|x\|_{\infty}=\max _{i}\left|x_{i}\right|</script><p>事实上，到目前为止所提出的所有三个范数都是$\ell_p$范数族的例子，它们由实数$p \geq 1$参数化，并定义为：</p><script type="math/tex; mode=display">\|x\|_{p}=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p}\right)^{1 / p}</script><p>也可以为矩阵定义范数，例如<strong>Frobenius</strong>范数:</p><script type="math/tex; mode=display">\|A\|_{F}=\sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} A_{i j}^{2}}=\sqrt{\operatorname{tr}\left(A^{T} A\right)}</script><p>许多其他更多的范数，但它们超出了这个复习材料的范围。</p><h4 id="3-6-线性相关性和秩"><a href="#3-6-线性相关性和秩" class="headerlink" title="3.6 线性相关性和秩"></a>3.6 线性相关性和秩</h4><p>一组向量${x_1,x_2, \cdots x_n} \in \mathbb{R}$， 如果没有向量可以表示为其余向量的线性组合，则称称该向量是线性无相关的。 相反，如果属于该组的一个向量可以表示为其余向量的线性组合，则称该向量是线性相关的。 也就是说，如果：</p><script type="math/tex; mode=display">x_{n}=\sum_{i=1}^{n-1} \alpha_{i} x_{i}</script><p>对于某些标量值$\alpha_1,\cdots \alpha_n-1 \in \mathbb{R}$，要么向量$x_1,x_2, \cdots x_n$是线性相关的; 否则，向量是线性无关的。 例如，向量：</p><script type="math/tex; mode=display">x_{1}=\left[\begin{array}{l}{1} \\ {2} \\ {3}\end{array}\right] \quad x_{2}=\left[\begin{array}{c}{4} \\ {1} \\ {5}\end{array}\right] \quad x_{3}=\left[\begin{array}{c}{2} \\ {-3} \\ {-1}\end{array}\right]</script><p>是线性相关的，因为：$x_3=-2x_1+x_2$。</p><p>矩阵$A  \in \mathbb{R}^{m \times n}$的<strong>列秩</strong>是构成线性无关集合的$A$的最大列子集的大小。 由于术语的多样性，这通常简称为$A$的线性无关列的数量。同样，行秩是构成线性无关集合的$A$的最大行数。 对于任何矩阵$A  \in \mathbb{R}^{m \times n}$，事实证明$A$的列秩等于$A$的行秩（尽管我们不会证明这一点），因此两个量统称为$A$的<strong>秩</strong>，用 $\text{rank}(A)$表示。 以下是秩的一些基本属性：</p><ul><li>对于  $A  \in \mathbb{R}^{m \times n}$，$\text{rank}(A) \leq min(m, n)$，如果$ \text(A) = \text{min} (m, n)$，则： $A$ 被称作<strong>满秩</strong>。</li><li>对于  $A  \in \mathbb{R}^{m \times n}$， $\text{rank}(A) = \text{rank}(A^T)$</li><li>对于  $A  \in \mathbb{R}^{m \times n}$,$B  \in \mathbb{R}^{n \times p}$ ,$\text{rank}(AB) \leq \text{min} ( \text{rank}(A), \text{rank}(B))$</li><li>对于  $A,B \in \mathbb{R}^{m \times n}$，$\text{rank}(A + B) \leq \text{rank}(A) + \text{rank}(B)$</li></ul><h4 id="3-7-方阵的逆"><a href="#3-7-方阵的逆" class="headerlink" title="3.7 方阵的逆"></a>3.7 方阵的逆</h4><p>方阵$A  \in \mathbb{R}^{n \times n}$的倒数表示为$A^{-1}$，并且是这样的独特矩阵:</p><script type="math/tex; mode=display">A^{-1}A=I=AA^{-1}</script><p>请注意，并非所有矩阵都具有逆。 例如，非方形矩阵根据定义没有逆。 然而，对于一些方形矩阵$A$，可能仍然存在$A^{-1}$可能不存在的情况。 特别是，如果$A^{-1}$存在，我们说$A$是<strong>可逆</strong>的或<strong>非奇异</strong>的，否则就是<strong>不可逆</strong>或<strong>奇异</strong>的。<br>为了使方阵A具有逆$A^{-1}$，则$A$必须是满秩。 我们很快就会发现，除了满秩之外，还有许多其它的充分必要条件。<br>以下是逆的属性; 假设$A,B  \in \mathbb{R}^{n \times n}$，而且是非奇异的：</p><ul><li>$(A^{-1})^{-1} = A$</li><li>$(AB)^{-1} = B^{-1}A^{-1}$</li><li>$(A^{-1})^{T} =(A^{T})^{-1} $因此，该矩阵通常表示为$A^{-T}$。<br>作为如何使用逆的示例，考虑线性方程组，$Ax = b$，其中$A  \in \mathbb{R}^{n \times n}$，$x,b\in \mathbb{R}$， 如果$A$是非奇异的（即可逆的），那么$x = A^{-1}b$。 （如果$A  \in \mathbb{R}^{m \times n}$不是方阵，这公式还有用吗？）</li></ul><h4 id="3-8-正交阵"><a href="#3-8-正交阵" class="headerlink" title="3.8 正交阵"></a>3.8 正交阵</h4><p>如果 $x^Ty=0$，则两个向量$x,y\in \mathbb{R}^{n}$ 是<strong>正交</strong>的。如果$|x|_2=1$，则向量$x\in \mathbb{R}^{n}$ 被归一化。如果一个方阵$U\in \mathbb{R}^{n \times n}$的所有列彼此正交并被归一化（这些列然后被称为正交），则方阵$U$是正交阵（注意在讨论向量时的意义不一样）。</p><p>它可以从正交性和正态性的定义中得出:</p><script type="math/tex; mode=display">U^ TU = I = U U^T</script><p>换句话说，正交矩阵的逆是其转置。 注意，如果$U$不是方阵 :即，$U\in \mathbb{R}^{m \times n}$，$n &lt;m$  ，但其列仍然是正交的，则$U^TU = I$，但是$UU^T \neq I$。我们通常只使用术语”正交”来描述先前的情况 ，其中$U$是方阵。<br>正交矩阵的另一个好的特性是在具有正交矩阵的向量上操作不会改变其欧几里德范数，即:</p><script type="math/tex; mode=display">\|U x\|_{2}=\|x\|_{2}</script><p>对于任何 $x\in \mathbb{R}$ , $U\in \mathbb{R}^{n}$是正交的。</p><h4 id="3-9-矩阵的值域和零空间"><a href="#3-9-矩阵的值域和零空间" class="headerlink" title="3.9 矩阵的值域和零空间"></a>3.9 矩阵的值域和零空间</h4><p>一组向量$\{x_{1}, \ldots x_{n}\}$是可以表示为$\{x_{1}, \ldots x_{n}\}$的线性组合的所有向量的集合。 即：</p><script type="math/tex; mode=display">\operatorname{span}\left(\left\{x_{1}, \ldots x_{n}\right\}\right)=\left\{v : v=\sum_{i=1}^{n} \alpha_{i} x_{i}, \quad \alpha_{i} \in \mathbb{R}\right\}</script><p>可以证明，如果$\{x_{1}, \ldots x_{n}\}$是一组$n$个线性无关的向量，其中每个$x_i \in \mathbb{R}^{n}$，则$\text{span}(\{x_{1}, \ldots x_{n}\})=\mathbb{R}^{n}$。 换句话说，任何向量$v\in \mathbb{R}^{n}$都可以写成$x_1$到$x_n$的线性组合。</p><p>向量$y\in \mathbb{R}^{m}$投影到$\{x_{1}, \ldots x_{n}\}$（这里我们假设$x_i \in \mathbb{R}^{m}$）得到向量$v \in \operatorname{span}(\{x_{1}, \ldots, x_{n}\})$，由欧几里德范数$|v  -  y|_2$可以得知，这样$v$尽可能接近$y$。</p><p>我们将投影表示为$\operatorname{Proj}\left(y ;\left\{x_{1}, \ldots x_{n}\right\}\right)$，并且可以将其正式定义为:</p><script type="math/tex; mode=display">\operatorname{Proj}\left(y ;\left\{x_{1}, \ldots x_{n}\right\}\right)=\operatorname{argmin}_{v \in \operatorname{span}\left(\left\{x_{1}, \ldots, x_{n}\right\}\right)}\|y-v\|_{2}</script><p>矩阵$A\in \mathbb{R}^{m \times n}$的值域（有时也称为列空间），表示为$\mathcal{R}(A)$，是$A$列的跨度。换句话说，</p><script type="math/tex; mode=display">\mathcal{R}(A)=\left\{v \in \mathbb{R}^{m} : v=A x, x \in \mathbb{R}^{n}\right\}</script><p>做一些技术性的假设（即$A$是满秩且$n &lt;m$），向量$y \in \mathbb{R}^{m}$到$A$的范围的投影由下式给出:</p><script type="math/tex; mode=display">\operatorname{Proj}(y ; A)=\operatorname{argmin}_{v \in \mathcal{R}(A)}\|v-y\|_{2}=A\left(A^{T} A\right)^{-1} A^{T} y</script><p>这个最后的方程应该看起来非常熟悉，因为它几乎与我们在课程中（我们将很快再次得出）得到的公式：用于参数的最小二乘估计一样。 看一下投影的定义，显而易见，这实际上是我们在最小二乘问题中最小化的目标（除了范数的平方这里有点不一样，这不会影响找到最优解），所以这些问题自然是非常相关的。 </p><p>当$A$只包含一列时，$a \in \mathbb{R}^{m}$，这给出了向量投影到一条线上的特殊情况：</p><script type="math/tex; mode=display">\operatorname{Proj}(y ; a)=\frac{a a^{T}}{a^{T} a} y</script><p>一个矩阵$A\in \mathbb{R}^{m \times n}$的零空间 $\mathcal{N}(A)$ 是所有乘以$A$时等于0向量的集合，即：</p><script type="math/tex; mode=display">\mathcal{N}(A)=\left\{x \in \mathbb{R}^{n} : A x=0\right\}</script><p>注意，$\mathcal{R}(A)$中的向量的大小为$m$，而 $\mathcal{N}(A)$ 中的向量的大小为$n$，因此$\mathcal{R}(A^T)$和 $\mathcal{N}(A)$ 中的向量的大小均为$\mathbb{R}^{n}$。 事实上，还有很多例子。 证明：</p><script type="math/tex; mode=display">\left\{w : w=u+v, u \in \mathcal{R}\left(A^{T}\right), v \in \mathcal{N}(A)\right\}=\mathbb{R}^{n} \text { and } \mathcal{R}\left(A^{T}\right) \cap \mathcal{N}(A)=\{\mathbf{0}\}</script><p>换句话说，$\mathcal{R}(A^T)$和 $\mathcal{N}(A)$ 是不相交的子集，它们一起跨越$\mathbb{R}^{n}$的整个空间。 这种类型的集合称为<strong>正交补</strong>，我们用$\mathcal{R}(A^T)= \mathcal{N}(A)^{\perp}$表示。</p><h4 id="3-10-行列式"><a href="#3-10-行列式" class="headerlink" title="3.10 行列式"></a>3.10 行列式</h4><p>一个方阵$A  \in \mathbb{R}^{n \times n}$的行列式是函数$\text {det}$：$\mathbb{R}^{n \times n} \rightarrow \mathbb{R}^{n} $，并且表示为$\left| A \right|$。 或者$\text{det} A$（有点像迹运算符，我们通常省略括号）。 从代数的角度来说，我们可以写出一个关于$A$行列式的显式公式。 因此，我们首先提供行列式的几何解释，然后探讨它的一些特定的代数性质。</p><p>给定一个矩阵：</p><script type="math/tex; mode=display">\left[\begin{array}{cccc}{-} & {a_{1}^{T}}  & {-} \\ {-} & {a_{2}^{T}} & {-} \\ {} & {\vdots} & {} \\  {-} & {a_{n}^{T}} & {-}\end{array}\right]</script><p>考虑通过采用$A$行向量$a_{1}, \ldots a_{n}\in  \mathbb{R}^{n}$的所有可能线性组合形成的点$S \subset \mathbb{R}^{n}$的集合，其中线性组合的系数都在0和1之间; 也就是说，集合$S$是$\text{span}(\{a_{1}, \ldots a_{n}\})$受到系数$a_{1}, \ldots a_{n}$的限制的线性组合，$\alpha_1, \cdots ,\alpha_n$满足$0 \leq \alpha_{i} \leq 1, i=1, \ldots, n$。从形式上看，</p><script type="math/tex; mode=display">S=\left\{v \in \mathbb{R}^{n} : v=\sum_{i=1}^{n} \alpha_{i} a_{i} \text { where } 0 \leq \alpha_{i} \leq 1, i=1, \ldots, n\right\}</script><p>事实证明，$A$的行列式的绝对值是对集合$S$的“体积”的度量。</p><p>比方说：一个$2 \times2$的矩阵(4)：</p><script type="math/tex; mode=display">A=\left[\begin{array}{ll}{1} & {3} \\ {3} & {2}\end{array}\right]</script><p>它的矩阵的行是：</p><script type="math/tex; mode=display">a_{1}=\left[\begin{array}{l}{1} \\ {3}\end{array}\right] \quad a_{2}=\left[\begin{array}{l}{3} \\ {2}\end{array}\right]</script><p>对应于这些行对应的集合$S$如图1所示。对于二维矩阵，$S$通常具有平行四边形的形状。 在我们的例子中，行列式的值是$\left| A \right| = -7$（可以使用本节后面显示的公式计算），因此平行四边形的面积为7。（请自己验证！）</p><p>在三维中，集合$S$对应于一个称为平行六面体的对象（一个有倾斜边的三维框，这样每个面都有一个平行四边形）。行定义$S$的$3×3$矩阵S的行列式的绝对值给出了平行六面体的三维体积。在更高的维度中，集合$S$是一个称为$n$维平行切的对象。</p><p><img src="images/fig1.png" alt=""></p><p>图1：（4）中给出的$2×2$矩阵$A$的行列式的图示。 这里，$a_1$和$a_2$是对应于$A$行的向量，并且集合$S$对应于阴影区域（即，平行四边形）。 这个行列式的绝对值，$\left| \text{det} A \right| = 7$，即平行四边形的面积。</p><p>在代数上，行列式满足以下三个属性（所有其他属性都遵循这些属性，包括通用公式）：</p><ol><li><p>恒等式的行列式为1, $\left| I \right|= 1$（几何上，单位超立方体的体积为1）。</p></li><li><p>给定一个矩阵 $A  \in \mathbb{R}^{n \times n}$, 如果我们将$A$中的一行乘上一个标量$t  \in \mathbb{R}$，那么新矩阵的行列式是$t\left| A \right|$</p><script type="math/tex; mode=display">\left|\left[\begin{array}{ccc}{-} & {t a_{1}^{T}} & {-} \\ {-} & {a_{2}^{T}} & {-} \\ {} & {\vdots} & {} \\ {} & {a_{m}^{T}} & {-}\end{array}\right]\right|=t|A|</script><p>几何上，将集合$S$的一个边乘以系数$t$，体积也会增加一个系数$t$。</p></li><li><p>如果我们交换任意两行在$a_i^T$和$a_j^T$，那么新矩阵的行列式是$-\left| A \right|$，例如：</p><script type="math/tex; mode=display">\left|\left[\begin{array}{ccc}{-} & {a_{2}^{T}} & {-} \\ {-} & {a_{1}^{T}} & {-} \\ {} & {\vdots} & {} \\ {-} & {a_{m}^{T}} & {-}\end{array}\right]\right|=-|A|</script><p>你一定很奇怪，满足上述三个属性的函数的存在并不多。事实上，这样的函数确实存在，而且是唯一的（我们在这里不再证明了）。</p></li></ol><p>从上述三个属性中得出的几个属性包括：</p><ul><li>对于 $A  \in \mathbb{R}^{n \times n}$, $\left| A \right| = \left| A^T \right|$</li><li>对于 $A,B \in \mathbb{R}^{n \times n}$, $\left| AB \right|= \left| A \right|\left| B \right|$</li><li>对于 $A  \in \mathbb{R}^{n \times n}$, 有且只有当$A$是奇异的（比如不可逆） ，则：$\left| A \right|= 0$</li><li>对于 $A  \in \mathbb{R}^{n \times n}$ 同时，$A$为非奇异的，则：$\left| A ^{−1}\right| = 1/\left| A \right|$</li></ul><p>在给出行列式的一般定义之前，我们定义，对于$A  \in \mathbb{R}^{n \times n}$，$A_{\backslash i, \backslash j}\in \mathbb{R}^{(n-1) \times (n-1)}$是由于删除第$i$行和第$j$列而产生的矩阵。 行列式的一般（递归）公式是：</p><script type="math/tex; mode=display">\begin{aligned}|A| &=\sum_{i=1}^{n}(-1)^{i+j} a_{i j}\left|A_{\backslash i, \backslash j}\right| \quad(\text { for any } j \in 1, \ldots, n) \\ &=\sum_{j=1}^{n}(-1)^{i+j} a_{i j}\left|A_{\backslash i, \backslash j}\right| \quad(\text { for any } i \in 1, \ldots, n) \end{aligned}</script><p>对于 $A  \in \mathbb{R}^{1 \times 1}$，初始情况为$\left| A \right|= a_{11}$。如果我们把这个公式完全展开为 $A  \in \mathbb{R}^{n \times n}$，就等于$n!$（$n$阶乘）不同的项。因此，对于大于$3×3$的矩阵，我们几乎没有明确地写出完整的行列式方程。然而，$3×3$大小的矩阵的行列式方程是相当常见的，建议好好地了解它们：</p><script type="math/tex; mode=display">\left|\left[a_{11}\right]\right|=a_{11}</script><script type="math/tex; mode=display">\left|\left[\begin{array}{ll}{a_{11}} & {a_{12}} \\ {a_{21}} & {a_{22}}\end{array}\right]\right|=a_{11} a_{22}-a_{12} a_{21}</script><script type="math/tex; mode=display">\left|\left[\begin{array}{l}{a_{11}} & {a_{12}} & {a_{13}} \\ {a_{21}} & {a_{22}} & {a_{23}} \\ {a_{31}} & {a_{32}} & {a_{33}}\end{array}\right]\right|=\quad \begin{array}{c}{a_{11} a_{22} a_{33}+a_{12} a_{23} a_{31}+a_{13} a_{21} a_{32}} \\\quad \quad {-a_{11} a_{23} a_{32}-a_{12} a_{21} a_{33}-a_{13} a_{22} a_{31}} \\ {}\end{array}</script><p>矩阵$A  \in \mathbb{R}^{n \times n}$的经典伴随矩阵（通常称为伴随矩阵）表示为$\operatorname{adj}(A)$，并定义为：</p><script type="math/tex; mode=display">\operatorname{adj}(A) \in \mathbb{R}^{n \times n}, \quad(\operatorname{adj}(A))_{i j}=(-1)^{i+j}\left|A_{\backslash j, \backslash i}\right|</script><p>（注意索引$A_{\backslash j, \backslash i}$中的变化）。可以看出，对于任何非奇异$A  \in \mathbb{R}^{n \times n}$，</p><script type="math/tex; mode=display">A^{-1}=\frac{1}{|A|} \operatorname{adj}(A)</script><p>虽然这是一个很好的“显式”的逆矩阵公式，但我们应该注意，从数字上讲，有很多更有效的方法来计算逆矩阵。</p><h4 id="3-11-二次型和半正定矩阵"><a href="#3-11-二次型和半正定矩阵" class="headerlink" title="3.11 二次型和半正定矩阵"></a>3.11 二次型和半正定矩阵</h4><p>给定方矩阵$A  \in \mathbb{R}^{n \times n}$和向量$x \in \mathbb{R}^{n}$，标量值$x^T Ax$被称为二次型。 写得清楚些，我们可以看到：</p><script type="math/tex; mode=display">x^{T} A x=\sum_{i=1}^{n} x_{i}(A x)_{i}=\sum_{i=1}^{n} x_{i}\left(\sum_{j=1}^{n} A_{i j} x_{j}\right)=\sum_{i=1}^{n} \sum_{j=1}^{n} A_{i j} x_{i} x_{j}</script><p>注意：</p><script type="math/tex; mode=display">x^{T} A x=\left(x^{T} A x\right)^{T}=x^{T} A^{T} x=x^{T}\left(\frac{1}{2} A+\frac{1}{2} A^{T}\right) x</script><p>第一个等号的是因为是标量的转置与自身相等，而第二个等号是因为是我们平均两个本身相等的量。 由此，我们可以得出结论，只有$A$的对称部分有助于形成二次型。 出于这个原因，我们经常隐含地假设以二次型出现的矩阵是对称阵。<br>我们给出以下定义：</p><ul><li><p>对于所有非零向量$x \in \mathbb{R}^n$，$x^TAx&gt;0$，对称阵$A \in \mathbb{S}^n$为<strong>正定</strong>（<strong>positive definite,PD</strong>）。这通常表示为$A\succ0$（或$A&gt;0$），并且通常将所有正定矩阵的集合表示为$\mathbb{S}_{++}^n$。</p></li><li><p>对于所有向量$x^TAx\geq 0$，对称矩阵$A \in \mathbb{S}^n$是<strong>半正定</strong>(<strong>positive semidefinite ,PSD</strong>)。 这写为（或$A \succeq 0$仅$A≥0$），并且所有半正定矩阵的集合通常表示为$\mathbb{S}_+^n$。</p></li><li><p>同样，对称矩阵$A \in \mathbb{S}^n$是<strong>负定</strong>（<strong>negative definite,ND</strong>），如果对于所有非零$x \in \mathbb{R}^n$，则$x^TAx &lt;0$表示为$A\prec0$（或$A &lt;0$）。</p></li><li><p>类似地，对称矩阵$A \in \mathbb{S}^n$是<strong>半负定</strong>(<strong>negative semidefinite,NSD</strong>），如果对于所有$x \in \mathbb{R}^n$，则$x^TAx \leq 0$表示为$A\preceq 0$（或$A≤0$）。</p></li><li><p>最后，对称矩阵$A \in \mathbb{S}^n$是<strong>不定</strong>的，如果它既不是正半定也不是负半定，即，如果存在$x_1,x_2 \in \mathbb{R}^n$，那么$x_1^TAx_1&gt;0$且$x_2^TAx_2&lt;0$。</p></li></ul><p>很明显，如果$A$是正定的，那么$−A$是负定的，反之亦然。同样，如果$A$是半正定的，那么$−A$是是半负定的，反之亦然。如果果$A$是不定的，那么$−A$是也是不定的。</p><p>正定矩阵和负定矩阵的一个重要性质是它们总是满秩，因此是可逆的。为了了解这是为什么，假设某个矩阵$A \in \mathbb{S}^n$不是满秩。然后，假设$A$的第$j$列可以表示为其他$n-1$列的线性组合：</p><script type="math/tex; mode=display">a_{j}=\sum_{i \neq j} x_{i} a_{i}</script><p>对于某些$x_1,\cdots x_{j-1},x_{j + 1} ,\cdots ,x_n\in \mathbb{R}$。设$x_j = -1$，则：</p><script type="math/tex; mode=display">Ax=\sum_{i \neq j} x_{i} a_{i}=0</script><p>但这意味着对于某些非零向量$x$，$x^T Ax = 0$，因此$A$必须既不是正定也不是负定。如果$A$是正定或负定，则必须是满秩。<br>最后，有一种类型的正定矩阵经常出现，因此值得特别提及。 给定矩阵$A  \in \mathbb{R}^{m \times n}$（不一定是对称或偶数平方），矩阵$G = A^T A$（有时称为<strong>Gram矩阵</strong>）总是半正定的。 此外，如果$m\geq n$（同时为了方便起见，我们假设$A$是满秩），则$G = A^T A$是正定的。</p><h4 id="3-12-特征值和特征向量"><a href="#3-12-特征值和特征向量" class="headerlink" title="3.12 特征值和特征向量"></a>3.12 特征值和特征向量</h4><p>给定一个方阵$A \in\mathbb{R}^{n\times n}$，我们认为在以下条件下，$\lambda \in\mathbb{C}$是$A$的<strong>特征值</strong>，$x\in\mathbb{C}^n$是相应的<strong>特征向量</strong>：</p><script type="math/tex; mode=display">Ax=\lambda x,x \ne 0</script><p>直观地说，这个定义意味着将$A$乘以向量$x$会得到一个新的向量，该向量指向与$x$相同的方向，但按系数$\lambda$缩放。值得注意的是，对于任何特征向量$x\in\mathbb{C}^n$和标量$t\in\mathbb{C}$，$A(cx)=cAx=c\lambda x=\lambda(cx)$，$cx$也是一个特征向量。因此，当我们讨论与$\lambda$相关的<strong>特征向量</strong>时，我们通常假设特征向量被标准化为长度为1（这仍然会造成一些歧义，因为$x$和$−x$都是特征向量，但我们必须接受这一点）。</p><p>我们可以重写上面的等式来说明$(\lambda,x)$是$A$的特征值和特征向量的组合：</p><script type="math/tex; mode=display">(\lambda I-A)x=0,x \ne 0</script><p>但是$(\lambda I-A)x=0$只有当$(\lambda I-A)$有一个非空零空间时，同时$(\lambda I-A)$是奇异的，$x$才具有非零解，即：</p><script type="math/tex; mode=display">|(\lambda I-A)|=0</script><p>现在，我们可以使用行列式的先前定义将表达式$|(\lambda I-A)|$扩展为$\lambda$中的（非常大的）多项式，其中，$\lambda$的度为$n$。它通常被称为矩阵$A$的特征多项式。</p><p>然后我们找到这个特征多项式的$n$（可能是复数）根，并用$\lambda_1,\cdots,\lambda_n$表示。这些都是矩阵$A$的特征值，但我们注意到它们可能不明显。为了找到特征值$\lambda_i$对应的特征向量，我们只需解线性方程$(\lambda I-A)x=0$，因为$(\lambda I-A)$是奇异的，所以保证有一个非零解（但也可能有多个或无穷多个解）。</p><p>应该注意的是，这不是实际用于数值计算特征值和特征向量的方法（记住行列式的完全展开式有$n!$项），这是一个数学上的争议。</p><p>以下是特征值和特征向量的属性（所有假设在$A \in\mathbb{R}^{n\times n}$具有特征值$\lambda_1,\cdots,\lambda_n$的前提下）：</p><ul><li><p>$A$的迹等于其特征值之和</p><script type="math/tex; mode=display">\operatorname{tr} A=\sum_{i=1}^{n} \lambda_{i}</script></li><li><p>$A$的行列式等于其特征值的乘积</p><script type="math/tex; mode=display">|A|=\prod_{i=1}^{n} \lambda_{i}</script></li><li><p>$A$的秩等于$A$的非零特征值的个数</p></li><li><p>假设$A$非奇异，其特征值为$\lambda$和特征向量为$x$。那么$1/\lambda$是具有相关特征向量$x$的$A^{-1}$的特征值，即$A^{-1}x=(1/\lambda)x$。（要证明这一点，取特征向量方程，$Ax=\lambda x$，两边都左乘$A^{-1}$）</p></li><li><p>对角阵的特征值$d=diag(d_1，\cdots,d_n)$实际上就是对角元素$d_1，\cdots,d_n$</p></li></ul><h4 id="3-13-对称矩阵的特征值和特征向量"><a href="#3-13-对称矩阵的特征值和特征向量" class="headerlink" title="3.13 对称矩阵的特征值和特征向量"></a>3.13 对称矩阵的特征值和特征向量</h4><p>通常情况下，一般的方阵的特征值和特征向量的结构可以很细微地表示出来。<br>值得庆幸的是，在机器学习的大多数场景下，处理对称实矩阵就足够了，其处理的对称实矩阵的特征值和特征向量具有显着的特性。</p><p>在本节中，我们假设$A$是实对称矩阵, 具有以下属性：</p><ol><li><p>$A$的所有特征值都是实数。 我们用用$\lambda_1,\cdots,\lambda_n$表示。</p></li><li><p>存在一组特征向量$u_1，\cdots u_n$，对于所有$i$，$u_i$是具有特征值$\lambda_{i}$和$b$的特征向量。$u_1，\cdots u_n$是单位向量并且彼此正交。</p></li></ol><p>设$U$是包含$u_i$作为列的正交矩阵：</p><script type="math/tex; mode=display">U=\left[\begin{array}{cccc}{ |} & { |} & {} & { |} \\ {u_{1}} & {u_{2}} & {\cdots} & {u_{n}} \\ { |} & { |} & {} & { |}\end{array}\right]</script><p>设$\Lambda= diag(\lambda_1,\cdots,\lambda_n)$是包含$\lambda_1,\cdots,\lambda_n$作为对角线上的元素的对角矩阵。 使用2.3节的方程（2）中的矩阵 - 矩阵向量乘法的方法，我们可以验证：</p><script type="math/tex; mode=display">A U=\left[\begin{array}{cccc}{ |} & { |} & {} & { |} \\ {A u_{1}} & {A u_{2}} & {\cdots} & {A u_{n}} \\ { |} & { |} & {} & { |}\end{array}\right]=\left[\begin{array}{ccc}{ |} & { |} & { |} & { |}\\ {\lambda_{1} u_{1}} & {\lambda_{2} u_{2}} & {\cdots} & {\lambda_{n} u_{n}} \\ { |} & { |} & {|} & { |}\end{array}\right]=U \operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{n}\right)=U \Lambda</script><p>考虑到正交矩阵$U$满足$UU^T=I$，利用上面的方程，我们得到：</p><script type="math/tex; mode=display">A=AUU^T=U\Lambda U^T</script><p>这种$A$的新的表示形式为$U\Lambda U^T$，通常称为矩阵$A$的对角化。术语对角化是这样来的：通过这种表示，我们通常可以有效地将对称矩阵$A$视为对角矩阵 , 这更容易理解。关于由特征向量$U$定义的基础， 我们将通过几个例子详细说明。</p><p><strong>背景知识</strong>：代表另一个基的向量。</p><p>任何正交矩阵$U=\left[\begin{array}{cccc}{ |} &amp; { |} &amp; {} &amp; { |} \ {u_{1}} &amp; {u_{2}} &amp; {\cdots} &amp; {u_{n}} \ { |} &amp; { |} &amp; {} &amp; { |}\end{array}\right]$定义了一个新的属于$\mathbb {R}^{n}$的基（坐标系），意义如下：对于任何向量$x \in\mathbb{R}^{n}$都可以表示为$u_1，\cdots u_n$的线性组合，其系数为$x_1,\cdots x_n$：</p><script type="math/tex; mode=display">x=\hat x_1u_1+\cdots +\cdots \hat x_nu_n=U\hat x</script><p>在第二个等式中，我们使用矩阵和向量相乘的方法。 实际上，这种$\hat x$是唯一存在的:</p><script type="math/tex; mode=display">x=U \hat{x} \Leftrightarrow U^{T} x=\hat{x}</script><p>换句话说，向量$\hat x=U^Tx$可以作为向量$x$的另一种表示，与$U$定义的基有关。</p><p><strong>“对角化”矩阵向量乘法</strong>。 通过上面的设置，我们将看到左乘矩阵$A$可以被视为左乘以对角矩阵关于特征向量的基。 假设$x$是一个向量，$\hat x$表示$U$的基。设$z=Ax$为矩阵向量积。现在让我们计算关于$U$的基$z$：<br>然后，再利用$UU^T=U^T=I$和方程$A=AUU^T=U\Lambda U^T$，我们得到：</p><script type="math/tex; mode=display">\hat{z}=U^{T} z=U^{T} A x=U^{T} U \Lambda U^{T} x=\Lambda \hat{x}=\left[\begin{array}{c}{\lambda_{1} \hat{x}_{1}} \\ {\lambda_{2} \hat{x}_{2}} \\ {\vdots} \\ {\lambda_{n} \hat{x}_{n}}\end{array}\right]</script><p>我们可以看到，原始空间中的左乘矩阵$A$等于左乘对角矩阵$\Lambda$相对于新的基，即仅将每个坐标缩放相应的特征值。<br>在新的基上，矩阵多次相乘也变得简单多了。例如，假设$q=AAAx$。根据$A$的元素导出$q$的分析形式，使用原始的基可能是一场噩梦，但使用新的基就容易多了：</p><script type="math/tex; mode=display">\hat{q}=U^{T} q=U^{T} AAA x=U^{T} U \Lambda U^{T} U \Lambda U^{T} U \Lambda U^{T} x=\Lambda^{3} \hat{x}=\left[\begin{array}{c}{\lambda_{1}^{3} \hat{x}_{1}} \\ {\lambda_{2}^{3} \hat{x}_{2}} \\ {\vdots} \\ {\lambda_{n}^{3} \hat{x}_{n}}\end{array}\right]</script><p><strong>“对角化”二次型</strong>。作为直接的推论，二次型$x^TAx$也可以在新的基上简化。</p><script type="math/tex; mode=display">x^{T} A x=x^{T} U \Lambda U^{T} x=\hat{x} \Lambda \hat{x}=\sum_{i=1}^{n} \lambda_{i} \hat{x}_{i}^{2}</script><p>(回想一下，在旧的表示法中，$x^{T} A x=\sum_{i=1, j=1}^{n} x_{i} x_{j} A_{i j}$涉及一个$n^2$项的和，而不是上面等式中的$n$项。)利用这个观点，我们还可以证明矩阵$A$的正定性完全取决于其特征值的符号：</p><ol><li>如果所有的$\lambda_i&gt;0$，则矩阵$A$正定的，因为对于任意的$\hat x \ne 0$,$x^{T} A x=\sum_{i=1}^{n} \lambda_{i} \hat{x}_{i}^{2}&gt;0$</li><li>如果所有的$\lambda_i\geq 0$，则矩阵$A$是为正半定，因为对于任意的$\hat x $,$x^{T} A x=\sum_{i=1}^{n} \lambda_{i} \hat{x}_{i}^{2} \geq 0$</li><li>同样，如果所有$\lambda_i&lt;0$或$\lambda_i\leq 0$，则矩阵$A$分别为负定或半负定。</li><li>最后，如果$A$同时具有正特征值和负特征值，比如λ$\lambda_i&gt;0$和$\lambda_j<0$，那么它是不定的。这是因为如果我们让$\hat x$满足$\hat x_i=1$和$\hat x_k=0$，同时所有的$k\ne i$，那么$x^{T} A x=\sum_{i=1}^{n} \lambda_{i} \hat{x}_{i}^{2}>0$ ,我们让$\hat x$满足$\hat x_i=1$和$\hat x_k=0$，同时所有的$k\ne i$，那么$x^{T} A x=\sum_{i=1}^{n} \lambda_{i} \hat{x}_{i}^{2}&lt;0$ </li></ol><p>特征值和特征向量经常出现的应用是最大化矩阵的某些函数。特别是对于矩阵$A \in \mathbb{S}^{n}$，考虑以下最大化问题：</p><script type="math/tex; mode=display">\max _{x \in \mathbb{R}^{n}} \ x^{T} A x=\sum_{i=1}^{n} \lambda_{i} \hat{x}_{i}^{2} \quad \text { subject to }\|x\|_{2}^{2}=1</script><p>也就是说，我们要找到（范数1）的向量，它使二次型最大化。假设特征值的阶数为$\lambda_1 \geq \lambda _2 \geq \cdots \lambda_n$，此优化问题的最优值为$\lambda_1$，且与$\lambda_1$对应的任何特征向量$u_1$都是最大值之一。（如果$\lambda_1 &gt; \lambda_2$，那么有一个与特征值$\lambda_1$对应的唯一特征向量，它是上面那个优化问题的唯一最大值。）<br>我们可以通过使用对角化技术来证明这一点：注意，通过公式$|U x|_{2}=|x|_{2}$推出$|x|_{2}=|\hat{x}|_{2}$，并利用公式：</p><p>$x^{T} A x=x^{T} U \Lambda U^{T} x=\hat{x} \Lambda \hat{x}=\sum_{i=1}^{n} \lambda_{i} \hat{x}_{i}^{2}$，我们可以将上面那个优化问题改写为：</p><script type="math/tex; mode=display">\max _{\hat{x} \in \mathbb{R}^{n}}\ \hat{x}^{T} \Lambda \hat{x}=\sum_{i=1}^{n} \lambda_{i} \hat{x}_{i}^{2} \quad \text { subject to }\|\hat{x}\|_{2}^{2}=1</script><p>然后，我们得到目标的上界为$\lambda_1$：</p><script type="math/tex; mode=display">\hat{x}^{T} \Lambda \hat{x}=\sum_{i=1}^{n} \lambda_{i} \hat{x}_{i}^{2} \leq \sum_{i=1}^{n} \lambda_{1} \hat{x}_{i}^{2}=\lambda_{1}</script><p>此外，设置$\hat{x}=\left[\begin{array}{c}{1} \ {0} \ {\vdots} \ {0}\end{array}\right]$可让上述等式成立，这与设置$x=u_1$相对应。</p><h3 id="4-矩阵微积分"><a href="#4-矩阵微积分" class="headerlink" title="4.矩阵微积分"></a>4.矩阵微积分</h3><p>虽然前面章节中的主题通常包含在线性代数的标准课程中，但似乎很少涉及（我们将广泛使用）的一个主题是微积分扩展到向量设置展。尽管我们使用的所有实际微积分都是相对微不足道的，但是符号通常会使事情看起来比实际困难得多。 在本节中，我们将介绍矩阵微积分的一些基本定义，并提供一些示例。</p><h4 id="4-1-梯度"><a href="#4-1-梯度" class="headerlink" title="4.1 梯度"></a>4.1 梯度</h4><p>假设$f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$是将维度为$m \times n$的矩阵$A\in \mathbb{R}^{m \times n}$作为输入并返回实数值的函数。 然后$f$的梯度（相对于$A\in \mathbb{R}^{m \times n}$）是偏导数矩阵，定义如下：</p><script type="math/tex; mode=display">\nabla_{A} f(A) \in \mathbb{R}^{m \times n}=\left[\begin{array}{cccc}{\frac{\partial f(A)}{\partial A_{11}}} & {\frac{\partial f(A)}{\partial A_{12}}} & {\cdots} & {\frac{\partial f(A)}{\partial A_{1n}}} \\ {\frac{\partial f(A)}{\partial A_{21}}} & {\frac{\partial f(A)}{\partial A_{22}}} & {\cdots} & {\frac{\partial f(A)}{\partial A_{2 n}}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial f(A)}{\partial A_{m 1}}} & {\frac{\partial f(A)}{\partial A_{m 2}}} & {\cdots} & {\frac{\partial f(A)}{\partial A_{m n}}}\end{array}\right]</script><p>即，$m \times n$矩阵:</p><script type="math/tex; mode=display">\left(\nabla_{A} f(A)\right)_{i j}=\frac{\partial f(A)}{\partial A_{i j}}</script><p>请注意，$\nabla_{A} f(A) $的维度始终与$A$的维度相同。特殊情况，如果$A$只是向量$A\in \mathbb{R}^{n}$，则</p><script type="math/tex; mode=display">\nabla_{x} f(x)=\left[\begin{array}{c}{\frac{\partial f(x)}{\partial x_{1}}} \\ {\frac{\partial f(x)}{\partial x_{2}}} \\ {\vdots} \\ {\frac{\partial f(x)}{\partial x_{n}}}\end{array}\right]</script><p>重要的是要记住，只有当函数是实值时，即如果函数返回标量值，才定义函数的梯度。例如，$A\in \mathbb{R}^{m \times n}$相对于$x$，我们不能取$Ax$的梯度，因为这个量是向量值。<br>它直接从偏导数的等价性质得出：</p><ul><li><p>$\nabla_{x}(f(x)+g(x))=\nabla_{x} f(x)+\nabla_{x} g(x)$</p></li><li><p>对于$t \in \mathbb{R}$ ，$\nabla_{x}(t f(x))=t \nabla_{x} f(x)$</p></li></ul><p>原则上，梯度是偏导数对多变量函数的自然延伸。然而，在实践中，由于符号的原因，使用梯度有时是很困难的。例如，假设$A\in \mathbb{R}^{m \times n}$是一个固定系数矩阵，假设$b\in \mathbb{R}^{m}$是一个固定系数向量。设$f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$为$f(z)=z^Tz$定义的函数，因此$\nabla_{z}f(z)=2z$。但现在考虑表达式，</p><script type="math/tex; mode=display">\nabla f(Ax)</script><p>该表达式应该如何解释？ 至少有两种可能性：<br>1.在第一个解释中，回想起$\nabla_{z}f(z)=2z$。 在这里，我们将$\nabla f(Ax)$解释为评估点$Ax$处的梯度，因此:</p><script type="math/tex; mode=display">\nabla f(A x)=2(A x)=2 A x \in \mathbb{R}^{m}</script><p>2.在第二种解释中，我们将数量$f(Ax)$视为输入变量$x$的函数。 更正式地说，设$g(x) =f(Ax)$。 然后在这个解释中:</p><script type="math/tex; mode=display">\nabla f(A x)=\nabla_{x} g(x) \in \mathbb{R}^{n}</script><p>在这里，我们可以看到这两种解释确实不同。 一种解释产生$m$维向量作为结果，而另一种解释产生$n$维向量作为结果！ 我们怎么解决这个问题？</p><p>这里，关键是要明确我们要区分的变量。<br>在第一种情况下，我们将函数$f$与其参数$z$进行区分，然后替换参数$Ax$。<br>在第二种情况下，我们将复合函数$g(x)=f(Ax)$直接与$x$进行微分。</p><p>我们将第一种情况表示为$\nabla zf(Ax)$，第二种情况表示为$\nabla xf(Ax)$。</p><p>保持符号清晰是非常重要的，以后完成课程作业时候你就会发现。</p><h4 id="4-2-黑塞矩阵"><a href="#4-2-黑塞矩阵" class="headerlink" title="4.2 黑塞矩阵"></a>4.2 黑塞矩阵</h4><p>假设$f: \mathbb{R}^{n} \rightarrow \mathbb{R}$是一个函数，它接受$\mathbb{R}^{n}$中的向量并返回实数。那么关于$x$的<strong>黑塞矩阵</strong>（也有翻译作海森矩阵），写做：$\nabla_x ^2 f(A x)$，或者简单地说，$H$是$n \times n$矩阵的偏导数：</p><script type="math/tex; mode=display">\nabla_{x}^{2} f(x) \in \mathbb{R}^{n \times n}=\left[\begin{array}{cccc}{\frac{\partial^{2} f(x)}{\partial x_{1}^{2}}} & {\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{1}}} & {\frac{\partial^{2} f(x)}{\partial x_{2}^{2}}} & {\cdots} & {\frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{n}}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial^{2} f(x)}{\partial x_{n} \partial x_{1}}} & {\frac{\partial^{2} f(x)}{\partial x_{n} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f(x)}{\partial x_{n}^{2}}}\end{array}\right]</script><p>换句话说，$\nabla_{x}^{2} f(x) \in \mathbb{R}^{n \times n}$，其：</p><script type="math/tex; mode=display">\left(\nabla_{x}^{2} f(x)\right)_{i j}=\frac{\partial^{2} f(x)}{\partial x_{i} \partial x_{j}}</script><p>注意：黑塞矩阵通常是对称阵：</p><script type="math/tex; mode=display">\frac{\partial^{2} f(x)}{\partial x_{i} \partial x_{j}}=\frac{\partial^{2} f(x)}{\partial x_{j} \partial x_{i}}</script><p>与梯度相似，只有当$f(x)$为实值时才定义黑塞矩阵。</p><p>很自然地认为梯度与向量函数的一阶导数的相似，而黑塞矩阵与二阶导数的相似（我们使用的符号也暗示了这种关系）。 这种直觉通常是正确的，但需要记住以下几个注意事项。<br>首先，对于一个变量$f: \mathbb{R} \rightarrow \mathbb{R}$的实值函数，它的基本定义：二阶导数是一阶导数的导数，即：</p><script type="math/tex; mode=display">\frac{\partial^{2} f(x)}{\partial x^{2}}=\frac{\partial}{\partial x} \frac{\partial}{\partial x} f(x)</script><p>然而，对于向量的函数，函数的梯度是一个向量，我们不能取向量的梯度，即:</p><script type="math/tex; mode=display">\nabla_{x} \nabla_{x} f(x)=\nabla_{x}\left[\begin{array}{c}{\frac{\partial f(x)}{\partial x_{1}}} \\ {\frac{\partial f(x)}{\partial x_{2}}} \\ {\vdots} \\ {\frac{\partial f(x)}{\partial x_{n}}}\end{array}\right]</script><p>上面这个表达式没有意义。 因此，黑塞矩阵不是梯度的梯度。 然而，下面这种情况却这几乎是正确的：如果我们看一下梯度$\left(\nabla_{x} f(x)\right)_{i}=\partial f(x) / \partial x_{i}$的第$i$个元素，并取关于于$x$的梯度我们得到：</p><script type="math/tex; mode=display">\nabla_{x} \frac{\partial f(x)}{\partial x_{i}}=\left[\begin{array}{c}{\frac{\partial^{2} f(x)}{\partial x_{i} \partial x_{1}}} \\ {\frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{2}}} \\ {\vdots} \\ {\frac{\partial f(x)}{\partial x_{i} \partial x_{n}}}\end{array}\right]</script><p>这是黑塞矩阵第$i$行（列）,所以：</p><script type="math/tex; mode=display">\nabla_{x}^{2} f(x)=\left[\nabla_{x}\left(\nabla_{x} f(x)\right)_{1} \quad \nabla_{x}\left(\nabla_{x} f(x)\right)_{2} \quad \cdots \quad \nabla_{x}\left(\nabla_{x} f(x)\right)_{n}\right]</script><p>简单地说：我们可以说由于：$\nabla_{x}^{2} f(x)=\nabla_{x}\left(\nabla_{x} f(x)\right)^{T}$，只要我们理解，这实际上是取$\nabla_{x} f(x)$的每个元素的梯度，而不是整个向量的梯度。</p><p>最后，请注意，虽然我们可以对矩阵$A\in \mathbb{R}^{n}$取梯度，但对于这门课，我们只考虑对向量$x \in \mathbb{R}^{n}$取黑塞矩阵。<br>这会方便很多（事实上，我们所做的任何计算都不要求我们找到关于矩阵的黑森方程），因为关于矩阵的黑塞方程就必须对矩阵所有元素求偏导数$\partial^{2} f(A) /\left(\partial A_{i j} \partial A_{k \ell}\right)$，将其表示为矩阵相当麻烦。</p><h4 id="4-3-二次函数和线性函数的梯度和黑塞矩阵"><a href="#4-3-二次函数和线性函数的梯度和黑塞矩阵" class="headerlink" title="4.3 二次函数和线性函数的梯度和黑塞矩阵"></a>4.3 二次函数和线性函数的梯度和黑塞矩阵</h4><p>现在让我们尝试确定几个简单函数的梯度和黑塞矩阵。 应该注意的是，这里给出的所有梯度都是<strong>CS229</strong>讲义中给出的梯度的特殊情况。</p><p>对于$x \in \mathbb{R}^{n}$, 设$f(x)=b^Tx$  的某些已知向量$b \in \mathbb{R}^{n}$ ，则：</p><script type="math/tex; mode=display">f(x)=\sum_{i=1}^{n} b_{i} x_{i}</script><p>所以：</p><script type="math/tex; mode=display">\frac{\partial f(x)}{\partial x_{k}}=\frac{\partial}{\partial x_{k}} \sum_{i=1}^{n} b_{i} x_{i}=b_{k}</script><p>由此我们可以很容易地看出$\nabla_{x} b^{T} x=b$。 这应该与单变量微积分中的类似情况进行比较，其中$\partial /(\partial x) a x=a$。<br>现在考虑$A\in \mathbb{S}^{n}$的二次函数$f(x)=x^TAx$。 记住这一点：</p><script type="math/tex; mode=display">f(x)=\sum_{i=1}^{n} \sum_{j=1}^{n} A_{i j} x_{i} x_{j}</script><p>为了取偏导数，我们将分别考虑包括$x_k$和$x_2^k$因子的项：</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial f(x)}{\partial x_{k}} &=\frac{\partial}{\partial x_{k}} \sum_{i=1}^{n} \sum_{j=1}^{n} A_{i j} x_{i} x_{j} \\ &=\frac{\partial}{\partial x_{k}}\left[\sum_{i \neq k} \sum_{j \neq k} A_{i j} x_{i} x_{j}+\sum_{i \neq k} A_{i k} x_{i} x_{k}+\sum_{j \neq k} A_{k j} x_{k} x_{j}+A_{k k} x_{k}^{2}\right] \\ &=\sum_{i \neq k} A_{i k} x_{i}+\sum_{j \neq k} A_{k j} x_{j}+2 A_{k k} x_{k} \\ &=\sum_{i=1}^{n} A_{i k} x_{i}+\sum_{j=1}^{n} A_{k j} x_{j}=2 \sum_{i=1}^{n} A_{k i} x_{i} \end{aligned}</script><p>最后一个等式，是因为$A$是对称的（我们可以安全地假设，因为它以二次形式出现）。 注意，$\nabla_{x} f(x)$的第$k$个元素是$A$和$x$的第$k$行的内积。 因此，$\nabla_{x} x^{T} A x=2 A x$。 同样，这应该提醒你单变量微积分中的类似事实，即$\partial /(\partial x) a x^{2}=2 a x$。</p><p>最后，让我们来看看二次函数$f(x)=x^TAx$黑塞矩阵（显然，线性函数$b^Tx$的黑塞矩阵为零）。在这种情况下:</p><script type="math/tex; mode=display">\frac{\partial^{2} f(x)}{\partial x_{k} \partial x_{\ell}}=\frac{\partial}{\partial x_{k}}\left[\frac{\partial f(x)}{\partial x_{\ell}}\right]=\frac{\partial}{\partial x_{k}}\left[2 \sum_{i=1}^{n} A_{\ell i} x_{i}\right]=2 A_{\ell k}=2 A_{k \ell}</script><p>因此，应该很清楚$\nabla_{x}^2 x^{T} A x=2 A$，这应该是完全可以理解的（同样类似于$\partial^2 /(\partial x^2) a x^{2}=2a$的单变量事实）。</p><p>简要概括起来：</p><ul><li><p>$\nabla_{x} b^{T} x=b$ </p></li><li><p>$\nabla_{x} x^{T} A x=2 A x$ (如果$A$是对称阵)</p></li><li><p>$\nabla_{x}^2 x^{T} A x=2 A $  (如果$A$是对称阵)</p></li></ul><h4 id="4-4-最小二乘法"><a href="#4-4-最小二乘法" class="headerlink" title="4.4 最小二乘法"></a>4.4 最小二乘法</h4><p>让我们应用上一节中得到的方程来推导最小二乘方程。假设我们得到矩阵$A\in \mathbb{R}^{m \times n}$（为了简单起见，我们假设$A$是满秩）和向量$b\in \mathbb{R}^{m}$，从而使$b \notin \mathcal{R}(A)$。在这种情况下，我们将无法找到向量$x\in \mathbb{R}^{n}$，由于$Ax = b$，因此我们想要找到一个向量$x$，使得$Ax$尽可能接近 $b$，用欧几里德范数的平方$|A x-b|_{2}^{2} $来衡量。</p><p>使用公式$|x|^{2}=x^Tx$，我们可以得到：</p><script type="math/tex; mode=display">\begin{aligned}\|A x-b\|_{2}^{2} &=(A x-b)^{T}(A x-b) \\ &=x^{T} A^{T} A x-2 b^{T} A x+b^{T} b \end{aligned}</script><p>根据$x$的梯度，并利用上一节中推导的性质：</p><script type="math/tex; mode=display">\begin{aligned} \nabla_{x}\left(x^{T} A^{T} A x-2 b^{T} A x+b^{T} b\right) &=\nabla_{x} x^{T} A^{T} A x-\nabla_{x} 2 b^{T} A x+\nabla_{x} b^{T} b \\ &=2 A^{T} A x-2 A^{T} b \end{aligned}</script><p>将最后一个表达式设置为零，然后解出$x$，得到了正规方程：</p><script type="math/tex; mode=display">x = (A^TA)^{-1}A^Tb</script><p>这和我们在课堂上得到的相同。</p><h4 id="4-5-行列式的梯度"><a href="#4-5-行列式的梯度" class="headerlink" title="4.5 行列式的梯度"></a>4.5 行列式的梯度</h4><p>现在让我们考虑一种情况，我们找到一个函数相对于矩阵的梯度，也就是说，对于$A\in \mathbb{R}^{n \times n}$，我们要找到$\nabla_{A}|A|$。回想一下我们对行列式的讨论：</p><script type="math/tex; mode=display">|A|=\sum_{i=1}^{n}(-1)^{i+j} A_{i j}\left|A_{\backslash i, \backslash j}\right| \quad(\text { for any } j \in 1, \ldots, n)</script><p>所以：</p><script type="math/tex; mode=display">\frac{\partial}{\partial A_{k \ell}}|A|=\frac{\partial}{\partial A_{k \ell}} \sum_{i=1}^{n}(-1)^{i+j} A_{i j}\left|A_{\backslash i, \backslash j}\right|=(-1)^{k+\ell}\left|A_{\backslash k,\backslash \ell}\right|=(\operatorname{adj}(A))_{\ell k}</script><p>从这里可以知道，它直接从伴随矩阵的性质得出：</p><script type="math/tex; mode=display">\nabla_{A}|A|=(\operatorname{adj}(A))^{T}=|A| A^{-T}</script><p>现在我们来考虑函数$f : \mathbb{S}_{++}^{n} \rightarrow \mathbb{R}$，$f(A)=\log |A|$。注意，我们必须将$f$的域限制为正定矩阵，因为这确保了$|A|&gt;0$，因此$|A|$的对数是实数。在这种情况下，我们可以使用链式法则（没什么奇怪的，只是单变量演算中的普通链式法则）来看看：</p><script type="math/tex; mode=display">\frac{\partial \log |A|}{\partial A_{i j}}=\frac{\partial \log |A|}{\partial|A|} \frac{\partial|A|}{\partial A_{i j}}=\frac{1}{|A|} \frac{\partial|A|}{\partial A_{i j}}</script><p>从这一点可以明显看出：</p><script type="math/tex; mode=display">\nabla_{A} \log |A|=\frac{1}{|A|} \nabla_{A}|A|=A^{-1}</script><p>我们可以在最后一个表达式中删除转置，因为$A$是对称的。注意与单值情况的相似性，其中$\partial /(\partial x) \log x=1 / x$。</p><h4 id="4-6-特征值优化"><a href="#4-6-特征值优化" class="headerlink" title="4.6 特征值优化"></a>4.6 特征值优化</h4><p>最后，我们使用矩阵演算以直接导致特征值/特征向量分析的方式求解优化问题。 考虑以下等式约束优化问题：</p><script type="math/tex; mode=display">\max _{x \in \mathbb{R}^{n}} x^{T} A x \quad \text { subject to }\|x\|_{2}^{2}=1</script><p>对于对称矩阵$A\in \mathbb{S}^{n}$。求解等式约束优化问题的标准方法是采用<strong>拉格朗日</strong>形式，一种包含等式约束的目标函数，在这种情况下，拉格朗日函数可由以下公式给出：</p><script type="math/tex; mode=display">\mathcal{L}(x, \lambda)=x^{T} A x-\lambda x^{T} x</script><p>其中，$\lambda $被称为与等式约束关联的拉格朗日乘子。可以确定，要使$x^<em>$成为问题的最佳点，拉格朗日的梯度必须在$x^</em>$处为零（这不是唯一的条件，但它是必需的）。也就是说，</p><script type="math/tex; mode=display">\nabla_{x} \mathcal{L}(x, \lambda)=\nabla_{x}\left(x^{T} A x-\lambda x^{T} x\right)=2 A^{T} x-2 \lambda x=0</script><p>请注意，这只是线性方程$Ax =\lambda x$。 这表明假设$x^T x = 1$，可能最大化（或最小化）$x^T Ax$的唯一点是$A$的特征向量。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;线性代数复习和参考&quot;&gt;&lt;a href=&quot;#线性代数复习和参考&quot; class=&quot;headerlink&quot; title=&quot;线性代数复习和参考&quot;&gt;&lt;/a&gt;线性代数复习和参考&lt;/h2&gt;&lt;h3 id=&quot;1-基础概念和符号&quot;&gt;&lt;a href=&quot;#1-基础概念和符号&quot; class</summary>
      
    
    
    
    <category term="机器学习" scheme="https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习基础" scheme="https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
</feed>
