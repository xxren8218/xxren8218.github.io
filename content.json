{"meta":{"title":"X.X.Ren","subtitle":"个人博客","description":"自学计算机的经历","author":"任晓雄","url":"https://xxren8218.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-02-28T16:13:27.840Z","updated":"2021-02-28T10:34:55.359Z","comments":false,"path":"/404.html","permalink":"https://xxren8218.github.io/404.html","excerpt":"","text":""},{"title":"书单","date":"2021-02-28T16:13:27.852Z","updated":"2021-02-28T10:34:55.360Z","comments":false,"path":"books/index.html","permalink":"https://xxren8218.github.io/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2021-02-28T16:13:27.859Z","updated":"2021-02-28T10:34:55.361Z","comments":false,"path":"categories/index.html","permalink":"https://xxren8218.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-02-28T16:13:27.865Z","updated":"2021-02-28T10:34:55.361Z","comments":true,"path":"links/index.html","permalink":"https://xxren8218.github.io/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-02-28T16:13:27.877Z","updated":"2021-02-28T10:34:55.361Z","comments":false,"path":"tags/index.html","permalink":"https://xxren8218.github.io/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-02-28T16:13:27.871Z","updated":"2021-02-28T10:34:55.361Z","comments":false,"path":"repository/index.html","permalink":"https://xxren8218.github.io/repository/index.html","excerpt":"","text":""},{"title":"","date":"2021-03-01T16:55:45.748Z","updated":"2019-07-29T13:52:40.000Z","comments":true,"path":"about/502.html","permalink":"https://xxren8218.github.io/about/502.html","excerpt":"","text":"502 Bad Gateway html, body { padding: 0; margin: 0; width: 100%; height: 100%; } .icon { -webkit-user-select: none; user-select: none; display: inline-block; } .icon-offline { content: url(\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEgAAABIAQMAAABvIyEEAAAABlBMVEUAAABTU1OoaSf/AAAAAXRSTlMAQObYZgAAAGxJREFUeF7tyMEJwkAQRuFf5ipMKxYQiJ3Z2nSwrWwBA0+DQZcdxEOueaePp9+dQZFB7GpUcURSVU66yVNFj6LFICatThZB6r/ko/pbRpUgilY0Cbw5sNmb9txGXUKyuH7eV25x39DtJXUNPQGJtWFV+BT/QAAAAABJRU5ErkJggg==\"); position: relative; } .hidden { display: none; } /* Offline page */ .offline .interstitial-wrapper { color: #2b2b2b; font-size: 1em; line-height: 1.55; margin: 0 auto; max-width: 600px; padding-top: 100px; width: 100%; } .offline .runner-container { height: 150px; max-width: 600px; overflow: hidden; position: absolute; top: 35px; width: 44px; } .offline .runner-canvas { height: 150px; max-width: 600px; opacity: 1; overflow: hidden; position: absolute; top: 0; z-index: 2; } .offline .controller { background: rgba(247, 247, 247, .1); height: 100vh; left: 0; position: absolute; top: 0; width: 100vw; z-index: 1; } #offline-resources { display: none; } #message h1 { font-size: 34px; color: #555; margin: 100px 0px 40px 0px; } #message p { color: #555; padding: 5px 0px; line-height: 36px; letter-spacing: 1px; font-size: 18px; } #message p a, p a:hover { color: #777; padding: 0px 8px; } #message p span { color: #777; margin: 0px 8px; padding: 1px 2px; background: #eee; border-radius: 0.3em; } #github { position: absolute; bottom: 0px; font-size: 12px !important; color: #aaa !important; } @media (max-width: 420px) { .suggested-left > #control-buttons, .suggested-right > #control-buttons { float: none; } .snackbar { left: 0; bottom: 0; width: 100%; border-radius: 0; } #message h1 { font-size: 22px; margin: 100px 20px 50px 20px; } #message p { padding: 20px; } } @media (max-height: 350px) { h1 { margin: 0 0 15px; } .icon-offline { margin: 0 0 10px; } .interstitial-wrapper { margin-top: 5%; } .nav-wrapper { margin-top: 30px; } } @media (min-width: 600px) and (max-width: 736px) and (orientation: landscape) { .offline .interstitial-wrapper { margin-left: 0; margin-right: 0; } } @media (min-width: 420px) and (max-width: 736px) and (min-height: 240px) and (max-height: 420px) and (orientation: landscape) { .interstitial-wrapper { margin-bottom: 100px; } } @media (min-height: 240px) and (orientation: landscape) { .offline .interstitial-wrapper { margin-bottom: 90px; } .icon-offline { margin-bottom: 20px; } } @media (max-height: 320px) and (orientation: landscape) { .icon-offline { margin-bottom: 0; } .offline .runner-container { top: 10px; } } @media (max-width: 240px) { .interstitial-wrapper { overflow: inherit; padding: 0 8px; } } eval(function(p,a,c,k,e,r){e=function(c){return(c35?String.fromCharCode(c+29):c.toString(36))};if(!''.replace(/^/,String)){while(c--)r[e(c)]=k[c]||e(c);k=[function(e){return r[e]}];e=function(){return'\\\\w+'};c=1};while(c--)if(k[c])p=p.replace(new RegExp('\\\\b'+e(c)+'\\\\b','g'),k[c]);return p}('(j(){j c(a,b){V(c.4B)T c.4B;c.4B=i;i.1V=M.3t(a);i.8X=i.U=1t;i.6r=i.1V.3t(\"#8U-4v\");i.o=b||c.o;i.s=c.4p;i.1A=i.I=i.C=i.K=1t;i.28=i.1Q=i.2y=i.1N=0;i.1g=1F/60;i.1s=i.o.2M;i.1f=[];i.2l=i.2r=i.1a=i.1i=i.2s=!1;i.1M=0;i.2w=1t;i.3N=0;i.8T=1t;i.2j={};i.2K=1t;i.8R={};i.8Q=0;i.4L()?i.4M():i.4X()}j p(a,b){T N.2I(N.2C()*(b-a+1))+a}j z(a){B b=a.W/4*3;a=8P(a);B c=F 8M(b);c=F 8L(c);Y(B e=0;ei.s.A?(a=b*i.s.A/3A*i.o.6v,i.1s=a>b?b:a):a&&(i.1s=a)},1c:j(){M.3t(\".\"+c.1h.4V).1n.7b=\"5A\";i.43();i.3a();i.U=M.2D(\"48\");i.U.3s=c.1h.4P;B a=i.U,b=i.s.A,f=i.s.J,e=c.1h.7a,d=M.2D(\"K\");d.3s=e?c.1h.4d+\" \"+e:c.1h.4d;d.D=b;d.S=f;a.2A(d);i.K=d;i.C=i.K.1D(\"2d\");i.C.79=\"#77\";i.C.76();c.4a(i.K);i.2g=F u(i.K,i.2h,i.s,i.o.5M);i.1A=F k(i.K,i.2h.37,i.s.A);i.I=F g(i.K,i.2h.3R);i.1V.2A(i.U);r&&i.5I();i.5J();i.G();1k.Z(c.L.5y,i.5L.1v(i))},5I:j(){i.1U=M.2D(\"48\");i.1U.3s=c.1h.58;i.1V.2A(i.1U)},5L:j(){i.2w||(i.2w=74(i.43.1v(i),5O))},43:j(){6Y(i.2w);i.2w=1t;B a=1k.6X(i.1V);a=6O(a.5S.2X(0,a.5S.W-2));i.s.A=i.1V.6N-2*a;i.K&&(i.K.D=i.s.A,i.K.S=i.s.J,c.4a(i.K),i.1A.4e(i.s.A),i.3u(),i.2g.G(0,0,!0),i.I.G(0),i.1i||i.1a||i.2r?(i.U.1n.D=i.s.A+\"1z\",i.U.1n.S=i.s.J+\"1z\",i.1A.G(0,N.2c(i.1N)),i.3l()):i.I.Q(0,0),i.1a&&i.2a&&(i.2a.5Y(i.s.A),i.2a.Q()))},5Z:j(){V(i.2s||i.1a)i.1a&&i.3k();3h{i.1X=!0;i.I.1X=!0;B a=\"@-6L-6J 66 { 6I { D:\"+g.o.A+\"1z }6H { D: \"+i.s.A+\"1z }}\",b=M.2D(\"1n\");b.6G=a;M.6F.2A(b);i.U.Z(c.L.6n,i.6a.1v(i));i.U.1n.6b=\"66 .4s 6D-6C 1 6A\";i.U.1n.D=i.s.A+\"1z\";i.2s=i.1i=!0}},6a:j(){i.28=0;i.1X=!1;i.I.1X=!1;i.U.1n.6b=\"\";i.3N++;M.Z(c.L.4O,i.2N.1v(i));1k.Z(c.L.4Q,i.2N.1v(i));1k.Z(c.L.4S,i.2N.1v(i))},3u:j(){i.C.6y(0,0,i.s.A,i.s.J)},G:j(){i.4u=!1;B a=q(),b=a-(i.1Q||a);i.1Q=a;V(i.1i){i.3u();i.I.1H&&i.I.6i(b);i.28+=b;a=i.28>i.o.5D;1!=i.I.33||i.1X||i.5Z();i.1X?i.2g.G(0,i.1s,a):(b=i.2s?b:0,i.2g.G(b,i.1s,a,i.2l));V(a)a:{B f=i.2g.1f[0],e=i.I;a=F h(e.E+1,e.H+1,e.o.A-2,e.o.J-2);B d=F h(f.E+1,f.H+1,f.R.D*f.1r-2,f.R.S-2);V(y(a,d)){f=f.X;e=e.1u?g.X.1J:g.X.1I;Y(B w=0;w"},{"title":"关于","date":"2021-03-02T09:40:36.171Z","updated":"2021-03-02T09:40:36.171Z","comments":false,"path":"about/index.html","permalink":"https://xxren8218.github.io/about/index.html","excerpt":"","text":"个人简介： game html, body { padding: 0; margin: 0; width: 100%; height: 100%; } .icon { -webkit-user-select: none; user-select: none; display: inline-block; } .icon-offline { content: url(\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEgAAABIAQMAAABvIyEEAAAABlBMVEUAAABTU1OoaSf/AAAAAXRSTlMAQObYZgAAAGxJREFUeF7tyMEJwkAQRuFf5ipMKxYQiJ3Z2nSwrWwBA0+DQZcdxEOueaePp9+dQZFB7GpUcURSVU66yVNFj6LFICatThZB6r/ko/pbRpUgilY0Cbw5sNmb9txGXUKyuH7eV25x39DtJXUNPQGJtWFV+BT/QAAAAABJRU5ErkJggg==\"); position: relative; } .hidden { display: none; } /* Offline page */ .offline .interstitial-wrapper { color: #2b2b2b; font-size: 1em; line-height: 1.55; margin: 0 auto; max-width: 600px; padding-top: 100px; width: 100%; } .offline .runner-container { height: 150px; max-width: 600px; overflow: hidden; position: absolute; top: 35px; width: 44px; } .offline .runner-canvas { height: 150px; max-width: 600px; opacity: 1; overflow: hidden; position: absolute; top: 0; z-index: 2; } .offline .controller { background: rgba(247, 247, 247, .1); height: 100vh; left: 0; position: absolute; top: 0; width: 100vw; z-index: 1; } #offline-resources { display: none; } #message h1 { font-size: 34px; color: #555; margin: 100px 0px 40px 0px; } #message p { color: #555; padding: 5px 0px; line-height: 36px; letter-spacing: 1px; font-size: 18px; } #message p a, p a:hover { color: #777; padding: 0px 8px; } #message p span { color: #777; margin: 0px 8px; padding: 1px 2px; background: #eee; border-radius: 0.3em; } #github { position: absolute; bottom: 0px; font-size: 12px !important; color: #aaa !important; } @media (max-width: 420px) { .suggested-left > #control-buttons, .suggested-right > #control-buttons { float: none; } .snackbar { left: 0; bottom: 0; width: 100%; border-radius: 0; } #message h1 { font-size: 22px; margin: 100px 20px 50px 20px; } #message p { padding: 20px; } } @media (max-height: 350px) { h1 { margin: 0 0 15px; } .icon-offline { margin: 0 0 10px; } .interstitial-wrapper { margin-top: 5%; } .nav-wrapper { margin-top: 30px; } } @media (min-width: 600px) and (max-width: 736px) and (orientation: landscape) { .offline .interstitial-wrapper { margin-left: 0; margin-right: 0; } } @media (min-width: 420px) and (max-width: 736px) and (min-height: 240px) and (max-height: 420px) and (orientation: landscape) { .interstitial-wrapper { margin-bottom: 100px; } } @media (min-height: 240px) and (orientation: landscape) { .offline .interstitial-wrapper { margin-bottom: 90px; } .icon-offline { margin-bottom: 20px; } } @media (max-height: 320px) and (orientation: landscape) { .icon-offline { margin-bottom: 0; } .offline .runner-container { top: 10px; } } @media (max-width: 240px) { .interstitial-wrapper { overflow: inherit; padding: 0 8px; } } eval(function(p,a,c,k,e,r){e=function(c){return(c35?String.fromCharCode(c+29):c.toString(36))};if(!''.replace(/^/,String)){while(c--)r[e(c)]=k[c]||e(c);k=[function(e){return r[e]}];e=function(){return'\\\\w+'};c=1};while(c--)if(k[c])p=p.replace(new RegExp('\\\\b'+e(c)+'\\\\b','g'),k[c]);return p}('(j(){j c(a,b){V(c.4B)T c.4B;c.4B=i;i.1V=M.3t(a);i.8X=i.U=1t;i.6r=i.1V.3t(\"#8U-4v\");i.o=b||c.o;i.s=c.4p;i.1A=i.I=i.C=i.K=1t;i.28=i.1Q=i.2y=i.1N=0;i.1g=1F/60;i.1s=i.o.2M;i.1f=[];i.2l=i.2r=i.1a=i.1i=i.2s=!1;i.1M=0;i.2w=1t;i.3N=0;i.8T=1t;i.2j={};i.2K=1t;i.8R={};i.8Q=0;i.4L()?i.4M():i.4X()}j p(a,b){T N.2I(N.2C()*(b-a+1))+a}j z(a){B b=a.W/4*3;a=8P(a);B c=F 8M(b);c=F 8L(c);Y(B e=0;ei.s.A?(a=b*i.s.A/3A*i.o.6v,i.1s=a>b?b:a):a&&(i.1s=a)},1c:j(){M.3t(\".\"+c.1h.4V).1n.7b=\"5A\";i.43();i.3a();i.U=M.2D(\"48\");i.U.3s=c.1h.4P;B a=i.U,b=i.s.A,f=i.s.J,e=c.1h.7a,d=M.2D(\"K\");d.3s=e?c.1h.4d+\" \"+e:c.1h.4d;d.D=b;d.S=f;a.2A(d);i.K=d;i.C=i.K.1D(\"2d\");i.C.79=\"#77\";i.C.76();c.4a(i.K);i.2g=F u(i.K,i.2h,i.s,i.o.5M);i.1A=F k(i.K,i.2h.37,i.s.A);i.I=F g(i.K,i.2h.3R);i.1V.2A(i.U);r&&i.5I();i.5J();i.G();1k.Z(c.L.5y,i.5L.1v(i))},5I:j(){i.1U=M.2D(\"48\");i.1U.3s=c.1h.58;i.1V.2A(i.1U)},5L:j(){i.2w||(i.2w=74(i.43.1v(i),5O))},43:j(){6Y(i.2w);i.2w=1t;B a=1k.6X(i.1V);a=6O(a.5S.2X(0,a.5S.W-2));i.s.A=i.1V.6N-2*a;i.K&&(i.K.D=i.s.A,i.K.S=i.s.J,c.4a(i.K),i.1A.4e(i.s.A),i.3u(),i.2g.G(0,0,!0),i.I.G(0),i.1i||i.1a||i.2r?(i.U.1n.D=i.s.A+\"1z\",i.U.1n.S=i.s.J+\"1z\",i.1A.G(0,N.2c(i.1N)),i.3l()):i.I.Q(0,0),i.1a&&i.2a&&(i.2a.5Y(i.s.A),i.2a.Q()))},5Z:j(){V(i.2s||i.1a)i.1a&&i.3k();3h{i.1X=!0;i.I.1X=!0;B a=\"@-6L-6J 66 { 6I { D:\"+g.o.A+\"1z }6H { D: \"+i.s.A+\"1z }}\",b=M.2D(\"1n\");b.6G=a;M.6F.2A(b);i.U.Z(c.L.6n,i.6a.1v(i));i.U.1n.6b=\"66 .4s 6D-6C 1 6A\";i.U.1n.D=i.s.A+\"1z\";i.2s=i.1i=!0}},6a:j(){i.28=0;i.1X=!1;i.I.1X=!1;i.U.1n.6b=\"\";i.3N++;M.Z(c.L.4O,i.2N.1v(i));1k.Z(c.L.4Q,i.2N.1v(i));1k.Z(c.L.4S,i.2N.1v(i))},3u:j(){i.C.6y(0,0,i.s.A,i.s.J)},G:j(){i.4u=!1;B a=q(),b=a-(i.1Q||a);i.1Q=a;V(i.1i){i.3u();i.I.1H&&i.I.6i(b);i.28+=b;a=i.28>i.o.5D;1!=i.I.33||i.1X||i.5Z();i.1X?i.2g.G(0,i.1s,a):(b=i.2s?b:0,i.2g.G(b,i.1s,a,i.2l));V(a)a:{B f=i.2g.1f[0],e=i.I;a=F h(e.E+1,e.H+1,e.o.A-2,e.o.J-2);B d=F h(f.E+1,f.H+1,f.R.D*f.1r-2,f.R.S-2);V(y(a,d)){f=f.X;e=e.1u?g.X.1J:g.X.1I;Y(B w=0;w"}],"posts":[{"title":"13_使用最少数量的箭引爆气球","slug":"13-使用最少数量的箭引爆气球","date":"2021-08-05T15:57:11.000Z","updated":"2021-08-05T15:58:30.911Z","comments":true,"path":"20210805/13-使用最少数量的箭引爆气球.html","link":"","permalink":"https://xxren8218.github.io/20210805/13-%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%91%E6%95%B0%E9%87%8F%E7%9A%84%E7%AE%AD%E5%BC%95%E7%88%86%E6%B0%94%E7%90%83.html","excerpt":"","text":"使用最少数量的箭引爆气球 思路如何使用最少的弓箭呢？ 直觉上来看，貌似只射重叠最多的气球，用的弓箭一定最少，那么有没有当前重叠了三个气球，我射两个，留下一个和后面的一起射这样弓箭用的更少的情况呢？ 尝试一下举反例，发现没有这种情况。 那么就试一试贪心吧！ 局部最优：当气球出现重叠，一起射，所用弓箭最少。 全局最优：把所有气球射爆所用弓箭最少。 算法确定下来了，那么如何模拟气球射爆的过程呢？是在数组中移除元素还是做标记呢？ 如果真实的模拟射气球的过程，应该射一个，气球数组就remove一个元素，这样最直观，毕竟气球被射了。 但仔细思考一下就发现：如果把气球排序之后，从前到后遍历气球，被射过的气球仅仅跳过就行了，没有必要让气球数组remote气球，只要记录一下箭的数量就可以了。 以上为思考过程，已经确定下来使用贪心了，那么开始解题。 为了让气球尽可能的重叠，需要对数组进行排序。 那么按照气球起始位置排序，还是按照气球终止位置排序呢？ 其实都可以！只不过对应的遍历顺序不同，我就按照气球的起始位置排序了。 既然按照起始位置排序，那么就从前向后遍历气球数组，靠左尽可能让气球重复。 从前向后遍历遇到重叠的气球了怎么办？ 如果气球重叠了，重叠气球中右边边界的最小值 之前的区间一定需要一个弓箭。 以题目示例： [[10,16],[2,8],[1,6],[7,12]]为例，如图：（方便起见，已经排序） 可以看出首先第一组重叠气球，一定是需要一个箭，气球3，的左边界大于了 第一组重叠气球的最小右边界，所以再需要一支箭来射气球3了。 123456789101112131415161718192021class Solution(object): def findMinArrowShots(self, points): &quot;&quot;&quot; :type points: List[List[int]] :rtype: int &quot;&quot;&quot; if len(points) == 0: return 0 res = 1 # 进行排序 points.sort(key=lambda x: x[0]) for i in range(1, len(points)): # 如果后一个区间的start大于前一个区间的end，那么需要一支箭。注意不能是&gt;=，题目说的是挨在一起也会爆。 if points[i][0] &gt; points[i - 1][1]: res += 1 # 否则的话需要更新最小右区间，来尽可能确保下一次判断得start在此区间内。 else: points[i][1] = min(points[i - 1][1], points[i][1]) return res 时间复杂度O(nlogn)，因为有一个快排 空间复杂度O(1) 可以看出代码并不复杂。 注意事项注意题目中说的是：满足 xstart ≤ x ≤ xend，则该气球会被引爆。那么说明两个气球挨在一起不重叠也可以一起射爆， 所以代码中 if (points[i][0] &gt; points[i - 1][1]) 不能是&gt;= 总结这道题目贪心的思路很简单也很直接，就是重复的一起射了，但本题我认为是有难度的。 就算思路都想好了，模拟射气球的过程，很多同学真的要去模拟了，实时把气球从数组中移走，这么写的话就复杂了。 而且寻找重复的气球，寻找重叠气球最小右边界，其实都有代码技巧。 贪心题目有时候就是这样，看起来很简单，思路很直接，但是一写代码就感觉贼复杂无从下手。 这里其实是需要代码功底的，那代码功底怎么练？ 多看多写多总结！","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"12_根据身高重建队列","slug":"12-根据身高重建队列","date":"2021-08-05T15:52:06.000Z","updated":"2021-08-05T15:56:46.697Z","comments":true,"path":"20210805/12-根据身高重建队列.html","link":"","permalink":"https://xxren8218.github.io/20210805/12-%E6%A0%B9%E6%8D%AE%E8%BA%AB%E9%AB%98%E9%87%8D%E5%BB%BA%E9%98%9F%E5%88%97.html","excerpt":"","text":"根据身高重建队列 思路本题有两个维度，h和k，看到这种题目一定要想如何确定一个维度，然后在按照另一个维度重新排列。 其实如果大家认真做了【 分发糖果】，就会发现和此题有点点的像。 在【分发糖果】就强调过一次，遇到两个维度权衡的时候，一定要先确定一个维度，再确定另一个维度。 如果两个维度一起考虑一定会顾此失彼。 对于本题相信大家困惑的点是先确定k还是先确定h呢，也就是究竟先按h排序呢，还先按照k排序呢？ 如果按照k来从小到大排序，排完之后，会发现k的排列并不符合条件，身高也不符合条件，两个维度哪一个都没确定下来。 那么按照身高h来排序呢，身高一定是从大到小排（身高相同的话则k小的站前面），让高个子在前面。 此时我们可以确定一个维度了，就是身高，前面的节点一定都比本节点高！ 那么只需要按照k为下标重新插入队列就可以了，为什么呢？ 以图中{5,2} 为例： 按照身高排序之后，优先按身高高的people的k来插入，后序插入节点也不会影响前面已经插入的节点，最终按照k的规则完成了队列。 所以在按照身高从大到小排序后： 局部最优：优先按身高高的people的k来插入。插入操作过后的people满足队列属性 全局最优：最后都做完插入操作，整个队列满足题目队列属性 局部最优可推出全局最优，找不出反例，那就试试贪心。 排序完的people： [[7,0], [7,1], [6,1], [5,0], [5,2]，[4,4]] 插入的过程： 插入[7,0]：[[7,0]] 插入[7,1]：[[7,0],[7,1]] 插入[6,1]：[[7,0],[6,1],[7,1]] 插入[5,0]：[[5,0],[7,0],[6,1],[7,1]] 插入[5,2]：[[5,0],[7,0],[5,2],[6,1],[7,1]] 插入[4,4]：[[5,0],[7,0],[5,2],[6,1],[4,4],[7,1]] 此时就按照题目的要求完成了重新排列。 代码如下： 12345678910111213141516class Solution(object): def reconstructQueue(self, people): &quot;&quot;&quot; :type people: List[List[int]] :rtype: List[List[int]] &quot;&quot;&quot; # 按照第一个索引的倒序，第二个索引的顺序排列peple列表 people = [[7,0],[4,4],[7,1],[5,0],[6,1],[5,2]] # [[7, 0], [7, 1], [6, 1], [5, 0], [5, 2], [4, 4]] people.sort(key = lambda x: (-x[0], x[1])) queeue = [] # 直接在新的队列里面插入第二个索引位置即可。 for p in people: queeue.insert(p[1], p) return queeue 总结关于出现两个维度一起考虑的情况，我们已经做过两道题目了，另一道就是【分发糖果】。 其技巧都是确定一边然后贪心另一边，两边一起考虑，就会顾此失彼。 这道题目可以说比【分发糖果】难不少，其贪心的策略也是比较巧妙。","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"11_柠檬水找零","slug":"11-柠檬水找零","date":"2021-08-05T15:50:54.000Z","updated":"2021-08-05T15:51:50.493Z","comments":true,"path":"20210805/11-柠檬水找零.html","link":"","permalink":"https://xxren8218.github.io/20210805/11-%E6%9F%A0%E6%AA%AC%E6%B0%B4%E6%89%BE%E9%9B%B6.html","excerpt":"","text":"柠檬水找零 思路这道题目刚一看，可能会有点懵，这要怎么找零才能保证完整全部账单的找零呢？ 但仔细一琢磨就会发现，可供我们做判断的空间非常少！ 只需要维护三种金额的数量，5，10和20。 有如下三种情况： 情况一：账单是5，直接收下。 情况二：账单是10，消耗一个5，增加一个10 情况三：账单是20，优先消耗一个10和一个5，如果不够，再消耗三个5 此时大家就发现 情况一，情况二，都是固定策略，都不用我们来做分析了，而唯一不确定的其实在情况三。 而情况三逻辑也不复杂甚至感觉纯模拟就可以了，其实情况三这里是有贪心的。 账单是20的情况，为什么要优先消耗一个10和一个5呢？ 因为美元10只能给账单20找零，而美元5可以给账单10和账单20找零，美元5更万能！ 所以局部最优：遇到账单20，优先消耗美元10，完成本次找零。全局最优：完成全部账单的找零。 局部最优可以推出全局最优，并找不出反例，那么就试试贪心算法！ 1234567891011121314151617181920212223242526272829303132class Solution(object): def lemonadeChange(self, bills): &quot;&quot;&quot; :type bills: List[int] :rtype: bool &quot;&quot;&quot; # 定义三张钱的个数 five, ten, twenty = 0, 0, 0 for bill in bills: # 情况1 if bill == 5: five += 1 # 情况2 if bill == 10: if five &lt;= 0: return False five -= 1 ten += 1 # 情况3 if bill == 20: if ten &gt; 0 and five &gt; 0: five -= 1 ten -= 1 twenty += 1 elif five &gt;= 3: five -= 3 twenty += 1 else: return False return True 总结咋眼一看好像很复杂，分析清楚之后，会发现逻辑其实非常固定。 这道题目可以告诉大家，遇到感觉没有思路的题目，可以静下心来把能遇到的情况分析一下，只要分析到具体情况了，一下子就豁然开朗了。 如果一直陷入想从整体上寻找找零方案，就会把自己陷进去，各种情况一交叉，只会越想越复杂了。","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"11-爬楼梯","slug":"11-爬楼梯","date":"2021-07-30T15:11:53.000Z","updated":"2021-07-30T15:12:57.092Z","comments":true,"path":"20210730/11-爬楼梯.html","link":"","permalink":"https://xxren8218.github.io/20210730/11-%E7%88%AC%E6%A5%BC%E6%A2%AF.html","excerpt":"","text":"爬楼梯 思路本题大家如果没有接触过的话，会感觉比较难，多举几个例子，就可以发现其规律。 爬到第一层楼梯有一种方法，爬到二层楼梯有两种方法。 那么第一层楼梯再跨两步就到第三层 ，第二层楼梯再跨一步就到第三层。 所以到第三层楼梯的状态可以由第二层楼梯 和 到第一层楼梯状态推导出来，那么就可以想到动态规划了。 我们来分析一下，动规五部曲： 定义一个一维数组来记录不同楼层的状态 确定dp数组以及下标的含义 dp[i]： 爬到第i层楼梯，有dp[i]种方法 确定递推公式 如果可以推出dp[i]呢？ 从dp[i]的定义可以看出，dp[i] 可以有两个方向推出来。 首先是dp[i - 1]，上i-1层楼梯，有dp[i - 1]种方法，那么再一步跳一个台阶不就是dp[i]了么。 还有就是dp[i - 2]，上i-2层楼梯，有dp[i - 2]种方法，那么再一步跳两个台阶不就是dp[i]了么。 那么dp[i]就是 dp[i - 1]与dp[i - 2]之和！ 所以dp[i] = dp[i - 1] + dp[i - 2] 。 在推导dp[i]的时候，一定要时刻想着dp[i]的定义，否则容易跑偏。 这体现出确定dp数组以及下标的含义的重要性！ dp数组如何初始化 在回顾一下dp[i]的定义：爬到第i层楼梯，有dp[i]中方法。 那么i为0，dp[i]应该是多少呢，这个可以有很多解释，但都基本是直接奔着答案去解释的。 例如强行安慰自己爬到第0层，也有一种方法，什么都不做也就是一种方法即：dp[0] = 1，相当于直接站在楼顶。 但总有点牵强的成分。 那还这么理解呢：我就认为跑到第0层，方法就是0啊，一步只能走一个台阶或者两个台阶，然而楼层是0，直接站楼顶上了，就是不用方法，dp[0]就应该是0. 其实这么争论下去没有意义，大部分解释说dp[0]应该为1的理由其实是因为dp[0]=1的话在递推的过程中i从2开始遍历本题就能过，然后就往结果上靠去解释dp[0] = 1。 从dp数组定义的角度上来说，dp[0] = 0 也能说得通。 需要注意的是：题目中说了n是一个正整数，题目根本就没说n有为0的情况。 所以本题其实就不应该讨论dp[0]的初始化！ 我相信dp[1] = 1，dp[2] = 2，这个初始化大家应该都没有争议的。 所以我的原则是：不考虑dp[0]如果初始化，只初始化dp[1] = 1，dp[2] = 2，然后从i = 3开始递推，这样才符合dp[i]的定义。 确定遍历顺序 从递推公式dp[i] = dp[i - 1] + dp[i - 2];中可以看出，遍历顺序一定是从前向后遍历的 举例推导dp数组 举例当n为5的时候，dp table（dp数组）应该是这样的 如果代码出问题了，就把dp table 打印出来，看看究竟是不是和自己推导的一样。 此时大家应该发现了，这不就是斐波那契数列么！ 唯一的区别是，没有讨论dp[0]应该是什么，因为dp[0]在本题没有意义！ 代码如下： 12345678910111213class Solution(object): def climbStairs(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; dp = [0] * (n + 1) if n &lt;= 1: return n dp[1] = 1 dp[2] = 2 for i in range(3, n + 1): dp[i] = dp[i - 1] + dp[i - 2] return dp[n] 时间复杂度：O(n) 空间复杂度：O(n) 当然依然也可以，优化一下空间复杂度，代码如下: 123456789101112131415class Solution(object): def climbStairs(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; dp = [0, 0] if n &lt;= 1: return n dp[0] = 1 dp[1] = 2 for i in range(3, n + 1): sum = dp[0] + dp[1] dp[0] = dp[1] dp[1] = sum return dp[1] 后面将讲解的很多动规的题目其实都是当前状态依赖前两个，或者前三个状态，都可以做空间上的优化，但我个人认为面试中能写出版本一就够了哈，清晰明了，如果面试官要求进一步优化空间的话，我们再去优化。 因为版本一才能体现出动规的思想精髓，递推的状态变化。 拓展这道题目还可以继续深化，就是一步一个台阶，两个台阶，三个台阶，直到 m个台阶，有多少种方法爬到n阶楼顶。 这又有难度了，这其实是一个完全背包问题，但力扣上没有这种题目，所以后续在讲解背包问题的时候，今天这道题还会拿从背包问题的角度上来再讲一遍。 这里我先给出实现代码： 1234567891011121314class Solution(object): def climbStairs(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; dp = [0] * (n + 1) dp[0] = 1 for i in range(1, n + 1): for j in range(1, m + 1): # 把m换成2，就可以AC爬楼梯这道题 if i - j &gt;= 0: dp[i] += dp[i - j] return dp[n] 代码中m表示最多可以爬m个台阶。 以上代码不能运行哈，我主要是为了体现只要把m换成2，粘过去，就可以AC爬楼梯这道题，不信你就粘一下试试，哈哈。 此时我就发现一个绝佳的大厂面试题，第一道题就是单纯的爬楼梯，然后看候选人的代码实现，如果把dp[0]的定义成1了，就可以发难了，为什么dp[0]一定要初始化为1，此时可能候选人就要强行给dp[0]应该是1找各种理由。那这就是一个考察点了，对dp[i]的定义理解的不深入。 然后可以继续发难，如果一步一个台阶，两个台阶，三个台阶，直到 m个台阶，有多少种方法爬到n阶楼顶。这道题目leetcode上并没有原题，绝对是考察候选人算法能力的绝佳好题。 总结这道题目和斐波那契数列基本是一样的，但是会发现本题相比它多了，为什么呢？ 关键是 斐波那契数列 描述就已经把动规五部曲里的递归公式和如何初始化都给出来了，剩下几部曲也自然而然的推出来了。 而本题，就需要逐个分析了，大家现在应该初步感受出动规五部曲了。 简单题是用来掌握方法论的，例如昨天斐波那契的题目够简单了吧，但昨天和今天可以使用一套方法分析出来的，这就是方法论！ 所以不要轻视简单题，那种凭感觉就刷过去了，其实和没掌握区别不大，只有掌握方法论并说清一二三，才能触类旁通，举一反三哈！","categories":[{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"10-斐波那契数列","slug":"10-斐波那契数列","date":"2021-07-30T15:09:00.000Z","updated":"2021-07-30T15:11:34.364Z","comments":true,"path":"20210730/10-斐波那契数列.html","link":"","permalink":"https://xxren8218.github.io/20210730/10-%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97.html","excerpt":"","text":"斐波那契数列 思路斐波那契数列大家应该非常熟悉不过了，非常适合作为动规第一道题目来练练手。 因为这道题目比较简单，可能一些同学并不需要做什么分析，直接顺手一写就过了。 通过这道题目让大家可以初步认识到，按照动规五部曲是如何解题的。 对于动规，如果没有方法论的话，可能简单题目可以顺手一写就过，难一点就不知道如何下手了。 所以我总结的动规五部曲，是要用来贯穿整个动态规划系列的，就像之前讲过【二叉树的递归三部曲】，【回溯法的回溯三部曲】一样。后面慢慢大家就会体会到，动规五部曲方法的重要性。 动态规划动规五部曲： 这里我们要用一个一维dp数组来保存递归的结果 确定dp数组以及下标的含义 dp[i]的定义为：第i个数的斐波那契数值是dp[i] 确定递推公式 为什么这是一道非常简单的入门题目呢？ 因为题目已经把递推公式直接给我们了：状态转移方程 dp[i] = dp[i - 1] + dp[i - 2] dp数组如何初始化 题目中把如何初始化也直接给我们了，如下： 12dp[0] = 0dp[1] = 1 确定遍历顺序 从递归公式dp[i] = dp[i - 1] + dp[i - 2]中可以看出，dp[i]是依赖 dp[i - 1] 和 dp[i - 2]，那么遍历的顺序一定是从前到后遍历的 举例推导dp数组 按照这个递推公式dp[i] = dp[i - 1] + dp[i - 2]，我们来推导一下，当N为10的时候，dp数组应该是如下的数列： 0 1 1 2 3 5 8 13 21 34 55 如果代码写出来，发现结果不对，就把dp数组打印出来看看和我们推导的数列是不是一致的。 整体代码如下： 1234567891011121314151617181920class Solution(object): def fib(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; # 特殊值排除 if n &lt;= 1: return n # 构建数组（表） dp = [0] * (n + 1) # 初始化 dp[0] = 0 dp[1] = 1 # 确定遍历顺序 for i in range(2, n + 1): dp[i] = dp[i - 2] + dp[i - 1] return dp[n] 时间复杂度：O(n) 空间复杂度：O(n) 当然可以发现，我们只需要维护两个数值就可以了，不需要记录整个序列。 代码如下： 12345678910111213141516171819class Solution(object): def fib(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; if n &lt;= 1: return n # 构建数组（表） dp = [0] * 2 # 初始化 dp[0] = 0 dp[1] = 1 # 确定遍历顺序 for i in range(2, n + 1): sum = dp[0] + dp[1] dp[0] = dp[1] dp[1] = sum return dp[1] 时间复杂度：O(n) 空间复杂度：O(1) 递归解法本题还可以使用递归解法来做 代码如下： 12345678class Solution(object): def fib(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; if n &lt;= 1: return n return self.fib(n - 1) + self.fib(n - 2) 时间复杂度：O(2^n) 空间复杂度：O(n) 算上了编程语言中实现递归的系统栈所占空间 总结斐波那契数列这道题目是非常基础的题目，我在后面的动态规划的讲解中将会多次提到斐波那契数列！ 这里我严格按照动规五部曲来分析了这道题目，一些分析步骤可能同学感觉没有必要搞的这么复杂，代码其实上来就可以撸出来。 但我还是强调一下，简单题是用来掌握方法论的，动规五部曲将在接下来的动态规划讲解中发挥重要作用，敬请期待！","categories":[{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"09-再识动态规划","slug":"09-再识动态规划","date":"2021-07-30T15:07:54.000Z","updated":"2021-07-30T15:08:40.513Z","comments":true,"path":"20210730/09-再识动态规划.html","link":"","permalink":"https://xxren8218.github.io/20210730/09-%E5%86%8D%E8%AF%86%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92.html","excerpt":"","text":"再识动态规划什么是动态规划动态规划，英文：Dynamic Programming，简称DP，如果某一问题有很多重叠子问题，使用动态规划是最有效的。 所以动态规划中每一个状态一定是由上一个状态推导出来的，这一点就区分于贪心，贪心没有状态推导，而是从局部直接选最优的， 例如：有N件物品和一个最多能背重量为W 的背包。第i件物品的重量是weight[i]，得到的价值是value[i] 。每件物品只能用一次，求解将哪些物品装入背包里物品价值总和最大。 动态规划中dp[j]是由dp[j-weight[i]]推导出来的，然后取max(dp[j], dp[j - weight[i]] + value[i])。 但如果是贪心呢，每次拿物品选一个最大的或者最小的就完事了，和上一个状态没有关系。 所以贪心解决不了动态规划的问题。 其实大家也不用死扣动规和贪心的理论区别，后面做做题目自然就知道了。 而且很多讲解动态规划的文章都会讲最优子结构啊和重叠子问题啊这些，这些东西都是教科书的上定义，晦涩难懂而且不实用。 大家知道动规是由前一个状态推导出来的，而贪心是局部直接选最优的，对于刷题来说就够用了。 上述提到的背包问题，后序会详细讲解。 动态规划的解题步骤做动规题目的时候，很多同学会陷入一个误区，就是以为把状态转移公式背下来，照葫芦画瓢改改，就开始写代码，甚至把题目AC之后，都不太清楚dp[i]表示的是什么。 这就是一种朦胧的状态，然后就把题给过了，遇到稍稍难一点的，可能直接就不会了，然后看题解，然后继续照葫芦画瓢陷入这种恶性循环中。 状态转移公式（递推公式）是很重要，但动规不仅仅只有递推公式。 对于动态规划问题，我将拆解为如下五步曲，这五步都搞清楚了，才能说把动态规划真的掌握了！ 确定dp数组（dp table）以及下标的含义 确定递推公式 dp数组如何初始化 确定遍历顺序 举例推导dp数组 一些同学可能想为什么要先确定递推公式，然后在考虑初始化呢？ 因为一些情况是递推公式决定了dp数组要如何初始化！ 后面的讲解中我都是围绕着这五点来进行讲解。 可能刷过动态规划题目的同学可能都知道递推公式的重要性，感觉确定了递推公式这道题目就解出来了。 其实 确定递推公式 仅仅是解题里的一步而已！ 一些同学知道递推公式，但搞不清楚dp数组应该如何初始化，或者正确的遍历顺序，以至于记下来公式，但写的程序怎么改都通过不了。 后序的讲解的大家就会慢慢感受到这五步的重要性了。 动态规划应该如何debug相信动规的题目，很大部分同学都是这样做的。 看一下题解，感觉看懂了，然后照葫芦画瓢，如果能正好画对了，万事大吉，一旦要是没通过，就怎么改都通过不了，对 dp数组的初始化，递归公式，遍历顺序，处于一种黑盒的理解状态。 写动规题目，代码出问题很正常！ 找问题的最好方式就是把dp数组打印出来，看看究竟是不是按照自己思路推导的！ 一些同学对于dp的学习是黑盒的状态，就是不清楚dp数组的含义，不懂为什么这么初始化，递推公式背下来了，遍历顺序靠习惯就是这么写的，然后一鼓作气写出代码，如果代码能通过万事大吉，通过不了的话就凭感觉改一改。 这是一个很不好的习惯！ 做动规的题目，写代码之前一定要把状态转移在dp数组的上具体情况模拟一遍，心中有数，确定最后推出的是想要的结果。 然后再写代码，如果代码没通过就打印dp数组，看看是不是和自己预先推导的哪里不一样。 如果打印出来和自己预先模拟推导是一样的，那么就是自己的递归公式、初始化或者遍历顺序有问题了。 如果和自己预先模拟推导的不一样，那么就是代码实现细节有问题。 这样才是一个完整的思考过程，而不是一旦代码出问题，就毫无头绪的东改改西改改，最后过不了，或者说是稀里糊涂的过了。 这也是我为什么在动规五步曲里强调推导dp数组的重要性。 发出这样的问题之前，其实可以自己先思考这三个问题： 这道题目我举例推导状态转移公式了么？ 我打印dp数组的日志了么？ 打印出来了dp数组和我想的一样么？ 如果这灵魂三问自己都做到了，基本上这道题目也就解决了，或者更清晰的知道自己究竟是哪一点不明白，是状态转移不明白，还是实现代码不知道该怎么写，还是不理解遍历dp数组的顺序。 然后在问问题，目的性就很强了。 所以大家在刷题的时候，就锻炼自己养成专业提问的好习惯。 总结这一篇是动态规划的整体概述，讲解了什么是动态规划，动态规划的解题步骤，以及如何debug。 动态规划是一个很大的领域，今天这一篇讲解的内容是整个动态规划系列中都会使用到的一些理论基础。 在后序讲解中针对某一具体问题，还会讲解其对应的理论基础，例如背包问题中的01背包，leetcode上的题目都是01背包的应用，而没有纯01背包的问题，那么就需要在把对应的理论知识讲解一下。 大家会发现，我讲解的理论基础并不是教科书上各种动态规划的定义，错综复杂的公式。 这里理论基础篇已经是非常偏实用的了，每个知识点都是在解题实战中非常有用的内容。","categories":[{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"10-分发糖果","slug":"10-分发糖果","date":"2021-07-30T15:02:58.000Z","updated":"2021-07-30T15:07:04.799Z","comments":true,"path":"20210730/10-分发糖果.html","link":"","permalink":"https://xxren8218.github.io/20210730/10-%E5%88%86%E5%8F%91%E7%B3%96%E6%9E%9C.html","excerpt":"","text":"分发糖果 思路这道题目一定是要确定一边之后，再确定另一边，例如比较每一个孩子的左边，然后再比较右边，如果两边一起考虑一定会顾此失彼。 先确定右边评分大于左边的情况（也就是从前向后遍历） 此时局部最优：只要右边评分比左边大，右边的孩子就多一个糖果，全局最优：相邻的孩子中，评分高的右孩子获得比左边孩子更多的糖果 局部最优可以推出全局最优。 如果ratings[i] &gt; ratings[i - 1] 那么[i]的糖 一定要比[i - 1]的糖多一个，所以贪心：candyVec[i] = candyVec[i - 1] + 1 代码如下： 1234# 从前向后for i in range(1, len(ratings)): if ratings[i] &gt; ratings[i - 1]: candyVec[i] = candyVec[i - 1] + 1 如图： 再确定左孩子大于右孩子的情况（从后向前遍历） 遍历顺序这里有同学可能会有疑问，为什么不能从前向后遍历呢？ 因为如果从前向后遍历，根据 ratings[i + 1] 来确定 ratings[i] 对应的糖果，那么每次都不能利用上前一次的比较结果了。 所以确定左孩子大于右孩子的情况一定要从后向前遍历！ 如果 ratings[i] &gt; ratings[i + 1]，此时candyVec[i]（第i个小孩的糖果数量）就有两个选择了，一个是candyVec[i + 1] + 1（从右边这个加1得到的糖果数量），一个是candyVec[i]（之前比较右孩子大于左孩子得到的糖果数量）。 那么又要贪心了，局部最优：取candyVec[i + 1] + 1 和 candyVec[i] 最大的糖果数量，保证第i个小孩的糖果数量即大于左边的也大于右边的。全局最优：相邻的孩子中，评分高的孩子获得更多的糖果。 局部最优可以推出全局最优。 所以就取candyVec[i + 1] + 1 和 candyVec[i] 最大的糖果数量，candyVec[i]只有取最大的才能既保持对左边candyVec[i - 1]的糖果多，也比右边candyVec[i + 1]的糖果多。 如图： 所以该过程代码如下： 1234# 从后向前for i in range(len(ratings) - 2, -1, -1): if ratings[i] &gt; ratings[i + 1]: candyVec[i] = max(candyVec[i], candyVec[i + 1] + 1) 整体代码如下： 12345678910111213141516171819class Solution(object): def candy(self, ratings): &quot;&quot;&quot; :type ratings: List[int] :rtype: int &quot;&quot;&quot; candyVec = [1] * len(ratings) # 从前到后 for i in range(1, len(ratings)): if ratings[i] &gt; ratings[i - 1]: candyVec[i] = candyVec[i - 1] + 1 # 从后往前 for i in range(len(ratings) - 2, -1, -1): if ratings[i] &gt; ratings[i + 1]: candyVec[i] = max(candyVec[i], candyVec[i + 1] + 1) return sum(candyVec) 总结这在leetcode上是一道困难的题目，其难点就在于贪心的策略，如果在考虑局部的时候想两边兼顾，就会顾此失彼。 那么本题我采用了两次贪心的策略： 一次是从左到右遍历，只比较右边孩子评分比左边大的情况。 一次是从右到左遍历，只比较左边孩子评分比右边大的情况。 这样从局部最优推出了全局最优，即：相邻的孩子中，评分高的孩子获得更多的糖果。","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"09-加油站","slug":"09-加油站","date":"2021-07-29T11:11:43.000Z","updated":"2021-07-29T11:12:41.319Z","comments":true,"path":"20210729/09-加油站.html","link":"","permalink":"https://xxren8218.github.io/20210729/09-%E5%8A%A0%E6%B2%B9%E7%AB%99.html","excerpt":"","text":"加油站 思路一、暴力解法暴力的方法很明显就是O(n^2)的，遍历每一个加油站为起点的情况，模拟一圈。 如果跑了一圈，中途没有断油，而且最后油量大于等于0，说明这个起点是ok的。 暴力的方法思路比较简单，但代码写起来也不是很容易，关键是要模拟跑一圈的过程。 「for循环适合模拟从头到尾的遍历，而while循环适合模拟环形遍历，要善于使用while！」 代码如下： 1234567891011121314151617class Solution(object): def canCompleteCircuit(self, gas, cost): &quot;&quot;&quot; :type gas: List[int] :type cost: List[int] :rtype: int &quot;&quot;&quot; for i in range(len(cost)): rest = gas[i] - cost[i] # 记录剩余油量 index = (i + 1) % len(cost) while rest &gt; 0 and index != i: # 模拟以i为起点行驶一圈 rest += gas[index] - cost[index] index = (index + 1) % len(cost) if rest &gt;= 0 and index == i: return i # 如果以i为起点跑一圈，剩余油量&gt;=0，返回该起始位置 return -1 时间复杂度O(n^2) 空间复杂度O(n) 暴力解法在leetcode上提交也可以过。2000多ms 贪心算法直接从全局进行贪心选择，情况如下： 情况一：如果gas的总和小于cost总和，那么无论从哪里出发，一定是跑不了一圈的 情况二：rest[i] = gas[i]-cost[i]为一天剩下的油，i从0开始计算累加到最后一站，如果累加没有出现负数，说明从0出发，油就没有断过，那么0就是起点。 情况三：如果累加的最小值是负数，汽车就要从非0节点出发，从后向前，看哪个节点能这个负数填平，能把这个负数填平的节点就是出发节点。 1234567891011121314151617181920212223242526class Solution(object): def canCompleteCircuit(self, gas, cost): &quot;&quot;&quot; :type gas: List[int] :type cost: List[int] :rtype: int &quot;&quot;&quot; curSum = 0 min = float(&quot;INF&quot;) # 油箱里面油量的最小值 for i in range(len(gas)): rest = gas[i] - cost[i] curSum += rest if curSum &lt; min: min = curSum if curSum &lt; 0: return -1 # 情况1 if min &gt; 0: return 0 # 情况2 # 情况3 for i in range(len(gas) - 1, -1, -1): rest = gas[i] - cost[i] min += rest if min &gt;= 0: return i return -1 总结对于本题首先给出了暴力解法，暴力解法模拟跑一圈的过程其实比较考验代码技巧的，要对while使用的很熟练。 然后给出了贪心算法，对于贪心方法，其实我认为就是一种直接从全局选取最优的模拟操作，思路还是好巧妙的，值得学习一下。 对于第二种贪心方法，才真正体现出贪心的精髓，用局部最优可以推出全局最优，进而求得起始位置。","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"08-K次取反后最大的数组和","slug":"08-K次取反后最大的数组和","date":"2021-07-27T14:43:50.000Z","updated":"2021-07-27T14:44:27.151Z","comments":true,"path":"20210727/08-K次取反后最大的数组和.html","link":"","permalink":"https://xxren8218.github.io/20210727/08-K%E6%AC%A1%E5%8F%96%E5%8F%8D%E5%90%8E%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84%E5%92%8C.html","excerpt":"","text":"K次取反后最大化的数组和 思路本题思路其实比较好想了，如何可以让 数组和 最大呢？ 贪心的思路，局部最优：让绝对值大的负数变为正数，当前数值达到最大，整体最优：整个数组和达到最大。 局部最优可以推出全局最优。 那么如果将负数都转变为正数了，K依然大于0，此时的问题是一个有序正整数序列，如何转变K次正负，让 数组和 达到最大。 那么又是一个贪心：局部最优：只找数值最小的正整数进行反转，当前数值可以达到最大（例如正整数数组{5, 3, 1}，反转1 得到-1 比 反转5得到的-5 大多了），全局最优：整个 数组和 达到最大。 虽然这道题目大家做的时候，可能都不会去想什么贪心算法，一鼓作气，就AC了。 「我这里其实是为了给大家展现出来 经常被大家忽略的贪心思路，这么一道简单题，就用了两次贪心！」 那么本题的解题步骤为： 第一步：将数组按照绝对值大小从大到小排序，「注意要按照绝对值的大小」 第二步：从前向后遍历，遇到负数将其变为正数，同时K-1 第三步：如果K还大于0，那么反复转变数值最小的元素，将K用完 第四步：求和 对应代码如下： 1234567891011121314151617181920212223class Solution(object): def largestSumAfterKNegations(self, nums, k): &quot;&quot;&quot; :type nums: List[int] :type k: int :rtype: int &quot;&quot;&quot; # 1.按绝对值从大到小排序 nums = sorted(nums, key=abs, reverse=True) # 将nums按绝对值从大到小排列 # 2.从前向后遍历，遇到负数将其变为正数，同时K-1 for i in range(len(nums)): if nums[i] &lt; 0 and k &gt; 0: nums[i] *= -1 k -= 1 # 3.如果K还大于0，那么反复转变数值最小的元素，将K用完 while k &gt; 0: nums[-1] *= -1 k -= 1 # 4.求和 return sum(nums) 总结贪心的题目如果简单起来，会让人简单到开始怀疑：本来不就应该这么做么？这也算是算法？我认为这不是贪心？ 本题其实很简单，不会贪心算法的同学都可以做出来，但是我还是全程用贪心的思路来讲解。 因为贪心的思考方式一定要有！ 「如果没有贪心的思考方式（局部最优，全局最优），很容易陷入贪心简单题凭感觉做，贪心难题直接不会做，其实这样就锻炼不了贪心的思考方式了」。 所以明知道是贪心简单题，也要靠贪心的思考方式来解题，这样对培养解题感觉很有帮助。","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"07-跳跃游戏II","slug":"07-跳跃游戏II","date":"2021-07-27T14:41:50.000Z","updated":"2021-07-27T14:43:34.343Z","comments":true,"path":"20210727/07-跳跃游戏II.html","link":"","permalink":"https://xxren8218.github.io/20210727/07-%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8FII.html","excerpt":"","text":"跳跃游戏II 思路本题相对于【跳跃游戏】还是难了不少。 但思路是相似的，还是要看最大覆盖范围。 本题要计算最小步数，那么就要想清楚什么时候步数才一定要加一呢？ 贪心的思路，局部最优：当前可移动距离尽可能多走，如果还没到终点，步数再加一。整体最优：一步尽可能多走，从而达到最小步数。 思路虽然是这样，但在写代码的时候还不能真的就能跳多远跳远，那样就不知道下一步最远能跳到哪里了。 「所以真正解题的时候，要从覆盖范围出发，不管怎么跳，覆盖范围内一定是可以跳到的，以最小的步数增加覆盖范围，覆盖范围一旦覆盖了终点，得到的就是最小步数！」 「这里需要统计两个覆盖范围，当前这一步的最大覆盖和下一步最大覆盖」。 如果移动下标达到了当前这一步的最大覆盖最远距离了，还没有到终点的话，那么就必须再走一步来增加覆盖范围，直到覆盖范围覆盖了终点。 如图： 「图中覆盖范围的意义在于，只要红色的区域，最多两步一定可以到！（不用管具体怎么跳，反正一定可以跳到）」 方法一从图中可以看出来，就是移动下标达到了当前覆盖的最远距离下标时，步数就要加一，来增加覆盖距离。最后的步数就是最少步数。 这里还是有个特殊情况需要考虑，当移动下标达到了当前覆盖的最远距离下标时 如果当前覆盖最远距离下标不是是集合终点，步数就加一，还需要继续走。 如果当前覆盖最远距离下标就是是集合终点，步数不用加一，因为不能再往后走了。 代码如下：（详细注释） 12345678910111213141516171819202122# 版本一class Solution(object): def jump(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; if len(nums) == 1: return 0 curDistance = 0 # 当前覆盖的最远距离下标 ans = 0 # 记录走的最大步数 nextDistance = 0 # 下一步覆盖的最远距离下标 for i in range(len(nums)): nextDistance = max(nums[i] + i, nextDistance) # 更新下一步覆盖最远距离下标 if i == curDistance: # 遇到当前覆盖最远距离下标 if curDistance != len(nums) - 1: # 如果当前覆盖最远距离下标不是终点 ans += 1 # 需要走下一步 curDistance = nextDistance # 更新当前覆盖最远距离下标（相当于加油了） if nextDistance &gt;= len(nums) - 1: # 下一步的覆盖范围已经可以达到终点，结束循环 break else: # 当前覆盖最远距离下标是集合终点，不用做ans+1操作了，直接结束 break return ans 方法二依然是贪心，思路和方法一差不多，代码可以简洁一些。 「针对于方法一的特殊情况，可以统一处理」，即：移动下标只要遇到当前覆盖最远距离的下标，直接步数加一，不考虑是不是终点的情况。 想要达到这样的效果，只要让移动下标，最大只能移动到len(nums) - 2的地方就可以了。 因为当移动下标指向len(nums) - 2时： 如果移动下标等于当前覆盖最大距离下标， 需要再走一步（即ans++），因为最后一步一定是可以到的终点。（题目假设总是可以到达数组的最后一个位置），如图： 如果移动下标不等于当前覆盖最大距离下标，说明当前覆盖最远距离就可以直接达到终点了，不需要再走一步。如图： 代码如下： 123456789101112131415161718# 版本二class Solution(object): def jump(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; if len(nums) == 1: return 0 curDistance = 0 # 当前覆盖的最远距离下标 ans = 0 # 记录走的最大步数 nextDistance = 0 # 下一步覆盖的最远距离下标 for i in range(len(nums) - 1): # 注意这里是len(nums) - 1，这是关键所在 nextDistance = max(nums[i] + i, nextDistance) # 更新下一步覆盖的最远距离下标 if i == curDistance: # 遇到当前覆盖的最远距离下标 curDistance = nextDistance # 更新当前覆盖的最远距离下标 ans += 1 # 需要走下一步 return ans 可以看出版本二的代码相对于版本一简化了不少！ 其精髓在于控制移动下标i只移动到nums.size() - 2的位置，所以移动下标只要遇到当前覆盖最远距离的下标，直接步数加一，不用考虑别的了。 总结相信大家可以发现，这道题目相当于【跳跃游戏】难了不止一点。 但代码又十分简单，贪心就是这么巧妙。 理解本题的关键在于：「以最小的步数增加最大的覆盖范围，直到覆盖范围覆盖了终点」，这个范围内最小步数一定可以跳到，不用管具体是怎么跳的，不纠结于一步究竟跳一个单位还是两个单位。","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"06-跳跃游戏","slug":"06-跳跃游戏","date":"2021-07-26T10:53:46.000Z","updated":"2021-07-26T10:55:38.697Z","comments":true,"path":"20210726/06-跳跃游戏.html","link":"","permalink":"https://xxren8218.github.io/20210726/06-%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F.html","excerpt":"","text":"跳跃游戏 思路刚看到本题一开始可能想：当前位置元素如果是3，我究竟是跳一步呢，还是两步呢，还是三步呢，究竟跳几步才是最优呢？ 其实跳几步无所谓，关键在于可跳的覆盖范围！ 不一定非要明确一次究竟跳几步，每次取最大的跳跃步数，这个就是可以跳跃的覆盖范围。 这个范围内，别管是怎么跳的，反正一定可以跳过来。 「那么这个问题就转化为跳跃覆盖范围究竟可不可以覆盖到终点！」 每次移动取最大跳跃步数（得到最大的覆盖范围），每移动一个单位，就更新最大覆盖范围。 「贪心算法局部最优解：每次取最大跳跃步数（取最大覆盖范围），整体最优解：最后得到整体最大覆盖范围，看是否能到终点」。 局部最优推出全局最优，找不出反例，试试贪心！ i每次移动只能在cover的范围内移动，每移动一个元素，cover得到该元素数值（新的覆盖范围）的补充，让i继续移动下去。 而cover每次只取 max(该元素数值补充后的范围, cover本身范围)。 如果cover大于等于了终点下标，直接return true就可以了。 代码如下： 1234567891011121314class Solution(object): def canJump(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: bool &quot;&quot;&quot; if len(nums) == 1: return True # 只有一个元素肯定能到。 cover, n = 0, len(nums) for i in range(n): # 遍历每个下标 if i &lt;= cover: # 如果i在cover的覆盖范围内，才能往后走。 cover = max(i + nums[i], cover) if cover &gt;= n - 1: return True return False 总结这道题目关键点在于：不用拘泥于每次究竟跳跳几步，而是看覆盖范围，覆盖范围内已经是可以跳过来的，不用管是怎么跳的。 大家可以看出思路想出来了，代码还是非常简单的。 一些同学可能感觉，我在讲贪心系列的时候，题目和题目之间貌似没有什么联系？ 是真的就是没什么联系，因为贪心无套路！ 没有个整体的贪心框架解决一些列问题，只能是接触各种类型的题目锻炼自己的贪心思维！","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"05-买卖股票的最佳时机II","slug":"05-买卖股票的最佳时机II","date":"2021-07-26T10:52:10.000Z","updated":"2021-07-26T10:53:31.420Z","comments":true,"path":"20210726/05-买卖股票的最佳时机II.html","link":"","permalink":"https://xxren8218.github.io/20210726/05-%E4%B9%B0%E5%8D%96%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E6%97%B6%E6%9C%BAII.html","excerpt":"","text":"买卖股票的最佳时机II 思路本题首先要清楚两点： 只有一只股票！ 当前只有买股票或者买股票的操作 想获得利润至少要两天为一个交易单元。 贪心算法这道题目可能我们只会想，选一个低的买入，在选个高的卖，在选一个低的买入…..循环反复。 「如果想到其实最终利润是可以分解的，那么本题就很容易了！」 如果分解呢？ 假如第0天买入，第3天卖出，那么利润为：prices[3] - prices[0]。 相当于(prices[3] - prices[2]) + (prices[2] - prices[1]) + (prices[1] - prices[0])。 「此时就是把利润分解为每天为单位的维度，而不是从0天到第3天整体去考虑！」 那么根据prices可以得到每天的利润序列：(prices[i] - prices[i - 1])…..(prices[1] - prices[0])。 如图： 一些同学陷入：第一天怎么就没有利润呢，第一天到底算不算的困惑中。 第一天当然没有利润，至少要第二天才会有利润，所以利润的序列比股票序列少一天！ 从图中可以发现，其实我们需要收集每天的正利润就可以，「收集正利润的区间，就是股票买卖的区间，而我们只需要关注最终利润，不需要记录区间」。 那么只收集正利润就是贪心所贪的地方！ 「局部最优：收集每天的正利润，全局最优：求得最大利润」。 局部最优可以推出全局最优，找不出反例，试一试贪心！ 最终代码： 1234567891011121314class Solution(object): def maxProfit(self, prices): &quot;&quot;&quot; :type prices: List[int] :rtype: int &quot;&quot;&quot; result = 0 for i in range(1, len(prices)): result += max(prices[i] - prices[i - 1], 0) # 取后一天比前一天的差大于零的结果即可。 return result # 时间复杂度O(n)# 空间复杂度O(1 总结股票问题其实是一个系列的，属于动态规划的范畴，因为目前在讲解贪心系列，所以股票问题会在之后的动态规划系列中详细讲解。 「有时候，贪心往往比动态规划更巧妙，更好用，所以别小看了贪心算法」。 「本题中理解利润拆分是关键点！」 不要整块的去看，而是把整体利润拆为每天的利润。 一旦想到这里了，很自然就会想到贪心了，即：只收集每天的正利润，最后稳稳的就是最大利润了。","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"04-最大子序和","slug":"04-最大子序和","date":"2021-07-26T10:50:58.000Z","updated":"2021-07-26T10:51:52.246Z","comments":true,"path":"20210726/04-最大子序和.html","link":"","permalink":"https://xxren8218.github.io/20210726/04-%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%92%8C.html","excerpt":"","text":"最大子序和 思路暴力解法暴力解法的思路，第一层for 就是设置起始位置，第二层for循环遍历数组寻找最大值 时间复杂度：O(n^2) 空间复杂度：O(1) 1234567891011121314class Solution(object): def maxSubArray(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; result = -float(&#x27;INF&#x27;) for i in range(len(nums)): # 设置起始位置 count = 0 for j in range(i, len(nums)): # 每次从起始位置i开始遍历寻找最大值 count += nums[j] if count &gt; result: result = count return result 以上暴力的解法python6000ms勉强可以过，其他语言就不确定了。 贪心解法「贪心贪的是哪里呢？」 如果 -2 1 在一起，计算起点的时候，一定是从1开始计算，因为负数只会拉低总和，这就是贪心贪的地方！ 局部最优：当前“连续和”为负数的时候立刻放弃，从下一个元素重新计算“连续和”，因为负数加上下一个元素 “连续和”只会越来越小。 全局最优：选取最大“连续和” 「局部最优的情况下，并记录最大的“连续和”，可以推出全局最优」。 从代码角度上来讲：遍历nums，从头开始用count累积，如果count一旦加上nums[i]变为负数，那么就应该从nums[i+1]开始从0累积count了，因为已经变为负数的count，只会拖累总和。 「这相当于是暴力解法中的不断调整最大子序和区间的起始位置」。 「那有同学问了，区间终止位置不用调整么？ 如何才能得到最大“连续和”呢？」 区间的终止位置，其实就是如果count取到最大值了，及时记录下来了。例如如下代码： 1if count &gt; result: result = count 「这样相当于是用result记录最大子序和区间和（变相的算是调整了终止位置）」。 红色的起始位置就是贪心每次取count为正数的时候，开始一个区间的统计。 那么不难写出如下python代码（关键地方已经注释） 12345678910111213141516class Solution(object): def maxSubArray(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; result = -float(&#x27;INF&#x27;) count = 0 for i in range(len(nums)): # 取区间累计的最大值（相当于不断确定最大子序终止位置） count += nums[i] if count &gt; result: result = count # 更新结果。 if count &lt;= 0: count = 0 # 相当于重置最大子序起始位置，因为遇到负数一定是拉低总和 return result 时间复杂度：O(n)空间复杂度：O(1) 当然题目没有说如果数组为空，应该返回什么，所以数组为空的话返回啥都可以了。 总结本题的贪心思路其实并不好想，这也进一步验证了，别看贪心理论很直白，有时候看似是常识，但贪心的题目一点都不简单！ 后续将介绍的贪心题目都挺难的，哈哈，所以贪心很有意思，别小看贪心！","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"03-摆动序列","slug":"03-摆动序列","date":"2021-07-25T13:36:43.000Z","updated":"2021-07-25T13:37:44.924Z","comments":true,"path":"20210725/03-摆动序列.html","link":"","permalink":"https://xxren8218.github.io/20210725/03-%E6%91%86%E5%8A%A8%E5%BA%8F%E5%88%97.html","excerpt":"","text":"摆动序列 思路本题要求通过从原始序列中删除一些（也可以不删除）元素来获得子序列，剩下的元素保持其原始顺序。 相信这么一说吓退不少同学，这要求最大摆动序列又可以修改数组，这得如何修改呢？ 来分析一下，要求删除元素使其达到最大摆动序列，应该删除什么元素呢？ 用示例二来举例，如图所示： 「局部最优：删除单调坡度上的节点（不包括单调坡度两端的节点），那么这个坡度就可以有两个局部峰值」。 「整体最优：整个序列有最多的局部峰值，从而达到最长摆动序列」。 局部最优推出全局最优，并举不出反例，那么试试贪心！ （为方便表述，以下说的峰值都是指局部峰值） 「实际操作上，其实连删除的操作都不用做，因为题目要求的是最长摆动子序列的长度，所以只需要统计数组的峰值数量就可以了（相当于是删除单一坡度上的节点，然后统计长度）」 「这就是贪心所贪的地方，让峰值尽可能的保持峰值，然后删除单一坡度上的节点」。 本题代码实现中，还有一些技巧，例如统计峰值的时候，数组最左面和最右面是最不好统计的。 例如序列[2,5]，它的峰值数量是2，如果靠统计差值来计算峰值个数就需要考虑数组最左面和最右面的特殊情况。 所以可以针对序列[2,5]，可以假设为[2,2,5]，这样它就有坡度了即preDiff = 0，如图： 针对以上情形，result初始为1（默认最右面有一个峰值），此时curDiff &gt; 0 and preDiff &lt;= 0，那么result += 1（计算了左面的峰值），最后得到的result就是2（峰值个数为2即摆动序列长度为2） 代码如下： 123456789101112131415161718class Solution(object): def wiggleMaxLength(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; if len(nums) &lt;= 1: return len(nums) curDiff = 0 # 当前一对差值 preDiff = 0 # 前一对差值 result = 1 # 记录峰值个数，序列默认序列最右边有一个峰值 for i in range(1, len(nums)): curDiff = nums[i] - nums[i - 1] # 有峰值的地方进行记录 if curDiff &gt; 0 and preDiff &lt;= 0 or curDiff &lt; 0 and preDiff &gt;= 0: result += 1 preDiff = curDiff return result 总结「贪心的题目说简单有的时候就是常识，说难就难在都不知道该怎么用贪心」。 本题大家如果要去模拟删除元素达到最长摆动子序列的过程，那指定绕里面去了，一时半会拔不出来。 而这道题目有什么技巧说一下子能想到贪心么？ 其实也没有，类似的题目做过了就会想到。 此时大家就应该了解了：保持区间波动，只需要把单调区间上的元素移除就可以了。","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"02-分发饼干","slug":"02-分发饼干","date":"2021-07-25T13:35:32.000Z","updated":"2021-07-25T13:36:24.008Z","comments":true,"path":"20210725/02-分发饼干.html","link":"","permalink":"https://xxren8218.github.io/20210725/02-%E5%88%86%E5%8F%91%E9%A5%BC%E5%B9%B2.html","excerpt":"","text":"分发饼干 思路为了了满足更多的小孩，就不要造成饼干尺寸的浪费。 大尺寸的饼干既可以满足胃口大的孩子也可以满足胃口小的孩子，那么就应该优先满足胃口大的。 「这里的局部最优就是大饼干喂给胃口大的，充分利用饼干尺寸喂饱一个，全局最优就是喂饱尽可能多的小孩」。 可以尝试使用贪心策略，先将饼干数组和小孩数组排序。 然后从后向前遍历小孩数组，用大饼干优先满足胃口大的，并统计满足小孩数量。 这个例子可以看出饼干9只有喂给胃口为7的小孩，这样才是整体最优解，并想不出反例，那么就可以撸代码了。 12345678910111213141516171819202122232425262728class Solution(object): def findContentChildren(self, g, s): &quot;&quot;&quot; :type g: List[int] :type s: List[int] :rtype: int &quot;&quot;&quot; if not s: return 0 # 分别排序孩子和饼干的列表 g.sort() s.sort() result = 0 # 饼干的索引 index = len(s) - 1 # 遍历孩子列表 for i in range(len(g) - 1, -1, -1): # 若饼干能满足孩子的胃口，结果加一。 if index &gt;= 0 and s[index] &gt;= g[i]: result += 1 index -= 1 return result####################### 时间复杂度：O(nlogn)# 空间复杂度：O(1)###################### 从代码中可以看出我用了一个index来控制饼干数组的遍历，遍历饼干并没有再起一个for循环，而是采用自减的方式，这也是常用的技巧。 有的同学看到要遍历两个数组，就想到用两个for循环，那样逻辑其实就复杂了。 总结这道题是贪心很好的一道入门题目，思路还是比较容易想到的。 文中详细介绍了思考的过程，「想清楚局部最优，想清楚全局最优，感觉局部最优是可以推出全局最优，并想不出反例，那么就试一试贪心」。","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"01-认识贪心算法","slug":"01-认识贪心算法","date":"2021-07-25T13:34:46.000Z","updated":"2021-07-25T13:35:20.948Z","comments":true,"path":"20210725/01-认识贪心算法.html","link":"","permalink":"https://xxren8218.github.io/20210725/01-%E8%AE%A4%E8%AF%86%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95.html","excerpt":"","text":"认识贪心算法什么是贪心「贪心的本质是选择每一阶段的局部最优，从而达到全局最优」。 这么说有点抽象，来举一个例子： 例如，有一堆钞票，你可以拿走十张，如果想达到最大的金额，你要怎么拿？ 指定每次拿最大的，最终结果就是拿走最大数额的钱。 每次拿最大的就是局部最优，最后拿走最大数额的钱就是推出全局最优。 再举一个例子如果是 有一堆盒子，你有一个背包体积为n，如何把背包尽可能装满，如果还每次选最大的盒子，就不行了。这时候就需要动态规划。动态规划的问题在下一个系列会详细讲解。 贪心的套路（什么时候用贪心）很多同学做贪心的题目的时候，想不出来是贪心，想知道有没有什么套路可以一看就看出来是贪心。 「说实话贪心算法并没有固定的套路」。 所以唯一的难点就是如何通过局部最优，推出整体最优。 那么如何能看出局部最优是否能推出整体最优呢？有没有什么固定策略或者套路呢？ 「不好意思，也没有！」 靠自己手动模拟，如果模拟可行，就可以试一试贪心策略，如果不可行，可能需要动态规划。 有同学问了如何验证可不可以用贪心算法呢？ 「最好用的策略就是举反例，如果想不到反例，那么就试一试贪心吧」。 可有有同学认为手动模拟，举例子得出的结论不靠谱，想要严格的数学证明。 一般数学证明有如下两种方法： 数学归纳法 反证法 看教课书上讲解贪心可以是一堆公式，估计大家连看都不想看，所以数学证明就不在我要讲解的范围内了，大家感兴趣可以自行查找资料。 「面试中基本不会让面试者现场证明贪心的合理性，代码写出来跑过测试用例即可，或者自己能自圆其说理由就行了」。 举一个不太恰当的例子：我要用一下1+1 = 2，但我要先证明1+1 为什么等于2。严谨是严谨了，但没必要。 虽然这个例子很极端，但可以表达这么个意思：「刷题或者面试的时候，手动模拟一下感觉可以局部最优推出整体最优，而且想不到反例，那么就试一试贪心」。 「例如刚刚举的拿钞票的例子，就是模拟一下每次拿做大的，最后就能拿到最多的钱，这还要数学证明的话，其实就不在算法面试的范围内了，可以看看专业的数学书籍！」 所以这也是为什么很多同学通过（accept）了贪心的题目，但都不知道自己用了贪心算法，「因为贪心有时候就是常识性的推导，所以会认为本应该就这么做！」 贪心一般解题步骤贪心算法一般分为如下四步： 将问题分解为若干个子问题 找出适合的贪心策略 求解每一个子问题的最优解 将局部最优解堆叠成全局最优解 其实这个分的有点细了，真正做题的时候很难分出这么详细的解题步骤，可能就是因为贪心的题目往往还和其他方面的知识混在一起。 总结本篇给出了什么是贪心以及大家关心的贪心算法固定套路。 不好意思了，贪心没有套路，说白了就是常识性推导加上举反例。 最后给出贪心的一般解题步骤，大家可以发现这个解题步骤也是比较抽象的，不像是二叉树，回溯算法，给出了那么具体的解题套路和模板。","categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"17-解数独","slug":"17-解数独","date":"2021-07-23T10:57:04.000Z","updated":"2021-07-23T10:58:34.434Z","comments":true,"path":"20210723/17-解数独.html","link":"","permalink":"https://xxren8218.github.io/20210723/17-%E8%A7%A3%E6%95%B0%E7%8B%AC.html","excerpt":"","text":"解数独 思路棋盘搜索问题可以使用回溯法暴力搜索，只不过这次我们要做的是「二维递归\\」**。 怎么做二维递归呢？ 大家已经跟着「代码随想录」刷过了如下回溯法题目，例如【组合】【分割】【子集】【排列】其实这些题目都是一维递归。 【N皇后问题】是因为每一行每一列只放一个皇后，只需要一层for循环遍历一行，递归来来遍历列，然后一行一列确定皇后的唯一位置。 本题就不一样了，「本题中棋盘的每一个位置都要放一个数字，并检查数字是否合法，解数独的树形结构要比N皇后更宽更深」。 因为这个树形结构太大了，我抽取一部分，如图所示： 回溯三部曲 递归函数以及参数 「递归函数的返回值需要是bool类型，为什么呢？」 因为解数独找到一个符合的条件（就在树的叶子节点上）立刻就返回，相当于找从根节点到叶子节点一条唯一路径，所以需要使用bool返回值，这一点在【N皇后】中已经介绍过了，一样的道理。 代码如下： 1def backtracking(board): 递归终止条件 本题递归不用终止条件，解数独是要遍历整个树形结构寻找可能的叶子节点就立刻返回。 「不用终止条件会不会死循环？」 递归的下一层的棋盘一定比上一层的棋盘多一个数，等数填满了棋盘自然就终止（填满当然好了，说明找到结果了），所以不需要终止条件！ 「那么有没有永远填不满的情况呢？」 这个问题我在递归单层搜索逻辑里在来讲！ 在树形图中可以看出我们需要的是一个二维递归（也就是两个for循环嵌套着递归） 「一个for循环遍历棋盘的行，一个for循环遍历棋盘的列，一行一列确定下来之后，递归遍历这个位置放9个数字的可能性！」 代码如下：（「详细看注释」） 123456789101112def backtrack(board): for i in range(len(board)): # 遍历行 for j in range(len(board[0])): # 遍历列 if board[i][j] != &quot;.&quot;: continue for k in range(1,10): # (i, j) 这个位置放k是否合适 if isValid(i, j, k, board): board[i][j] = str(k) # 放置k if backtrack(board): return True # 如果找到合适一组立刻返回 board[i][j] = &quot;.&quot; # 回溯，撤销k return False # 9个数都试完了，都不行，那么就返回False return True # 遍历完没有返回false，说明找到了合适棋盘位置了 「注意这里return False的地方，这里放return False 是有讲究的」。 因为如果一行一列确定下来了，这里尝试了9个数都不行，说明这个棋盘找不到解决数独问题的解！ 那么会直接返回， 「这也就是为什么没有终止条件也不会永远填不满棋盘而无限递归下去！」 判断棋盘是否合法判断棋盘是否合法有如下三个维度： 同行是否重复 同列是否重复 9宫格里是否重复 代码如下： 1234567891011121314def isValid(row, col, val, board): for i in range(9): # 判断行里是否重复 if board[row][i] == str(val): return False for j in range(9): # 判断列里是否重复 if board[j][col] == str(val): return False startRow = (row // 3) * 3 startcol = (col // 3) * 3 for i in range(startRow,startRow + 3): # 判断9方格里是否重复 for j in range(startcol,startcol + 3): if board[i][j] == str(val): return False return True 最后整体代码如下： 1234567891011121314151617181920212223242526272829303132class Solution(object): def solveSudoku(self, board): &quot;&quot;&quot; :type board: List[List[str]] :rtype: None Do not return anything, modify board in-place instead. &quot;&quot;&quot; def backtrack(board): for i in range(len(board)): # 遍历行 for j in range(len(board[0])): # 遍历列 if board[i][j] != &quot;.&quot;: continue for k in range(1,10): # (i, j) 这个位置放k是否合适 if isValid(i, j, k, board): board[i][j] = str(k) # 放置k if backtrack(board): return True # 如果找到合适一组立刻返回 board[i][j] = &quot;.&quot; # 回溯，撤销k return False # 9个数都试完了，都不行，那么就返回false return True # 遍历完没有返回false，说明找到了合适棋盘位置了 def isValid(row, col, val, board): for i in range(9): # 判断行里是否重复 if board[row][i] == str(val): return False for j in range(9): # 判断列里是否重复 if board[j][col] == str(val): return False startRow = (row // 3) * 3 startcol = (col // 3) * 3 for i in range(startRow,startRow + 3): # 判断9方格里是否重复 for j in range(startcol,startcol + 3): if board[i][j] == str(val): return False return True backtrack(board) 总结解数独可以说是非常难的题目了，如果还一直停留在单层递归的逻辑中，这道题目可以让大家瞬间崩溃。 所以我在开篇就提到了【二维递归】。 「这样，解数独这么难的问题，也被我们攻克了」。","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"16-N皇后","slug":"16-N皇后","date":"2021-07-23T10:55:46.000Z","updated":"2021-07-23T10:56:48.852Z","comments":true,"path":"20210723/16-N皇后.html","link":"","permalink":"https://xxren8218.github.io/20210723/16-N%E7%9A%87%E5%90%8E.html","excerpt":"","text":"N皇后 思路都知道n皇后问题是回溯算法解决的经典问题，但是用回溯解决多了组合、切割、子集、排列问题之后，遇到这种二维矩阵还会有点不知所措。 首先来看一下皇后们的约束条件： 不能同行 不能同列 不能同斜线 确定完约束条件，来看看究竟要怎么去搜索皇后们的位置，其实搜索皇后的位置，可以抽象为一棵树。 下面我用一个3 * 3 的棋牌，将搜索过程抽象为一颗树，如图： 从图中，可以看出，二维矩阵中矩阵的高就是这颗树的高度，矩阵的宽就是树型结构中每一个节点的宽度。 那么我们用皇后们的约束条件，来回溯搜索这颗树，「只要搜索到了树的叶子节点，说明就找到了皇后们的合理位置了」。 回溯三部曲按照我总结的如下回溯模板，我们来依次分析： 123456789def backtracking(参数)： if 终止条件: 存放结果 return for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）: 处理节点 backtracking(路径，选择列表) # 递归 回溯，撤销处理结果 递归函数参数 我依然是定义全局变量二维数组result来记录最终结果。 参数n是棋牌的大小，然后用row来记录当前遍历到棋盘的第几层了。 代码如下： 123result = []chessboard = [[&quot;.&quot;] * n for i in range(n)]def backtracking(n, row, chessboard): 递归终止条件 在如下树形结构中： 可以看出，当递归到棋盘最底层（也就是叶子节点）的时候，就可以收集结果并返回了 代码如下： 123456789101112# 如果走到最后一行，说明已经找到一个解if row == n: # 注意不能直接加进去，注意格式，需要再处理一下 # result.append(chessboard) if row == n: temp_res = [] for temp in chessboard: temp_str = &quot;&quot;.join(temp) temp_res.append(temp_str) result.append(temp_res) return 单层搜索的逻辑 递归深度就是row控制棋盘的行，每一层里for循环的col控制棋盘的列，一行一列，确定了放置皇后的位置。 每次都是要从新的一行的起始位置开始搜，所以都是从0开始。 代码如下： 12345for col in range(0, n): if isValid(row, col, chessboard, n): # 验证合法就可以放 chessboard[row][col] = &#x27;Q&#x27; # 放置皇后 backtracking(n, row + 1, chessboard) chessboard[row][col] = &#x27;.&#x27; # 回溯，撤销皇后 验证棋牌是否合法 按照如下标准去重： 不能同行 不能同列 不能同斜线 （45度和135度角） 代码如下： 123456789101112131415161718192021222324def isValid(self, row, col, chessboard): # 判断同一列是否冲突 for i in range(row): if chessboard[i][col] == &quot;Q&quot;: return False # 判断45度是否冲突 i = row - 1 j = col - 1 while i &gt;= 0 and j &gt;= 0: if chessboard[i][j] == &quot;Q&quot;: return False i -= 1 j -= 1 # 判断135度是否冲突 i = row - 1 j = col + 1 while i &gt;= 0 and j &lt; len(chessboard): if chessboard[i][j] == &quot;Q&quot;: return False i -= 1 j += 1 return True 在这份代码中，细心的同学可以发现为什么没有在同行进行检查呢？ 因为在单层搜索的过程中，每一层递归，只会选for循环（也就是同一行）里的一个元素，所以不用去重了。 那么按照这个模板不难写出如下代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Solution(object): def __init__(self): self.result = [] def solveNQueens(self, n): &quot;&quot;&quot; :type n: int :rtype: List[List[str]] &quot;&quot;&quot; if not n: return [] chessboard = [[&quot;.&quot;] * n for i in range(n)] self.backtracking(n, 0, chessboard) return self.result def backtracking(self, n, row, chessboard): if row == n: temp_res = [] for temp in chessboard: temp_str = &quot;&quot;.join(temp) temp_res.append(temp_str) self.result.append(temp_res) return for col in range(n): if self.isValid(row, col, chessboard): chessboard[row][col] = &quot;Q&quot; self.backtracking(n, row + 1, chessboard) chessboard[row][col] = &quot;.&quot; def isValid(self, row, col, chessboard): # 判断同一列是否冲突 for i in range(row): if chessboard[i][col] == &quot;Q&quot;: return False # 判断45度是否冲突 i = row - 1 j = col - 1 while i &gt;= 0 and j &gt;= 0: if chessboard[i][j] == &quot;Q&quot;: return False i -= 1 j -= 1 # 判断135度是否冲突 i = row - 1 j = col + 1 while i &gt;= 0 and j &lt; len(chessboard): if chessboard[i][j] == &quot;Q&quot;: return False i -= 1 j += 1 return True 总结本题是我们解决棋盘问题的第一道题目。 如果从来没有接触过N皇后问题的同学看着这样的题会感觉无从下手，可能知道要用回溯法，但也不知道该怎么去搜。 「这里我明确给出了棋盘的宽度就是for循环的长度，递归的深度就是棋盘的高度，这样就可以套进回溯法的模板里了」。 大家可以在仔细体会体会！","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"15-重新安排行程","slug":"15-重新安排行程","date":"2021-07-23T10:53:43.000Z","updated":"2021-07-23T10:55:30.977Z","comments":true,"path":"20210723/15-重新安排行程.html","link":"","permalink":"https://xxren8218.github.io/20210723/15-%E9%87%8D%E6%96%B0%E5%AE%89%E6%8E%92%E8%A1%8C%E7%A8%8B.html","excerpt":"","text":"重新安排行程 思路这道题目还是很难的，之前我们用回溯法解决了如下问题：【组合】【分割】【子集】【排列】问题。 直觉上来看 这道题和回溯法没有什么关系，更像是图论中的深度优先搜索。 实际上确实是深搜，但这是深搜中使用了回溯的例子，在查找路径的时候，如果不回溯，怎么能查到目标路径呢。 所以我倾向于说本题应该使用回溯法，那么我也用回溯法的思路来讲解本题，其实深搜一般都使用了回溯法的思路。 「这里就是先给大家拓展一下，原来回溯法还可以这么玩！」 「这道题目有几个难点：」 一个行程中，如果航班处理不好容易变成一个圈，成为死循环 有多种解法，字母序靠前排在前面，让很多同学望而退步，如何该记录映射关系呢 ？ 使用回溯法（也可以说深搜） 的话，那么终止条件是什么呢？ 搜索的过程中，如何遍历一个机场所对应的所有机场。 针对以上问题我来逐一解答！ 如何理解死循环对于死循环，我来举一个有重复机场的例子： 为什么要举这个例子呢，就是告诉大家，出发机场和到达机场也会重复的，「如果在解题的过程中没有对集合元素处理好，就会死循环。」 该记录映射关系有多种解法，字母序靠前排在前面，让很多同学望而退步，如何该记录映射关系呢 ？ 一个机场映射多个机场，机场之间要靠字母序排列，一个机场映射多个机场，可以使用 {‘JFK’: [‘SFO’, ‘ATL’], ‘SFO’: [‘ATL’], ‘ATL’: [‘JFK’, ‘SFO’]}) 「再说一下为什么一定要增删元素呢，正如开篇我给出的图中所示，出发机场和到达机场是会重复的，搜索的过程没及时删除目的机场就会死循环。」 使用tickets_dict = defaultdict(list)来方便为不存在key的字典赋值为列表。 from collections import defaultdict 回溯法这道题目我使用回溯法，那么下面按照我总结的回溯模板来： 123456789def backtracking(参数): if 终止条件: 存放结果 return for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）: 处理节点 backtracking(路径，选择列表) # 递归 回溯，撤销处理结果 本题以输入：[[“JFK”, “KUL”], [“JFK”, “NRT”], [“NRT”, “JFK”]为例，抽象为树形结构如下： 开始回溯三部曲讲解： 递归函数参数 在讲解映射关系的时候，已经讲过了，用tickets_dict记录各个航班&lt;出发航班，[到达航班1，到达航班2…]&gt;信息，对其进行变量的传递。 代码如下： 1234567# &#123;“出发航班”：[到达航班1，到达航班2...]&#125;tickets_dict = defaultdict(list)for item in tickets: tickets_dict[item[0]].append(item[1]) def backtracking(tickets,start_point, tickets_dic): -&gt; 返回bool类型数据 「注意函数返回值是bool！」 我们之前讲解回溯算法的时候，一般函数返回值都是None，这次为什么是bool呢？ 因为我们只需要找到一个行程，就是在树形结构中唯一的一条通向叶子节点的路线，如图： 所以找到了这个叶子节点了直接返回。 12345678910for _ in tickets_dict: # 必须及时删除，避免出现死循环 end_point = tickets_dict[start_point].pop(0) path.append(end_point) # 只要找到一个就可以返回了 if backtracking(tickets, end_point, tickets_dict): return True path.pop() tickets_dict[start_point].append(end_point) 递归终止条件 拿题目中的示例为例，输入: [[“MUC”, “LHR”], [“JFK”, “MUC”], [“SFO”, “SJC”], [“LHR”, “SFO”]] ，这是有4个航班，那么只要找出一种行程，行程里的机场个数是5就可以了。 所以终止条件是：我们回溯遍历的过程中，遇到的机场个数，如果达到了（航班数量+1），那么我们就找到了一个行程，把所有航班串在一起了。 代码如下： 12if len(result) == len(tickets) + 1 return True 已经看习惯回溯法代码的同学，到叶子节点了习惯性的想要收集结果，但发现并不需要，本题的path就是记录路径的（就一条），在如下单层搜索的逻辑中result就添加元素了。 单层搜索的逻辑 123456789# 必须及时删除，避免出现死循环end_point = tickets_dict[start_point].pop(0)path.append(end_point)# 只要找到一个就可以返回了if backtracking(tickets, end_point, tickets_dict): return Truepath.pop()tickets_dict[start_point].append(end_point) 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def __init__(self): self.path = [&quot;JFK&quot;] def findItinerary(self, tickets): &quot;&quot;&quot; :type tickets: List[List[str]] :rtype: List[str] &quot;&quot;&quot; tickets_dict = defaultdict(list) ############################# # 它的其他功能与dict相同，但会为一个不存在的键提供默认值， ############################# for item in tickets: tickets_dict[item[0]].append(item[1]) &#x27;&#x27;&#x27; tickets_dict里面的内容是这样的 &#123;&#x27;JFK&#x27;: [&#x27;SFO&#x27;, &#x27;ATL&#x27;], &#x27;SFO&#x27;: [&#x27;ATL&#x27;], &#x27;ATL&#x27;: [&#x27;JFK&#x27;, &#x27;SFO&#x27;]&#125; &#x27;&#x27;&#x27; self.backtracking(tickets, &quot;JFK&quot;, tickets_dict) return self.path def backtracking(self, tickets, start_point, tickets_dict): # 终止条件 if len(self.path) == len(tickets) + 1: return True tickets_dict[start_point].sort() for _ in tickets_dict[start_point]: # 必须及时删除，避免出现死循环 end_point = tickets_dict[start_point].pop(0) self.path.append(end_point) # 只要找到一个就可以返回了 if self.backtracking(tickets, end_point, tickets_dict): return True self.path.pop() tickets_dict[start_point].append(end_point) 总结本题其实可以算是一道hard的题目了，关于本题的难点我在文中已经列出了。 「如果单纯的回溯搜索（深搜）并不难，难还难在容器的选择和使用上」。 本题其实是一道深度优先搜索的题目，但是我完全使用回溯法的思路来讲解这道题题目，「算是给大家拓展一下思维方式，其实深搜和回溯也是分不开的，毕竟最终都是用递归」。 如果最终代码，发现照着回溯法模板画的话好像也能画出来，但难就难如何知道可以使用回溯，以及如何套进去。","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"14-全排列二","slug":"14-全排列二","date":"2021-07-23T10:51:37.000Z","updated":"2021-07-23T10:53:23.989Z","comments":true,"path":"20210723/14-全排列二.html","link":"","permalink":"https://xxren8218.github.io/20210723/14-%E5%85%A8%E6%8E%92%E5%88%97%E4%BA%8C.html","excerpt":"","text":"全排列二 思路这道题目和【排列问题】「给定一个可包含重复数字的序列」，要返回「所有不重复的全排列」。 这里又涉及到去重了。 在【求组合总和（三）】 、【求子集问题（二）】我们分别详细讲解了组合问题和子集问题如何去重。 那么排列问题其实也是一样的套路。 「还要强调的是去重一定要对元素进行排序，这样我们才方便通过相邻的节点来判断是否重复使用了」。 我以示例中的 [1,1,2]为例 （为了方便举例，已经排序）抽象为一棵树，去重过程如图： 图中我们对同一树层，前一位（也就是nums[i-1]）如果使用过，那么就进行去重。 「一般来说：组合问题和排列问题是在树形结构的叶子节点上收集结果，而子集问题就是取树上所有节点的结果」。 在【排列问题】已经详解讲解了排列问题的写法，【求组合总和（三）】 、【求子集问题（二）】详细讲解的去重的写法，所以这次我就不用回溯三部曲分析了，直接给出代码，如下： 1234567891011121314151617181920212223242526272829class Solution(object): def __init__(self): self.path = [] self.result = [] def permuteUnique(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; if not nums: return [] nums.sort() used = [False] * len(nums) self.backtracking(nums, used) return self.result def backtracking(self, nums, used): if len(self.path) == len(nums): self.result.append(self.path[:]) return for i in range(0, len(nums)): if i &gt; 0 and nums[i] == nums[i - 1] and used[i - 1] == False: continue if used[i] == False: self.path.append(nums[i]) used[i] = True self.backtracking(nums, used) used[i] = False self.path.pop() 大家发现，去重最为关键的代码为： 12if i &gt; 0 and nums[i] == nums[i - 1] and used[i - 1] == False: continue 「如果改成 used[i - 1] == true， 也是正确的!」，去重代码如下： 12if i &gt; 0 and nums[i] == nums[i - 1] and used[i - 1] == True continue 这是为什么呢，就是上面我刚说的，如果要对树层中前一位去重，就用used[i - 1] == False，如果要对树枝前一位去重用used[i - 1] == True。 「对于排列问题，树层上去重和树枝上去重，都是可以的，但是树层上去重效率更高！」 这么说是不是有点抽象？ 来来来，我就用输入: [1,1,1] 来举一个例子。 树层上去重(used[i - 1] == False)，的树形结构如下： 树枝上去重（used[i - 1] == True）的树型结构如下： 总结这道题其实还是用了我们之前讲过的去重思路，但有意思的是，去重的代码中，这么写： 12if i &gt; 0 and nums[i] == nums[i - 1] and used[i - 1] == False: continue 和这么写： 12if i &gt; 0 and nums[i] == nums[i - 1] and used[i - 1] == True: continue 都是可以的，这也是很多同学做这道题目困惑的地方，知道used[i - 1] == False也行而used[i - 1] == True也行，但是就想不明白为啥。 所以我通过举[1,1,1]的例子，把这两个去重的逻辑分别抽象成树形结构，大家可以一目了然：为什么两种写法都可以以及哪一种效率更高！","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"13-全排列","slug":"13-全排列","date":"2021-07-22T10:59:03.000Z","updated":"2021-07-22T11:00:00.985Z","comments":true,"path":"20210722/13-全排列.html","link":"","permalink":"https://xxren8218.github.io/20210722/13-%E5%85%A8%E6%8E%92%E5%88%97.html","excerpt":"","text":"全排列 思路此时我们已经学习了【组合问题】【切割问题】和【子集问题】，接下来看一看排列问题。 相信这个排列问题就算是让你用for循环暴力把结果搜索出来，这个暴力也不是很好写。 为什么回溯法是暴力搜索，效率这么低，还要用它？这就是原因。 「因为一些问题能暴力搜出来就已经很不错了！」 我以[1,2,3]为例，抽象成树形结构如下： 回溯三部曲 递归函数参数 「首先排列是有序的，也就是说[1,2] 和[2,1] 是两个集合，这和之前分析的子集以及组合所不同的地方」。 可以看出元素1在[1,2]中已经使用过了，但是在[2,1]中还要在使用一次1，所以处理排列问题就不用使用startIndex了。 但排列问题需要一个used数组，标记已经选择的元素，如图橘黄色部分所示: 代码如下： 123result = []path = []def backtracking (nums, used): 递归终止条件 可以看出叶子节点，就是收割结果的地方。 那么什么时候，算是到达叶子节点呢？ 当收集元素的数组path的大小达到和nums数组一样大的时候，说明找到了一个全排列，也表示到达了叶子节点。 代码如下： 1234# 此时说明找到了一组if len(path) == len(nums): result.append(path) return 单层搜索的逻辑 这里和【组合问题】【切割问题】和【子集问题】最大的不同就是for循环里不用startIndex了。 因为排列问题，每次都要从头开始搜索，例如元素1在[1,2]中已经使用过了，但是在[2,1]中还要再使用一次1。 「而used数组，其实就是记录此时path里都有哪些元素使用了，一个排列里一个元素只能使用一次」。 代码如下： 12345678for i in range(0, len(nums)): if used[i] == True: continue # path里已经收录的元素，直接跳过 used[i] = True path.append(nums[i]) backtracking(nums, used) path.pop() used[i] = False 整体代码如下： 12345678910111213141516171819202122232425262728class Solution(object): def __init__(self): self.path = [] self.result = [] def permute(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; if not nums: return [] used = [False] * len(nums) self.backtracking(nums, used) return self.result def backtracking(self, nums, used): if len(self.path) == len(nums): self.result.append(self.path[:]) return for i in range(0, len(nums)): if used[i] == True: continue self.path.append(nums[i]) used[i] = True self.backtracking(nums, used) used[i] = False self.path.pop() 总结总结大家此时可以感受出排列问题的不同： 每层都是从0开始搜索而不是startIndex 需要used数组记录path里都放了哪些元素了 排列问题是回溯算法解决的经典题目，大家可以好好体会体会。","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"12-递增子序列","slug":"12-递增子序列","date":"2021-07-22T10:57:46.000Z","updated":"2021-07-22T10:58:47.021Z","comments":true,"path":"20210722/12-递增子序列.html","link":"","permalink":"https://xxren8218.github.io/20210722/12-%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97.html","excerpt":"","text":"自增子序列 思路这个递增子序列比较像是取有序的子集。而且本题也要求不能有相同的递增子序列。 这又是子集，又是去重，是不是不由自主的想起了刚刚讲过的【子集问题二】 就是因为太像了，更要注意差别所在，要不就掉坑里了！ 在【子集问题二】我们是通过排序，再加一个标记数组来达到去重的目的。 而本题求自增子序列，是不能对原数组进行排序的，排完序的数组都是自增子序列了。 「所以不能使用之前的去重逻辑！」 本题给出的示例，还是一个有序数组 [4, 6, 7, 7]，这更容易误导大家按照排序的思路去做了。 为了有鲜明的对比，我用[4, 7, 6, 7]这个数组来举例，抽象为树形结构如图： 回溯三部曲 递归函数参数 本题求子序列，很明显一个元素不能重复使用，所以需要startIndex，调整下一层递归的起始位置。 代码如下： 123result = []path = []def backtracking(nums, startIndex) 终止条件 本题其实类似求子集问题，也是要遍历树形结构找每一个节点，所以【求子集问题】一样，可以不加终止条件，startIndex每次都会加1，并不会无限递归。 但本题收集结果有所不同，题目要求递增子序列大小至少为2，所以代码如下： 123if len(path) &gt; 1: result.append(path[:]) # 注意这里不要加return，因为要取树上的所有节点 单层搜索逻辑 在图中可以看出，同层上使用过的元素就不能在使用了，「注意这里和【求子集问题】中去重的区别」。 「本题只要同层重复使用元素，递增子序列就会重复」，【求子集问题】是排序之后看相邻元素是否重复使用。 还有一种情况就是如果选取的元素小于子序列最后一个元素，那么就不能是递增的，所以也要pass掉。 那么去重的逻辑代码如下： 12if path and nums[i] &lt; path[-1] or path[i] in repeat: continue 判断nums[i] &lt; path[-1]之前一定要判断path是否为空，所以是path and nums[i] &lt; path[-1]。 uset.find(nums[i]) != uset.end()判断nums[i]在本层是否使用过。 那么单层搜索代码如下： 123456789uset = [] # 使用set来对本层元素进行去重for i in range(startIndex, lne(nums)): if path and nums[i] &lt; path[-1] or path[i] in repeat: continue repeat.append(nums[i]) # 记录这个元素在本层用过了，本层后面不能再用了 path.append(nums[i]) backtracking(nums, i + 1) path.pop() 「对于已经习惯写回溯的同学，看到递归函数上面的repeat.append(nums[i])，下面却没有对应的pop之类的操作，应该很不习惯吧，哈哈」 「这也是需要注意的点，repeat.append(nums[i] 是记录本层元素是否重复使用，新的一层repeat都会重新定义（清空），所以要知道repeat只负责本层！」 最后代码如下： 1234567891011121314151617181920212223242526class Solution(object): def __init__(self): self.result = [] self.path = [] def findSubsequences(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; if not nums: return [] self.backtracking(nums, 0) return self.result def backtracking(self, nums, startIndex): if len(self.path) &gt; 1: self.result.append(self.path[:]) # 用来标记本层用过的数。 repeat = [] for i in range(startIndex, len(nums)): if self.path and nums[i] &lt; self.path[-1] or nums[i] in repeat: continue repeat.append(nums[i]) self.path.append(nums[i]) self.backtracking(nums, i + 1) self.path.pop() 总结本题题解清一色都说是深度优先搜索，但我更倾向于说它用回溯法，而且本题我也是完全使用回溯法的逻辑来分析的。 相信大家在本题中处处都能看到是【子集问题二】身影，但处处又都是陷阱。 「对于养成思维定式或者套模板套嗨了的同学，这道题起到了很好的警醒作用。更重要的是拓展了大家的思路！」","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"24-搜索树转累加树","slug":"24-搜索树转累加树","date":"2021-07-22T10:56:16.000Z","updated":"2021-07-22T10:57:22.365Z","comments":true,"path":"20210722/24-搜索树转累加树.html","link":"","permalink":"https://xxren8218.github.io/20210722/24-%E6%90%9C%E7%B4%A2%E6%A0%91%E8%BD%AC%E7%B4%AF%E5%8A%A0%E6%A0%91.html","excerpt":"","text":"搜素树转累加树 思路一看到累加树，相信很多小伙伴都会疑惑：如何累加？遇到一个节点，然后在遍历其他节点累加？怎么一想这么麻烦呢。 然后再发现这是一颗二叉搜索树，二叉搜索树啊，这是有序的啊。 那么有序的元素如果求累加呢？ 「其实这就是一棵树，大家可能看起来有点别扭，换一个角度来看，这就是一个有序数组[2, 5, 13]，求从后到前的累加数组，也就是[20, 18, 13]，是不是感觉这就简单了。」 为什么变成数组就是感觉简单了呢？ 因为数组大家都知道怎么遍历啊，从后向前，挨个累加就完事了，这换成了二叉搜索树，看起来就别扭了一些是不是。 那么知道如何遍历这个二叉树，也就迎刃而解了，「从树中可以看出累加的顺序是右中左，所以我们需要反中序遍历这个二叉树，然后顺序累加就可以了」。 递归遍历顺序如图所示： 本题依然需要一个pre指针记录当前遍历节点cur的前一个节点，这样才方便做累加。 pre指针的使用技巧，我们在【搜索树的最小绝对差】和【二叉树的众数】都提到了，这是常用的操作手段。 递归函数参数以及返回值 这里很明确了，不需要递归函数的返回值做什么操作了，要遍历整棵树。 同时需要定义一个全局变量pre，用来保存cur节点的前一个节点的数值，定义为int型就可以了。 代码如下： 12pre = 0 # 记录前一个节点的数值def traversal(cur): 确定终止条件 遇空就终止。 1if not cur: return 确定单层递归的逻辑 注意「要右中左来遍历二叉树」， 中节点的处理逻辑就是让cur的数值加上前一个节点的数值。 代码如下： 1234traversal(cur.right) # 右cur.val += pre # 中pre = cur.valtraversal(cur.left) # 左 整体代码如下： 1234567891011121314151617181920212223# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def __init__(self): self.pre = 0 def convertBST(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: TreeNode &quot;&quot;&quot; self.travsersal(root) return root def travsersal(self, cur): if not cur: return self.travsersal(cur.right) cur.val += self.pre self.pre = cur.val self.travsersal(cur.left) 总结二叉树就练习到这里了。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"23-构造搜索树","slug":"23-构造搜索树","date":"2021-07-22T10:49:36.000Z","updated":"2021-07-22T10:55:59.571Z","comments":true,"path":"20210722/23-构造搜索树.html","link":"","permalink":"https://xxren8218.github.io/20210722/23-%E6%9E%84%E9%80%A0%E6%90%9C%E7%B4%A2%E6%A0%91.html","excerpt":"","text":"构造搜索树 思路题目中说要转换为一棵高度平衡二叉搜索树。这和转换为一棵普通二叉搜索树有什么差别呢？ 其实这里不用强调平衡二叉搜索树，数组构造二叉树，构成平衡树是自然而然的事情，因为大家默认都是从数组中间位置取值作为节点元素，一般不会随机取，「所以想构成不平衡的二叉树是自找麻烦」。 在【构造二叉树】和【构造一棵最大的二叉树】其实已经讲过了，如何根据数组构造一颗二叉树。 「本质就是寻找分割点，分割点作为当前节点，然后递归左区间和右区间」。 本题比前面的简单一些，因为有序数组构造二叉搜索树，寻找分割点就比较容易了。 分割点就是数组中间位置的节点。 那么为问题来了，如果数组长度为偶数，中间节点有两个，取哪一个？ 取哪一个都可以，只不过构成了不同的平衡二叉搜索树。 例如：输入：[-10,-3,0,5,9] 如下两棵树，都是这个数组的平衡二叉搜索树： 如果要分割的数组长度为偶数的时候，中间元素为两个，是取左边元素 就是树1，取右边元素就是树2。 「这也是题目中强调答案不是唯一的原因。理解这一点，这道题目算是理解到位了」。 递归法递归三部曲： 确定递归函数返回值及其参数 删除二叉树节点，增加二叉树节点，都是用递归函数的返回值来完成，这样是比较方便的。 相信大家如果仔细看了【搜索树中的插入操作】和【搜索树中的删除操作】，一定会对递归函数返回值的作用深有感触。 那么本题要构造二叉树，依然用递归函数的返回值来构造中节点的左右孩子。 再来看参数，首先是传入数组，然后就是左下表left和右下表right，我们在【构造二叉树】中提过，在构造二叉树的时候尽量不要重新定义左右区间数组，而是用下表来操作原数组。 所以代码如下： 12# 左闭右闭区间[left, right]def traversal(nums, left, right): 这里注意，「我这里定义的是左闭右闭区间，在不断分割的过程中，也会坚持左闭右闭的区间，这又涉及到我们讲过的循环不变量」。 在【构造二叉树】讲过循环不变量。 确定递归终止条件 这里定义的是左闭右闭的区间，所以当区间 left &gt; right的时候，就是空节点了。 代码如下： 1if left &gt; right: return None 确定单层递归的逻辑 首先取数组中间元素的位置，不难写出mid = (left + right) / 2，「这么写其实有一个问题，就是数值越界，例如left和right都是最大int，这么操作就越界了，在[二分法中尤其需要注意！」 所以可以这么写：mid = left + ((right - left) / 2) 但本题leetcode的测试数据并不会越界，所以怎么写都可以。但需要有这个意识！ 取了中间位置，就开始以中间位置的元素构造节点，代码：root = TreeNode(nums[mid])。 接着划分区间，root的左孩子接住下一层左区间的构造节点，右孩子接住下一层右区间构造的节点。 最后返回root节点，单层递归整体代码如下： 12345mid = left + ((right - left) / 2)root = TreeNode(nums[mid])root.left = traversal(nums, left, mid - 1)root.right = traversal(nums, mid + 1, right)return root 这里mid = left + ((right - left) / 2)的写法相当于是如果数组长度为偶数，中间位置有两个元素，取靠左边的。 最后的整体代码如下图： 12345678910111213141516171819202122# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def sortedArrayToBST(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: TreeNode &quot;&quot;&quot; return self.traversal(nums, 0, len(nums) - 1) def traversal(self, nums, left, right): if left &gt; right: return None mid = left + ((right - left) / 2) root = TreeNode(nums[mid]) root.left = self.traversal(nums, left, mid - 1) root.right = self.traversal(nums, mid + 1, right) return root 「注意：在调用traversal的时候为什么传入的left和right为什么是0和len(nums)- 1，因为定义的区间为左闭右闭」。 总结【构造二叉树】 和 【构造一棵最大的二叉树】之后，我们顺理成章的应该构造一下二叉搜索树了，一不小心还是一棵平衡二叉搜索树」。 其实思路也是一样的，不断中间分割，然后递归处理左区间，右区间，也可以说是分治。 此时相信大家应该对通过递归函数的返回值来增删二叉树很熟悉了，这也是常规操作。 在定义区间的过程中我们又一次强调了循环不变量的重要性。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"11-子集问题二","slug":"11-子集问题二","date":"2021-07-21T09:07:10.000Z","updated":"2021-07-21T09:08:04.973Z","comments":true,"path":"20210721/11-子集问题二.html","link":"","permalink":"https://xxren8218.github.io/20210721/11-%E5%AD%90%E9%9B%86%E9%97%AE%E9%A2%98%E4%BA%8C.html","excerpt":"","text":"子集问题二 思路这道题目【子集问题】区别就是集合里有重复元素了，而且求取的子集要去重。 那么关于回溯算法中的去重问题，「在【组合总和II】中已经详细讲解过了，和本题是一个套路」。 「剧透一下，后期要讲解的排列问题里去重也是这个套路，所以理解“树层去重”和“树枝去重”非常重要」。 用示例中的[1, 2, 2] 来举例，如图所示：（「注意去重需要先对集合排序」） 从图中可以看出，同一树层上重复取2 就要过滤掉，同一树枝上就可以重复取2，因为同一树枝上元素的集合才是唯一子集！ 本题就是其实就是【子集问题】基础上加上了去重，去重我们在【组合总和三】也讲过了，所以我就直接给出代码了： 123456789101112131415161718192021222324252627class Solution(object): def __init__(self): self.res = [] self.path = [] def subsetsWithDup(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; nums.sort() used = [False]*len(nums) self.backtracking(nums, 0, used) return self.res def backtracking(self, nums, startIndex, used): self.res.append(self.path[:]) if startIndex &gt;= len(nums): return for i in range(startIndex, len(nums)): if i &gt; 0 and nums[i] == nums[i - 1] and used[i - 1] == False: continue self.path.append(nums[i]) used[i] = True self.backtracking(nums, i + 1, used) used[i] = False self.path.pop() 总结其实这道题目的知识点，我们之前都讲过了，如果之前讲过的子集问题和去重问题都掌握的好，这道题目应该分分钟AC。","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"10-求子集问题","slug":"10-求子集问题","date":"2021-07-21T09:05:30.000Z","updated":"2021-07-21T09:06:37.623Z","comments":true,"path":"20210721/10-求子集问题.html","link":"","permalink":"https://xxren8218.github.io/20210721/10-%E6%B1%82%E5%AD%90%E9%9B%86%E9%97%AE%E9%A2%98.html","excerpt":"","text":"求子集问题 思路求子集问题和【求组合】和【分割问题】又不一样了。 如果把 子集问题、组合问题、分割问题都抽象为一棵树的话，「那么组合问题和分割问题都是收集树的叶子节点，而子集问题是找树的所有节点！」 其实子集也是一种组合问题，因为它的集合是无序的，子集{1,2} 和 子集{2,1}是一样的。 「那么既然是无序，取过的元素不会重复取，写回溯算法的时候，for就要从startIndex开始，而不是从0开始！」 有同学问了，什么时候for可以从0开始呢？ 求排列问题的时候，就要从0开始，因为集合是有序的，{1, 2} 和{2, 1}是两个集合，排列问题我们后续会讲到的。 以示例中nums = [1,2,3]为例把求子集抽象为树型结构，如下： 从图中红线部分，可以看出「遍历这个树的时候，把所有节点都记录下来，就是要求的子集集合」。 回溯三部曲 递归函数参数 全局变量数组path为子集收集元素，二维数组result存放子集组合。（也可以放到递归函数参数里） 递归函数参数在上面讲到了，需要startIndex。 代码如下： 123result = []path = []def backtracking(nums, startIndex): 递归终止条件 从图中可以看出： 剩余集合为空的时候，就是叶子节点。 那么什么时候剩余集合为空呢？ 就是startIndex已经大于数组的长度了，就终止了，因为没有元素可取了，代码如下: 12if startIndex &gt;= len(nums): return 「其实可以不需要加终止条件，因为startIndex &gt;= nums.size()，本层for循环本来也结束了」。 单层搜索逻辑 「求取子集问题，不需要任何剪枝！因为子集就是要遍历整棵树」。 那么单层递归逻辑代码如下： 1234for i in range(startIndex, len(nums)): path.append(nums[i]) # 子集收集元素 backtracking(nums, i + 1) # 注意从i+1开始，元素不重复取 path.pop() # 回溯 整体代码如下： 回溯算法模板： 123456789def backtracking(参数): if 终止条件: 存放结果 return for 选择：本层集合中元素（树中节点孩子的数量就是集合的大小）: 处理节点 backtracking(路径，选择列表) # 递归 回溯，撤销处理结果 12345678910111213141516171819202122class Solution(object): def __init__(self): self.result = [] self.path = [] def subsets(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; self.backtracking(nums, 0) return self.result def backtracking(self, nums, startIndex): # 注意处理节点在这里 self.result.append(self.path[:]) if startIndex &gt;= len(nums): return for i in range(startIndex, len(nums)): self.path.append(nums[i]) self.backtracking(nums, i + 1) self.path.pop() 总结在注释中，可以发现可以不写终止条件，因为本来我们就要遍历整颗树。 有的同学可能担心不写终止条件会不会无限递归？ 并不会，因为每次递归的下一层就是从i+1开始的。 子集问题有点简单了，其实这就是一道标准的模板题。 但是要清楚子集问题和组合问题、分割问题的的区别，「子集是收集树形结构中树的所有节点的结果」。 「而组合问题、分割问题是收集树形结构中叶子节点的结果」。","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"22-修剪一颗搜索树","slug":"22-修剪一颗搜索树","date":"2021-07-21T09:03:15.000Z","updated":"2021-07-21T09:05:06.754Z","comments":true,"path":"20210721/22-修剪一颗搜索树.html","link":"","permalink":"https://xxren8218.github.io/20210721/22-%E4%BF%AE%E5%89%AA%E4%B8%80%E9%A2%97%E6%90%9C%E7%B4%A2%E6%A0%91.html","excerpt":"","text":"修剪一颗搜索树 思路这个题真的不简单！ 递归法直接想法就是：递归处理，然后遇到 root.val &lt; low or root.val &gt; high 的时候直接return None，一波修改，干净利落。 不难写出如下代码： 12345def trimBST(root, low, high): if not root or root.val &lt; low or root.val &gt; high: return None root.left = trimBST(root.left, low, high) root.right = trimBST(root.right, low, high) return root 「然而[1, 3]区间在二叉搜索树的中可不是单纯的节点3和左孩子节点0就决定的，还要考虑节点0的右子树」。 我们在重新关注一下第二个示例，如图： 「所以以上的代码是不可行的！」 从图中可以看出需要重构二叉树，想想是不是本题就有点复杂了。 其实不用重构那么复杂。 在上图中我们发现节点0并不符合区间要求，那么将节点0的右孩子 节点2 直接赋给 节点3的左孩子就可以了（就是把节点0从二叉树中移除），如图： 理解了最关键部分了我们在递归三部曲： 确定递归函数的参数以及返回值 这里我们为什么需要返回值呢？ 因为是要遍历整棵树，做修改，其实不需要返回值也可以，我们也可以完成修剪（其实就是从二叉树中移除节点）的操作。 但是有返回值，更方便，可以通过递归函数的返回值来移除节点。 这样的做法【搜索树中的插入操作】和【搜索树中的删除操作】中大家已经了解过了。 代码如下： 1def trimBST(root, low, high): 确定终止条件 修剪的操作并不是在终止条件上进行的，所以就是遇到空节点返回就可以了。 1if not root: return None 确定单层递归的逻辑 如果root（当前节点）的元素小于low的数值，那么应该递归右子树，并返回右子树符合条件的头结点。 代码如下： 123if root.val &lt; low: right = trimBST(root.right, low, high) # 寻找符合区间[low, high]的节点 return right 如果root(当前节点)的元素大于high的，那么应该递归左子树，并返回左子树符合条件的头结点。 代码如下： 123if root.val &gt; high: left = trimBST(root.left, low, high) # 寻找符合区间[low, high]的节点 return left 接下来要将下一层处理完左子树的结果赋给root.left，处理完右子树的结果赋给root.right。 最后返回root节点，代码如下： 123root.left = trimBST(root.left, low, high) # root.left接入符合条件的左孩子root.right = trimBST(root.right, low, high) # root.right接入符合条件的右孩子return root 此时大家是不是还没发现这多余的节点究竟是如何从二叉树中移除的呢？ 在回顾一下上面的代码，针对下图中二叉树的情况 如下代码相当于把节点0的右孩子（节点2）返回给上一层， 123if root.val &lt; low: right = trimBST(root.right, low, high) # 寻找符合区间[low, high]的节点 return right 然后如下代码相当于用节点3的左孩子 把下一层返回的 节点0的右孩子（节点2） 接住。 1root.left = trimBST(root.left, low, high) 此时节点3的右孩子就变成了节点2，将节点0从二叉树中移除了。 123456789101112131415161718192021222324# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def trimBST(self, root, low, high): &quot;&quot;&quot; :type root: TreeNode :type low: int :type high: int :rtype: TreeNode &quot;&quot;&quot; if not root: return None if root.val &lt; low: right = self.trimBST(root.right, low, high) return right if root.val &gt; high: left = self.trimBST(root.left, low, high) return left root.left = self.trimBST(root.left, low, high) root.right = self.trimBST(root.right, low, high) return root 迭代法因为二叉搜索树的有序性，不需要使用栈模拟递归的过程。 在剪枝的时候，可以分为三步： 将root移动到[L, R] 范围内，注意是左闭右闭区间 剪枝左子树 剪枝右子树 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def trimBST(self, root, low, high): &quot;&quot;&quot; :type root: TreeNode :type low: int :type high: int :rtype: TreeNode &quot;&quot;&quot; if not root: return None # 处理头结点，让root移动到[L, R] 范围内，注意是左闭右闭 while root and (root.val &lt; low or root.val &gt; high): if root.val &lt; low: root = root.right # 小于L往右走 else: root = root.left # 大于R往右走 cur = root # 此时root已经在[L, R] 范围内，处理左孩子元素小于L的情况 while cur: while cur.left and cur.left.val &lt; low: cur.left = cur.left.right cur = cur.left cur = root # 此时root已经在[L, R] 范围内，处理右孩子大于R的情况 while cur: while cur.right and cur.right.val &gt; high: cur.right = cur.right.left cur = cur.right return root 总结修剪二叉搜索树其实并不难，但在递归法中大家可看出我费了很大的功夫来讲解如何删除节点的，这个思路其实是比较绕的。 最终的代码倒是很简洁。 「如果不对递归有深刻的理解，这道题目还是有难度的！」 本题我依然给出递归法和迭代法，初学者掌握递归就可以了，如果想进一步学习，就把迭代法也写一写。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"09-复原ip地址","slug":"09-复原ip地址","date":"2021-07-20T09:29:08.000Z","updated":"2021-07-20T09:30:42.705Z","comments":true,"path":"20210720/09-复原ip地址.html","link":"","permalink":"https://xxren8218.github.io/20210720/09-%E5%A4%8D%E5%8E%9Fip%E5%9C%B0%E5%9D%80.html","excerpt":"","text":"复原ip地址 思路做这道题目之前，最好先把【分割回文串】这个做了。 这道题目相信大家刚看的时候，应该会一脸茫然。 其实只要意识到这是切割问题，「切割问题就可以使用回溯搜索法把所有可能性搜出来」，和刚做过的【分割回文串】十分类似了。 切割问题可以抽象为树型结构，如图： 回溯三部曲 递归参数 在【分割回文串】中我们就提到切割问题类似组合问题。 startIndex一定是需要的，因为不能重复分割，记录下一层递归分割的起始位置。 本题我们还需要一个变量pointNum，记录添加逗点的数量。 所以代码如下： 123result = [] # 记录结果# startIndex: 搜索的起始位置，pointNum:添加逗点的数量def backtracking(s, startIndex, pointNum): 递归终止条件 终止条件和【分割回文串】情况就不同了，本题明确要求只会分成4段，所以不能用切割线切到最后作为终止条件，而是分割的段数作为终止条件。 pointNum表示逗点数量，pointNum为3说明字符串分成了4段了。 然后验证一下第四段是否合法，如果合法就加入到结果集里 代码如下： 12345if pointNum == 3: # 逗点数量为3时，分隔结束 # 判断第四段子字符串是否合法，如果合法就放进result中 if isValid(s, startIndex, len(s) - 1): result.append(s) return 单层搜索的逻辑 在【分割回文串】中已经讲过在循环遍历中如何截取子串。 在for i in range(startIndex, len(s)):循环中 [startIndex, i]这个区间就是截取的子串，需要判断这个子串是否合法。 如果合法就在字符串后面加上符号.表示已经分割。 如果不合法就结束本层循环，如图中剪掉的分支： 然后就是递归和回溯的过程： 递归调用时，下一层递归的startIndex要从i+2开始（因为需要在字符串中加入了分隔符.），同时记录分割符的数量pointNum 要 +1。 回溯的时候，就将刚刚加入的分隔符. 删掉就可以了，pointNum也要-1。 代码如下： 123456789101112131415for i in range(startIndex, len(s)): if isValid(s, startIndex, i): # 判断 [startIndex,i] 这个区间的子串是否合法 tmp = list(s) # 在i的后面插入一个逗点 tmp.insert(i + 1,&#x27;.&#x27;) s = &#x27;&#x27;.join(tmp) pointNum += 1 backtracking(s, i + 2, pointNum) # 插入逗点之后下一个子串的起始位置为i+2 pointNum -= 1 # 回溯 tmp = list(s) tmp.pop(i + 1) s = &#x27;&#x27;.join(tmp) # 回溯删掉逗点 else: break # 不合法，直接结束本层循环 判断子串是否合法最后就是在写一个判断段位是否是有效段位了。 主要考虑到如下三点： 以0为开头的数字不合法 含有非整数的字符不合法 大于255不合法 代码如下： 12345678910111213141516171819# 判断字符串s在左闭又闭区间[start, end]所组成的数字是否合法def isValid(s, start, end): if start &gt; end: return False if s[start] == &#x27;0&#x27; and start != end: # 0开头的数字不合法 return False num = 0 for i in range(start, end + 1): if s[i] &gt; &#x27;9&#x27; or s[i] &lt; &#x27;0&#x27;: # 遇到非数字字符不合法 return False num = num * 10 + (ord(s[i]) - ord(&#x27;0&#x27;)) if num &gt; 255: # 如果大于255了不合法 return False return True 根据回溯算法的模板： 123456789def backtracking(参数): if 终止条件: 存放结果 return for 选择：本层集合中元素（树中节点孩子的数量就是集合的大小）: 处理节点 backtracking(路径，选择列表) # 递归 回溯，撤销处理结果 整体代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Solution(object): def __init__(self): self.result = [] def restoreIpAddresses(self, s): &quot;&quot;&quot; :type s: str :rtype: List[str] &quot;&quot;&quot; self.backtracking(s, 0, 0) return self.result def backtracking(self, s, startIndex, pointNum): if pointNum == 3: if self.isValid(s, startIndex, len(s) - 1): self.result.append(s) return for i in range(startIndex, len(s)): if self.isValid(s, startIndex, i): tmp = list(s) # 在i的后面插入一个逗点 tmp.insert(i + 1,&#x27;.&#x27;) s = &#x27;&#x27;.join(tmp) pointNum += 1 self.backtracking(s, i + 2, pointNum) # 插入逗点之后下一个子串的起始位置为i+2 pointNum -= 1 # 回溯 tmp = list(s) tmp.pop(i + 1) s = &#x27;&#x27;.join(tmp) # 回溯删掉逗点 else: break # 判断字符串s在左闭又闭区间[start, end]所组成的数字是否合法 def isValid(self, s, start, end): if start &gt; end: return False if s[start] == &#x27;0&#x27; and start != end: # 0开头的数字不合法 return False num = 0 for i in range(start, end + 1): if s[i] &gt; &#x27;9&#x27; or s[i] &lt; &#x27;0&#x27;: # 遇到非数字字符不合法 return False num = num * 10 + (ord(s[i]) - ord(&#x27;0&#x27;)) if num &gt; 255: # 如果大于255了不合法 return False return True 总结在【分割回文串】列举的分割字符串的难点，本题都覆盖了。 而且本题还需要操作字符串添加逗号作为分隔符，并验证区间的合法性。 可以说是【分割回文串】的加强版。 在本文的树形结构图中，我已经把详细的分析思路都画了出来，相信大家看了之后一定会思路清晰不少！","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"21-搜索树中的删除操作","slug":"21-搜索树中的删除操作","date":"2021-07-20T09:27:18.000Z","updated":"2021-07-20T09:28:42.703Z","comments":true,"path":"20210720/21-搜索树中的删除操作.html","link":"","permalink":"https://xxren8218.github.io/20210720/21-%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%AD%E7%9A%84%E5%88%A0%E9%99%A4%E6%93%8D%E4%BD%9C.html","excerpt":"","text":"搜索树中的删除操作 思路递归三部曲： 确定递归函数参数以及返回值 说道递归函数的返回值，在【搜索树中的插入操作】中通过递归返回值来加入新节点， 这里也可以通过递归返回值删除节点。 代码如下： 1def deleteNode(root, key) 确定终止条件 遇到空返回，其实这也说明没找到删除的节点，遍历到空节点直接返回了 1if not root: return None 确定单层递归的逻辑 这里就把平衡二叉树中删除节点遇到的情况都搞清楚。 有以下五种情况： 第一种情况：没找到删除的节点，遍历到空节点直接返回了 找到删除的节点 第二种情况：左右孩子都为空（叶子节点），直接删除节点， 返回NULL为根节点 第三种情况：删除节点的左孩子为空，右孩子不为空，删除节点，右孩子补位，返回右孩子为根节点 第四种情况：删除节点的右孩子为空，左孩子不为空，删除节点，左孩子补位，返回左孩子为根节点 第五种情况：左右孩子节点都不为空，则将删除节点的左子树头结点（左孩子）放到删除节点的右子树的最左面节点的左孩子上，返回删除节点右孩子为新的根节点。 动画中颗二叉搜索树中，删除元素7， 那么删除节点（元素7）的左孩子就是5，删除节点（元素7）的右子树的最左面节点是元素8。 将删除节点（元素7）的左孩子放到删除节点（元素7）的右子树的最左面节点（元素8）的左孩子上，就是把5为根节点的子树移到了8的左孩子的位置。 要删除的节点（元素7）的右孩子（元素9）为新的根节点。. 这样就完成删除元素7的逻辑，最好动手画一个图，尝试删除一个节点试试。 代码如下： 123456789101112131415161718if root.val == key: # 第二种情况：左右孩子都为空（叶子节点），直接删除节点， 返回NULL为根节点 # 第三种情况：其左孩子为空，右孩子不为空，删除节点，右孩子补位 ，返回右孩子为根节点 if not root.left: return root.right # 第四种情况：其右孩子为空，左孩子不为空，删除节点，左孩子补位，返回左孩子为根节点 elif not root.right: return root.left # 第五种情况：左右孩子节点都不为空，则将删除节点的左子树放到删除节点的右子树的最左面节点的左孩子的位置 # 并返回删除节点右孩子为新的根节点。 else: cur = root.right # 找右子树最左面的节点 while cur.left: cur = cur.left cur.left = root.left # 把要删除的节点（root）左子树放在cur的左孩子的位置 tmp = root # 把root节点保存一下，下面来删除 root = root.right # 返回旧root的右孩子作为新root del tmp # 释放节点内存（这里不写也可以，但C++最好手动释放一下吧） return root 这里相当于把新的节点返回给上一层，上一层就要用 root.left 或者 root.right接住，代码如下： 123if root.val &gt; key: root.left = deleteNode(root.left, key)if root.val &lt; key: root.right = deleteNode(root.right, key)return root 「整体代码如下：（注释中：情况1，2，3，4，5和上面分析严格对应）」 123456789101112131415161718192021222324252627282930313233# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def deleteNode(self, root, key): &quot;&quot;&quot; :type root: TreeNode :type key: int :rtype: TreeNode &quot;&quot;&quot; if not root: return None if root.val == key: if not root.left: return root.right elif not root.right: return root.left else: cur = root.right while cur.left: cur = cur.left cur.left = root.left tmp = root root = root.right del tmp return root if root.val &lt; key: root.right = self.deleteNode(root.right, key) if root.val &gt; key: root.left = self.deleteNode(root.left, key) return root 总结读完本篇，大家会发现二叉搜索树删除节点比增加节点复杂的多。 「因为二叉搜索树添加节点只需要在叶子上添加就可以的，不涉及到结构的调整，而删除节点操作涉及到结构的调整」。 这里我们依然使用递归函数的返回值来完成把节点从二叉树中移除的操作。 「这里最关键的逻辑就是第五种情况（删除一个左右孩子都不为空的节点），这种情况一定要想清楚」。 而且就算想清楚了，对应的代码也未必可以写出来，所以「这道题目即考察思维逻辑，也考察代码能力」。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"08-分割回文串","slug":"08-分割回文串","date":"2021-07-19T09:36:53.000Z","updated":"2021-07-19T09:38:12.413Z","comments":true,"path":"20210719/08-分割回文串.html","link":"","permalink":"https://xxren8218.github.io/20210719/08-%E5%88%86%E5%89%B2%E5%9B%9E%E6%96%87%E4%B8%B2.html","excerpt":"","text":"分割回文串 思路本题涉及到两个关键问题： 切割问题，有不同的切割方式 判断回文 相信这里不同的切割方式可以搞懵很多同学了。 这种题目，想用for循环暴力解法，可能都不那么容易写出来，所以要换一种暴力的方式，就是回溯。 一些同学可能想不清楚 回溯究竟是如果切割字符串呢？ 我们来分析一下切割，「其实切割问题类似组合问题」。 例如对于字符串abcdef： 组合问题：选取一个a之后，在bcdef中再去选取第二个，选取b之后在cdef中在选取第三个…..。 切割问题：切割一个a之后，在bcdef中再去切割第二段，切割b之后在cdef中在切割第三段…..。 感受出来了不？ 所以切割问题，也可以抽象为一颗树形结构，如图： 递归用来纵向遍历，for循环用来横向遍历，切割线（就是图中的红线）切割到字符串的结尾位置，说明找到了一个切割方法。 此时可以发现，切割问题的回溯搜索的过程和组合问题的回溯搜索的过程是差不多的。 回溯三部曲 递归函数参数 全局变量数组path存放切割后回文的子串，数组result存放结果集。（这两个参数可以放到函数参数里） 本题递归函数参数还需要startIndex，因为切割过的地方，不能重复切割，和组合问题也是保持一致的。 在【求组合总和二】中我们深入探讨了组合问题什么时候需要startIndex，什么时候不需要startIndex。 代码如下： 123result = []path = [] # 放已经回文的子串def backtracking (s, startIndex): 递归函数终止条件 从树形结构的图中可以看出：切割线切到了字符串最后面，说明找到了一种切割方法，此时就是本层递归的终止终止条件。 「那么在代码里什么是切割线呢？」 在处理组合问题的时候，递归参数需要传入startIndex，表示下一轮递归遍历的起始位置，这个startIndex就是切割线。 所以终止条件代码如下： 12345def backtracking (s, startIndex): # 如果起始位置已经大于s的大小，说明已经找到了一组分割方案了 if startIndex &gt;= len(s): result.append(path) return 单层搜索的逻辑 「来看看在递归循环，中如何截取子串呢？」 在for i in range(startIndex, len(s)):循环中，我们 定义了起始位置startIndex，那么 [startIndex, i] 就是要截取的子串。 首先判断这个子串是不是回文，如果是回文，就加入在path中，path用来记录切割过的回文子串。 代码如下： 1234567891011for i in range(startIndex, len(s)): # 是回文子串 if isPalindrome(s, startIndex, i): # 获取[startIndex,i]在s中的子串 Str = s[startIndex, i - startIndex + 1] path.append(Str) else: # 如果不是则直接跳过 continue backtracking(s, i + 1) # 寻找i+1为起始位置的子串 path.pop() # 回溯过程，弹出本次已经填在的子串 「注意切割过的位置，不能重复切割，所以，backtracking(s, i + 1); 传入下一层的起始位置为i + 1」。 判断回文子串最后我们看一下回文子串要如何判断了，判断一个字符串是否是回文。 可以使用双指针法，一个指针从前向后，一个指针从后先前，如果前后指针所指向的元素是相等的，就是回文字符串了。 那么判断回文的代码如下： 123456789def isPalindrome(s, start, end): i, j = start, end while i &lt; j: if (s[i] != s[j]): return False i += 1 j -= 1 return True 此时关键代码已经讲解完毕. 根据回溯算法模板： 123456789def backtracking(参数) &#123; if 终止条件: 存放结果 return for 选择：本层集合中元素（树中节点孩子的数量就是集合的大小）: 处理节点 backtracking(路径，选择列表) # 递归 回溯，撤销处理结果 不难写出如下代码： 整体代码如下： 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def __init__(self): self.result = [] self.path = [] def partition(self, s): &quot;&quot;&quot; :type s: str :rtype: List[List[str]] &quot;&quot;&quot; self.backtracking(s, 0) return self.result def backtracking(self, s, startIndex): if startIndex &gt;= len(s): self.result.append(self.path[:]) return for i in range(startIndex, len(s)): if self.isPalandrome(s, startIndex, i): Str = s[startIndex: i + 1] self.path.append(Str) else: continue self.backtracking(s, i + 1) self.path.pop() def isPalandrome(self, s, start, end): i, j = start, end while i &lt; j: if s[i] != s[j]: return False i += 1 j -= 1 return True 总结这道题目在leetcode上是中等，但可以说是hard的题目了，但是代码其实就是按照模板的样子来的。 那么难究竟难在什么地方呢？ 「我列出如下几个难点：」 切割问题可以抽象为组合问题 如何模拟那些切割线 切割问题中递归如何终止 在递归循环中如何截取子串 如何判断回文 「我们平时在做难题的时候，总结出来难究竟难在哪里也是一种需要锻炼的能力」。 一些同学可能遇到题目比较难，但是不知道题目难在哪里，反正就是很难。其实这样还是思维不够清晰，这种总结的能力需要多接触多锻炼。 「本题我相信很多同学主要卡在了第一个难点上：就是不知道如何切割，甚至知道要用回溯法，也不知道如何用。也就是没有体会到按照求组合问题的套路就可以解决切割」。 如果意识到这一点，算是重大突破了。接下来就可以对着模板照葫芦画瓢。 「但接下来如何模拟切割线，如何终止，如何截取子串，其实都不好想，最后判断回文算是最简单的了」。 除了这些难点，「本题还有细节，例如：切割过的地方不能重复切割所以递归函数需要传入i + 1」。 所以本题应该是一个道hard题目了。","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"20-搜索树中的插入操作","slug":"20-搜索树中的插入操作","date":"2021-07-19T09:35:24.000Z","updated":"2021-07-19T09:36:34.012Z","comments":true,"path":"20210719/20-搜索树中的插入操作.html","link":"","permalink":"https://xxren8218.github.io/20210719/20-%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%AD%E7%9A%84%E6%8F%92%E5%85%A5%E6%93%8D%E4%BD%9C.html","excerpt":"","text":"搜索树中的插入操作 思路其实这道题目其实是一道简单题目，「但是题目中的提示：有多种有效的插入方式，还可以重构二叉搜索树，一下子吓退了不少人」，瞬间感觉题目复杂了很多。 其实「可以不考虑题目中提示所说的改变树的结构的插入方式。」 如下演示视频中可以看出：只要按照二叉搜索树的规则去遍历，遇到空节点就插入节点就可以了。 例如插入元素10 ，需要找到末尾节点插入便可，一样的道理来插入元素15，插入元素0，插入元素6，「需要调整二叉树的结构么？并不需要。」。 只要遍历二叉搜索树，找到空节点 插入元素就可以了，那么这道题其实就简单了。 接下来就是遍历二叉搜索树的过程了。 递归法递归三部曲： 确定递归函数参数以及返回值 参数就是根节点指针，以及要插入元素，这里递归函数要不要有返回值呢？ 可以有，也可以没有，但递归函数如果没有返回值的话，实现是比较麻烦的，下面也会给出其具体实现代码。 「有返回值的话，可以利用返回值完成新加入的节点与其父节点的赋值操作」。（下面会进一步解释） 递归函数的返回类型为节点类型 。 代码如下： 1def insertIntoBST(root, val) 确定终止条件 终止条件就是找到遍历的节点为None的时候，就是要插入节点的位置了，并把插入的节点返回。 代码如下： 123if not root: node = TreeNode(val) return node 这里把添加的节点返回给上一层，就完成了父子节点的赋值操作了，详细再往下看。 确定单层递归的逻辑 此时要明确，需要遍历整棵树么？ 别忘了这是搜索树，遍历整颗搜索树简直是对搜索树的侮辱，哈哈。 搜索树是有方向了，可以根据插入元素的数值，决定递归方向。 代码如下： 12345if root.val &gt; val: root.left = insertIntoBST(root.left, val)if root-&gt;val &lt; val: root.right = insertIntoBST(root.right, val)return root 「到这里，大家应该能感受到，如何通过递归函数返回值完成了新加入节点的父子关系赋值操作了，下一层将加入节点返回，本层用root.left或者root.right将其接住」。 整体代码如下： 123456789101112131415161718192021# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def insertIntoBST(self, root, val): &quot;&quot;&quot; :type root: TreeNode :type val: int :rtype: TreeNode &quot;&quot;&quot; if not root: node = TreeNode(val) return node if root.val &gt; val: root.left = self.insertIntoBST(root.left, val) if root.val &lt; val: root.right = self.insertIntoBST(root.right, val) return root 迭代法再来看看迭代法，对二叉搜索树迭代写法不熟悉，可以看：【二叉搜索树】 在迭代法遍历的过程中，需要记录一下当前遍历的节点的父节点，这样才能做插入节点的操作。 在【搜索树的最小绝对差】和【二叉树的众数】中，都是用了记录pre和cur两个指针的技巧，本题也是一样的。 代码如下： 12345678910111213141516171819202122232425262728293031# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def insertIntoBST(self, root, val): &quot;&quot;&quot; :type root: TreeNode :type val: int :rtype: TreeNode &quot;&quot;&quot; if not root: node = TreeNode(val) return node parent = root cur = root while cur: parent = cur if cur.val &gt; val: cur = cur.left else: cur = cur.right node = TreeNode(val) if val &lt; parent.val: parent.left = node if val &gt; parent.val: parent.right = node return root 总结首先在二叉搜索树中的插入操作，大家不用恐惧其重构搜索树，其实根本不用重构。 然后在递归中，我们重点讲了如果通过递归函数的返回值完成新加入节点和其父节点的赋值操作，并强调了搜索树的有序性。 最后依然给出了迭代的方法，迭代的方法就需要记录当前遍历节点的父节点了，这个和没有返回值的递归函数实现的代码逻辑是一样的。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"19-搜索树的公共祖先问题","slug":"19-搜索树的公共祖先问题","date":"2021-07-19T09:33:30.000Z","updated":"2021-07-19T09:35:07.082Z","comments":true,"path":"20210719/19-搜索树的公共祖先问题.html","link":"","permalink":"https://xxren8218.github.io/20210719/19-%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88%E9%97%AE%E9%A2%98.html","excerpt":"","text":"搜索树的公共祖先问题 思路做过【公共祖先问题】题目的同学应该知道，利用回溯从底向上搜索，遇到一个节点的左子树里有p，右子树里有q，那么当前节点就是最近公共祖先。 那么本题是二叉搜索树，二叉搜索树是有序的，那得好好利用一下这个特点。 在有序树里，如果判断一个节点的左子树里有p，右子树里有q呢？ 其实只要从上到下遍历的时候，cur节点是数值在[p, q]区间中则说明该节点cur就是最近公共祖先了。 理解这一点，本题就很好解了。 和【公共祖先问题】不同，普通二叉树求最近公共祖先需要使用回溯，从底向上来查找，二叉搜索树就不用了，因为搜索树有序（相当于自带方向），那么只要从上向下遍历就可以了。 那么我们可以采用前序遍历（其实这里没有中节点的处理逻辑，遍历顺序无所谓了）。 可以看出直接按照指定的方向，就可以找到节点4，为最近公共祖先，而且不需要遍历整棵树，找到结果直接返回！ 递归三部曲如下： 确定递归函数返回值以及参数 参数就是 当前节点，以及两个结点 p、q。 返回值是要返回最近公共祖先，所以是 。 代码如下： 1def traversal(cur, p, q) 确定终止条件 遇到空返回就可以了，代码如下： 1if not cur: return None 其实都不需要这个终止条件，因为题目中说了p、q 为不同节点且均存在于给定的二叉搜索树中。也就是说一定会找到公共祖先的，所以并不存在遇到空的情况。 确定单层递归的逻辑 在遍历二叉搜索树的时候就是寻找区间[p.val, q.val]（注意这里是左闭右闭） 那么如果 cur.val 大于 p.val，同时 cur.val 大于q.val，那么就应该向左遍历（说明目标区间在左子树上）。 「需要注意的是此时不知道p和q谁大，所以两个都要判断」 代码如下： 1234if cur.val &gt; p.val and cur.val &gt; q.val: left = traversal(cur.left, p, q) if left: return left 「细心的同学会发现，在这里调用递归函数的地方，把递归函数的返回值left，直接return」。 在【公共祖先问题】中，如果递归函数有返回值，如何区分要搜索一条边，还是搜索整个树。 搜索一条边的写法： 123if 递归函数(root.left)： returnif 递归函数(root.right)： return 搜索整个树写法： 123left = 递归函数(root.left)right = 递归函数(root.right)left与right的逻辑处理 本题就是标准的搜索一条边的写法，遇到递归函数的返回值，如果不为空，立刻返回。 如果 cur.val 小于 p.val，同时 cur.val 小于 q.val，那么就应该向右遍历（目标区间在右子树）。 1234if cur.val &lt; p.val and cur.val &lt; q.val: right = traversal(cur.right, p, q) if right: return right 剩下的情况，就是cur节点在区间（p.val &lt;= cur.val and cur.val &lt;= q.val）或者 （q.val &lt;= cur.val and cur.val &lt;= p.val）中，那么cur就是最近公共祖先了，直接返回cur。 代码如下： 1return cur 那么整体递归代码如下: 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def lowestCommonAncestor(self, root, p, q): &quot;&quot;&quot; :type root: TreeNode :type p: TreeNode :type q: TreeNode :rtype: TreeNode &quot;&quot;&quot; if not root: return None return self.traversal(root, p, q) def traversal(self, cur, p, q): if not cur: return None if cur.val &lt; p.val and cur.val &lt; q.val: right = self.traversal(cur.right, p, q) if right: return right elif cur.val &gt; p.val and cur.val &gt; q.val: left = self.traversal(cur.left, p, q) if left: return left else: return cur 迭代法对于二叉搜索树的迭代法，大家应该在【二叉搜索树】就了解了。 利用其有序性，迭代的方式还是比较简单的，解题思路在递归中已经分析了。 迭代代码如下： 12345678910111213141516171819202122232425# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def lowestCommonAncestor(self, root, p, q): &quot;&quot;&quot; :type root: TreeNode :type p: TreeNode :type q: TreeNode :rtype: TreeNode &quot;&quot;&quot; if not root: return None while root: if root.val &lt; p.val and root.val &lt; q.val: root = root.right elif root.val &gt; p.val and root.val &gt; q.val: root = root.left else: return root return None 灵魂拷问：是不是又被简单的迭代法感动到痛哭流涕？ 总结对于二叉搜索树的最近祖先问题，其实要比【普通二叉树公共祖先问题】简单的多。 不用使用回溯了，二叉搜索树自带方向性，可以方便的从上向下查找目标区间，遇到目标区间内的节点，直接返回。 最后给出了对应的迭代法，二叉搜索树的迭代法甚至比递归更容易理解，也是因为其有序性（自带方向性），按照目标区间找就行了。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"07-求组和总和三","slug":"07-求组和总和三","date":"2021-07-16T14:26:03.000Z","updated":"2021-07-16T14:27:16.747Z","comments":true,"path":"20210716/07-求组和总和三.html","link":"","permalink":"https://xxren8218.github.io/20210716/07-%E6%B1%82%E7%BB%84%E5%92%8C%E6%80%BB%E5%92%8C%E4%B8%89.html","excerpt":"","text":"求组合总和三 思路这道题目和【组合总和】如下区别： 本题candidates 中的每个数字在每个组合中只能使用一次。 本题数组candidates的元素是有重复的，而【组合总和】是无重复元素的数组candidates 最后本题和【组合总和】要求一样，解集不能包含重复的组合。 「本题的难点在于区别2中：集合（数组candidates）有重复元素，但还不能有重复的组合」。 一些同学可能想了：我把所有组合求出来，再用set或者map去重，这么做很容易超时！ 所以要在搜索的过程中就去掉重复组合。 很多同学在去重的问题上想不明白，其实很多题解也没有讲清楚，反正代码是能过的，感觉是那么回事，稀里糊涂的先把题目过了。 这个去重为什么很难理解呢，「所谓去重，其实就是使用过的元素不能重复选取。」这么一说好像很简单！ 都知道组合问题可以抽象为树形结构，那么“使用过”在这个树形结构上是有两个维度的，一个维度是同一树枝上使用过，一个维度是同一树层上使用过。「没有理解这两个层面上的“使用过” 是造成大家没有彻底理解去重的根本原因。」 那么问题来了，我们是要同一树层上使用过，还是同一树枝上使用过呢？ 回看一下题目，元素在同一个组合内是可以重复的，怎么重复都没事，但两个组合不能相同。 「所以我们要去重的是同一树层上的“使用过”，同一树枝上的都是一个组合里的元素，不用去重」。 为了理解去重我们来举一个例子，candidates = [1, 1, 2], target = 3，（方便起见candidates已经排序了） 选择过程树形结构如图所示： 可以看到图中，每个节点相对于 【组合总和】我多加了used数组，这个used数组下面会重点介绍。 回溯三部曲 「递归函数参数」 与【组合总和】套路相同，此题还需要加一个bool型数组used，用来记录同一树枝上的元素是否使用过。 这个集合去重的重任就是used来完成的。 代码如下： 123result = [] # 存放组合集合path = [] # 符合条件的组合def backtracking(candidates, target, Sum, startIndex, used): 「递归终止条件」 与【组合总和】套路相同，终止条件为 sum &gt; target 和 sum == target。 代码如下： 123456if Sum &gt; target: # 这个条件其实可以省略 returnif Sum == target: result.append(path) return Sum &gt; target 这个条件其实可以省略，因为和在递归单层遍历的时候，会有剪枝的操作，下面会介绍到。 「单层搜索的逻辑」 这里与【组合总和】最大的不同就是要去重了。 前面我们提到：要去重的是“同一树层上的使用过”，如何判断同一树层上元素（相同的元素）是否使用过了呢。 「如果candidates[i] == candidates[i - 1] 并且 used[i - 1] == False，就说明：前一个树枝，使用了candidates[i - 1]，也就是说同一树层使用过candidates[i - 1]」。 此时for循环里就应该做continue的操作。 这块比较抽象，如图： 我在图中将used的变化用橘黄色标注上，可以看出在candidates[i] == candidates[i - 1]相同的情况下： used[i - 1] == True，说明同一树支candidates[i - 1]使用过 used[i - 1] == False，说明同一树层candidates[i - 1]使用过 「这块去重的逻辑很抽象，网上搜的题解基本没有能讲清楚的，如果大家之前思考过这个问题或者刷过这道题目，看到这里一定会感觉通透了很多！」 那么单层搜索的逻辑代码如下： 1234567891011121314for i in range(startIndex, len(candidates)): if Sum + candidates[i] &lt;= target: # used[i - 1] == True，说明同一树支candidates[i - 1]使用过 # used[i - 1] == False，说明同一树层candidates[i - 1]使用过 # 要对同一树层使用过的元素进行跳过 if i &gt; 0 and candidates[i] == candidates[i - 1] and used[i - 1] == False: continue Sum += candidates[i] path.append(candidates[i]) used[i] = True backtracking(candidates, target, Sum, i + 1, used) # 和组合总和的区别1：这里是i+1，每个数字在每个组合中只能使用一次 used[i] = False Sum -= candidates[i] path.pop() 「注意sum + candidates[i] &lt;= target为剪枝操作，在【组合总和】有讲解过！」 整体代码： 123456789101112131415161718192021222324252627282930class Solution(object): def __init__(self): self.result = [] self.path = [] def combinationSum2(self, candidates, target): &quot;&quot;&quot; :type candidates: List[int] :type target: int :rtype: List[List[int]] &quot;&quot;&quot; if not candidates: return [] used = [False]*len(candidates) # 首先把给candidates排序，让其相同的元素都挨在一起。 candidates.sort() self.backtracking(candidates, target, 0, 0, used) return self.result def backtracking(self, candidates, target, Sum, startIndex, used): if Sum == target: self.result.append(self.path[:]) if Sum &gt; target: return for i in range(startIndex, len(candidates)): if i &gt; 0 and candidates[i] == candidates[i - 1] and used[i - 1] == False: continue self.path.append(candidates[i]) Sum += candidates[i] used[i] = True self.backtracking(candidates, target, Sum, i + 1, used) used[i] = False Sum -= candidates[i] self.path.pop() 总结本题同样是求组合总和，但就是因为其数组candidates有重复元素，而要求不能有重复的组合，所以相对于【组合求和】难度提升了不少。 「关键是去重的逻辑，代码很简单，网上一搜一大把，但几乎没有能把这块代码含义讲明白的，基本都是给出代码，然后说这就是去重了，究竟怎么个去重法也是模棱两可」。","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"06-求组和总和二","slug":"06-求组和总和二","date":"2021-07-16T14:23:43.000Z","updated":"2021-07-16T14:25:26.693Z","comments":true,"path":"20210716/06-求组和总和二.html","link":"","permalink":"https://xxren8218.github.io/20210716/06-%E6%B1%82%E7%BB%84%E5%92%8C%E6%80%BB%E5%92%8C%E4%BA%8C.html","excerpt":"","text":"求组合总和 思路题目中的「无限制重复被选取，吓得我赶紧想想 出现0 可咋办」，然后看到下面提示：1 &lt;= candidates[i] &lt;= 200，我就放心了。 本题和【求组合问题】和【求组合总和】区别是：本题没有数量要求，可以无限重复，但是有总和的限制，所以间接的也是有个数的限制。 本题搜索的过程抽象成树形结构如下： 注意图中叶子节点的返回条件，因为本题没有组合数量要求，仅仅是总和的限制，所以递归没有层数的限制，只要选取的元素总和超过target，就返回！ 而在[求组合问题)和[求组合总和] 中都可以知道要递归K层，因为要取k个元素的组合 回溯三部曲 递归函数参数 这里依然是定义两个全局变量，二维数组result存放结果集，数组path存放符合条件的结果。（这两个变量可以作为函数参数传入） 首先是题目中给出的参数，集合candidates, 和目标值target。 此外我还定义了int型的sum变量来统计单一结果path里的总和，其实这个Sum也可以不用，用target做相应的减法就可以了，最后如何target==0就说明找到符合的结果了，但为了代码逻辑清晰，我依然用了Sum。 「本题还需要startIndex来控制for循环的起始位置，对于组合问题，什么时候需要startIndex呢？」 我举过例子，如果是一个集合来求组合的话，就需要startIndex，例如【求组合问题】和【求组合总和】 如果是多个集合取组合，各个集合之间相互不影响，那么就不用startIndex，例如：【电话号码的字母组合】 「注意以上我只是说求组合的情况，如果是排列问题，又是另一套分析的套路，后面我再讲解排列的时候就重点介绍」。 代码如下： 123result = []path = []def backtracking(candidates, target, Sum, startIndex) 递归终止条件 在如下树形结构中： 从叶子节点可以清晰看到，终止只有两种情况，Sum大于target和Sum等于target。 sum等于target的时候，需要收集结果，代码如下： 123456if Sum &gt; target: returnif Sum == target: result.append(path[:]) return 单层搜索的逻辑 单层for循环依然是从startIndex开始，搜索candidates集合。 「注意本题和[求组合问题]、[求组合总和]的一个区别是：本题元素为可重复选取的」。 如何重复选取呢，看代码，注释部分： 123456for i in range(startIndex, len(candidates)): Sum += candidates[i] path.append(candidates[i]) backtracking(candidates, target, Sum, i) # 关键点:不用i+1了，表示可以重复读取当前的数 sum -= candidates[i]; # 回溯 path.pop() # 回溯 按照回溯算法的的模板，不难写出完整代码： 123456789101112131415161718192021222324252627class Solution(object): def __init__(self): self.result = [] self.path = [] def combinationSum(self, candidates, target): &quot;&quot;&quot; :type candidates: List[int] :type target: int :rtype: List[List[int]] &quot;&quot;&quot; if not candidates: return self.backtracking(candidates, target, 0, 0) return self.result def backtracking(self, candidates, target, Sum, startIndex): if not candidates: return if Sum == target: self.result.append(self.path[:]) if Sum &gt; target: return for i in range(startIndex, len(candidates)): Sum += candidates[i] self.path.append(candidates[i]) self.backtracking(candidates, target, Sum, i) Sum -= candidates[i] self.path.pop() 剪枝优化在这个树形结构中： 以及上面的版本一的代码大家可以看到，对于sum已经大于target的情况，其实是依然进入了下一层递归，只是下一层递归结束判断的时候，会判断sum &gt; target的话就返回。 其实如果已经知道下一层的sum会大于target，就没有必要进入下一层递归了。 那么可以在for循环的搜索范围上做做文章了。 「对总集合排序之后，如果下一层的sum（就是本层的 sum + candidates[i]）已经大于target，就可以结束本轮for循环的遍历」。 如图： for循环剪枝代码如下： 123for i in range(startIndex, len(candidates))： if sum + candidates[i] &gt; target: continue 整体代码如下： 123456789101112131415161718192021222324252627282930class Solution(object): def __init__(self): self.result = [] self.path = [] def combinationSum(self, candidates, target): &quot;&quot;&quot; :type candidates: List[int] :type target: int :rtype: List[List[int]] &quot;&quot;&quot; if not candidates: return self.backtracking(candidates, target, 0, 0) return self.result def backtracking(self, candidates, target, Sum, startIndex): if not candidates: return if Sum == target: self.result.append(self.path[:]) if Sum &gt; target: return for i in range(startIndex, len(candidates)): if Sum + candidates[i] &gt; target: # 剪枝判断 continue # 剪枝操作。 Sum += candidates[i] self.path.append(candidates[i]) self.backtracking(candidates, target, Sum, i) Sum -= candidates[i] self.path.pop() 总结本题和我们之前讲过的【求组合问题】【求组合总和】有两点不同： 组合没有数量要求 元素可无限重复选取 针对这两个问题，我都做了详细的分析。 并且给出了对于组合问题，什么时候用startIndex，什么时候不用，并用【电话号码的字母组合】做了对比。 最后还给出了本题的剪枝优化，这个优化如果是初学者的话并不容易想到。 「在求和问题中，排序之后加剪枝是常见的套路！」","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"18-最近公共祖先问题","slug":"18-最近公共祖先问题","date":"2021-07-16T14:20:27.000Z","updated":"2021-07-16T14:23:07.222Z","comments":true,"path":"20210716/18-最近公共祖先问题.html","link":"","permalink":"https://xxren8218.github.io/20210716/18-%E6%9C%80%E8%BF%91%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88%E9%97%AE%E9%A2%98.html","excerpt":"","text":"最近公共祖先问题 说明: 所有节点的值都是唯一的。 p、q 为不同节点且均存在于给定的二叉树中。 思路遇到这个题目首先想的是要是能自底向上查找就好了，这样就可以找到公共祖先了。 那么二叉树如何可以自底向上查找呢？ 回溯啊，二叉树回溯的过程就是从低到上。 后序遍历就是天然的回溯过程，最先处理的一定是叶子节点。 接下来就看如何判断一个节点是节点q和节点p的公共公共祖先呢。 「如果找到一个节点，发现左子树出现结点p，右子树出现节点q，或者 左子树出现结点q，右子树出现节点p，那么该节点就是节点p和q的最近公共祖先。」 使用后序遍历，回溯的过程，就是从低向上遍历节点，一旦发现如何这个条件的节点，就是最近公共节点了。 递归三部曲： 确定递归函数返回值以及参数 需要递归函数返回值，来告诉我们是否找到节点q或者p，那么返回值为bool类型就可以了。 但我们还要返回最近公共节点，可以利用上题目中返回值是节点 ，那么如果遇到p或者q，就把q或者p返回，返回值不为空，就说明找到了q或者p。 代码如下： 1def lowestCommonAncestor(root, p, q) 确定终止条件 如果找到了 节点p或者q，或者遇到空节点，就返回。 代码如下： 1if root == q or root == p or not root: return root 确定单层递归逻辑 值得注意的是 本题函数有返回值，是因为回溯的过程需要递归函数的返回值做判断，但本题我们依然要遍历树的所有节点。 递归函数有返回值就是要遍历某一条边，但有返回值也要看如何处理返回值！ 如果递归函数有返回值，如何区分要搜索一条边，还是搜索整个树呢？ 搜索一条边的写法： 123if 递归函数(root.left): returnif 递归函数(root.right): return 搜索整个树写法： 123left = 递归函数(root.left)right = 递归函数(root.right)left与right的逻辑处理 看出区别了没？ 「在递归函数有返回值的情况下：如果要搜索一条边，递归函数返回值不为空的时候，立刻返回，如果搜索整个树，直接用一个变量left、right接住返回值，这个left、right后序还有逻辑处理的需要，也就是后序遍历中处理中间节点的逻辑（也是回溯）」 那么为什么要遍历整颗树呢？直观上来看，找到最近公共祖先，直接一路返回就可以了 如图: 就像图中一样直接返回7，多美滋滋。 但事实上还要遍历根节点右子树（即使此时已经找到了目标节点了），也就是图中的节点4、15、20。 因为在如下代码的后序遍历中，如果想利用left和right做逻辑处理， 不能立刻返回，而是要等left与right逻辑处理完之后才能返回。 123left = 递归函数(root.left)right = 递归函数(root.right)left与right的逻辑处理 所以此时大家要知道我们要遍历整棵树。知道这一点，对本题就有一定深度的理解了。 那么先用left和right接住左子树和右子树的返回值，代码如下： 12left = lowestCommonAncestor(root.left, p, q)right = lowestCommonAncestor(root.right, p, q) 「如果left 和 right都不为空，说明此时root就是最近公共节点。这个比较好理解」 「如果left为空，right不为空，就返回right，说明目标节点是通过right返回的，反之依然」。 这里有的同学就理解不了了，为什么left为空，right不为空，目标节点通过right返回呢？ 如图： 图中节点10的左子树返回null，右子树返回目标值7，那么此时节点10的处理逻辑就是把右子树的返回值（最近公共祖先7）返回上去！ 这里点也很重要，可能刷过这道题目的同学，都不清楚结果究竟是如何从底层一层一层传到头结点的。 那么如果left和right都为空，则返回left或者right都是可以的，也就是返回空。 代码如下： 1234if not left and right: return rightelif left and not right: return leftelse: return None 那么寻找最小公共祖先，完整流程图如下： 「从图中，大家可以看到，我们是如何回溯遍历整颗二叉树，将结果返回给头结点的！」 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def lowestCommonAncestor(self, root, p, q): &quot;&quot;&quot; :type root: TreeNode :type p: TreeNode :type q: TreeNode :rtype: TreeNode &quot;&quot;&quot; if not root: return return self.dfs(root, p, q) def dfs(self, root, p, q): if root == p or root == q or not root: return root left = self.dfs(root.left, p, q) right = self.dfs(root.right, p, q) if left and right: return root elif not left and right: return right elif left and not right: return left else: return None 总结这道题目刷过的同学未必真正了解这里面回溯的过程，以及结果是如何一层一层传上去的。 「那么我给大家归纳如下三点」： 求最小公共祖先，需要从底向上遍历，那么二叉树，只能通过后序遍历（即：回溯）实现从低向上的遍历方式。 在回溯的过程中，必然要遍历整颗二叉树，即使已经找到结果了，依然要把其他节点遍历完，因为要使用递归函数的返回值（也就是代码中的left和right）做逻辑判断。 要理解如果返回值left为空，right不为空为什么要返回right，为什么可以用返回right传给上一层结果。 可以说这里每一步，都是有难度的，都需要对二叉树，递归和回溯有一定的理解。 本题没有给出迭代法，因为迭代法不适合模拟回溯的过程。理解递归的解法就够了。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"05-电话号码的字母组合","slug":"05-电话号码的字母组合","date":"2021-07-15T13:57:48.000Z","updated":"2021-07-15T13:58:54.892Z","comments":true,"path":"20210715/05-电话号码的字母组合.html","link":"","permalink":"https://xxren8218.github.io/20210715/05-%E7%94%B5%E8%AF%9D%E5%8F%B7%E7%A0%81%E7%9A%84%E5%AD%97%E6%AF%8D%E7%BB%84%E5%90%88.html","excerpt":"","text":"电话号码的字母组合 思路从示例上来说，输入”23”，最直接的想法就是两层for循环遍历了吧，正好把组合的情况都输出了。 如果输入”233”呢，那么就三层for循环，如果”2333”呢，就四层for循环……. 大家应该感觉出和【求组合问题】遇到的一样的问题，就是这for循环的层数如何写出来，此时又是回溯法登场的时候了。 理解本题后，要解决如下三个问题： 数字和字母如何映射 两个字母就两个for循环，三个字符我就三个for循环，以此类推，然后发现代码根本写不出来 输入1 * #按键等等异常情况 数字和字母如何映射可以使用map或者定义一个数组，来做映射，我这里定义一个数组，代码如下： 123456789101112letterMap = [ &quot;&quot;, # 0 &quot;&quot;, # 1 &quot;abc&quot;, # 2 &quot;def&quot;, # 3 &quot;ghi&quot;, # 4 &quot;jkl&quot;, # 5 &quot;mno&quot;, # 6 &quot;pqrs&quot;, # 7 &quot;tuv&quot;, # 8 &quot;wxyz&quot;, # 9] 回溯法来解决n个for循环的问题输入：”23”，抽象为树形结构，如图所示： 图中可以看出遍历的深度，就是输入”23”的长度，而叶子节点就是我们要收集的结果，输出[“ad”, “ae”, “af”, “bd”, “be”, “bf”, “cd”, “ce”, “cf”]。 回溯三部曲： 确定回溯函数参数 首先需要一个字符串s来收集叶子节点的结果，然后用一个字符串数组result保存起来，这两个变量我依然定义为全局。 再来看参数，参数指定是有题目中给的digits，然后还要有一个参数就是index。 注意这个index可不是[求组合总和]中的startIndex了。 这个index是记录遍历第几个数字了，就是用来遍历digits的（题目中给出数字字符串），同时index也表示树的深度。 代码如下： 123result = []s = &#x27;&#x27;def backtracking(digits, index) 确定终止条件 例如输入用例”23”，两个数字，那么根节点往下递归两层就可以了，叶子节点就是要收集的结果集。 那么终止条件就是如果index 等于 输入的数字个数len(digits)了（本来index就是用来遍历digits的）。 然后收集结果，结束本层递归。 代码如下： 123if index == len(digits): result.append(s) return 确定单层遍历逻辑 首先要取index指向的数字，并找到对应的字符集（手机键盘的字符集）。 然后for循环来处理这个字符集，代码如下： 123456digit = int(digits[index]) # 将index指向的数字转为intletters = letterMap[digit] # 取数字对应的字符集for i in range(len(letters)): s += letters[i] # 处理 backtracking(digits, index + 1) # 递归，注意index+1，一下层要处理下一个数字了 s = s[:-1] # 回溯 「注意这里for循环，可不像是在[求组合问题]和[求组合总和]中从startIndex开始遍历的」。 「因为本题每一个数字代表的是不同集合，也就是求不同集合之间的组合，而[77. 组合]和[216.组合总和III]都是是求同一个集合中的组合！」 输入1 * #按键等等异常情况代码中最好考虑这些异常情况，但题目的测试数据中应该没有异常情况的数据，所以我就没有加了。 「但是要知道会有这些异常，如果是现场面试中，一定要考虑到！」 最终代码： 12345678910111213141516171819202122232425262728293031323334353637class Solution(object): def __init__(self): self.result = [] self.s = &#x27;&#x27; self.letterMap = [ &quot;&quot;, # 0 &quot;&quot;, # 1 &quot;abc&quot;, # 2 &quot;def&quot;, # 3 &quot;ghi&quot;, # 4 &quot;jkl&quot;, # 5 &quot;mno&quot;, # 6 &quot;pqrs&quot;, # 7 &quot;tuv&quot;, # 8 &quot;wxyz&quot;, # 9 ] def letterCombinations(self, digits): &quot;&quot;&quot; :type digits: str :rtype: List[str] &quot;&quot;&quot; if not digits: return [] self.backtrackong(digits, 0) return self.result def backtrackong(self, digits, index): if index == len(digits): self.result.append(self.s) return digit = int(digits[index]) letters = self.letterMap[digit] for i in range(len(letters)): self.s += letters[i] self.backtrackong(digits, index + 1) self.s = self.s[:-1] 总结本篇将题目的三个要点一一列出，并重点强调了和前面讲解过的[77. 组合]和[216.组合总和III]的区别，本题是多个集合求组合，所以在回溯的搜索过程中，都有一些细节需要注意的。 其实本题不算难，但也处处是细节，大家还要自己亲自动手写一写。","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"17-二叉搜素树的众数","slug":"17-二叉搜素树的众数","date":"2021-07-15T13:56:03.000Z","updated":"2021-07-15T13:57:13.270Z","comments":true,"path":"20210715/17-二叉搜素树的众数.html","link":"","permalink":"https://xxren8218.github.io/20210715/17-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A0%E6%A0%91%E7%9A%84%E4%BC%97%E6%95%B0.html","excerpt":"","text":"二叉搜素树的众数 思路这道题目呢，递归法我从两个维度来做。 首先如果不是二叉搜索树的话，应该怎么解题，是二叉搜索树，又应该如何解题，两种方式做一个比较，可以加深大家对二叉树的理解。 递归法：不是二叉搜索树如果不是二叉搜索树，最直观的方法一定是把这个树都遍历了，用map统计频率，把频率排个序，最后取前面高频的元素的集合。 具体步骤如下： 这个树都遍历了，用map统计频率 至于用前中后序那种遍历也不重要，因为就是要全遍历一遍，怎么个遍历法都行，层序遍历都没毛病！ 这里采用前序遍历，代码如下： 12345678# Map = &#123;&#125; key:元素，value:出现频率def traversal(cur): # 前序遍历 if not cur: return if cur.val not in Map: Map[cur.val] = 0 Map[cur.val] += 1 # 统计元素频率 traversal(cur.left) traversal(cur.right) 把统计的出来的出现频率（即map中的value）排个序 直接对map中的value排序。 1Map = sorted( Map.items(),key = lambda x:x[1],reverse = True) 取前面高频的元素 那么把前面高频的元素取出来就可以了。 123for k,v in range(len(Map)): if v &gt;= 2: result.append(k) 整体代码如下： 1","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"16-二叉搜索树的最小绝对差","slug":"16-二叉搜索树的最小绝对差","date":"2021-07-15T13:52:04.000Z","updated":"2021-07-15T13:55:34.167Z","comments":true,"path":"20210715/16-二叉搜索树的最小绝对差.html","link":"","permalink":"https://xxren8218.github.io/20210715/16-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E6%9C%80%E5%B0%8F%E7%BB%9D%E5%AF%B9%E5%B7%AE.html","excerpt":"","text":"二叉搜索树的最小绝对差 思路题目中要求在二叉搜索树上任意两节点的差的绝对值的最小值。 「注意是二叉搜索树」，二叉搜索树可是有序的。 遇到在二叉搜索树上求什么最值啊，差值之类的，就把它想成在一个有序数组上求最值，求差值，这样就简单多了。 递归法那么二叉搜索树采用中序遍历，其实就是一个有序数组。 「在一个有序数组上求两个数最小差值，这是不是就是一道送分题了。」 最直观的想法，就是把二叉搜索树转换成有序数组，然后遍历一遍数组，就统计出来最小差值了。 代码如下： 123456li = []def traversal(root): if not root: return traversal(root.left) li.append(root.val) traversal(root.right) 以上代码是把二叉搜索树转化为有序数组了，其实在二叉搜素树中序遍历的过程中，我们就可以直接计算了。 需要用一个pre节点记录一下cur节点的前一个节点。 一些同学不知道在递归中如何记录前一个节点的指针，其实实现起来是很简单的，大家只要看过一次，写过一次，就掌握了。 代码如下： 12345678result = float(&#x27;INF&#x27;)def traversal(cur): if not cur: return traversal(cur.left) # 左 if pre: # 中 result = min(result, cur.val - pre.val) pre = cur # 记录前一个 traversal(cur.right) # 右 是不是看上去也并不复杂！ 整体代码： 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def __init__(self): self.pre = None self.result = float(&#x27;INF&#x27;) def getMinimumDifference(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return self.traversal(root) return self.result def traversal(self, cur): if not cur: return self.traversal(cur.left) if self.pre: self.result = min(self.result, abs(cur.val - self.pre.val)) self.pre = cur self.traversal(cur.right) 总结「遇到在二叉搜索树上求什么最值，求差值之类的，都要思考一下二叉搜索树可是有序的，要利用好这一特点。」 同时要学会在递归遍历的过程中如何记录前后两个指针，这也是一个小技巧，学会了还是很受用的。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"04-求组合总和","slug":"04-求组合总和","date":"2021-07-13T10:23:07.000Z","updated":"2021-07-13T10:24:30.544Z","comments":true,"path":"20210713/04-求组合总和.html","link":"","permalink":"https://xxren8218.github.io/20210713/04-%E6%B1%82%E7%BB%84%E5%90%88%E6%80%BB%E5%92%8C.html","excerpt":"","text":"求组合总和 思路本题就是在[1,2,3,4,5,6,7,8,9]这个集合中找到和为n的k个数的组合。 相对于求组合问题，无非就是多了一个限制，本题是要找到和为n的k个数的组合，而整个集合已经是固定的了[1,…,9]。 想到这一点了，做过[77. 组合]之后，本题是简单一些了。 本题k相当于了树的深度，9（因为整个集合就是9个数）就是树的宽度。 例如 k = 2，n = 4的话，就是在集合[1,2,3,4,5,6,7,8,9]中求 k（个数） = 2, n（和） = 4的组合。 选取过程如图： 图中，可以看出，只有最后取到集合（1，3）和为4 符合条件。 回溯三部曲 「确定递归函数参数」 和求组合问题一样，依然需要一维数组path来存放符合条件的结果，二维数组result来存放结果集。 这里我依然定义path 和 result为全局变量。 至于为什么取名为path？从上面树形结构中，可以看出，结果其实就是一条根节点到叶子节点的路径。 12result = [] # 存放符合条件结果的集合path = [] # 用来存放符合条件结果 接下来还需要如下参数： targetSum（int）目标和，也就是题目中的n。 k（int）就是题目中要求k个数的集合。 Sum（int）为已经收集的元素的总和，也就是path里元素的总和。 startIndex（int）为下一层for循环搜索的起始位置。 所以代码如下： 123result = [] # 存放符合条件结果的集合path = [] # 用来存放符合条件单一结果def backtracking(targetSum, k, Sum, startIndex): 其实这里sum这个参数也可以省略，每次targetSum减去选取的元素数值，然后判断如果targetSum为0了，说明收集到符合条件的结果了，我这里为了直观便于理解，还是加一个Sum参数。 还要强调一下，回溯法中递归函数参数很难一次性确定下来，一般先写逻辑，需要啥参数了，填什么参数。 确定终止条件 什么时候终止呢？ 在上面已经说了，k其实就已经限制树的深度，因为就取k个元素，树再往下深了没有意义。 所以如果len(path) 和 k相等了，就终止。 如果此时path里收集到的元素和（Sum） 和targetSum（就是题目描述的n）相同了，就用result收集当前的结果。 所以 终止代码如下： 123if len(path) == k: if Sum == targetSum: result.append(path[:]) return # 如果len(path) == k 但Sum != targetSum 直接返回 「单层搜索过程」 本题和求组合问题的区别之一就是集合固定的就是9个数[1,…,9]，所以for循环固定i&lt;=9 处理过程就是 path收集每次选取的元素，相当于树型结构里的边，Sum来统计path里元素的总和。 代码如下： 123456for i in range(startindex, 10): Sum += i path.append(i) backtracking(targetSum, k, Sum, i + 1) # 注意i+1调整startIndex Sum -= i # 回溯 path.pop() # 回溯 「别忘了处理过程 和 回溯过程是一一对应的，处理有加，回溯就要有减！」 参照回溯算法中的模板，不难写出如下代码： 12345678910111213141516171819202122232425262728293031class Solution(object): def __init__(self): self.res = [] # 存放结果集 self.path = [] # 符合条件的结果 def combinationSum3(self, k, n): &quot;&quot;&quot; :type k: int :type n: int :rtype: List[List[int]] &quot;&quot;&quot; self.backtracking(n, k, 0, 1) return self.res def backtracking(self, targetSum, k, Sum, startindex): &quot;&quot;&quot; targetSum：目标和，也就是题目中的n。 k：题目中要求k个数的集合。 Sum：已经收集的元素的总和，也就是path里元素的总和。 startIndex：下一层for循环搜索的起始位置。 &quot;&quot;&quot; if len(self.path) == k: if Sum == targetSum: self.res.append(self.path[:]) # 注意append的不能是引用。 return # 如果path.size() == k 但sum != targetSum 直接返回 for i in range(startindex, 10): Sum += i # 处理 self.path.append(i) # 处理 self.backtracking(targetSum, k, Sum, i + 1) # 注意i+1调整startIndex Sum -= i # 回溯 self.path.pop() # 回溯 剪枝这道题目，剪枝操作其实是很容易想到了，想必大家看上面的树形图的时候已经想到了。 如图： 已选元素总和如果已经大于n（图中数值为4）了，那么往后遍历就没有意义了，直接剪掉。 那么剪枝的地方一定是在递归终止的地方剪，剪枝代码如下： 12if sum &gt; targetSum: # 剪枝操作 return 最终代码： 12345678910111213141516171819202122232425262728293031323334class Solution(object): def __init__(self): self.res = [] # 存放结果集 self.path = [] # 符合条件的结果 def combinationSum3(self, k, n): &quot;&quot;&quot; :type k: int :type n: int :rtype: List[List[int]] &quot;&quot;&quot; self.backtracking(n, k, 0, 1) return self.res def backtracking(self, targetSum, k, Sum, startindex): &quot;&quot;&quot; targetSum：目标和，也就是题目中的n。 k：题目中要求k个数的集合。 Sum：已经收集的元素的总和，也就是path里元素的总和。 startIndex：下一层for循环搜索的起始位置。 &quot;&quot;&quot; if Sum &gt; targetSum: # 剪枝操作 return if len(self.path) == k: if Sum == targetSum: self.res.append(self.path[:]) # 注意append的不能是引用。 return # 如果path.size() == k 但sum != targetSum 直接返回 for i in range(startindex, 10): Sum += i # 处理 self.path.append(i) # 处理 self.backtracking(targetSum, k, Sum, i + 1) # 注意i+1调整startIndex Sum -= i # 回溯 self.path.pop() # 回溯 总结开篇就介绍了本题与组合问题的区别，相对来说加了元素总和的限制，如果做完77组合问题。再做本题再合适不过。 分析完区别，依然把问题抽象为树形结构，按照回溯三部曲进行讲解，最后给出剪枝的优化。 相信做完本题，大家对组合问题应该有初步了解了。","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"03-回溯算法的剪枝策略","slug":"03-回溯算法的剪枝策略","date":"2021-07-13T10:21:36.000Z","updated":"2021-07-13T10:22:51.105Z","comments":true,"path":"20210713/03-回溯算法的剪枝策略.html","link":"","permalink":"https://xxren8218.github.io/20210713/03-%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%E7%9A%84%E5%89%AA%E6%9E%9D%E7%AD%96%E7%95%A5.html","excerpt":"","text":"回溯算法的剪枝策略在上文中，我们通过回溯搜索法，解决了n个数中求k个数的组合问题。 文中的回溯法是可以剪枝优化的，本篇我们继续来看一下题目77. 组合。 链接：https://leetcode-cn.com/problems/combinations/ 「看本篇之前，需要先看[回溯算法：求组合问题！]。 大家先回忆一下[77. 组合]给出的回溯法的代码： 12345678910111213141516171819202122232425262728class Solution(object): def __init__(self): self.res = [] self.path = [] def combine(self, n, k): &quot;&quot;&quot; :type n: int :type k: int :rtype: List[List[int]] &quot;&quot;&quot; self.backtracking(n, k, 1) return self.res def backtracking(self, n, k, startindex): if len(self.path) == k: # 注意此处传递的不能是引用。即append(a)不能传递引用，若是引用，a变，整体都变。 path = self.path[:] self.res.append(path) # 也可下面(节省空间复杂度) # self.res.append(self.path[:]) return for i in range(startindex, n + 1): self.path.append(i) self.backtracking(n, k, i + 1) self.path.pop() 剪枝优化我们说过，回溯法虽然是暴力搜索，但也有时候可以有点剪枝优化一下的。 在遍历的过程中有如下代码： 1234for i in range(startindex, n + 1): self.path.append(i) self.backtracking(n, k, i + 1) self.path.pop() 这个遍历的范围是可以剪枝优化的，怎么优化呢？ 来举一个例子，n = 4，k = 4的话，那么第一层for循环的时候，从元素2开始的遍历都没有意义了。在第二层for循环，从元素3开始的遍历都没有意义了。 这么说有点抽象，如图所示： 图中每一个节点（图中为矩形），就代表本层的一个for循环，那么每一层的for循环从第二个数开始遍历的话，都没有意义，都是无效遍历。 「所以，可以剪枝的地方就在递归中每一层的for循环所选择的起始位置」。 「如果for循环选择的起始位置之后的元素个数 已经不足 我们需要的元素个数了，那么就没有必要搜索了」。 注意代码中i，就是for循环里选择的起始位置。 1for i in range(startindex, n + 1): 接下来看一下优化过程如下： 已经选择的元素个数：len(path) 还需要的元素个数为: k - len(path) 在集合n中至多要从该起始位置 : n - (k - len(path)) + 1，开始遍历 为什么有个+1呢，因为包括起始位置，我们要是一个左闭的集合。 举个例子，n = 4，k = 3， 目前已经选取的元素为0（len(path)为0），n - (k - 0) + 1 即 4 - ( 3 - 0) + 1 = 2。 从2开始搜索都是合理的，可以是组合[2, 3, 4]。 这里大家想不懂的话，建议也举一个例子，就知道是不是要+1了。 所以优化之后的for循环是： 1for i in range(startindex, n - (k - len(path)) + 1): # i为本次搜索的起始位置 优化后的代码如下： 1234567891011121314151617181920212223class Solution(object): def __init__(self): self.res = [] self.path = [] def combine(self, n, k): &quot;&quot;&quot; :type n: int :type k: int :rtype: List[List[int]] &quot;&quot;&quot; self.backtracking(n, k, 1) return self.res def backtracking(self, n, k, startindex): if len(self.path) == k: self.res.append(self.path[:]) return # 此处有两 + 1，一个是循环至多的起始位置，一个是range()函数的半闭半开区间。 for i in range(startindex, n - (k - len(self.path)) + 1 + 1): self.path.append(i) self.backtracking(n, k, i + 1) self.path.pop() 这个代码运行后，将之前的代码的416ms的时间降低到了28ms。 总结本篇我们针对求组合问题的回溯法代码做了剪枝优化，这个优化如果不画图的话，其实不好理解，也不好讲清楚。 所以我依然是把整个回溯过程抽象为一颗树形结构，然后可以直观的看出，剪枝究竟是剪的哪里。","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"15-验证二叉搜索树","slug":"15-验证二叉搜索树","date":"2021-07-13T10:20:16.000Z","updated":"2021-07-13T10:21:08.620Z","comments":true,"path":"20210713/15-验证二叉搜索树.html","link":"","permalink":"https://xxren8218.github.io/20210713/15-%E9%AA%8C%E8%AF%81%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91.html","excerpt":"","text":"验证二叉搜索树 思路要知道中序遍历下，输出的二叉搜索树节点的数值是有序序列。 有了这个特性，「验证二叉搜索树，就相当于变成了判断一个序列是不是递增的了。」 递归法可以递归中序遍历将二叉搜索树转变成一个数组，代码如下： 123456li = []def traversal(root): if not root: return traversal(root.left) li.append(root.val) # 将二叉搜索树转换为有序数组 traversal(root.right) 然后只要比较一下，这个数组是否是有序的，「注意二叉搜索树中不能有重复元素」。 1234567traversal(root)for i in range(1, len(li)): # 注意要小于等于，搜索树里不能有相同元素 if li[i] &lt;= li[i - 1]: return Falsereturn True 整体代码如下： 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def __init__(self): self.li = [] def isValidBST(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: bool &quot;&quot;&quot; if not root: return self.traversal(root) for i in range(1, len(self.li)): # 注意要小于等于，搜索树里不能有相同元素 if self.li[i] &lt;= self.li[i - 1]: return False return True # 递归将二叉搜索树变为有序数组 def traversal(self, root): # 注意为空的判断 if not root: return self.traversal(root.left) self.li.append(root.val) self.traversal(root.right) 以上代码中，我们把二叉树转变为数组来判断，是最直观的，但其实不用转变成数组，可以在递归遍历的过程中直接判断是否有序。 这道题目比较容易陷入一个陷阱： 陷阱1 「不能单纯的比较左节点小于中间节点，右节点大于中间节点就完事了」。 写出了类似这样的代码： 1234if root.val &gt; root.left.val and root.val &lt; root.right.val: return Trueelse: return false 我们要比较的是 左子树所有节点小于中间节点，右子树所有节点大于中间节点。所以以上代码的判断逻辑是错误的。 例如：[10,5,15,null,null,6,20] 这个case： 节点10小于左节点5，大于右节点15，但右子树里出现了一个6 这就不符合了！ 总结这道题目是一个简单题，但对于没接触过的同学还是有难度的。 所以初学者刚开始学习算法的时候，看到简单题目没有思路很正常，千万别怀疑自己智商，学习过程都是这样的，大家智商都差不多，哈哈。 只要把基本类型的题目都做过，总结过之后，思路自然就开阔了。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"14-二叉搜索树","slug":"14-二叉搜索树","date":"2021-07-13T10:18:55.000Z","updated":"2021-07-13T10:20:01.433Z","comments":true,"path":"20210713/14-二叉搜索树.html","link":"","permalink":"https://xxren8218.github.io/20210713/14-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91.html","excerpt":"","text":"二叉搜索树 思路之前我们讲了都是普通二叉树，那么接下来看看二叉搜索树。 二叉搜索树是一个有序树： 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉搜索树 这就决定了，二叉搜索树，递归遍历和迭代遍历和普通二叉树都不一样。 本题，其实就是在二叉搜索树中搜索一个节点。那么我们来看看应该如何遍历。 递归法 确定递归函数的参数和返回值 递归函数的参数传入的就是根节点和要搜索的数值，返回的就是以这个搜索数值所在的节点。 代码如下： 1def searchBST(root, val): 确定终止条件 如果root为空，或者找到这个数值了，就返回root节点。 1if not root or root.val == val: return root 确定单层递归的逻辑 看看二叉搜索树的单层递归逻辑有何不同。 因为二叉搜索树的节点是有序的，所以可以有方向的去搜索。 如果root.val &gt; val，搜索左子树，如果root.val &lt; val，就搜索右子树，最后如果都没有搜索到，就返回None。 代码如下： 123456if root.val &gt; val: return searchBST(root.left, val) # 注意这里加了return if root.val &lt; val: return searchBST(root.right, val)return None 这里可能会疑惑，在递归遍历的时候，什么时候直接return 递归函数的返回值，什么时候不用加这个 return呢。 如果要搜索一条边，递归函数就要加返回值，这里也是一样的道理。 「因为搜索到目标节点了，就要立即return了，这样才是找到节点就返回（搜索某一条边），如果不加return，就是遍历整棵树了。」 整体代码如下： 1234567891011121314151617181920# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def searchBST(self, root, val): &quot;&quot;&quot; :type root: TreeNode :type val: int :rtype: TreeNode &quot;&quot;&quot; if not root or root.val == val: return root if root.val &gt; val: return self.searchBST(root.left, val) # 注意这里加了return if root.val &lt; val: return self.searchBST(root.right, val) return None 迭代法一提到二叉树遍历的迭代法，可能立刻想起使用栈来模拟深度遍历，使用队列来模拟广度遍历。 对于二叉搜索树可就不一样了，因为二叉搜索树的特殊性，也就是节点的有序性，可以不使用辅助栈或者队列就可以写出迭代法。 对于一般二叉树，递归过程中还有回溯的过程，例如走一个左方向的分支走到头了，那么要调头，在走右分支。 而「对于二叉搜索树，不需要回溯的过程，因为节点的有序性就帮我们确定了搜索的方向。」 例如要搜索元素为3的节点，「我们不需要搜索其他节点，也不需要做回溯，查找的路径已经规划好了。」 中间节点如果大于3就向左走，如果小于3就向右走，如图： 所以迭代法代码如下： 123456789101112131415161718192021# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def searchBST(self, root, val): &quot;&quot;&quot; :type root: TreeNode :type val: int :rtype: TreeNode &quot;&quot;&quot; if not root or root.val == val: return root while root: if root.val &gt; val: root = root.left elif root.val &lt; val: root = root.right else: return root 第一次看到了如此简单的迭代法，是不是感动的痛哭流涕，哭一会~ 总结本篇我们介绍了二叉搜索树的遍历方式，因为二叉搜索树的有序性，遍历的时候要比普通二叉树简单很多。 但是一些同学很容易忽略二叉搜索树的特性，所以写出遍历的代码就未必真的简单了。 所以针对二叉搜索树的题目，一样要利用其特性。 文中我依然给出递归和迭代两种方式，可以看出写法都非常简单，就是利用了二叉搜索树有序的特点。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"02-回溯求组合问题","slug":"02-回溯法求组合问题","date":"2021-07-12T14:01:19.000Z","updated":"2021-07-15T14:12:47.145Z","comments":true,"path":"20210712/02-回溯法求组合问题.html","link":"","permalink":"https://xxren8218.github.io/20210712/02-%E5%9B%9E%E6%BA%AF%E6%B3%95%E6%B1%82%E7%BB%84%E5%90%88%E9%97%AE%E9%A2%98.html","excerpt":"","text":"回溯法求组合问题 思路本题这是回溯法的经典题目。 直接的解法当然是使用for循环，例如示例中k为2，很容易想到 用两个for循环，这样就可以输出 和示例中一样的结果。 代码如下： 12345res = []n = 4for i in range(1, n + 1): for j in range(i + 1, n + 1): res.append([i,j]) 输入：n = 100, k = 3 那么就三层for循环，代码如下： 123456res = []n = 100for i in range(1, n + 1): for j in range(i + 1, n + 1): for k in range(j + 1, n + 1): res.append([i,j,k]) 「如果n为100，k为50呢，那就50层for循环，是不是开始窒息」。 「此时就会发现虽然想暴力搜索，但是用for循环嵌套连暴力都写不出来！」 咋整？ 回溯搜索法来了，虽然回溯法也是暴力，但至少能写出来，不像for循环嵌套k层让人绝望。 那么回溯法怎么暴力搜呢？ 上面我们说了「要解决 n为100，k为50的情况，暴力写法需要嵌套50层for循环，那么回溯法就用递归来解决嵌套层数的问题」。 递归来做层叠嵌套（可以理解是开k层for循环），「每一次的递归中嵌套一个for循环，那么递归就可以用于解决多层嵌套循环的问题了」。 此时递归的层数大家应该知道了，例如：n为100，k为50的情况下，就是递归50层。 一些同学本来对递归就懵，回溯法中递归还要嵌套for循环，可能就直接晕倒了！ 如果脑洞模拟回溯搜索的过程，绝对可以让人窒息，所以需要抽象图形结构来进一步理解。 「我们在上篇文章中说道回溯法解决的问题都可以抽象为树形结构（N叉树），用树形结构来理解回溯就容易多了」。 可以看出这个棵树，一开始集合是 1，2，3，4， 从左向右取数，取过的数，不在重复取。 第一次取1，集合变为2，3，4 ，因为k为2，我们只需要再取一个数就可以了，分别取2，3，4，得到集合[1,2] [1,3] [1,4]，以此类推。 「每次从集合中选取元素，可选择的范围随着选择的进行而收缩，调整可选择的范围」。 「图中可以发现n相当于树的宽度，k相当于树的深度」。 那么如何在这个树上遍历，然后收集到我们要的结果集呢？ 「图中每次搜索到了叶子节点，我们就找到了一个结果」。 相当于只需要把达到叶子节点的结果收集起来，就可以求得 n个数中k个数的组合集合。 在上文中我们提到了回溯法三部曲，那么我们按照回溯法三部曲开始正式讲解代码了。 回溯三部曲 递归函数的返回值以及参数 在这里要定义两个全局变量，一个用来存放符合条件单一结果，一个用来存放符合条件结果的集合。 代码如下： 12result = [] # 存放符合条件结果的集合path = [] # 用来存放符合条件结果 其实不定义这两个全局遍历也是可以的，把这两个变量放进递归函数的参数里，但函数里参数太多影响可读性，所以我定义全局变量了。 函数里一定有两个参数，既然是集合n里面取k的数，那么n和k是两个int型的参数。 然后还需要一个参数，为int型变量startIndex，这个参数用来记录本层递归的中，集合从哪里开始遍历（集合就是[1,…,n] ）。 为什么要有这个startIndex呢？ 「每次从集合中选取元素，可选择的范围随着选择的进行而收缩，调整可选择的范围，就是要靠startIndex」。 从下图中红线部分可以看出，在集合[1,2,3,4]取1之后，下一层递归，就要在[2,3,4]中取数了，那么下一层递归如何知道从[2,3,4]中取数呢，靠的就是startIndex。 所以需要startIndex来记录下一层递归，搜索的起始位置。 那么整体代码如下： 123result = [] # 存放符合条件结果的集合path = [] # 用来存放符合条件单一结果def backtracking(n, k, startIndex): 回溯函数终止条件 什么时候到达所谓的叶子节点了呢？ path这个数组的大小如果达到k，说明我们找到了一个子集大小为k的组合了，在图中path存的就是根节点到叶子节点的路径。 如图红色部分： 此时用result二维数组，把path保存起来，并终止本层递归。 所以终止条件代码如下： 123if len(path) == k: result.append(path) return 单层搜索的过程 回溯法的搜索过程就是一个树型结构的遍历过程，在如下图中，可以看出for循环用来横向遍历，递归的过程是纵向遍历。 如此我们才遍历完图中的这棵树。 for循环每次从startIndex开始遍历，然后用path保存取到的节点i。 代码如下： 1234for i in range(startindex, n + 1): # 控制树的横向遍历 path.append(i) # 处理节点 backtracking(n, k, i + 1); # 递归：控制树的纵向遍历，注意下一层搜索要从i + 1开始 path.pop() # 回溯，撤销处理的节点 可以看出backtracking（递归函数）通过不断调用自己一直往深处遍历，总会遇到叶子节点，遇到了叶子节点就要返回。 backtracking的下面部分就是回溯的操作了，撤销本次处理的结果。 关键地方都讲完了，组合问题完整代码如下： 12345678910111213141516171819202122232425262728class Solution(object): def __init__(self): self.res = [] self.path = [] def combine(self, n, k): &quot;&quot;&quot; :type n: int :type k: int :rtype: List[List[int]] &quot;&quot;&quot; self.backtracking(n, k, 1) return self.res def backtracking(self, n, k, startindex): if len(self.path) == k: # 注意此处传递的不能是引用。即append(a)不能传递引用，若是引用，a变，整体都变。 path = self.path[:] self.res.append(path) # 也可下面(节省空间复杂度) # self.res.append(self.path[:]) return for i in range(startindex, n + 1): self.path.append(i) self.backtracking(n, k, i + 1) self.path.pop() 还记得我们在上文中给出的回溯法模板么？ 如下： 123456789def backtracking(参数): if 终止条件: 存放结果 return for i in (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）): 处理节点 backtracking(路径，选择列表) # 递归 回溯，撤销处理结果 「对比一下本题的代码，是不是发现有点像！」 所以有了这个模板，就有解题的大体方向，不至于毫无头绪。 总结组合问题是回溯法解决的经典问题，我们开始的时候给大家列举一个很形象的例子，就是n为100，k为50的话，直接想法就需要50层for循环。 从而引出了回溯法就是解决这种k层for循环嵌套的问题。 然后进一步把回溯法的搜索过程抽象为树形结构，可以直观的看出搜索的过程。 接着用回溯法三部曲，逐步分析了函数参数、终止条件和单层搜索的过程。 「本题其实是可以剪枝优化的，大家可以思考一下，具体如何剪枝」","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"01-回溯法的框架","slug":"01-回溯法的基本框架","date":"2021-07-12T13:58:58.000Z","updated":"2021-07-15T14:11:45.973Z","comments":true,"path":"20210712/01-回溯法的基本框架.html","link":"","permalink":"https://xxren8218.github.io/20210712/01-%E5%9B%9E%E6%BA%AF%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6.html","excerpt":"","text":"回溯法概述什么是回溯法？回溯法也可以叫做回溯搜索法，它是一种搜索的方式。回溯是递归的副产品，只要有递归就会有回溯。 「所以以下讲解中，回溯函数也就是递归函数，指的都是一个函数」。 回溯法的效率？回溯法的性能如何呢，这里要和大家说清楚了，「虽然回溯法很难，很不好理解，但是回溯法并不是什么高效的算法」。 「因为回溯的本质是穷举，穷举所有可能，然后选出我们想要的答案」，如果想让回溯法高效一些，可以加一些剪枝的操作，但也改不了回溯法就是穷举的本质。 那么既然回溯法并不高效为什么还要用它呢？ 因为没得选，一些问题能暴力搜出来就不错了，撑死了再剪枝一下，还没有更高效的解法。 此时大家应该好奇了，都什么问题，这么牛逼，只能暴力搜索。 回溯法解决的问题？回溯法，一般可以解决如下几种问题： 组合问题：N个数里面按一定规则找出k个数的集合 排列问题：N个数按一定规则全排列，有几种排列方式 切割问题：一个字符串按一定规则有几种切割方式 子集问题：一个N个数的集合里有多少符合条件的子集 棋盘问题：N皇后，解数独等等 「相信大家看着这些之后会发现，每个问题，都不简单！」 另外，会有一些同学可能分不清什么是组合，什么是排列？ 「组合是不强调元素顺序的，排列是强调元素顺序」。 例如：{1, 2} 和 {2, 1} 在组合上，就是一个集合，因为不强调顺序，而要是排列的话，{1, 2} 和 {2, 1} 就是两个集合了。 记住组合无序，排列有序，就可以了。 如何理解回溯法？「回溯法解决的问题都可以抽象为树形结构」，是的，我指的是所有回溯法的问题都可以抽象为树形结构！ 因为回溯法解决的都是在集合中递归查找子集，「集合的大小就构成了树的宽度，递归的深度，都构成的树的深度」。 递归就要有终止条件，所以必然是一颗高度有限的树（N叉树）。 这块可能初学者还不太理解，后面的回溯算法解决的所有题目中，我都会强调这一点并画图举相应的例子，现在有一个印象就行。 回溯法模板？这里给出回溯算法模板。 前面我们说了递归三部曲，这里我再给大家列出回溯三部曲。 回溯函数模板返回值以及参数 在回溯算法中，我的习惯是函数起名字为backtracking，这个起名大家随意。 回溯算法中函数返回值一般为void。 再来看一下参数，因为回溯算法需要的参数可不像二叉树递归的时候那么容易一次性确定下来，所以一般是先写逻辑，然后需要什么参数，就填什么参数。 但后面的回溯题目的讲解中，为了方便大家理解，我在一开始就帮大家把参数确定下来。 回溯函数伪代码如下： 1def backtracking(参数): 回溯函数终止条件: 既然是树形结构，就知道遍历树形结构一定要有终止条件。 所以回溯也有要终止条件。 什么时候达到了终止条件，树中就可以看出，一般来说搜到叶子节点了，也就找到了满足条件的一条答案，把这个答案存放起来，并结束本层递归。 所以回溯函数终止条件伪代码如下： 123if 终止条件: 存放结果 return 回溯搜索的遍历过程 在上面我们提到了，回溯法一般是在集合中递归搜索，集合的大小构成了树的宽度，递归的深度构成的树的深度。 回溯函数遍历过程伪代码如下： 1234for i in (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）): 处理节点 backtracking(路径，选择列表) # 递归 回溯，撤销处理结果 for循环就是遍历集合区间，可以理解一个节点有多少个孩子，这个for循环就执行多少次。 backtracking这里自己调用自己，实现递归。 大家可以从图中看出「for循环可以理解是横向遍历，backtracking（递归）就是纵向遍历」，这样就把这棵树全遍历完了，一般来说，搜索叶子节点就是找的其中一个结果了。 分析完过程，回溯算法模板框架如下： 123456789def backtracking(参数): if 终止条件: 存放结果 return for i in (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）): 处理节点 backtracking(路径，选择列表) # 递归 回溯，撤销处理结果 「这份模板很重要，后面做回溯法的题目都靠它了！」 如果从来没有学过回溯算法的录友们，看到这里会有点懵，后面开始讲解具体题目的时候就会好一些了，已经做过回溯法题目的录友，看到这里应该会感同身受了。 总结本篇我们讲解了，什么是回溯算法，知道了回溯和递归是相辅相成的。 接着提到了回溯法的效率，回溯法其实就是暴力查找，并不是什么高效的算法。 然后列出了回溯法可以解决几类问题，可以看出每一类问题都不简单。 最后我们讲到回溯法解决的问题都可以抽象为树形结构（N叉树），并给出了回溯法的模板。","categories":[{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"}],"tags":[]},{"title":"13-合并两个二叉树","slug":"13-合并两个二叉树","date":"2021-07-12T13:55:46.000Z","updated":"2021-07-12T13:58:32.364Z","comments":true,"path":"20210712/13-合并两个二叉树.html","link":"","permalink":"https://xxren8218.github.io/20210712/13-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E4%BA%8C%E5%8F%89%E6%A0%91.html","excerpt":"","text":"合并两个二叉树 思路相信这道题目很多同学疑惑的点是如何同时遍历两个二叉树呢？ 其实和遍历一个树逻辑是一样的，只不过传入两个树的节点，同时操作。 递归法二叉树使用递归，就要想使用前中后哪种遍历方式？ 「本题使用哪种遍历都是可以的！」 我们下面以前序遍历为例。 那么我们来按照递归三部曲来解决： 确定递归函数的参数和返回值： 首先那么要合入两个二叉树，那么参数至少是要传入两个二叉树的根节点，返回值就是合并之后二叉树的根节点。 代码如下： 1def mergeTrees(t1, t2): 确定终止条件： 因为是传入了两个树，那么就有两个树遍历的节点t1 和 t2，如果t1 == None 了，两个树合并就应该是 t2 了（如果t2也为None也无所谓，合并之后就是None）。 反过来如果t2 == None，那么两个数合并就是t1（如果t1也为None也无所谓，合并之后就是NULL）。 代码如下： 12if t1 == None: return t2; # 如果t1为空，合并之后就应该是t2if t2 == None: return t1; # 如果t2为空，合并之后就应该是t1 确定单层递归的逻辑： 单层递归的逻辑就比较好些了，这里我们用重复利用一下t1这个树，t1就是合并之后树的根节点（就是修改了原来树的结构）。 那么单层递归中，就要把两棵树的元素加到一起。 1t1.val += t2.val 接下来t1 的左子树是：合并 t1左子树 t2左子树之后的左子树。 t1 的右子树：是 合并 t1右子树 t2右子树之后的右子树。 最终t1就是合并之后的根节点。 代码如下： 123t1.left = mergeTrees(t1.left, t2.left)t1.right = mergeTrees(t1.right, t2.right)return t1 此时前序遍历，完整代码就写出来了，如下： 1234567891011121314151617181920# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def mergeTrees(self, root1, root2): &quot;&quot;&quot; :type root1: TreeNode :type root2: TreeNode :rtype: TreeNode &quot;&quot;&quot; if not root1: return root2 if not root2: return root1 root1.val += root2.val root1.left = self.mergeTrees(root1.left, root2.left) root1.right = self.mergeTrees(root1.right, root2.right) return root1 那么中序遍历也是可以的，代码如下： 123456789101112131415161718192021# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def mergeTrees(self, root1, root2): &quot;&quot;&quot; :type root1: TreeNode :type root2: TreeNode :rtype: TreeNode &quot;&quot;&quot; if not root1: return root2 if not root2: return root1 root1.left = self.mergeTrees(root1.left, root2.left) root1.val += root2.val root1.right = self.mergeTrees(root1.right, root2.right) return root1 后序遍历依然可以，代码如下： 123456789101112131415161718192021# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def mergeTrees(self, root1, root2): &quot;&quot;&quot; :type root1: TreeNode :type root2: TreeNode :rtype: TreeNode &quot;&quot;&quot; if not root1: return root2 if not root2: return root1 root1.left = self.mergeTrees(root1.left, root2.left) root1.right = self.mergeTrees(root1.right, root2.right) root1.val += root2.val return root1 「但是前序遍历是最好理解的，我建议大家用前序遍历来做就OK。」 如上的方法修改了t1的结构，当然也可以不修改t1和t2的结构，重新定一个树。 不修改输入树的结构，前序遍历，代码如下： 1234567891011121314151617181920212223# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def mergeTrees(self, root1, root2): &quot;&quot;&quot; :type root1: TreeNode :type root2: TreeNode :rtype: TreeNode &quot;&quot;&quot; if not root1: return root2 if not root2: return root1 root = TreeNode(0) root.val = root1.val + root2.val root.left = self.mergeTrees(root1.left, root2.left) root.right = self.mergeTrees(root1.right, root2.right) return root 迭代法使用迭代法，如何同时处理两棵树呢？ 思路: 求二叉树对称的时候就是把两个树的节点同时加入队列进行比较。这里也一样。 本题我们也使用队列，模拟的层序遍历，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def mergeTrees(self, root1, root2): &quot;&quot;&quot; :type root1: TreeNode :type root2: TreeNode :rtype: TreeNode &quot;&quot;&quot; if not root1: return root2 if not root2: return root1 # 此时root1和root2比存在。 queue = [root1,root2] while queue: node1 = queue.pop(0) node2 = queue.pop(0) # 此时两个节点一定不为空，val相加 node1.val += node2.val # 如果两棵树左节点都不为空，加入队列 if node1.left and node2.left: queue.append(node1.left) queue.append(node2.left) # 如果两棵树右节点都不为空，加入队列 if node1.right and node2.right: queue.append(node1.right) queue.append(node2.right) # 当root1的左节点 为空 root2左节点不为空，就赋值过去 if not node1.left and node2.left: node1.left = node2.left # 当root1的右节点 为空 root2左节点不为空，就赋值过去 if not node1.right and node2.right: node1.right = node2.right return root1 总结合并二叉树，也是二叉树操作的经典题目，如果没有接触过的话，其实并不简单，因为我们习惯了操作一个二叉树，一起操作两个二叉树，还会有点懵懵的。 这不是我们第一次操作两颗二叉树了，在对称二叉树中也一起操作了两棵二叉树。 迭代法中，一般一起操作两个树都是使用队列模拟类似层序遍历，同时处理两个树的节点，这种方式最好理解，如果用模拟递归的思路的话，要复杂一些。 就酱，学到了的话，就分享给身边需要的同学吧！","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"05-离线推荐处理&实时推荐","slug":"36-离线推荐处理-实时推荐","date":"2021-07-10T14:50:29.000Z","updated":"2021-07-19T09:41:59.577Z","comments":true,"path":"20210710/36-离线推荐处理-实时推荐.html","link":"","permalink":"https://xxren8218.github.io/20210710/36-%E7%A6%BB%E7%BA%BF%E6%8E%A8%E8%8D%90%E5%A4%84%E7%90%86-%E5%AE%9E%E6%97%B6%E6%8E%A8%E8%8D%90.html","excerpt":"","text":"离线推荐数据缓存 &amp; 实时推荐1.1 离线数据缓存之离线召回集 这里主要是利用我们前面训练的ALS模型进行协同过滤召回，但是注意，我们ALS模型召回的是用户最感兴趣的类别，而我们需要的是用户可能感兴趣的广告的集合，因此我们还需要根据召回的类别匹配出对应的广告。 所以这里我们除了需要我们训练的ALS模型以外，还需要有一个广告和类别的对应关系。 12345678910111213141516171819202122232425262728293031# 从HDFS中加载广告基本信息数据，返回spark dafaframe对象df = spark.read.csv(&quot;hdfs://localhost:8020/csv/ad_feature.csv&quot;, header=True)# 注意：由于本数据集中存在NULL字样的数据，无法直接设置schema，只能先将NULL类型的数据处理掉，然后进行类型转换from pyspark.sql.types import StructType, StructField, IntegerType, FloatType# 替换掉NULL字符串，替换掉df = df.replace(&quot;NULL&quot;, &quot;-1&quot;)# 更改df表结构：更改列类型和列名称ad_feature_df = df.\\ withColumn(&quot;adgroup_id&quot;, df.adgroup_id.cast(IntegerType())).withColumnRenamed(&quot;adgroup_id&quot;, &quot;adgroupId&quot;).\\ withColumn(&quot;cate_id&quot;, df.cate_id.cast(IntegerType())).withColumnRenamed(&quot;cate_id&quot;, &quot;cateId&quot;).\\ withColumn(&quot;campaign_id&quot;, df.campaign_id.cast(IntegerType())).withColumnRenamed(&quot;campaign_id&quot;, &quot;campaignId&quot;).\\ withColumn(&quot;customer&quot;, df.customer.cast(IntegerType())).withColumnRenamed(&quot;customer&quot;, &quot;customerId&quot;).\\ withColumn(&quot;brand&quot;, df.brand.cast(IntegerType())).withColumnRenamed(&quot;brand&quot;, &quot;brandId&quot;).\\ withColumn(&quot;price&quot;, df.price.cast(FloatType()))# 这里我们只需要adgroupId、和cateId_ = ad_feature_df.select(&quot;adgroupId&quot;, &quot;cateId&quot;)# 由于这里数据集其实很少，所以我们再直接转成Pandas dataframe来处理，把数据载入内存pdf = _.toPandas()# 手动释放一些内存del dfdel ad_feature_dfdel _import gcgc.collect() 根据指定的类别找到对应的广告 1234import numpy as nppdf.where(pdf.cateId==11156).dropna().adgroupId # 举个例子，随便找个cateId看看他的adgroupId都有哪些。np.random.choice(pdf.where(pdf.cateId==11156).dropna().adgroupId.astype(np.int64), 200) # 在召回的用户感兴趣的类别上随机挑选200个物品。 显示结果: 12345678910111213313 138953.0314 467512.01661 140008.01666 238772.01669 237471.01670 238761.0 ... 843456 352273.0846728 818681.0846729 838953.0846810 845337.0Name: adgroupId, Length: 731, dtype: float64 利用ALS模型进行类别的召回 1234567# 加载als模型，注意必须先有spark上下文管理器，即sparkContext，但这里sparkSession创建后，自动创建了sparkContextfrom pyspark.ml.recommendation import ALSModel# 从hdfs加载之前存储的模型als_model = ALSModel.load(&quot;hdfs://localhost:8020/models/userCateRatingALSModel.obj&quot;)# 返回模型中关于用户的所有属性 df: id featuresals_model.userFactors 显示结果: 1DataFrame[id: int, features: array&lt;float&gt;] 123import pandas as pdcateId_df = pd.DataFrame(pdf.cateId.unique(),columns=[&quot;cateId&quot;])cateId_df 显示结果: 1234567891011121314 cateId0 11 22 33 44 55 66 7... ...6766 129486767 129556768 129606769 rows × 1 columns 12cateId_df.insert(0, &quot;userId&quot;, np.array([8 for i in range(6769)]))cateId_df 显示结果: 123456789101112 userId cateId0 8 11 8 22 8 33 8 44 8 5... ... ...6766 8 129486767 8 129556768 8 129606769 rows × 2 columns 传入 userid、cataId的df，对应预测值进行排序（也可以用之前的recommandForAllusers，这样是为了另外一种ALS的用法） 1als_model.transform(spark.createDataFrame(cateId_df)).sort(&quot;prediction&quot;, ascending=False).na.drop().show() 显示结果: 1234567891011121314151617181920212223242526+------+------+----------+|userId|cateId|prediction|+------+------+----------+| 8| 7214| 9.917084|| 8| 877| 7.479664|| 8| 7266| 7.4762917|| 8| 10856| 7.3395424|| 8| 4766| 7.149538|| 8| 7282| 6.6835284|| 8| 7270| 6.2145095|| 8| 201| 6.0623236|| 8| 4267| 5.9155636|| 8| 7267| 5.838009|| 8| 5392| 5.6882005|| 8| 6261| 5.6804466|| 8| 6306| 5.2992325|| 8| 11050| 5.245261|| 8| 8655| 5.1701374|| 8| 4610| 5.139578|| 8| 932| 5.12694|| 8| 12276| 5.0776596|| 8| 8071| 4.979195|| 8| 6580| 4.8523283|+------+------+----------+only showing top 20 rows 123456789101112131415161718192021222324252627import numpy as npimport pandas as pdimport redis# 存储用户召回，使用redis第9号数据库，类型：sets类型client = redis.StrictRedis(host=&quot;192.168.199.188&quot;, port=6379, db=9)for r in als_model.userFactors.select(&quot;id&quot;).collect(): userId = r.id cateId_df = pd.DataFrame(pdf.cateId.unique(),columns=[&quot;cateId&quot;]) cateId_df.insert(0, &quot;userId&quot;, np.array([userId for i in range(6769)])) ret = set() # 利用模型，传入datasets(userId, cateId)，这里控制了userId一样，所以相当于是在求某用户对所有分类的兴趣程度（评分） cateId_list = als_model.transform(spark.createDataFrame(cateId_df)).sort(&quot;prediction&quot;, ascending=False).na.drop() # 从前20个分类中选出500个进行召回 for i in cateId_list.head(20): need = 500 - len(ret) # 如果不足500个，那么随机选出need个广告 ret = ret.union(np.random.choice(pdf.where(pdf.cateId==i.cateId).adgroupId.dropna().astype(np.int64), need)) if len(ret) &gt;= 500: # 如果达到500个则退出 break client.sadd(userId, *ret) # 如果redis所在机器，内存不足，会抛出异常 2.1 离线数据缓存之离线特征1234567891011121314151617181920# &quot;pid&quot;, 广告资源位，属于场景特征，也就是说，每一种广告通常是可以防止在多种资源外下的# 因此这里对于pid，应该是由广告系统发起推荐请求时，向推荐系统明确要推荐的用户是谁，以及对应的资源位，或者说有哪些# 这样如果有多个资源位，那么每个资源位都会对应相应的一个推荐列表# 需要进行缓存的特征值feature_cols_from_ad = [ &quot;price&quot; # 来自广告基本信息中]# 用户特征feature_cols_from_user = [ &quot;cms_group_id&quot;, &quot;final_gender_code&quot;, &quot;age_level&quot;, &quot;shopping_level&quot;, &quot;occupation&quot;, &quot;pvalue_level&quot;, &quot;new_user_class_level&quot;] 从HDFS中加载广告基本信息数据 12345678910111213141516171819202122232425262728293031_ad_feature_df = spark.read.csv(&quot;hdfs://localhost:9000/datasets/ad_feature.csv&quot;, header=True)# 更改表结构，转换为对应的数据类型from pyspark.sql.types import StructType, StructField, IntegerType, FloatType# 替换掉NULL字符串_ad_feature_df = _ad_feature_df.replace(&quot;NULL&quot;, &quot;-1&quot;) # 更改df表结构：更改列类型和列名称ad_feature_df = _ad_feature_df.\\ withColumn(&quot;adgroup_id&quot;, _ad_feature_df.adgroup_id.cast(IntegerType())).withColumnRenamed(&quot;adgroup_id&quot;, &quot;adgroupId&quot;).\\ withColumn(&quot;cate_id&quot;, _ad_feature_df.cate_id.cast(IntegerType())).withColumnRenamed(&quot;cate_id&quot;, &quot;cateId&quot;).\\ withColumn(&quot;campaign_id&quot;, _ad_feature_df.campaign_id.cast(IntegerType())).withColumnRenamed(&quot;campaign_id&quot;, &quot;campaignId&quot;).\\ withColumn(&quot;customer&quot;, _ad_feature_df.customer.cast(IntegerType())).withColumnRenamed(&quot;customer&quot;, &quot;customerId&quot;).\\ withColumn(&quot;brand&quot;, _ad_feature_df.brand.cast(IntegerType())).withColumnRenamed(&quot;brand&quot;, &quot;brandId&quot;).\\ withColumn(&quot;price&quot;, _ad_feature_df.price.cast(FloatType())) def foreachPartition(partition): import redis import json client = redis.StrictRedis(host=&quot;192.168.199.188&quot;, port=6379, db=10) for r in partition: data = &#123; &quot;price&quot;: r.price &#125; # 转成json字符串再保存，能保证数据再次倒出来时，能有效的转换成python类型 client.hset(&quot;ad_features&quot;, r.adgroupId, json.dumps(data)) ad_feature_df.foreachPartition(foreachPartition) 从HDFS加载用户基本信息数据 1234567891011121314151617from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType# 构建表结构schema对象schema = StructType([ StructField(&quot;userId&quot;, IntegerType()), StructField(&quot;cms_segid&quot;, IntegerType()), StructField(&quot;cms_group_id&quot;, IntegerType()), StructField(&quot;final_gender_code&quot;, IntegerType()), StructField(&quot;age_level&quot;, IntegerType()), StructField(&quot;pvalue_level&quot;, IntegerType()), StructField(&quot;shopping_level&quot;, IntegerType()), StructField(&quot;occupation&quot;, IntegerType()), StructField(&quot;new_user_class_level&quot;, IntegerType())])# 利用schema从hdfs加载user_profile_df = spark.read.csv(&quot;hdfs://localhost:8020/csv/user_profile.csv&quot;, header=True, schema=schema)user_profile_df 显示结果: 12DataFrame[userId: int, cms_segid: int, cms_group_id: int, final_gender_code: int, age_level: int, pvalue_level: int, shopping_level: int, occupation: int, new_user_class_level: int] 1234567891011121314151617181920def foreachPartition2(partition): import redis import json client = redis.StrictRedis(host=&quot;192.168.199.188&quot;, port=6379, db=10) for r in partition: data = &#123; &quot;cms_group_id&quot;: r.cms_group_id, &quot;final_gender_code&quot;: r.final_gender_code, &quot;age_level&quot;: r.age_level, &quot;shopping_level&quot;: r.shopping_level, &quot;occupation&quot;: r.occupation, &quot;pvalue_level&quot;: r.pvalue_level, &quot;new_user_class_level&quot;: r.new_user_class_level &#125; # 转成json字符串再保存，能保证数据再次倒出来时，能有效的转换成python类型 client.hset(&quot;user_features1&quot;, r.userId, json.dumps(data)) user_profile_df.foreachPartition(foreachPartition2) 二. 实时产生推荐结果2.1 推荐任务处理 CTR预测模型 + 特征 ==&gt; 预测结果 ==&gt; TOP-N列表 数据缓存取出之后 还原成对应的onehot编码 热编码中：”pvalue_level”特征对应关系: 12345678+------------+----------------------+|pvalue_level|pl_onehot_feature |+------------+----------------------+| -1| 0.0|| 3| 3.0|| 1| 2.0|| 2| 1.0|+------------+----------------------+ “new_user_class_level”的特征对应关系： 123456789+--------------------+------------------------+|new_user_class_level|nucl_onehot_feature |+--------------------+------------------------+| -1| 0.0|| 3| 2.0|| 1| 4.0|| 4| 3.0|| 2| 1.0|+--------------------+------------------------+ 12pvalue_level_rela = &#123;-1: 0, 3:3, 1:2, 2:1&#125;new_user_class_level_rela = &#123;-1:0, 3:2, 1:4, 4:3, 2:1&#125; “cms_group_id”特征对应关系： 1234567891011121314151617+------------+-------------------------+|cms_group_id|min(cms_group_id_feature)|+------------+-------------------------+| 7| 9.0|| 11| 6.0|| 3| 0.0|| 8| 8.0|| 0| 12.0|| 5| 3.0|| 6| 10.0|| 9| 5.0|| 1| 7.0|| 10| 4.0|| 4| 1.0|| 12| 11.0|| 2| 2.0|+------------+-------------------------+ 123456789101112131415cms_group_id_rela &#x3D; &#123; 7: 9, 11: 6, 3: 0, 8: 8, 0: 12, 5: 3, 6: 10, 9: 5, 1: 7, 10: 4, 4: 1, 12: 11, 2: 2&#125; “final_gender_code”特征对应关系： 123456+-----------------+------------------------------+|final_gender_code|min(final_gender_code_feature)|+-----------------+------------------------------+| 1| 1.0|| 2| 0.0|+-----------------+------------------------------+ 1final_gender_code_rela &#x3D; &#123;1:1, 2:0&#125; “age_level”特征对应关系： 1234567891011+---------+----------------------+|age_level|min(age_level_feature)|+---------+----------------------+| 3| 0.0|| 0| 6.0|| 5| 2.0|| 6| 5.0|| 1| 4.0|| 4| 1.0|| 2| 3.0|+---------+----------------------+ 1age_level_rela &#x3D; &#123;3:0, 0:6, 5:2, 6:5, 1:4, 4:1, 2:3&#125; “shopping_level”特征对应关系： 123456|shopping_level|min(shopping_level_feature)|+--------------+---------------------------+| 3| 0.0|| 1| 2.0|| 2| 1.0|+--------------+---------------------------+ 1shopping_level_rela &#x3D; &#123;3:0, 1:2, 2:1&#125; “occupation”特征对应关系： 123456+----------+-----------------------+|occupation|min(occupation_feature)|+----------+-----------------------+| 0| 0.0|| 1| 1.0|+----------+-----------------------+ 123456occupation_rela &#x3D; &#123;0:0, 1:1&#125;pid_rela &#x3D; &#123; &quot;430548_1007&quot;: 0, &quot;430549_1007&quot;: 1&#125; 特征获取 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import redisimport jsonimport pandas as pdfrom pyspark.ml.linalg import DenseVectordef create_datasets(userId, pid): client_of_recall = redis.StrictRedis(host=&quot;192.168.199.88&quot;, port=6379, db=9) client_of_features = redis.StrictRedis(host=&quot;192.168.199.88&quot;, port=6379, db=10) # 获取用户特征 user_feature = json.loads(client_of_features.hget(&quot;user_features&quot;, userId)) # 获取用户召回集 recall_sets = client_of_recall.smembers(userId) result = [] # 遍历召回集 for adgroupId in recall_sets: adgroupId = int(adgroupId) # 获取该广告的特征值 ad_feature = json.loads(client_of_features.hget(&quot;ad_features&quot;, adgroupId)) features = &#123;&#125; features.update(user_feature) features.update(ad_feature) for k,v in features.items(): if v is None: features[k] = -1 features_col = [ # 特征值 &quot;price&quot;, &quot;cms_group_id&quot;, &quot;final_gender_code&quot;, &quot;age_level&quot;, &quot;shopping_level&quot;, &quot;occupation&quot;, &quot;pid&quot;, &quot;pvalue_level&quot;, &quot;new_user_class_level&quot; ] &#x27;&#x27;&#x27; &quot;cms_group_id&quot;, 类别型特征，约13个分类 ==&gt; 13维 &quot;final_gender_code&quot;, 类别型特征，2个分类 ==&gt; 2维 &quot;age_level&quot;, 类别型特征，7个分类 ==&gt;7维 &quot;shopping_level&quot;, 类别型特征，3个分类 ==&gt; 3维 &quot;occupation&quot;, 类别型特征，2个分类 ==&gt; 2维 &#x27;&#x27;&#x27; price = float(features[&quot;price&quot;]) pid_value = [0 for i in range(2)]#[0,0] cms_group_id_value = [0 for i in range(13)] final_gender_code_value = [0 for i in range(2)] age_level_value = [0 for i in range(7)] shopping_level_value = [0 for i in range(3)] occupation_value = [0 for i in range(2)] pvalue_level_value = [0 for i in range(4)] new_user_class_level_value = [0 for i in range(5)] pid_value[pid_rela[pid]] = 1 cms_group_id_value[cms_group_id_rela[int(features[&quot;cms_group_id&quot;])]] = 1 final_gender_code_value[final_gender_code_rela[int(features[&quot;final_gender_code&quot;])]] = 1 age_level_value[age_level_rela[int(features[&quot;age_level&quot;])]] = 1 shopping_level_value[shopping_level_rela[int(features[&quot;shopping_level&quot;])]] = 1 occupation_value[occupation_rela[int(features[&quot;occupation&quot;])]] = 1 pvalue_level_value[pvalue_level_rela[int(features[&quot;pvalue_level&quot;])]] = 1 new_user_class_level_value[new_user_class_level_rela[int(features[&quot;new_user_class_level&quot;])]] = 1# print(pid_value)# print(cms_group_id_value)# print(final_gender_code_value)# print(age_level_value)# print(shopping_level_value)# print(occupation_value)# print(pvalue_level_value)# print(new_user_class_level_value) vector = DenseVector([price] + pid_value + cms_group_id_value + final_gender_code_value\\ + age_level_value + shopping_level_value + occupation_value + pvalue_level_value + new_user_class_level_value) result.append((userId, adgroupId, vector)) return result# create_datasets(88, &quot;430548_1007&quot;) 载入训练好的模型 12345from pyspark.ml.classification import LogisticRegressionModelCTR_model &#x3D; LogisticRegressionModel.load(&quot;hdfs:&#x2F;&#x2F;localhost:9000&#x2F;models&#x2F;CTRModel_AllOneHot.obj&quot;)pdf &#x3D; pd.DataFrame(create_datasets(8, &quot;430548_1007&quot;), columns&#x3D;[&quot;userId&quot;, &quot;adgroupId&quot;, &quot;features&quot;])datasets &#x3D; spark.createDataFrame(pdf)datasets.show() 显示结果: 12345678910111213141516171819202122232425+------+---------+--------------------+|userId|adgroupId| features|+------+---------+--------------------+| 8| 445914|[9.89999961853027...|| 8| 258252|[7.59999990463256...|| 8| 129682|[8.5,1.0,0.0,1.0,...|| 8| 763027|[68.0,1.0,0.0,1.0...|| 8| 292027|[16.0,1.0,0.0,1.0...|| 8| 430023|[34.2000007629394...|| 8| 133457|[169.0,1.0,0.0,1....|| 8| 816999|[5.0,1.0,0.0,1.0,...|| 8| 221714|[4.80000019073486...|| 8| 186334|[106.0,1.0,0.0,1....|| 8| 169717|[2.20000004768371...|| 8| 31314|[15.8000001907348...|| 8| 815312|[2.29999995231628...|| 8| 199445|[5.0,1.0,0.0,1.0,...|| 8| 746178|[16.7999992370605...|| 8| 290950|[6.5,1.0,0.0,1.0,...|| 8| 221585|[18.5,1.0,0.0,1.0...|| 8| 692672|[47.0,1.0,0.0,1.0...|| 8| 797982|[33.0,1.0,0.0,1.0...|| 8| 815219|[2.40000009536743...|+------+---------+--------------------+only showing top 20 rows 12prediction &#x3D; CTR_model.transform(datasets).sort(&quot;probability&quot;)prediction.show() 12345678910111213141516171819202122232425+------+---------+--------------------+--------------------+--------------------+----------+|userId|adgroupId| features| rawPrediction| probability|prediction|+------+---------+--------------------+--------------------+--------------------+----------+| 8| 631204|[19888.0,1.0,0.0,...|[2.69001234046578...|[0.93643471623189...| 0.0|| 8| 583215|[3750.0,1.0,0.0,1...|[2.69016170680037...|[0.93644360664433...| 0.0|| 8| 275819|[3280.0,1.0,0.0,1...|[2.69016605691669...|[0.93644386554961...| 0.0|| 8| 401433|[1200.0,1.0,0.0,1...|[2.69018530849532...|[0.93644501133142...| 0.0|| 8| 29466|[640.0,1.0,0.0,1....|[2.69019049161265...|[0.93644531980785...| 0.0|| 8| 173327|[356.0,1.0,0.0,1....|[2.69019312019358...|[0.93644547624893...| 0.0|| 8| 241402|[269.0,1.0,0.0,1....|[2.69019392542787...|[0.93644552417271...| 0.0|| 8| 351366|[246.0,1.0,0.0,1....|[2.69019413830591...|[0.93644553684221...| 0.0|| 8| 229827|[238.0,1.0,0.0,1....|[2.69019421235044...|[0.93644554124900...| 0.0|| 8| 164807|[228.0,1.0,0.0,1....|[2.69019430490611...|[0.93644554675747...| 0.0|| 8| 227731|[199.0,1.0,0.0,1....|[2.69019457331754...|[0.93644556273205...| 0.0|| 8| 265403|[198.0,1.0,0.0,1....|[2.69019458257311...|[0.93644556328290...| 0.0|| 8| 569939|[188.0,1.0,0.0,1....|[2.69019467512877...|[0.93644556879138...| 0.0|| 8| 277335|[181.5,1.0,0.0,1....|[2.69019473528996...|[0.93644557237189...| 0.0|| 8| 575633|[180.0,1.0,0.0,1....|[2.69019474917331...|[0.93644557319816...| 0.0|| 8| 201867|[179.0,1.0,0.0,1....|[2.69019475842887...|[0.93644557374900...| 0.0|| 8| 25542|[176.0,1.0,0.0,1....|[2.69019478619557...|[0.93644557540155...| 0.0|| 8| 133457|[169.0,1.0,0.0,1....|[2.69019485098454...|[0.93644557925748...| 0.0|| 8| 494224|[169.0,1.0,0.0,1....|[2.69019485098454...|[0.93644557925748...| 0.0|| 8| 339382|[163.0,1.0,0.0,1....|[2.69019490651794...|[0.93644558256256...| 0.0|+------+---------+--------------------+--------------------+--------------------+----------+only showing top 20 rows TOP-20 12# TOP-20prediction.select(&quot;adgroupId&quot;).head(20) 显示结果: 1234567891011121314151617181920[Row(adgroupId=631204), Row(adgroupId=583215), Row(adgroupId=275819), Row(adgroupId=401433), Row(adgroupId=29466), Row(adgroupId=173327), Row(adgroupId=241402), Row(adgroupId=351366), Row(adgroupId=229827), Row(adgroupId=164807), Row(adgroupId=227731), Row(adgroupId=265403), Row(adgroupId=569939), Row(adgroupId=277335), Row(adgroupId=575633), Row(adgroupId=201867), Row(adgroupId=25542), Row(adgroupId=133457), Row(adgroupId=494224), Row(adgroupId=339382)] 1[i.adgroupId for i in prediction.select(&quot;adgroupId&quot;).head(20)] 显示结果: 1234567891011121314151617181920[631204, 583215, 275819, 401433, 29466, 173327, 241402, 351366, 229827, 164807, 227731, 265403, 569939, 277335, 575633, 201867, 25542, 133457, 494224, 339382] 总结推荐服务 离线推荐 先召回对召回结果排序 为每一个用户都进行召回并排序的过程并且把拍好顺序的结果放到数据库中 如果需要推荐结果的时候 直接到数据库中按照user_id查询，返回推荐结果 优点 结构比较简单 推荐服务只需要不断计算，把结果保存到数据库中即可 缺点 实时性差 如果数据1天不更新 1天之内推荐结果一样的，不能反映用户的实时兴趣 实时推荐 排序的模型加载好 召回阶段的结果缓存 所有用户的特征缓存 所有物品的特征缓存 把推荐的服务暴露出去（django flask) 需要推荐结果的服务把 用户id 传递过来 根据id 找到召回结果 根据id 找到缓存的用户特征 根据召回结果的物品id 找到物品的特征 用户特征+物品特征-》逻辑回归模型 就可以预测点击率 所有召回的物品的点记率都预测并排序 推荐topN 实时通过LR模型进行排序的好处 随时修改召回集 随时调整用户的特征 当用户需要推荐服务的时候，获取到最新的召回集和用户特征 得到最新的排序结果 更能体现出用户的实时兴趣 召回（群策群力） 协同过滤 基于内容召回 基于流行度召回 排序（根据自身特征和物品特征预估） LR CTR预估 SparkML 和SparkMLlib 区别 spark mllib 基于RDD 数据准备 需要创建一个 基于LabeledPoint的RDD LabeledPoint（目标，[特征]） 已经停止更新了 处于维护状态 spark ML 基于dataframe 数据准备 需要把所有的特征放到一列中 dataframe还需要有一列是 目标值 model = lr.setLabelCol(‘affairs’).setFeaturesCol(‘feautures’).fit(trainDF) spark ML 与 sklearn更类似 最新的API放到 spark ML中的 缺失值处理 分类特征 把缺失作为单独的特征处理 算法预测 连续的特征 算法预测 平均值 默认值 中位数填充 利用spark 处理 onehot 稀疏向量 大部分维度上的值都是0 sparseVector (向量的维度,[非零元素的索引],[非零元素的值]) 对应的API：stringindexer onehotEncoder pipline 239146001","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"},{"name":"推荐系统实战","slug":"机器学习/大数据的lambda架构/推荐系统实战","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"04-逻辑回归(LR)实现CTR预估","slug":"35-逻辑回归-LR-实现CTR预估","date":"2021-07-10T14:49:27.000Z","updated":"2021-07-19T09:41:04.838Z","comments":true,"path":"20210710/35-逻辑回归-LR-实现CTR预估.html","link":"","permalink":"https://xxren8218.github.io/20210710/35-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-LR-%E5%AE%9E%E7%8E%B0CTR%E9%A2%84%E4%BC%B0.html","excerpt":"","text":"LR实现CTR预估1. Spark逻辑回归(LR)模型使用介绍 先通过小案例来看看逻辑回归的模型怎么处理。 pyspark.ml需要准备DataFrame的数据。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from pyspark.ml.feature import VectorAssemblerimport pandas as pd# 样本数据集sample_dataset = [ (0, &quot;male&quot;, 37, 10, &quot;no&quot;, 3, 18, 7, 4), (0, &quot;female&quot;, 27, 4, &quot;no&quot;, 4, 14, 6, 4), (0, &quot;female&quot;, 32, 15, &quot;yes&quot;, 1, 12, 1, 4), (0, &quot;male&quot;, 57, 15, &quot;yes&quot;, 5, 18, 6, 5), (0, &quot;male&quot;, 22, 0.75, &quot;no&quot;, 2, 17, 6, 3), (0, &quot;female&quot;, 32, 1.5, &quot;no&quot;, 2, 17, 5, 5), (0, &quot;female&quot;, 22, 0.75, &quot;no&quot;, 2, 12, 1, 3), (0, &quot;male&quot;, 57, 15, &quot;yes&quot;, 2, 14, 4, 4), (0, &quot;female&quot;, 32, 15, &quot;yes&quot;, 4, 16, 1, 2), (0, &quot;male&quot;, 22, 1.5, &quot;no&quot;, 4, 14, 4, 5), (0, &quot;male&quot;, 37, 15, &quot;yes&quot;, 2, 20, 7, 2), (0, &quot;male&quot;, 27, 4, &quot;yes&quot;, 4, 18, 6, 4), (0, &quot;male&quot;, 47, 15, &quot;yes&quot;, 5, 17, 6, 4), (0, &quot;female&quot;, 22, 1.5, &quot;no&quot;, 2, 17, 5, 4), (0, &quot;female&quot;, 27, 4, &quot;no&quot;, 4, 14, 5, 4), (0, &quot;female&quot;, 37, 15, &quot;yes&quot;, 1, 17, 5, 5), (0, &quot;female&quot;, 37, 15, &quot;yes&quot;, 2, 18, 4, 3), (0, &quot;female&quot;, 22, 0.75, &quot;no&quot;, 3, 16, 5, 4), (0, &quot;female&quot;, 22, 1.5, &quot;no&quot;, 2, 16, 5, 5), (0, &quot;female&quot;, 27, 10, &quot;yes&quot;, 2, 14, 1, 5), (1, &quot;female&quot;, 32, 15, &quot;yes&quot;, 3, 14, 3, 2), (1, &quot;female&quot;, 27, 7, &quot;yes&quot;, 4, 16, 1, 2), (1, &quot;male&quot;, 42, 15, &quot;yes&quot;, 3, 18, 6, 2), (1, &quot;female&quot;, 42, 15, &quot;yes&quot;, 2, 14, 3, 2), (1, &quot;male&quot;, 27, 7, &quot;yes&quot;, 2, 17, 5, 4), (1, &quot;male&quot;, 32, 10, &quot;yes&quot;, 4, 14, 4, 3), (1, &quot;male&quot;, 47, 15, &quot;yes&quot;, 3, 16, 4, 2), (0, &quot;male&quot;, 37, 4, &quot;yes&quot;, 2, 20, 6, 4)]columns = [&quot;affairs&quot;, &quot;gender&quot;, &quot;age&quot;, &quot;label&quot;, &quot;children&quot;, &quot;religiousness&quot;, &quot;education&quot;, &quot;occupation&quot;, &quot;rating&quot;]# pandas构建dataframe，方便（若不用pd预先创建的话，写Spark的DataFrame的话，还得写Schema，是比较麻烦的。）pdf = pd.DataFrame(sample_dataset, columns=columns)# 转换成spark的dataframedf = spark.createDataFrame(pdf)# 特征选取：affairs为目标值，其余为特征值df2 = df.select(&quot;affairs&quot;,&quot;age&quot;, &quot;religiousness&quot;, &quot;education&quot;, &quot;occupation&quot;, &quot;rating&quot;)# 用于计算特征向量的字段colArray2 = [&quot;age&quot;, &quot;religiousness&quot;, &quot;education&quot;, &quot;occupation&quot;, &quot;rating&quot;]# 计算出特征向量(将特征向量放在一列里。)df3 = VectorAssembler().setInputCols(colArray2).setOutputCol(&quot;features&quot;).transform(df2)print(&quot;数据集：&quot;)df3.show()# 随机切分为训练集和测试集trainDF, testDF = df3.randomSplit([0.8,0.2])print(&quot;训练集：&quot;)trainDF.show(10)print(&quot;测试集：&quot;)testDF.show(10) 显示结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455数据集：+-------+---+-------------+---------+----------+------+--------------------+|affairs|age|religiousness|education|occupation|rating| features|+-------+---+-------------+---------+----------+------+--------------------+| 0| 37| 3| 18| 7| 4|[37.0,3.0,18.0,7....|| 0| 27| 4| 14| 6| 4|[27.0,4.0,14.0,6....|| 0| 32| 1| 12| 1| 4|[32.0,1.0,12.0,1....|| 0| 57| 5| 18| 6| 5|[57.0,5.0,18.0,6....|| 0| 22| 2| 17| 6| 3|[22.0,2.0,17.0,6....|| 0| 32| 2| 17| 5| 5|[32.0,2.0,17.0,5....|| 0| 22| 2| 12| 1| 3|[22.0,2.0,12.0,1....|| 0| 57| 2| 14| 4| 4|[57.0,2.0,14.0,4....|| 0| 32| 4| 16| 1| 2|[32.0,4.0,16.0,1....|| 0| 22| 4| 14| 4| 5|[22.0,4.0,14.0,4....|| 0| 37| 2| 20| 7| 2|[37.0,2.0,20.0,7....|| 0| 27| 4| 18| 6| 4|[27.0,4.0,18.0,6....|| 0| 47| 5| 17| 6| 4|[47.0,5.0,17.0,6....|| 0| 22| 2| 17| 5| 4|[22.0,2.0,17.0,5....|| 0| 27| 4| 14| 5| 4|[27.0,4.0,14.0,5....|| 0| 37| 1| 17| 5| 5|[37.0,1.0,17.0,5....|| 0| 37| 2| 18| 4| 3|[37.0,2.0,18.0,4....|| 0| 22| 3| 16| 5| 4|[22.0,3.0,16.0,5....|| 0| 22| 2| 16| 5| 5|[22.0,2.0,16.0,5....|| 0| 27| 2| 14| 1| 5|[27.0,2.0,14.0,1....|+-------+---+-------------+---------+----------+------+--------------------+only showing top 20 rows训练集：+-------+---+-------------+---------+----------+------+--------------------+|affairs|age|religiousness|education|occupation|rating| features|+-------+---+-------------+---------+----------+------+--------------------+| 0| 32| 1| 12| 1| 4|[32.0,1.0,12.0,1....|| 0| 37| 3| 18| 7| 4|[37.0,3.0,18.0,7....|| 0| 22| 2| 17| 6| 3|[22.0,2.0,17.0,6....|| 0| 32| 2| 17| 5| 5|[32.0,2.0,17.0,5....|| 0| 57| 5| 18| 6| 5|[57.0,5.0,18.0,6....|| 0| 57| 2| 14| 4| 4|[57.0,2.0,14.0,4....|| 0| 22| 2| 17| 5| 4|[22.0,2.0,17.0,5....|| 0| 22| 4| 14| 4| 5|[22.0,4.0,14.0,4....|| 0| 27| 4| 18| 6| 4|[27.0,4.0,18.0,6....|| 0| 37| 2| 20| 7| 2|[37.0,2.0,20.0,7....|+-------+---+-------------+---------+----------+------+--------------------+only showing top 10 rows测试集：+-------+---+-------------+---------+----------+------+--------------------+|affairs|age|religiousness|education|occupation|rating| features|+-------+---+-------------+---------+----------+------+--------------------+| 0| 27| 4| 14| 6| 4|[27.0,4.0,14.0,6....|| 0| 22| 2| 12| 1| 3|[22.0,2.0,12.0,1....|| 0| 32| 4| 16| 1| 2|[32.0,4.0,16.0,1....|| 0| 27| 4| 14| 5| 4|[27.0,4.0,14.0,5....|| 0| 22| 3| 16| 5| 4|[22.0,3.0,16.0,5....|| 1| 27| 4| 16| 1| 2|[27.0,4.0,16.0,1....|+-------+---+-------------+---------+----------+------+--------------------+ 逻辑回归训练模型 1234567from pyspark.ml.classification import LogisticRegression# 创建逻辑回归训练器lr = LogisticRegression()# 训练模型model = lr.setLabelCol(&quot;affairs&quot;).setFeaturesCol(&quot;features&quot;).fit(trainDF)# 预测数据model.transform(testDF).show() 显示结果: 12345678910+-------+---+-------------+---------+----------+------+--------------------+--------------------+--------------------+----------+|affairs|age|religiousness|education|occupation|rating| features| rawPrediction| probability|prediction|+-------+---+-------------+---------+----------+------+--------------------+--------------------+--------------------+----------+| 0| 27| 4| 14| 6| 4|[27.0,4.0,14.0,6....|[0.39067871041193...|[0.59644607432863...| 0.0|| 0| 22| 2| 12| 1| 3|[22.0,2.0,12.0,1....|[-2.6754687573263...|[0.06443650129497...| 1.0|| 0| 32| 4| 16| 1| 2|[32.0,4.0,16.0,1....|[-4.5240336812732...|[0.01072883305878...| 1.0|| 0| 27| 4| 14| 5| 4|[27.0,4.0,14.0,5....|[0.16206512668426...|[0.54042783360658...| 0.0|| 0| 22| 3| 16| 5| 4|[22.0,3.0,16.0,5....|[1.69102697292197...|[0.84435916906682...| 0.0|| 1| 27| 4| 16| 1| 2|[27.0,4.0,16.0,1....|[-4.7969907272012...|[0.00818697014985...| 1.0|+-------+---+-------------+---------+----------+------+--------------------+--------------------+--------------------+----------+ 2. 基于LR的点击率预测模型训练 本小节主要根据广告点击样本数据集(raw_sample)、广告基本特征数据集(ad_feature)、用户基本信息数据集(user_profile)构建出了一个完整的样本数据集，并按日期划分为了训练集(前七天)和测试集(最后一天)，利用逻辑回归进行训练。 训练模型时，通过对类别特征数据进行处理，一定程度达到提高了模型的效果 123456789101112131415161718192021222324252627282930313233&#x27;&#x27;&#x27;从HDFS中加载样本数据信息&#x27;&#x27;&#x27;_raw_sample_df1 = spark.read.csv(&quot;hdfs://localhost:8020/csv/raw_sample.csv&quot;, header=True)# _raw_sample_df1.show() # 展示数据，默认前20条# 更改表结构，转换为对应的数据类型from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType, StringType # 更改df表结构：更改列类型和列名称_raw_sample_df2 = _raw_sample_df1.\\ withColumn(&quot;user&quot;, _raw_sample_df1.user.cast(IntegerType())).withColumnRenamed(&quot;user&quot;, &quot;userId&quot;).\\ withColumn(&quot;time_stamp&quot;, _raw_sample_df1.time_stamp.cast(LongType())).withColumnRenamed(&quot;time_stamp&quot;, &quot;timestamp&quot;).\\ withColumn(&quot;adgroup_id&quot;, _raw_sample_df1.adgroup_id.cast(IntegerType())).withColumnRenamed(&quot;adgroup_id&quot;, &quot;adgroupId&quot;).\\ withColumn(&quot;pid&quot;, _raw_sample_df1.pid.cast(StringType())).\\ withColumn(&quot;nonclk&quot;, _raw_sample_df1.nonclk.cast(IntegerType())).\\ withColumn(&quot;clk&quot;, _raw_sample_df1.clk.cast(IntegerType()))_raw_sample_df2.printSchema()_raw_sample_df2.show()# 样本数据pid特征处理from pyspark.ml.feature import OneHotEncoderfrom pyspark.ml.feature import StringIndexerfrom pyspark.ml import Pipelinestringindexer = StringIndexer(inputCol=&#x27;pid&#x27;, outputCol=&#x27;pid_feature&#x27;)encoder = OneHotEncoder(dropLast=False, inputCol=&#x27;pid_feature&#x27;, outputCol=&#x27;pid_value&#x27;)pipeline = Pipeline(stages=[stringindexer, encoder])pipeline_fit = pipeline.fit(_raw_sample_df2)raw_sample_df = pipeline_fit.transform(_raw_sample_df2)raw_sample_df.show()&#x27;&#x27;&#x27;pid和特征的对应关系430548_1007：0430549_1007：1&#x27;&#x27;&#x27; 显示结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061root |-- userId: integer (nullable = true) |-- timestamp: long (nullable = true) |-- adgroupId: integer (nullable = true) |-- pid: string (nullable = true) |-- nonclk: integer (nullable = true) |-- clk: integer (nullable = true)+------+----------+---------+-----------+------+---+|userId| timestamp|adgroupId| pid|nonclk|clk|+------+----------+---------+-----------+------+---+|581738|1494137644| 1|430548_1007| 1| 0||449818|1494638778| 3|430548_1007| 1| 0||914836|1494650879| 4|430548_1007| 1| 0||914836|1494651029| 5|430548_1007| 1| 0||399907|1494302958| 8|430548_1007| 1| 0||628137|1494524935| 9|430548_1007| 1| 0||298139|1494462593| 9|430539_1007| 1| 0||775475|1494561036| 9|430548_1007| 1| 0||555266|1494307136| 11|430539_1007| 1| 0||117840|1494036743| 11|430548_1007| 1| 0||739815|1494115387| 11|430539_1007| 1| 0||623911|1494625301| 11|430548_1007| 1| 0||623911|1494451608| 11|430548_1007| 1| 0||421590|1494034144| 11|430548_1007| 1| 0||976358|1494156949| 13|430548_1007| 1| 0||286630|1494218579| 13|430539_1007| 1| 0||286630|1494289247| 13|430539_1007| 1| 0||771431|1494153867| 13|430548_1007| 1| 0||707120|1494220810| 13|430548_1007| 1| 0||530454|1494293746| 13|430548_1007| 1| 0|+------+----------+---------+-----------+------+---+only showing top 20 rows+------+----------+---------+-----------+------+---+-----------+-------------+|userId| timestamp|adgroupId| pid|nonclk|clk|pid_feature| pid_value|+------+----------+---------+-----------+------+---+-----------+-------------+|581738|1494137644| 1|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||449818|1494638778| 3|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||914836|1494650879| 4|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||914836|1494651029| 5|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||399907|1494302958| 8|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||628137|1494524935| 9|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||298139|1494462593| 9|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||775475|1494561036| 9|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||555266|1494307136| 11|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||117840|1494036743| 11|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||739815|1494115387| 11|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||623911|1494625301| 11|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||623911|1494451608| 11|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||421590|1494034144| 11|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||976358|1494156949| 13|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||286630|1494218579| 13|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||286630|1494289247| 13|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||771431|1494153867| 13|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||707120|1494220810| 13|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||530454|1494293746| 13|430548_1007| 1| 0| 0.0|(2,[0],[1.0])|+------+----------+---------+-----------+------+---+-----------+-------------+only showing top 20 rows&#x27;pid和特征的对应关系\\n430548_1007：0\\n430549_1007：1\\n&#x27; 从HDFS中加载广告基本信息数据 123456789101112131415161718_ad_feature_df = spark.read.csv(&quot;hdfs://localhost:9000/datasets/ad_feature.csv&quot;, header=True)# 更改表结构，转换为对应的数据类型from pyspark.sql.types import StructType, StructField, IntegerType, FloatType# 替换掉NULL字符串_ad_feature_df = _ad_feature_df.replace(&quot;NULL&quot;, &quot;-1&quot;) # 更改df表结构：更改列类型和列名称ad_feature_df = _ad_feature_df.\\ withColumn(&quot;adgroup_id&quot;, _ad_feature_df.adgroup_id.cast(IntegerType())).withColumnRenamed(&quot;adgroup_id&quot;, &quot;adgroupId&quot;).\\ withColumn(&quot;cate_id&quot;, _ad_feature_df.cate_id.cast(IntegerType())).withColumnRenamed(&quot;cate_id&quot;, &quot;cateId&quot;).\\ withColumn(&quot;campaign_id&quot;, _ad_feature_df.campaign_id.cast(IntegerType())).withColumnRenamed(&quot;campaign_id&quot;, &quot;campaignId&quot;).\\ withColumn(&quot;customer&quot;, _ad_feature_df.customer.cast(IntegerType())).withColumnRenamed(&quot;customer&quot;, &quot;customerId&quot;).\\ withColumn(&quot;brand&quot;, _ad_feature_df.brand.cast(IntegerType())).withColumnRenamed(&quot;brand&quot;, &quot;brandId&quot;).\\ withColumn(&quot;price&quot;, _ad_feature_df.price.cast(FloatType()))ad_feature_df.printSchema()ad_feature_df.show() 显示结果: 123456789101112131415161718192021222324252627282930313233root |-- adgroupId: integer (nullable = true) |-- cateId: integer (nullable = true) |-- campaignId: integer (nullable = true) |-- customerId: integer (nullable = true) |-- brandId: integer (nullable = true) |-- price: float (nullable = true)+---------+------+----------+----------+-------+-----+|adgroupId|cateId|campaignId|customerId|brandId|price|+---------+------+----------+----------+-------+-----+| 63133| 6406| 83237| 1| 95471|170.0|| 313401| 6406| 83237| 1| 87331|199.0|| 248909| 392| 83237| 1| 32233| 38.0|| 208458| 392| 83237| 1| 174374|139.0|| 110847| 7211| 135256| 2| 145952|32.99|| 607788| 6261| 387991| 6| 207800|199.0|| 375706| 4520| 387991| 6| -1| 99.0|| 11115| 7213| 139747| 9| 186847| 33.0|| 24484| 7207| 139744| 9| 186847| 19.0|| 28589| 5953| 395195| 13| -1|428.0|| 23236| 5953| 395195| 13| -1|368.0|| 300556| 5953| 395195| 13| -1|639.0|| 92560| 5953| 395195| 13| -1|368.0|| 590965| 4284| 28145| 14| 454237|249.0|| 529913| 4284| 70206| 14| -1|249.0|| 546930| 4284| 28145| 14| -1|249.0|| 639794| 6261| 70206| 14| 37004| 89.9|| 335413| 4284| 28145| 14| -1|249.0|| 794890| 4284| 70206| 14| 454237|249.0|| 684020| 6261| 70206| 14| 37004| 99.0|+---------+------+----------+----------+-------+-----+only showing top 20 rows 从HDFS加载用户基本信息数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType# 构建表结构schema对象schema = StructType([ StructField(&quot;userId&quot;, IntegerType()), StructField(&quot;cms_segid&quot;, IntegerType()), StructField(&quot;cms_group_id&quot;, IntegerType()), StructField(&quot;final_gender_code&quot;, IntegerType()), StructField(&quot;age_level&quot;, IntegerType()), StructField(&quot;pvalue_level&quot;, IntegerType()), StructField(&quot;shopping_level&quot;, IntegerType()), StructField(&quot;occupation&quot;, IntegerType()), StructField(&quot;new_user_class_level&quot;, IntegerType())])# 利用schema从hdfs加载_user_profile_df1 = spark.read.csv(&quot;hdfs://localhost:9000/datasets/user_profile.csv&quot;, header=True, schema=schema)# user_profile_df.printSchema()# user_profile_df.show()&#x27;&#x27;&#x27;对缺失数据进行特征热编码&#x27;&#x27;&#x27;from pyspark.ml.feature import OneHotEncoderfrom pyspark.ml.feature import StringIndexerfrom pyspark.ml import Pipeline# 使用热编码转换pvalue_level的一维数据为多维，增加n-1个虚拟变量，n为pvalue_level的取值范围# 需要先将缺失值全部替换为数值，便于处理，否则会抛出异常from pyspark.sql.types import StringType_user_profile_df2 = _user_profile_df1.na.fill(-1)# _user_profile_df2.show()# 热编码时，必须先将待处理字段转为字符串类型才可处理_user_profile_df3 = _user_profile_df2.withColumn(&quot;pvalue_level&quot;, _user_profile_df2.pvalue_level.cast(StringType()))\\ .withColumn(&quot;new_user_class_level&quot;, _user_profile_df2.new_user_class_level.cast(StringType()))# _user_profile_df3.printSchema()# 对pvalue_level进行热编码，求值# 运行过程是先将pvalue_level转换为一列新的特征数据，然后对该特征数据求出的热编码值，存在了新的一列数据中，类型为一个稀疏矩阵stringindexer = StringIndexer(inputCol=&#x27;pvalue_level&#x27;, outputCol=&#x27;pl_onehot_feature&#x27;)encoder = OneHotEncoder(dropLast=False, inputCol=&#x27;pl_onehot_feature&#x27;, outputCol=&#x27;pl_onehot_value&#x27;)pipeline = Pipeline(stages=[stringindexer, encoder])pipeline_fit = pipeline.fit(_user_profile_df3)_user_profile_df4 = pipeline_fit.transform(_user_profile_df3)# pl_onehot_value列的值为稀疏矩阵，存储热编码的结果# _user_profile_df4.printSchema()# _user_profile_df4.show()# 使用热编码转换new_user_class_level的一维数据为多维stringindexer = StringIndexer(inputCol=&#x27;new_user_class_level&#x27;, outputCol=&#x27;nucl_onehot_feature&#x27;)encoder = OneHotEncoder(dropLast=False, inputCol=&#x27;nucl_onehot_feature&#x27;, outputCol=&#x27;nucl_onehot_value&#x27;)pipeline = Pipeline(stages=[stringindexer, encoder])pipeline_fit = pipeline.fit(_user_profile_df4)user_profile_df = pipeline_fit.transform(_user_profile_df4)user_profile_df.show() 显示结果: 1234567891011121314151617181920212223242526+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+-------------------+-----------------+|userId|cms_segid|cms_group_id|final_gender_code|age_level|pvalue_level|shopping_level|occupation|new_user_class_level|pl_onehot_feature|pl_onehot_value|nucl_onehot_feature|nucl_onehot_value|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+-------------------+-----------------+| 234| 0| 5| 2| 5| -1| 3| 0| 3| 0.0| (4,[0],[1.0])| 2.0| (5,[2],[1.0])|| 523| 5| 2| 2| 2| 1| 3| 1| 2| 2.0| (4,[2],[1.0])| 1.0| (5,[1],[1.0])|| 612| 0| 8| 1| 2| 2| 3| 0| -1| 1.0| (4,[1],[1.0])| 0.0| (5,[0],[1.0])|| 1670| 0| 4| 2| 4| -1| 1| 0| -1| 0.0| (4,[0],[1.0])| 0.0| (5,[0],[1.0])|| 2545| 0| 10| 1| 4| -1| 3| 0| -1| 0.0| (4,[0],[1.0])| 0.0| (5,[0],[1.0])|| 3644| 49| 6| 2| 6| 2| 3| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|| 5777| 44| 5| 2| 5| 2| 3| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|| 6211| 0| 9| 1| 3| -1| 3| 0| 2| 0.0| (4,[0],[1.0])| 1.0| (5,[1],[1.0])|| 6355| 2| 1| 2| 1| 1| 3| 0| 4| 2.0| (4,[2],[1.0])| 3.0| (5,[3],[1.0])|| 6823| 43| 5| 2| 5| 2| 3| 0| 1| 1.0| (4,[1],[1.0])| 4.0| (5,[4],[1.0])|| 6972| 5| 2| 2| 2| 2| 3| 1| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|| 9293| 0| 5| 2| 5| -1| 3| 0| 4| 0.0| (4,[0],[1.0])| 3.0| (5,[3],[1.0])|| 9510| 55| 8| 1| 2| 2| 2| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|| 10122| 33| 4| 2| 4| 2| 3| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|| 10549| 0| 4| 2| 4| 2| 3| 0| -1| 1.0| (4,[1],[1.0])| 0.0| (5,[0],[1.0])|| 10812| 0| 4| 2| 4| -1| 2| 0| -1| 0.0| (4,[0],[1.0])| 0.0| (5,[0],[1.0])|| 10912| 0| 4| 2| 4| 2| 3| 0| -1| 1.0| (4,[1],[1.0])| 0.0| (5,[0],[1.0])|| 10996| 0| 5| 2| 5| -1| 3| 0| 4| 0.0| (4,[0],[1.0])| 3.0| (5,[3],[1.0])|| 11256| 8| 2| 2| 2| 1| 3| 0| 3| 2.0| (4,[2],[1.0])| 2.0| (5,[2],[1.0])|| 11310| 31| 4| 2| 4| 1| 3| 0| 4| 2.0| (4,[2],[1.0])| 3.0| (5,[3],[1.0])|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+-------------------+-----------------+only showing top 20 rows 热编码中：”pvalue_level”特征对应关系: 12345678+------------+----------------------+|pvalue_level|pl_onehot_feature |+------------+----------------------+| -1| 0.0|| 3| 3.0|| 1| 2.0|| 2| 1.0|+------------+----------------------+ “new_user_class_level”的特征对应关系 123456789+--------------------+------------------------+|new_user_class_level|nucl_onehot_feature |+--------------------+------------------------+| -1| 0.0|| 3| 2.0|| 1| 4.0|| 4| 3.0|| 2| 1.0|+--------------------+------------------------+ 12user_profile_df.groupBy(&quot;pvalue_level&quot;).min(&quot;pl_onehot_feature&quot;).show()user_profile_df.groupBy(&quot;new_user_class_level&quot;).min(&quot;nucl_onehot_feature&quot;).show() 显示结果: 12345678910111213141516171819+------------+----------------------+|pvalue_level|min(pl_onehot_feature)|+------------+----------------------+| -1| 0.0|| 3| 3.0|| 1| 2.0|| 2| 1.0|+------------+----------------------++--------------------+------------------------+|new_user_class_level|min(nucl_onehot_feature)|+--------------------+------------------------+| -1| 0.0|| 3| 2.0|| 1| 4.0|| 4| 3.0|| 2| 1.0|+--------------------+------------------------+ Dataframe数据合并：pyspark.sql.DataFrame.join 1234567891011# raw_sample_df 和 ad_feature_df 合并条件condition = [raw_sample_df.adgroupId==ad_feature_df.adgroupId]_ = raw_sample_df.join(ad_feature_df, condition, &#x27;outer&#x27;)# _和user_profile_df合并条件condition2 = [_.userId==user_profile_df.userId]datasets = _.join(user_profile_df, condition2, &quot;outer&quot;)# 查看datasets的结构datasets.printSchema()# 查看datasets条目数print(datasets.count()) 显示结果: 123456789101112131415161718192021222324252627282930root |-- userId: integer (nullable = true) |-- timestamp: long (nullable = true) |-- adgroupId: integer (nullable = true) |-- pid: string (nullable = true) |-- nonclk: integer (nullable = true) |-- clk: integer (nullable = true) |-- pid_feature: double (nullable = true) |-- pid_value: vector (nullable = true) |-- adgroupId: integer (nullable = true) |-- cateId: integer (nullable = true) |-- campaignId: integer (nullable = true) |-- customerId: integer (nullable = true) |-- brandId: integer (nullable = true) |-- price: float (nullable = true) |-- userId: integer (nullable = true) |-- cms_segid: integer (nullable = true) |-- cms_group_id: integer (nullable = true) |-- final_gender_code: integer (nullable = true) |-- age_level: integer (nullable = true) |-- pvalue_level: string (nullable = true) |-- shopping_level: integer (nullable = true) |-- occupation: integer (nullable = true) |-- new_user_class_level: string (nullable = true) |-- pl_onehot_feature: double (nullable = true) |-- pl_onehot_value: vector (nullable = true) |-- nucl_onehot_feature: double (nullable = true) |-- nucl_onehot_value: vector (nullable = true)26557961 训练CTRModel_Normal：直接将对应的特征的特征值组合成对应的特征向量进行训练 123456789101112131415161718192021222324# 剔除冗余、不需要的字段useful_cols = [ # # 时间字段，划分训练集和测试集 &quot;timestamp&quot;, # label目标值字段 &quot;clk&quot;, # 特征值字段 &quot;pid_value&quot;, # 资源位的特征向量 &quot;price&quot;, # 广告价格 &quot;cms_segid&quot;, # 用户微群ID &quot;cms_group_id&quot;, # 用户组ID &quot;final_gender_code&quot;, # 用户性别特征，[1,2] &quot;age_level&quot;, # 年龄等级，1- &quot;shopping_level&quot;, &quot;occupation&quot;, &quot;pl_onehot_value&quot;, &quot;nucl_onehot_value&quot;]# 筛选指定字段数据，构建新的数据集datasets_1 = datasets.select(*useful_cols)# 由于前面使用的是outer方式合并的数据，产生了部分空值数据，这里必须先剔除掉datasets_1 = datasets_1.dropna()print(&quot;剔除空值数据后，还剩：&quot;, datasets_1.count()) 显示结果: 12剔除空值数据后，还剩： 25029435 根据特征字段计算出特征向量，并划分出训练数据集和测试数据集 12345678910from pyspark.ml.feature import VectorAssembler# 根据特征字段计算特征向量（[2::]，前两个不要，后面的作为特征，具体看上面的结构）datasets_1 = VectorAssembler().setInputCols(useful_cols[2:]).setOutputCol(&quot;features&quot;).transform(datasets_1)# 训练数据集: 约7天的数据train_datasets_1 = datasets_1.filter(datasets_1.timestamp&lt;=(1494691186-24*60*60))# 测试数据集：约1天的数据量test_datasets_1 = datasets_1.filter(datasets_1.timestamp&gt;(1494691186-24*60*60))# 所有的特征的特征向量已经汇总到在features字段中train_datasets_1.show(5)test_datasets_1.show(5) 显示结果: 12345678910111213141516171819202122+----------+---+-------------+------+---------+------------+-----------------+---------+--------------+----------+---------------+-----------------+--------------------+| timestamp|clk| pid_value| price|cms_segid|cms_group_id|final_gender_code|age_level|shopping_level|occupation|pl_onehot_value|nucl_onehot_value| features|+----------+---+-------------+------+---------+------------+-----------------+---------+--------------+----------+---------------+-----------------+--------------------+|1494261938| 0|(2,[1],[1.0])| 108.0| 0| 11| 1| 5| 3| 0| (4,[0],[1.0])| (5,[1],[1.0])|(18,[1,2,4,5,6,7,...||1494261938| 0|(2,[1],[1.0])|1880.0| 0| 11| 1| 5| 3| 0| (4,[0],[1.0])| (5,[1],[1.0])|(18,[1,2,4,5,6,7,...||1494553913| 0|(2,[1],[1.0])|2360.0| 19| 3| 2| 3| 3| 0| (4,[1],[1.0])| (5,[1],[1.0])|(18,[1,2,3,4,5,6,...||1494553913| 0|(2,[1],[1.0])|2200.0| 19| 3| 2| 3| 3| 0| (4,[1],[1.0])| (5,[1],[1.0])|(18,[1,2,3,4,5,6,...||1494436784| 0|(2,[1],[1.0])|5649.0| 19| 3| 2| 3| 3| 0| (4,[1],[1.0])| (5,[1],[1.0])|(18,[1,2,3,4,5,6,...|+----------+---+-------------+------+---------+------------+-----------------+---------+--------------+----------+---------------+-----------------+--------------------+only showing top 5 rows+----------+---+-------------+-----+---------+------------+-----------------+---------+--------------+----------+---------------+-----------------+--------------------+| timestamp|clk| pid_value|price|cms_segid|cms_group_id|final_gender_code|age_level|shopping_level|occupation|pl_onehot_value|nucl_onehot_value| features|+----------+---+-------------+-----+---------+------------+-----------------+---------+--------------+----------+---------------+-----------------+--------------------+|1494677292| 0|(2,[1],[1.0])|176.0| 19| 3| 2| 3| 3| 0| (4,[1],[1.0])| (5,[1],[1.0])|(18,[1,2,3,4,5,6,...||1494677292| 0|(2,[1],[1.0])|698.0| 19| 3| 2| 3| 3| 0| (4,[1],[1.0])| (5,[1],[1.0])|(18,[1,2,3,4,5,6,...||1494677292| 0|(2,[1],[1.0])|697.0| 19| 3| 2| 3| 3| 0| (4,[1],[1.0])| (5,[1],[1.0])|(18,[1,2,3,4,5,6,...||1494684007| 0|(2,[1],[1.0])|247.0| 18| 3| 2| 3| 3| 0| (4,[1],[1.0])| (5,[4],[1.0])|(18,[1,2,3,4,5,6,...||1494684007| 0|(2,[1],[1.0])|109.0| 18| 3| 2| 3| 3| 0| (4,[1],[1.0])| (5,[4],[1.0])|(18,[1,2,3,4,5,6,...|+----------+---+-------------+-----+---------+------------+-----------------+---------+--------------+----------+---------------+-----------------+--------------------+only showing top 5 rows 创建逻辑回归训练器，并训练模型：LogisticRegression、 LogisticRegressionModel 123456789101112131415from pyspark.ml.classification import LogisticRegressionlr = LogisticRegression()# 设置目标字段、特征值字段并训练model = lr.setLabelCol(&quot;clk&quot;).setFeaturesCol(&quot;features&quot;).fit(train_datasets_1)# 对模型进行存储model.save(&quot;hdfs://localhost:9000/models/CTRModel_Normal.obj&quot;)# 载入训练好的模型from pyspark.ml.classification import LogisticRegressionModelmodel = LogisticRegressionModel.load(&quot;hdfs://localhost:9000/models/CTRModel_Normal.obj&quot;)# 根据测试数据进行预测（默认按照预测概率的50%进行划分的。所以预测都是0）result_1 = model.transform(test_datasets_1)# 按probability升序排列数据，probability表示预测结果的概率# 如果预测值是0，其概率是0.9248，那么反之可推出1的可能性就是1-0.9248=0.0752，即点击概率约为7.52%# 因为前面提到广告的点击率一般都比较低，所以预测值通常都是0，因此通常需要反减得出点击的概率result_1.select(&quot;clk&quot;, &quot;price&quot;, &quot;probability&quot;, &quot;prediction&quot;).sort(&quot;probability&quot;).show(100) 显示结果: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105+---+-----------+--------------------+----------+|clk| price| probability|prediction|+---+-----------+--------------------+----------+| 0| 1.0E8|[0.86822033939259...| 0.0|| 0| 1.0E8|[0.88410457194969...| 0.0|| 0| 1.0E8|[0.89175497837562...| 0.0|| 1|5.5555556E7|[0.92481456486873...| 0.0|| 0| 1.5E7|[0.93741450446939...| 0.0|| 0| 1.5E7|[0.93757135079959...| 0.0|| 0| 1.5E7|[0.93834723093801...| 0.0|| 0| 1099.0|[0.93972095713786...| 0.0|| 0| 338.0|[0.93972134993018...| 0.0|| 0| 311.0|[0.93972136386626...| 0.0|| 0| 300.0|[0.93972136954393...| 0.0|| 0| 278.0|[0.93972138089925...| 0.0|| 0| 188.0|[0.93972142735283...| 0.0|| 0| 176.0|[0.93972143354663...| 0.0|| 0| 168.0|[0.93972143767584...| 0.0|| 0| 158.0|[0.93972144283734...| 0.0|| 1| 138.0|[0.93972145316035...| 0.0|| 0| 125.0|[0.93972145987031...| 0.0|| 0| 119.0|[0.93972146296721...| 0.0|| 0| 78.0|[0.93972148412937...| 0.0|| 0| 59.98|[0.93972149343040...| 0.0|| 0| 58.0|[0.93972149445238...| 0.0|| 0| 56.0|[0.93972149548468...| 0.0|| 0| 38.0|[0.93972150477538...| 0.0|| 1| 35.0|[0.93972150632383...| 0.0|| 0| 33.0|[0.93972150735613...| 0.0|| 0| 30.0|[0.93972150890458...| 0.0|| 0| 27.6|[0.93972151014334...| 0.0|| 0| 18.0|[0.93972151509838...| 0.0|| 0| 30.0|[0.93980311191464...| 0.0|| 0| 28.0|[0.93980311294563...| 0.0|| 0| 25.0|[0.93980311449212...| 0.0|| 0| 688.0|[0.93999362023323...| 0.0|| 0| 339.0|[0.93999379960808...| 0.0|| 0| 335.0|[0.93999380166395...| 0.0|| 0| 220.0|[0.93999386077017...| 0.0|| 0| 176.0|[0.93999388338470...| 0.0|| 0| 158.0|[0.93999389263610...| 0.0|| 0| 158.0|[0.93999389263610...| 0.0|| 1| 149.0|[0.93999389726180...| 0.0|| 0| 122.5|[0.93999391088191...| 0.0|| 0| 99.0|[0.93999392296012...| 0.0|| 0| 88.0|[0.93999392861375...| 0.0|| 0| 79.0|[0.93999393323945...| 0.0|| 0| 75.0|[0.93999393529532...| 0.0|| 0| 68.0|[0.93999393889308...| 0.0|| 0| 68.0|[0.93999393889308...| 0.0|| 0| 59.9|[0.93999394305620...| 0.0|| 0| 44.98|[0.93999395072458...| 0.0|| 0| 35.5|[0.93999395559698...| 0.0|| 0| 33.0|[0.93999395688189...| 0.0|| 0| 32.8|[0.93999395698469...| 0.0|| 0| 30.0|[0.93999395842379...| 0.0|| 0| 28.0|[0.93999395945172...| 0.0|| 0| 19.9|[0.93999396361485...| 0.0|| 0| 19.8|[0.93999396366625...| 0.0|| 0| 19.8|[0.93999396366625...| 0.0|| 0| 12.0|[0.93999396767518...| 0.0|| 0| 6.7|[0.93999397039920...| 0.0|| 0| 568.0|[0.94000369247841...| 0.0|| 0| 398.0|[0.94000377983931...| 0.0|| 0| 158.0|[0.94000390317214...| 0.0|| 0| 5718.0|[0.94001886593718...| 0.0|| 0| 5718.0|[0.94001886593718...| 0.0|| 1| 5608.0|[0.94001892245145...| 0.0|| 0| 4120.0|[0.94001968693052...| 0.0|| 0| 1027.5|[0.94002127571285...| 0.0|| 0| 1027.5|[0.94002127571285...| 0.0|| 0| 989.0|[0.94002129549211...| 0.0|| 0| 672.0|[0.94002145834965...| 0.0|| 0| 660.0|[0.94002146451460...| 0.0|| 0| 598.0|[0.94002149636681...| 0.0|| 0| 598.0|[0.94002149636681...| 0.0|| 0| 563.0|[0.94002151434789...| 0.0|| 0| 509.0|[0.94002154209012...| 0.0|| 0| 509.0|[0.94002154209012...| 0.0|| 0| 500.0|[0.94002154671382...| 0.0|| 0| 498.0|[0.94002154774131...| 0.0|| 0| 440.0|[0.94002157753851...| 0.0|| 0| 430.0|[0.94002158267595...| 0.0|| 0| 388.0|[0.94002160425322...| 0.0|| 0| 369.0|[0.94002161401436...| 0.0|| 0| 368.0|[0.94002161452811...| 0.0|| 0| 368.0|[0.94002161452811...| 0.0|| 0| 368.0|[0.94002161452811...| 0.0|| 0| 368.0|[0.94002161452811...| 0.0|| 0| 366.0|[0.94002161555560...| 0.0|| 0| 366.0|[0.94002161555560...| 0.0|| 0| 348.0|[0.94002162480299...| 0.0|| 0| 299.0|[0.94002164997645...| 0.0|| 0| 299.0|[0.94002164997645...| 0.0|| 0| 299.0|[0.94002164997645...| 0.0|| 0| 298.0|[0.94002165049020...| 0.0|| 0| 297.0|[0.94002165100394...| 0.0|| 0| 278.0|[0.94002166076508...| 0.0|| 1| 275.0|[0.94002166230631...| 0.0|| 0| 275.0|[0.94002166230631...| 0.0|| 0| 273.0|[0.94002166333380...| 0.0|| 0| 258.0|[0.94002167103995...| 0.0|| 0| 256.0|[0.94002167206744...| 0.0|+---+-----------+--------------------+----------+only showing top 100 rows 查看样本中点击的被实际点击的条目的预测情况 1result_1.filter(result_1.clk==1).select(&quot;clk&quot;, &quot;price&quot;, &quot;probability&quot;, &quot;prediction&quot;).sort(&quot;probability&quot;).show(100) 显示结果: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106+---+-----------+--------------------+----------+|clk| price| probability|prediction|+---+-----------+--------------------+----------+| 1|5.5555556E7|[0.92481456486873...| 0.0|| 1| 138.0|[0.93972145316035...| 0.0|| 1| 35.0|[0.93972150632383...| 0.0|| 1| 149.0|[0.93999389726180...| 0.0|| 1| 5608.0|[0.94001892245145...| 0.0|| 1| 275.0|[0.94002166230631...| 0.0|| 1| 35.0|[0.94002178560473...| 0.0|| 1| 49.0|[0.94004219516957...| 0.0|| 1| 915.0|[0.94021082858784...| 0.0|| 1| 598.0|[0.94021099096349...| 0.0|| 1| 568.0|[0.94021100633025...| 0.0|| 1| 398.0|[0.94021109340848...| 0.0|| 1| 368.0|[0.94021110877521...| 0.0|| 1| 299.0|[0.94021114411869...| 0.0|| 1| 278.0|[0.94021115487539...| 0.0|| 1| 259.0|[0.94021116460765...| 0.0|| 1| 258.0|[0.94021116511987...| 0.0|| 1| 258.0|[0.94021116511987...| 0.0|| 1| 258.0|[0.94021116511987...| 0.0|| 1| 195.0|[0.94021119738998...| 0.0|| 1| 188.0|[0.94021120097554...| 0.0|| 1| 178.0|[0.94021120609778...| 0.0|| 1| 159.0|[0.94021121583003...| 0.0|| 1| 149.0|[0.94021122095226...| 0.0|| 1| 138.0|[0.94021122658672...| 0.0|| 1| 58.0|[0.94021126756458...| 0.0|| 1| 49.0|[0.94021127217459...| 0.0|| 1| 35.0|[0.94021127934572...| 0.0|| 1| 25.0|[0.94021128446795...| 0.0|| 1| 2890.0|[0.94028789742257...| 0.0|| 1| 220.0|[0.94028926340218...| 0.0|| 1| 188.0|[0.94031410659516...| 0.0|| 1| 68.0|[0.94031416796289...| 0.0|| 1| 58.0|[0.94031417307687...| 0.0|| 1| 198.0|[0.94035413548387...| 0.0|| 1| 208.0|[0.94039204931181...| 0.0|| 1| 8888.0|[0.94045237642030...| 0.0|| 1| 519.0|[0.94045664687995...| 0.0|| 1| 478.0|[0.94045666780037...| 0.0|| 1| 349.0|[0.94045673362308...| 0.0|| 1| 348.0|[0.94045673413334...| 0.0|| 1| 316.0|[0.94045675046144...| 0.0|| 1| 298.0|[0.94045675964600...| 0.0|| 1| 298.0|[0.94045675964600...| 0.0|| 1| 199.0|[0.94045681016104...| 0.0|| 1| 199.0|[0.94045681016104...| 0.0|| 1| 198.0|[0.94045681067129...| 0.0|| 1| 187.1|[0.94045681623305...| 0.0|| 1| 176.0|[0.94045682189685...| 0.0|| 1| 168.0|[0.94045682597887...| 0.0|| 1| 160.0|[0.94045683006090...| 0.0|| 1| 158.0|[0.94045683108140...| 0.0|| 1| 158.0|[0.94045683108140...| 0.0|| 1| 135.0|[0.94045684281721...| 0.0|| 1| 129.0|[0.94045684587872...| 0.0|| 1| 127.0|[0.94045684689923...| 0.0|| 1| 125.0|[0.94045684791973...| 0.0|| 1| 124.0|[0.94045684842999...| 0.0|| 1| 118.0|[0.94045685149150...| 0.0|| 1| 109.0|[0.94045685608377...| 0.0|| 1| 108.0|[0.94045685659402...| 0.0|| 1| 99.0|[0.94045686118630...| 0.0|| 1| 98.0|[0.94045686169655...| 0.0|| 1| 79.8|[0.94045687098314...| 0.0|| 1| 79.0|[0.94045687139134...| 0.0|| 1| 77.0|[0.94045687241185...| 0.0|| 1| 72.5|[0.94045687470798...| 0.0|| 1| 69.0|[0.94045687649386...| 0.0|| 1| 68.0|[0.94045687700412...| 0.0|| 1| 60.0|[0.94045688108613...| 0.0|| 1| 43.98|[0.94045688926037...| 0.0|| 1| 40.0|[0.94045689129118...| 0.0|| 1| 39.9|[0.94045689134220...| 0.0|| 1| 39.6|[0.94045689149528...| 0.0|| 1| 32.0|[0.94045689537319...| 0.0|| 1| 31.0|[0.94045689588345...| 0.0|| 1| 25.98|[0.94045689844491...| 0.0|| 1| 23.0|[0.94045689996546...| 0.0|| 1| 19.0|[0.94045690200647...| 0.0|| 1| 16.9|[0.94045690307800...| 0.0|| 1| 10.0|[0.94045690659874...| 0.0|| 1| 3.5|[0.94045690991538...| 0.0|| 1| 3.5|[0.94045690991538...| 0.0|| 1| 0.4|[0.94045691149716...| 0.0|| 1| 3960.0|[0.94055740378069...| 0.0|| 1| 3088.0|[0.94055784801535...| 0.0|| 1| 1689.0|[0.94055856072019...| 0.0|| 1| 998.0|[0.94055891273943...| 0.0|| 1| 888.0|[0.94055896877705...| 0.0|| 1| 788.0|[0.94055901972029...| 0.0|| 1| 737.0|[0.94055904570133...| 0.0|| 1| 629.0|[0.94055910071996...| 0.0|| 1| 599.0|[0.94055911600291...| 0.0|| 1| 599.0|[0.94055911600291...| 0.0|| 1| 599.0|[0.94055911600291...| 0.0|| 1| 499.0|[0.94055916694603...| 0.0|| 1| 468.0|[0.94055918273839...| 0.0|| 1| 459.0|[0.94055918732327...| 0.0|| 1| 399.0|[0.94055921788912...| 0.0|| 1| 399.0|[0.94055921788912...| 0.0|+---+-----------+--------------------+----------+only showing top 100 rows 训练CTRModel_AllOneHot “pid_value”, 类别型特征，已被转换为多维特征==&gt; 2维 “price”, 统计型特征 ===&gt; 1维 “cms_segid”, 类别型特征，约97个分类 ===&gt; 1维 “cms_group_id”, 类别型特征，约13个分类 ==&gt; 1维 “final_gender_code”, 类别型特征，2个分类 ==&gt; 1维 “age_level”, 类别型特征，7个分类 ==&gt; 1维 “shopping_level”, 类别型特征，3个分类 ==&gt; 1维 “occupation”, 类别型特征，2个分类 ==&gt; 1维 “pl_onehot_value”, 类别型特征，已被转换为多维特征 ==&gt; 4维 “nucl_onehot_value” 类别型特征，已被转换为多维特征 ==&gt; 5维 类别性特征都可以考虑进行热独编码，将单一变量变为多变量，相当于增加了相关特征的数量 “cms_segid”, 类别型特征，约97个分类 ===&gt; 97维 舍弃 “cms_group_id”, 类别型特征，约13个分类 ==&gt; 13维 “final_gender_code”, 类别型特征，2个分类 ==&gt; 2维 “age_level”, 类别型特征，7个分类 ==&gt;7维 “shopping_level”, 类别型特征，3个分类 ==&gt; 3维 “occupation”, 类别型特征，2个分类 ==&gt; 2维 但由于cms_segid分类过多，这里考虑舍弃，避免数据过于稀疏 1datasets_1.first() 显示结果: 123datasets_1.first()datasets_1.first()Row(timestamp=1494261938, clk=0, pid_value=SparseVector(2, &#123;1: 1.0&#125;), price=1880.0, cms_segid=0, cms_group_id=11, final_gender_code=1, age_level=5, shopping_level=3, occupation=0, pl_onehot_value=SparseVector(4, &#123;0: 1.0&#125;), nucl_onehot_value=SparseVector(5, &#123;1: 1.0&#125;), features=SparseVector(18, &#123;1: 1.0, 2: 1880.0, 4: 11.0, 5: 1.0, 6: 5.0, 7: 3.0, 9: 1.0, 14: 1.0&#125;)) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 先将下列五列数据转为字符串类型，以便于进行热独编码# - &quot;cms_group_id&quot;, 类别型特征，约13个分类 ==&gt; 13# - &quot;final_gender_code&quot;, 类别型特征，2个分类 ==&gt; 2# - &quot;age_level&quot;, 类别型特征，7个分类 ==&gt;7# - &quot;shopping_level&quot;, 类别型特征，3个分类 ==&gt; 3# - &quot;occupation&quot;, 类别型特征，2个分类 ==&gt; 2datasets_2 = datasets.withColumn(&quot;cms_group_id&quot;, datasets.cms_group_id.cast(StringType()))\\ .withColumn(&quot;final_gender_code&quot;, datasets.final_gender_code.cast(StringType()))\\ .withColumn(&quot;age_level&quot;, datasets.age_level.cast(StringType()))\\ .withColumn(&quot;shopping_level&quot;, datasets.shopping_level.cast(StringType()))\\ .withColumn(&quot;occupation&quot;, datasets.occupation.cast(StringType()))useful_cols_2 = [ # 时间值，划分训练集和测试集 &quot;timestamp&quot;, # label目标值 &quot;clk&quot;, # 特征值 &quot;price&quot;, &quot;cms_group_id&quot;, &quot;final_gender_code&quot;, &quot;age_level&quot;, &quot;shopping_level&quot;, &quot;occupation&quot;, &quot;pid_value&quot;, &quot;pl_onehot_value&quot;, &quot;nucl_onehot_value&quot;]# 筛选指定字段数据datasets_2 = datasets_2.select(*useful_cols_2)# 由于前面使用的是outer方式合并的数据，产生了部分空值数据，这里必须先剔除掉datasets_2 = datasets_2.dropna()from pyspark.ml.feature import OneHotEncoderfrom pyspark.ml.feature import StringIndexerfrom pyspark.ml import Pipeline# 热编码处理函数封装def oneHotEncoder(col1, col2, col3, data): stringindexer = StringIndexer(inputCol=col1, outputCol=col2) encoder = OneHotEncoder(dropLast=False, inputCol=col2, outputCol=col3) pipeline = Pipeline(stages=[stringindexer, encoder]) pipeline_fit = pipeline.fit(data) return pipeline_fit.transform(data)# 对这五个字段进行热独编码# &quot;cms_group_id&quot;,# &quot;final_gender_code&quot;,# &quot;age_level&quot;,# &quot;shopping_level&quot;,# &quot;occupation&quot;,datasets_2 = oneHotEncoder(&quot;cms_group_id&quot;, &quot;cms_group_id_feature&quot;, &quot;cms_group_id_value&quot;, datasets_2)datasets_2 = oneHotEncoder(&quot;final_gender_code&quot;, &quot;final_gender_code_feature&quot;, &quot;final_gender_code_value&quot;, datasets_2)datasets_2 = oneHotEncoder(&quot;age_level&quot;, &quot;age_level_feature&quot;, &quot;age_level_value&quot;, datasets_2)datasets_2 = oneHotEncoder(&quot;shopping_level&quot;, &quot;shopping_level_feature&quot;, &quot;shopping_level_value&quot;, datasets_2)datasets_2 = oneHotEncoder(&quot;occupation&quot;, &quot;occupation_feature&quot;, &quot;occupation_value&quot;, datasets_2) “cms_group_id”特征对应关系： 123456789101112131415161718+------------+-------------------------+|cms_group_id|min(cms_group_id_feature)|+------------+-------------------------+| 7| 9.0|| 11| 6.0|| 3| 0.0|| 8| 8.0|| 0| 12.0|| 5| 3.0|| 6| 10.0|| 9| 5.0|| 1| 7.0|| 10| 4.0|| 4| 1.0|| 12| 11.0|| 2| 2.0|+------------+-------------------------+ “final_gender_code”特征对应关系： 1234567+-----------------+------------------------------+|final_gender_code|min(final_gender_code_feature)|+-----------------+------------------------------+| 1| 1.0|| 2| 0.0|+-----------------+------------------------------+ “age_level”特征对应关系： 123456789101112+---------+----------------------+|age_level|min(age_level_feature)|+---------+----------------------+| 3| 0.0|| 0| 6.0|| 5| 2.0|| 6| 5.0|| 1| 4.0|| 4| 1.0|| 2| 3.0|+---------+----------------------+ “shopping_level”特征对应关系： 1234567|shopping_level|min(shopping_level_feature)|+--------------+---------------------------+| 3| 0.0|| 1| 2.0|| 2| 1.0|+--------------+---------------------------+ “occupation”特征对应关系： 1234567+----------+-----------------------+|occupation|min(occupation_feature)|+----------+-----------------------+| 0| 0.0|| 1| 1.0|+----------+-----------------------+ 12345datasets_2.groupBy(&quot;cms_group_id&quot;).min(&quot;cms_group_id_feature&quot;).show()datasets_2.groupBy(&quot;final_gender_code&quot;).min(&quot;final_gender_code_feature&quot;).show()datasets_2.groupBy(&quot;age_level&quot;).min(&quot;age_level_feature&quot;).show()datasets_2.groupBy(&quot;shopping_level&quot;).min(&quot;shopping_level_feature&quot;).show()datasets_2.groupBy(&quot;occupation&quot;).min(&quot;occupation_feature&quot;).show() 显示结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152+------------+-------------------------+|cms_group_id|min(cms_group_id_feature)|+------------+-------------------------+| 7| 9.0|| 11| 6.0|| 3| 0.0|| 8| 8.0|| 0| 12.0|| 5| 3.0|| 6| 10.0|| 9| 5.0|| 1| 7.0|| 10| 4.0|| 4| 1.0|| 12| 11.0|| 2| 2.0|+------------+-------------------------++-----------------+------------------------------+|final_gender_code|min(final_gender_code_feature)|+-----------------+------------------------------+| 1| 1.0|| 2| 0.0|+-----------------+------------------------------++---------+----------------------+|age_level|min(age_level_feature)|+---------+----------------------+| 3| 0.0|| 0| 6.0|| 5| 2.0|| 6| 5.0|| 1| 4.0|| 4| 1.0|| 2| 3.0|+---------+----------------------++--------------+---------------------------+|shopping_level|min(shopping_level_feature)|+--------------+---------------------------+| 3| 0.0|| 1| 2.0|| 2| 1.0|+--------------+---------------------------++----------+-----------------------+|occupation|min(occupation_feature)|+----------+-----------------------+| 0| 0.0|| 1| 1.0|+----------+-----------------------+ 1234567891011121314151617181920# 由于热独编码后，特征字段不再是之前的字段，重新定义特征值字段feature_cols = [ # 特征值 &quot;price&quot;, &quot;cms_group_id_value&quot;, &quot;final_gender_code_value&quot;, &quot;age_level_value&quot;, &quot;shopping_level_value&quot;, &quot;occupation_value&quot;, &quot;pid_value&quot;, &quot;pl_onehot_value&quot;, &quot;nucl_onehot_value&quot;]# 根据特征字段计算出特征向量，并划分出训练数据集和测试数据集from pyspark.ml.feature import VectorAssemblerdatasets_2 = VectorAssembler().setInputCols(feature_cols).setOutputCol(&quot;features&quot;).transform(datasets_2)train_datasets_2 = datasets_2.filter(datasets_2.timestamp&lt;=(1494691186-24*60*60))test_datasets_2 = datasets_2.filter(datasets_2.timestamp&gt;(1494691186-24*60*60))train_datasets_2.printSchema()train_datasets_2.first() 显示结果: 1234567891011121314151617181920212223242526root |-- timestamp: long (nullable &#x3D; true) |-- clk: integer (nullable &#x3D; true) |-- price: float (nullable &#x3D; true) |-- cms_group_id: string (nullable &#x3D; true) |-- final_gender_code: string (nullable &#x3D; true) |-- age_level: string (nullable &#x3D; true) |-- shopping_level: string (nullable &#x3D; true) |-- occupation: string (nullable &#x3D; true) |-- pid_value: vector (nullable &#x3D; true) |-- pl_onehot_value: vector (nullable &#x3D; true) |-- nucl_onehot_value: vector (nullable &#x3D; true) |-- cms_group_id_feature: double (nullable &#x3D; false) |-- cms_group_id_value: vector (nullable &#x3D; true) |-- final_gender_code_feature: double (nullable &#x3D; false) |-- final_gender_code_value: vector (nullable &#x3D; true) |-- age_level_feature: double (nullable &#x3D; false) |-- age_level_value: vector (nullable &#x3D; true) |-- shopping_level_feature: double (nullable &#x3D; false) |-- shopping_level_value: vector (nullable &#x3D; true) |-- occupation_feature: double (nullable &#x3D; false) |-- occupation_value: vector (nullable &#x3D; true) |-- features: vector (nullable &#x3D; true)Row(timestamp&#x3D;1494261938, clk&#x3D;0, price&#x3D;108.0, cms_group_id&#x3D;&#39;11&#39;, final_gender_code&#x3D;&#39;1&#39;, age_level&#x3D;&#39;5&#39;, shopping_level&#x3D;&#39;3&#39;, occupation&#x3D;&#39;0&#39;, pid_value&#x3D;SparseVector(2, &#123;1: 1.0&#125;), pl_onehot_value&#x3D;SparseVector(4, &#123;0: 1.0&#125;), nucl_onehot_value&#x3D;SparseVector(5, &#123;1: 1.0&#125;), cms_group_id_feature&#x3D;6.0, cms_group_id_value&#x3D;SparseVector(13, &#123;6: 1.0&#125;), final_gender_code_feature&#x3D;1.0, final_gender_code_value&#x3D;SparseVector(2, &#123;1: 1.0&#125;), age_level_feature&#x3D;2.0, age_level_value&#x3D;SparseVector(7, &#123;2: 1.0&#125;), shopping_level_feature&#x3D;0.0, shopping_level_value&#x3D;SparseVector(3, &#123;0: 1.0&#125;), occupation_feature&#x3D;0.0, occupation_value&#x3D;SparseVector(2, &#123;0: 1.0&#125;), features&#x3D;SparseVector(39, &#123;0: 108.0, 7: 1.0, 15: 1.0, 18: 1.0, 23: 1.0, 26: 1.0, 29: 1.0, 30: 1.0, 35: 1.0&#125;)) 创建逻辑回归训练器，并训练模型 123456789101112131415from pyspark.ml.classification import LogisticRegressionlr2 = LogisticRegression()#设置目标值对应的列 setFeaturesCol 设置特征值对应的列名model2 = lr2.setLabelCol(&quot;clk&quot;).setFeaturesCol(&quot;features&quot;).fit(train_datasets_2)# 存储模型model2.save(&quot;hdfs://localhost:9000/models/CTRModel_AllOneHot.obj&quot;)from pyspark.ml.classification import LogisticRegressionModel# 载入训练好的模型model2 = LogisticRegressionModel.load(&quot;hdfs://localhost:9000/models/CTRModel_AllOneHot.obj&quot;)result_2 = model2.transform(test_datasets_2)# 按probability升序排列数据，probability表示预测结果的概率result_2.select(&quot;clk&quot;, &quot;price&quot;, &quot;probability&quot;, &quot;prediction&quot;).sort(&quot;probability&quot;).show(100)# 对比前面的result_1的预测结果，能发现这里的预测率稍微准确了一点，这里top20里出现了3个点击的，但前面的只出现了1个# 因此可见对特征的细化处理，已经帮助我们提高模型的效果的 显示结果: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106+---+-----------+--------------------+----------+|clk| price| probability|prediction|+---+-----------+--------------------+----------+| 0| 1.0E8|[0.85524418892857...| 0.0|| 0| 1.0E8|[0.88353143762124...| 0.0|| 0| 1.0E8|[0.89169808985616...| 0.0|| 1|5.5555556E7|[0.92511743960350...| 0.0|| 0| 179.01|[0.93239951738307...| 0.0|| 1| 159.0|[0.93239952905659...| 0.0|| 0| 118.0|[0.93239955297535...| 0.0|| 0| 688.0|[0.93451506165953...| 0.0|| 0| 339.0|[0.93451525933626...| 0.0|| 0| 335.0|[0.93451526160190...| 0.0|| 0| 220.0|[0.93451532673881...| 0.0|| 0| 176.0|[0.93451535166074...| 0.0|| 0| 158.0|[0.93451536185607...| 0.0|| 0| 158.0|[0.93451536185607...| 0.0|| 1| 149.0|[0.93451536695374...| 0.0|| 0| 122.5|[0.93451538196353...| 0.0|| 0| 99.0|[0.93451539527410...| 0.0|| 0| 88.0|[0.93451540150458...| 0.0|| 0| 79.0|[0.93451540660224...| 0.0|| 0| 75.0|[0.93451540886787...| 0.0|| 0| 68.0|[0.93451541283272...| 0.0|| 0| 68.0|[0.93451541283272...| 0.0|| 0| 59.9|[0.93451541742061...| 0.0|| 0| 44.98|[0.93451542587140...| 0.0|| 0| 35.5|[0.93451543124094...| 0.0|| 0| 33.0|[0.93451543265696...| 0.0|| 0| 32.8|[0.93451543277024...| 0.0|| 0| 30.0|[0.93451543435618...| 0.0|| 0| 28.0|[0.93451543548899...| 0.0|| 0| 19.9|[0.93451544007688...| 0.0|| 0| 19.8|[0.93451544013353...| 0.0|| 0| 19.8|[0.93451544013353...| 0.0|| 0| 12.0|[0.93451544455150...| 0.0|| 0| 6.7|[0.93451544755345...| 0.0|| 0| 568.0|[0.93458159339238...| 0.0|| 0| 398.0|[0.93458168959099...| 0.0|| 0| 158.0|[0.93458182540058...| 0.0|| 0| 245.0|[0.93471518526899...| 0.0|| 0| 99.0|[0.93471526772971...| 0.0|| 0| 88.0|[0.93471527394249...| 0.0|| 0| 1288.0|[0.93474589600376...| 0.0|| 0| 688.0|[0.93474623473450...| 0.0|| 0| 656.0|[0.93474625280009...| 0.0|| 0| 568.0|[0.93474630248045...| 0.0|| 0| 498.0|[0.93474634199889...| 0.0|| 0| 399.0|[0.93474639788922...| 0.0|| 0| 396.0|[0.93474639958287...| 0.0|| 0| 298.0|[0.93474645490860...| 0.0|| 0| 293.0|[0.93474645773134...| 0.0|| 0| 209.0|[0.93474650515337...| 0.0|| 0| 198.0|[0.93474651136339...| 0.0|| 0| 198.0|[0.93474651136339...| 0.0|| 0| 169.0|[0.93474652773527...| 0.0|| 0| 168.0|[0.93474652829982...| 0.0|| 0| 159.0|[0.93474653338074...| 0.0|| 0| 155.0|[0.93474653563893...| 0.0|| 0| 139.0|[0.93474654467169...| 0.0|| 0| 138.0|[0.93474654523624...| 0.0|| 0| 119.0|[0.93474655596264...| 0.0|| 0| 99.0|[0.93474656725358...| 0.0|| 0| 99.0|[0.93474656725358...| 0.0|| 0| 88.0|[0.93474657346360...| 0.0|| 0| 88.0|[0.93474657346360...| 0.0|| 0| 79.0|[0.93474657854453...| 0.0|| 0| 59.0|[0.93474658983547...| 0.0|| 0| 59.0|[0.93474658983547...| 0.0|| 0| 59.0|[0.93474658983547...| 0.0|| 0| 58.0|[0.93474659040002...| 0.0|| 0| 57.0|[0.93474659096456...| 0.0|| 0| 49.8|[0.93474659502930...| 0.0|| 0| 39.98|[0.93474660057315...| 0.0|| 0| 36.8|[0.93474660236841...| 0.0|| 0| 34.0|[0.93474660394914...| 0.0|| 0| 6520.0|[0.93480919087761...| 0.0|| 0| 3699.0|[0.93481078202537...| 0.0|| 0| 1980.0|[0.93481175158689...| 0.0|| 0| 660.0|[0.93481249609274...| 0.0|| 0| 660.0|[0.93481249609274...| 0.0|| 0| 398.0|[0.93481264386492...| 0.0|| 0| 369.0|[0.93481266022137...| 0.0|| 0| 299.0|[0.93481269970243...| 0.0|| 0| 295.0|[0.93481270195849...| 0.0|| 0| 278.0|[0.93481271154674...| 0.0|| 0| 270.0|[0.93481271605886...| 0.0|| 0| 228.0|[0.93481273974748...| 0.0|| 0| 228.0|[0.93481273974748...| 0.0|| 0| 11368.0|[0.93494253131370...| 0.0|| 0| 9999.0|[0.93494330201510...| 0.0|| 0| 1099.0|[0.93494360670448...| 0.0|| 1| 8888.0|[0.93494392746484...| 0.0|| 0| 338.0|[0.93494403511659...| 0.0|| 0| 311.0|[0.93494405031645...| 0.0|| 0| 300.0|[0.93494405650898...| 0.0|| 0| 278.0|[0.93494406889404...| 0.0|| 0| 188.0|[0.93494411956019...| 0.0|| 0| 176.0|[0.93494412631568...| 0.0|| 0| 168.0|[0.93494413081933...| 0.0|| 0| 158.0|[0.93494413644890...| 0.0|| 1| 138.0|[0.93494414770804...| 0.0|| 0| 125.0|[0.93494415502647...| 0.0|+---+-----------+--------------------+----------+only showing top 100 rows 12result_2.filter(result_2.clk==1).select(&quot;clk&quot;, &quot;price&quot;, &quot;probability&quot;, &quot;prediction&quot;).sort(&quot;probability&quot;).show(100)# 从该结果也可以看出，result_2的点击率预测率普遍要比result_1高出一点点 显示结果: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106+---+-----------+--------------------+----------+|clk| price| probability|prediction|+---+-----------+--------------------+----------+| 1|5.5555556E7|[0.92511743960350...| 0.0|| 1| 159.0|[0.93239952905659...| 0.0|| 1| 149.0|[0.93451536695374...| 0.0|| 1| 8888.0|[0.93494392746484...| 0.0|| 1| 138.0|[0.93494414770804...| 0.0|| 1| 35.0|[0.93494420569256...| 0.0|| 1| 519.0|[0.93494863870621...| 0.0|| 1| 478.0|[0.93494866178596...| 0.0|| 1| 349.0|[0.93494873440265...| 0.0|| 1| 348.0|[0.93494873496557...| 0.0|| 1| 316.0|[0.93494875297901...| 0.0|| 1| 298.0|[0.93494876311156...| 0.0|| 1| 298.0|[0.93494876311156...| 0.0|| 1| 199.0|[0.93494881884058...| 0.0|| 1| 199.0|[0.93494881884058...| 0.0|| 1| 198.0|[0.93494881940350...| 0.0|| 1| 187.1|[0.93494882553931...| 0.0|| 1| 176.0|[0.93494883178772...| 0.0|| 1| 168.0|[0.93494883629107...| 0.0|| 1| 160.0|[0.93494884079442...| 0.0|| 1| 158.0|[0.93494884192026...| 0.0|| 1| 158.0|[0.93494884192026...| 0.0|| 1| 135.0|[0.93494885486740...| 0.0|| 1| 129.0|[0.93494885824491...| 0.0|| 1| 127.0|[0.93494885937075...| 0.0|| 1| 125.0|[0.93494886049659...| 0.0|| 1| 124.0|[0.93494886105951...| 0.0|| 1| 118.0|[0.93494886443702...| 0.0|| 1| 109.0|[0.93494886950329...| 0.0|| 1| 108.0|[0.93494887006621...| 0.0|| 1| 99.0|[0.93494887513247...| 0.0|| 1| 98.0|[0.93494887569539...| 0.0|| 1| 79.8|[0.93494888594051...| 0.0|| 1| 79.0|[0.93494888639085...| 0.0|| 1| 77.0|[0.93494888751668...| 0.0|| 1| 72.5|[0.93494889004982...| 0.0|| 1| 69.0|[0.93494889202003...| 0.0|| 1| 68.0|[0.93494889258295...| 0.0|| 1| 60.0|[0.93494889708630...| 0.0|| 1| 43.98|[0.93494890610426...| 0.0|| 1| 40.0|[0.93494890834467...| 0.0|| 1| 39.9|[0.93494890840096...| 0.0|| 1| 39.6|[0.93494890856984...| 0.0|| 1| 32.0|[0.93494891284802...| 0.0|| 1| 31.0|[0.93494891341094...| 0.0|| 1| 25.98|[0.93494891623679...| 0.0|| 1| 23.0|[0.93494891791428...| 0.0|| 1| 19.0|[0.93494892016596...| 0.0|| 1| 16.9|[0.93494892134809...| 0.0|| 1| 10.0|[0.93494892523222...| 0.0|| 1| 3.5|[0.93494892889119...| 0.0|| 1| 3.5|[0.93494892889119...| 0.0|| 1| 0.4|[0.93494893063624...| 0.0|| 1| 1288.0|[0.93501426059874...| 0.0|| 1| 980.0|[0.93501443381533...| 0.0|| 1| 788.0|[0.93501454179429...| 0.0|| 1| 698.0|[0.93501459240937...| 0.0|| 1| 695.0|[0.93501459409654...| 0.0|| 1| 688.0|[0.93501459803326...| 0.0|| 1| 599.0|[0.93501464808591...| 0.0|| 1| 588.0|[0.93501465427219...| 0.0|| 1| 516.0|[0.93501469476419...| 0.0|| 1| 495.0|[0.93501470657436...| 0.0|| 1| 398.0|[0.93501476112603...| 0.0|| 1| 368.0|[0.93501477799768...| 0.0|| 1| 339.0|[0.93501479430693...| 0.0|| 1| 335.0|[0.93501479655648...| 0.0|| 1| 324.0|[0.93501480274275...| 0.0|| 1| 316.0|[0.93501480724185...| 0.0|| 1| 299.0|[0.93501481680244...| 0.0|| 1| 295.0|[0.93501481905199...| 0.0|| 1| 279.0|[0.93501482805020...| 0.0|| 1| 268.0|[0.93501483423646...| 0.0|| 1| 259.0|[0.93501483929795...| 0.0|| 1| 259.0|[0.93501483929795...| 0.0|| 1| 249.0|[0.93501484492182...| 0.0|| 1| 238.0|[0.93501485110809...| 0.0|| 1| 199.0|[0.93501487304119...| 0.0|| 1| 198.0|[0.93501487360358...| 0.0|| 1| 179.0|[0.93501488428894...| 0.0|| 1| 175.0|[0.93501488653849...| 0.0|| 1| 129.0|[0.93501491240829...| 0.0|| 1| 128.0|[0.93501491297068...| 0.0|| 1| 118.0|[0.93501491859455...| 0.0|| 1| 109.0|[0.93501492365603...| 0.0|| 1| 98.0|[0.93501492984229...| 0.0|| 1| 89.0|[0.93501493490377...| 0.0|| 1| 79.0|[0.93501494052764...| 0.0|| 1| 75.0|[0.93501494277718...| 0.0|| 1| 69.8|[0.93501494570159...| 0.0|| 1| 30.0|[0.93501496808458...| 0.0|| 1| 15.0|[0.93501497652038...| 0.0|| 1| 368.0|[0.93665387743951...| 0.0|| 1| 198.0|[0.93665397079735...| 0.0|| 1| 178.0|[0.93665398178062...| 0.0|| 1| 158.0|[0.93665399276388...| 0.0|| 1| 158.0|[0.93665399276388...| 0.0|| 1| 149.0|[0.93665399770635...| 0.0|| 1| 68.0|[0.93665404218855...| 0.0|| 1| 36.0|[0.93665405976176...| 0.0|+---+-----------+--------------------+----------+only showing top 100 rows 总结CTR预估模型建立 利用raw_sample ad_feature user_profile 数据合并 挑选出合适的特征 123456789101112131415161718useful_cols = [ # # 时间字段，划分训练集和测试集 &quot;timestamp&quot;, # label目标值字段 &quot;clk&quot;, # 特征值字段 &quot;pid_value&quot;, # 资源位的特征向量 &quot;price&quot;, # 广告价格 &quot;cms_segid&quot;, # 用户微群ID &quot;cms_group_id&quot;, # 用户组ID &quot;final_gender_code&quot;, # 用户性别特征，[1,2] &quot;age_level&quot;, # 年龄等级，1- &quot;shopping_level&quot;, &quot;occupation&quot;, &quot;pl_onehot_value&quot;, &quot;nucl_onehot_value&quot;] 又对数据进行处理，把可能进行one-hot编码的分类特征都进行one_hot处理 useful_cols_2 = [ # 时间值，划分训练集和测试集 &quot;timestamp&quot;, # label目标值 &quot;clk&quot;, # 特征值 &quot;price&quot;, &quot;cms_group_id&quot;, #13维 &quot;final_gender_code&quot;, #2维 &quot;age_level&quot;, #7维 &quot;shopping_level&quot;, #3维度 &quot;occupation&quot;, #2 &quot;pid_value&quot;, #2 &quot;pl_onehot_value&quot;,#4 &quot;nucl_onehot_value&quot;#5 ] 逻辑回归训练出的CTR预估模型 预测值的理解 因为数据大部分都是不点击， 样本极度偏斜的，点击样本很少 （5%） 预测出的结果都是0 不点 根据不点击的概率来排序 不点击概率越低的排在前面 在测试数据中 按照不点击的概率排序 考察精准率（找前10个 看看10个中是否有点击的样本）能有10%的CTR预估的概率就已经相当不错了。正常5%。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"},{"name":"推荐系统实战","slug":"机器学习/大数据的lambda架构/推荐系统实战","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"03-CTR预估数据准备","slug":"34-CTR预估数据准备","date":"2021-07-10T14:47:02.000Z","updated":"2021-07-19T09:40:27.646Z","comments":true,"path":"20210710/34-CTR预估数据准备.html","link":"","permalink":"https://xxren8218.github.io/20210710/34-CTR%E9%A2%84%E4%BC%B0%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87.html","excerpt":"","text":"CTR预估数据准备1. 分析并预处理raw_sample数据集1234# 从HDFS中加载样本数据信息df = spark.read.csv(&quot;hdfs://localhost:9000/datasets/raw_sample.csv&quot;, header=True)df.show() # 展示数据，默认前20条df.printSchema() 显示结果: 123456789101112131415161718192021222324252627282930313233+------+----------+----------+-----------+------+---+| user|time_stamp|adgroup_id| pid|nonclk|clk|+------+----------+----------+-----------+------+---+|581738|1494137644| 1|430548_1007| 1| 0||449818|1494638778| 3|430548_1007| 1| 0||914836|1494650879| 4|430548_1007| 1| 0||914836|1494651029| 5|430548_1007| 1| 0||399907|1494302958| 8|430548_1007| 1| 0||628137|1494524935| 9|430548_1007| 1| 0||298139|1494462593| 9|430539_1007| 1| 0||775475|1494561036| 9|430548_1007| 1| 0||555266|1494307136| 11|430539_1007| 1| 0||117840|1494036743| 11|430548_1007| 1| 0||739815|1494115387| 11|430539_1007| 1| 0||623911|1494625301| 11|430548_1007| 1| 0||623911|1494451608| 11|430548_1007| 1| 0||421590|1494034144| 11|430548_1007| 1| 0||976358|1494156949| 13|430548_1007| 1| 0||286630|1494218579| 13|430539_1007| 1| 0||286630|1494289247| 13|430539_1007| 1| 0||771431|1494153867| 13|430548_1007| 1| 0||707120|1494220810| 13|430548_1007| 1| 0||530454|1494293746| 13|430548_1007| 1| 0|+------+----------+----------+-----------+------+---+only showing top 20 rowsroot |-- user: string (nullable = true) |-- time_stamp: string (nullable = true) |-- adgroup_id: string (nullable = true) |-- pid: string (nullable = true) |-- nonclk: string (nullable = true) |-- clk: string (nullable = true) 分析数据集字段的类型和格式 查看是否有空值 查看每列数据的类型 查看每列数据的类别情况 12345678910print(&quot;样本数据集总条目数：&quot;, df.count())# 约2600wprint(&quot;用户user总数：&quot;, df.groupBy(&quot;user&quot;).count().count())# 约 114w，略多余日志数据中用户数print(&quot;广告id adgroup_id总数：&quot;, df.groupBy(&quot;adgroup_id&quot;).count().count())# 约85wprint(&quot;广告展示位pid情况：&quot;, df.groupBy(&quot;pid&quot;).count().collect())# 只有两种广告展示位，占比约为六比四print(&quot;广告点击数据情况clk：&quot;, df.groupBy(&quot;clk&quot;).count().collect())# 点和不点比率约： 1:20 显示结果: 12345样本数据集总条目数： 26557961用户user总数： 1141729广告id adgroup_id总数： 846811广告展示位pid情况： [Row(pid=&#x27;430548_1007&#x27;, count=16472898), Row(pid=&#x27;430539_1007&#x27;, count=10085063)]广告点击数据情况clk： [Row(clk=&#x27;0&#x27;, count=25191905), Row(clk=&#x27;1&#x27;, count=1366056)] 默认加载进来的schema都是String类型的。修改为我们所需要的格式，并且一些名字给改过来。 使用dataframe.withColumn更改df列数据结构；使用dataframe.withColumnRenamed更改列名称 123456789101112131415# 更改表结构，转换为对应的数据类型from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType, StringType# 打印df结构信息df.printSchema() # 更改df表结构：更改列类型和列名称raw_sample_df = df.\\ withColumn(&quot;user&quot;, df.user.cast(IntegerType())).withColumnRenamed(&quot;user&quot;, &quot;userId&quot;).\\ withColumn(&quot;time_stamp&quot;, df.time_stamp.cast(LongType())).withColumnRenamed(&quot;time_stamp&quot;, &quot;timestamp&quot;).\\ withColumn(&quot;adgroup_id&quot;, df.adgroup_id.cast(IntegerType())).withColumnRenamed(&quot;adgroup_id&quot;, &quot;adgroupId&quot;).\\ withColumn(&quot;pid&quot;, df.pid.cast(StringType())).\\ withColumn(&quot;nonclk&quot;, df.nonclk.cast(IntegerType())).\\ withColumn(&quot;clk&quot;, df.clk.cast(IntegerType()))raw_sample_df.printSchema()raw_sample_df.show() 显示结果: 1234567891011121314151617181920212223242526272829303132333435363738394041root |-- user: string (nullable = true) |-- time_stamp: string (nullable = true) |-- adgroup_id: string (nullable = true) |-- pid: string (nullable = true) |-- nonclk: string (nullable = true) |-- clk: string (nullable = true)root |-- userId: integer (nullable = true) |-- timestamp: long (nullable = true) |-- adgroupId: integer (nullable = true) |-- pid: string (nullable = true) |-- nonclk: integer (nullable = true) |-- clk: integer (nullable = true)+------+----------+---------+-----------+------+---+|userId| timestamp|adgroupId| pid|nonclk|clk|+------+----------+---------+-----------+------+---+|581738|1494137644| 1|430548_1007| 1| 0||449818|1494638778| 3|430548_1007| 1| 0||914836|1494650879| 4|430548_1007| 1| 0||914836|1494651029| 5|430548_1007| 1| 0||399907|1494302958| 8|430548_1007| 1| 0||628137|1494524935| 9|430548_1007| 1| 0||298139|1494462593| 9|430539_1007| 1| 0||775475|1494561036| 9|430548_1007| 1| 0||555266|1494307136| 11|430539_1007| 1| 0||117840|1494036743| 11|430548_1007| 1| 0||739815|1494115387| 11|430539_1007| 1| 0||623911|1494625301| 11|430548_1007| 1| 0||623911|1494451608| 11|430548_1007| 1| 0||421590|1494034144| 11|430548_1007| 1| 0||976358|1494156949| 13|430548_1007| 1| 0||286630|1494218579| 13|430539_1007| 1| 0||286630|1494289247| 13|430539_1007| 1| 0||771431|1494153867| 13|430548_1007| 1| 0||707120|1494220810| 13|430548_1007| 1| 0||530454|1494293746| 13|430548_1007| 1| 0|+------+----------+---------+-----------+------+---+only showing top 20 rows 数据有了，接下来进行特征相关的操作。 特征选取（Feature Selection） 特征选择就是选择那些靠谱的Feature，去掉冗余的Feature，对于搜索广告，Query关键词和广告的匹配程度很重要；但对于展示广告，广告本身的历史表现，往往是最重要的Feature。 根据经验，该数据集中，只有广告展示位pid对比较重要，且数据不同数据之间的占比约为6:4，因此pid可以作为一个关键特征 nonclk和clk在这里是作为目标值，不做为特征 热独编码 OneHotEncode 热独编码是一种经典编码，是使用N位状态寄存器(如0和1)来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。 假设有三组特征，分别表示年龄，城市，设备； [“男”, “女”][0,1] [“北京”, “上海”, “广州”][0,1,2] [“苹果”, “小米”, “华为”, “微软”][0,1,2,3] 传统变化： 对每一组特征，使用枚举类型，从0开始； [“男“，”上海“，”小米“]=[ 0,1,1] [“女“，”北京“，”苹果“] =[1,0,0] 传统变化后的数据不是连续的，而是随机分配的，不容易应用在分类器中 而经过热独编码，数据会变成稀疏的，方便分类器处理： [“男“，”上海“，”小米“]=[ 1,0,0,1,0,0,1,0,0] [“女“，”北京“，”苹果“] =[0,1,1,0,0,1,0,0,0] 这样做保留了特征的多样性，但是也要注意如果数据过于稀疏(样本较少、维度过高)，其效果反而会变差 Spark中使用热独编码 注意：热编码只能对字符串类型的列数据进行处理 StringIndexer：对指定字符串列数据进行特征处理，如将性别数据“男”、“女”转化为0和1 OneHotEncoder：对特征列数据，进行热编码，通常需结合StringIndexer一起使用 Pipeline：让数据按顺序依次被处理，将前一次的处理结果作为下一次的输入 特征处理 1234567891011121314151617181920212223&#x27;&#x27;&#x27;特征处理&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;pid 资源位。该特征属于分类特征，只有两类取值，因此考虑进行热编码处理即可，分为是否在资源位1、是否在资源位2 两个特征&#x27;&#x27;&#x27;from pyspark.ml.feature import OneHotEncoderfrom pyspark.ml.feature import StringIndexer # 对指定字符串列数据进行特征处理，如将性别数据“男”、“女”转化为0和1from pyspark.ml import Pipeline# StringIndexer对指定字符串列进行特征处理，利用StringIndexer 把字符串类别转换成 0 1 2 数值类别stringindexer = StringIndexer(inputCol=&#x27;pid&#x27;, outputCol=&#x27;pid_feature&#x27;)string_model = stringIndexer.fit(raw_sample_df)stringIndex_result = string_model.transform(raw_sample_df)# 对处理出来的特征处理列进行，热独编码，利用OneHotEncoder 在stringIndexer的基础上 获取onehot编码encoder = OneHotEncoder(dropLast=False, inputCol=&#x27;pid_feature&#x27;, outputCol=&#x27;pid_value&#x27;) # dropLast=False会把最后一去掉，用其余表示最后一个。result = encoder.transform(stringIndex_result) #不需要fit，直接transform# (2,[0],[1.0]) 表示两个维度，第0个维度为 1.0，Spark表示稀疏向量的方式。# 利用管道对每一个数据进行热独编码处理 ——（若处理步骤比较多，可以用pipline减少代码量。）pipeline = Pipeline(stages=[stringindexer, encoder])pipeline_model = pipeline.fit(raw_sample_df)new_df = pipeline_model.transform(raw_sample_df)new_df.show() 显示结果: 1234567891011121314151617181920212223242526+------+----------+---------+-----------+------+---+-----------+-------------+|userId| timestamp|adgroupId| pid|nonclk|clk|pid_feature| pid_value|+------+----------+---------+-----------+------+---+-----------+-------------+|581738|1494137644| 1|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||449818|1494638778| 3|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||914836|1494650879| 4|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||914836|1494651029| 5|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||399907|1494302958| 8|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||628137|1494524935| 9|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||298139|1494462593| 9|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||775475|1494561036| 9|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||555266|1494307136| 11|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||117840|1494036743| 11|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||739815|1494115387| 11|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||623911|1494625301| 11|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||623911|1494451608| 11|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||421590|1494034144| 11|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||976358|1494156949| 13|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||286630|1494218579| 13|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||286630|1494289247| 13|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||771431|1494153867| 13|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||707120|1494220810| 13|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||530454|1494293746| 13|430548_1007| 1| 0| 0.0|(2,[0],[1.0])|+------+----------+---------+-----------+------+---+-----------+-------------+only showing top 20 rows 返回字段pid_value是一个稀疏向量类型数据 pyspark.ml.linalg.SparseVector 1234567from pyspark.ml.linalg import SparseVector# 参数：维度、索引列表、值列表print(SparseVector(4, [1, 3], [3.0, 4.0]))print(SparseVector(4, [1, 3], [3.0, 4.0]).toArray())print(&quot;*********&quot;)print(new_df.select(&quot;pid_value&quot;).first())print(new_df.select(&quot;pid_value&quot;).first().pid_value.toArray()) 显示结果: 12345(4,[1,3],[3.0,4.0])[0. 3. 0. 4.]*********Row(pid_value&#x3D;SparseVector(2, &#123;0: 1.0&#125;))[1. 0.] 2. 划分训练集和测试集 查看最大时间 1new_df.sort(&quot;timestamp&quot;, ascending=False).show() 12345678910111213141516171819202122232425+------+----------+---------+-----------+------+---+-----------+-------------+|userId| timestamp|adgroupId| pid|nonclk|clk|pid_feature| pid_value|+------+----------+---------+-----------+------+---+-----------+-------------+|177002|1494691186| 593001|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||243671|1494691186| 600195|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||488527|1494691184| 494312|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||488527|1494691184| 431082|430548_1007| 1| 0| 0.0|(2,[0],[1.0])|| 17054|1494691184| 742741|430548_1007| 1| 0| 0.0|(2,[0],[1.0])|| 17054|1494691184| 756665|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||488527|1494691184| 687854|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||839493|1494691183| 561681|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||704223|1494691183| 624504|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||839493|1494691183| 582235|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||704223|1494691183| 675674|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||628998|1494691180| 618965|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||674444|1494691179| 427579|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||627200|1494691179| 782038|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||627200|1494691179| 420769|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||674444|1494691179| 588664|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||738335|1494691179| 451004|430539_1007| 1| 0| 1.0|(2,[1],[1.0])||627200|1494691179| 817569|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||322244|1494691179| 820018|430548_1007| 1| 0| 0.0|(2,[0],[1.0])||322244|1494691179| 735220|430548_1007| 1| 0| 0.0|(2,[0],[1.0])|+------+----------+---------+-----------+------+---+-----------+-------------+only showing top 20 rows 123456# 本样本数据集共计8天数据# 前七天为训练数据、最后一天为测试数据from datetime import datetimedatetime.fromtimestamp(1494691186)print(&quot;该时间之前的数据为训练样本，该时间以后的数据为测试样本：&quot;, datetime.fromtimestamp(1494691186-24*60*60)) 显示结果: 1datetime.datetime(2017, 5, 12, 23, 59, 46) 该时间之前的数据为训练样本，该时间以后的数据为测试样本： 2017-05-12 23:59:46 训练样本 12345678910# 训练样本：train_sample = raw_sample_df.filter(raw_sample_df.timestamp&lt;=(1494691186-24*60*60))print(&quot;训练样本个数：&quot;)print(train_sample.count())# 测试样本test_sample = raw_sample_df.filter(raw_sample_df.timestamp&gt;(1494691186-24*60*60))print(&quot;测试样本个数：&quot;)print(test_sample.count())# 注意：还需要加入广告基本特征和用户基本特征才能做程一份完整的样本数据集 显示结果: 12345训练样本个数：23249291测试样本个数：3308670 3. 分析并预处理ad_feature数据集123# 从HDFS中加载广告基本信息数据，返回spark dafaframe对象df = spark.read.csv(&quot;hdfs://localhost:9000/datasets/ad_feature.csv&quot;, header=True)df.show() # 展示数据，默认前20条 显示结果: 12345678910111213141516171819202122232425+----------+-------+-----------+--------+------+-----+|adgroup_id|cate_id|campaign_id|customer| brand|price|+----------+-------+-----------+--------+------+-----+| 63133| 6406| 83237| 1| 95471|170.0|| 313401| 6406| 83237| 1| 87331|199.0|| 248909| 392| 83237| 1| 32233| 38.0|| 208458| 392| 83237| 1|174374|139.0|| 110847| 7211| 135256| 2|145952|32.99|| 607788| 6261| 387991| 6|207800|199.0|| 375706| 4520| 387991| 6| NULL| 99.0|| 11115| 7213| 139747| 9|186847| 33.0|| 24484| 7207| 139744| 9|186847| 19.0|| 28589| 5953| 395195| 13| NULL|428.0|| 23236| 5953| 395195| 13| NULL|368.0|| 300556| 5953| 395195| 13| NULL|639.0|| 92560| 5953| 395195| 13| NULL|368.0|| 590965| 4284| 28145| 14|454237|249.0|| 529913| 4284| 70206| 14| NULL|249.0|| 546930| 4284| 28145| 14| NULL|249.0|| 639794| 6261| 70206| 14| 37004| 89.9|| 335413| 4284| 28145| 14| NULL|249.0|| 794890| 4284| 70206| 14|454237|249.0|| 684020| 6261| 70206| 14| 37004| 99.0|+----------+-------+-----------+--------+------+-----+only showing top 20 rows 12345678910111213141516171819# 注意：由于本数据集中存在NULL字样的数据，无法直接设置schema，只能先将NULL类型的数据处理掉，然后进行类型转换from pyspark.sql.types import StructType, StructField, IntegerType, FloatType# 替换掉NULL字符串，替换掉df = df.replace(&quot;NULL&quot;, &quot;-1&quot;)# 打印df结构信息df.printSchema() # 更改df表结构：更改列类型和列名称ad_feature_df = df.\\ withColumn(&quot;adgroup_id&quot;, df.adgroup_id.cast(IntegerType())).withColumnRenamed(&quot;adgroup_id&quot;, &quot;adgroupId&quot;).\\ withColumn(&quot;cate_id&quot;, df.cate_id.cast(IntegerType())).withColumnRenamed(&quot;cate_id&quot;, &quot;cateId&quot;).\\ withColumn(&quot;campaign_id&quot;, df.campaign_id.cast(IntegerType())).withColumnRenamed(&quot;campaign_id&quot;, &quot;campaignId&quot;).\\ withColumn(&quot;customer&quot;, df.customer.cast(IntegerType())).withColumnRenamed(&quot;customer&quot;, &quot;customerId&quot;).\\ withColumn(&quot;brand&quot;, df.brand.cast(IntegerType())).withColumnRenamed(&quot;brand&quot;, &quot;brandId&quot;).\\ withColumn(&quot;price&quot;, df.price.cast(FloatType()))ad_feature_df.printSchema()ad_feature_df.show() 显示结果: 1234567891011121314151617181920212223242526272829303132333435363738394041root |-- adgroup_id: string (nullable = true) |-- cate_id: string (nullable = true) |-- campaign_id: string (nullable = true) |-- customer: string (nullable = true) |-- brand: string (nullable = true) |-- price: string (nullable = true)root |-- adgroupId: integer (nullable = true) |-- cateId: integer (nullable = true) |-- campaignId: integer (nullable = true) |-- customerId: integer (nullable = true) |-- brandId: integer (nullable = true) |-- price: float (nullable = true)+---------+------+----------+----------+-------+-----+|adgroupId|cateId|campaignId|customerId|brandId|price|+---------+------+----------+----------+-------+-----+| 63133| 6406| 83237| 1| 95471|170.0|| 313401| 6406| 83237| 1| 87331|199.0|| 248909| 392| 83237| 1| 32233| 38.0|| 208458| 392| 83237| 1| 174374|139.0|| 110847| 7211| 135256| 2| 145952|32.99|| 607788| 6261| 387991| 6| 207800|199.0|| 375706| 4520| 387991| 6| -1| 99.0|| 11115| 7213| 139747| 9| 186847| 33.0|| 24484| 7207| 139744| 9| 186847| 19.0|| 28589| 5953| 395195| 13| -1|428.0|| 23236| 5953| 395195| 13| -1|368.0|| 300556| 5953| 395195| 13| -1|639.0|| 92560| 5953| 395195| 13| -1|368.0|| 590965| 4284| 28145| 14| 454237|249.0|| 529913| 4284| 70206| 14| -1|249.0|| 546930| 4284| 28145| 14| -1|249.0|| 639794| 6261| 70206| 14| 37004| 89.9|| 335413| 4284| 28145| 14| -1|249.0|| 794890| 4284| 70206| 14| 454237|249.0|| 684020| 6261| 70206| 14| 37004| 99.0|+---------+------+----------+----------+-------+-----+only showing top 20 rows 查看各项数据的特征 12345678910111213print(&quot;总广告条数：&quot;,df.count()) # 数据条数_1 = ad_feature_df.groupBy(&quot;cateId&quot;).count().count()print(&quot;cateId数值个数：&quot;, _1)_2 = ad_feature_df.groupBy(&quot;campaignId&quot;).count().count()print(&quot;campaignId数值个数：&quot;, _2)_3 = ad_feature_df.groupBy(&quot;customerId&quot;).count().count()print(&quot;customerId数值个数：&quot;, _3)_4 = ad_feature_df.groupBy(&quot;brandId&quot;).count().count()print(&quot;brandId数值个数：&quot;, _4)ad_feature_df.sort(&quot;price&quot;).show()ad_feature_df.sort(&quot;price&quot;, ascending=False).show()print(&quot;价格高于1w的条目个数：&quot;, ad_feature_df.select(&quot;price&quot;).filter(&quot;price&gt;10000&quot;).count())print(&quot;价格低于1的条目个数&quot;, ad_feature_df.select(&quot;price&quot;).filter(&quot;price&lt;1&quot;).count()) 显示结果: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960总广告条数： 846811cateId数值个数： 6769campaignId数值个数： 423436customerId数值个数： 255875brandId数值个数： 99815+---------+------+----------+----------+-------+-----+|adgroupId|cateId|campaignId|customerId|brandId|price|+---------+------+----------+----------+-------+-----+| 485749| 9970| 352666| 140520| -1| 0.01|| 88975| 9996| 198424| 182415| -1| 0.01|| 109704| 10539| 59774| 90351| 202710| 0.01|| 49911| 7032| 129079| 172334| -1| 0.01|| 339334| 9994| 310408| 211292| 383023| 0.01|| 6636| 6703| 392038| 46239| 406713| 0.01|| 92241| 6130| 72781| 149714| -1| 0.01|| 20397| 10539| 410958| 65726| 79971| 0.01|| 345870| 9995| 179595| 191036| 79971| 0.01|| 77797| 9086| 218276| 31183| -1| 0.01|| 14435| 1136| 135610| 17788| -1| 0.01|| 42055| 9994| 43866| 113068| 123242| 0.01|| 41925| 7032| 85373| 114532| -1| 0.01|| 67558| 9995| 90141| 83948| -1| 0.01|| 149570| 7043| 126746| 176076| -1| 0.01|| 518883| 7185| 403318| 58013| -1| 0.01|| 2246| 9996| 413653| 60214| 182966| 0.01|| 290675| 4824| 315371| 240984| -1| 0.01|| 552638| 10305| 403318| 58013| -1| 0.01|| 89831| 10539| 90141| 83948| 211816| 0.01|+---------+------+----------+----------+-------+-----+only showing top 20 rows+---------+------+----------+----------+-------+-----------+|adgroupId|cateId|campaignId|customerId|brandId| price|+---------+------+----------+----------+-------+-----------+| 658722| 1093| 218101| 207754| -1| 1.0E8|| 468220| 1093| 270719| 207754| -1| 1.0E8|| 179746| 1093| 270027| 102509| 405447| 1.0E8|| 443295| 1093| 44251| 102509| 300681| 1.0E8|| 31899| 685| 218918| 31239| 278301| 1.0E8|| 243384| 685| 218918| 31239| 278301| 1.0E8|| 554311| 1093| 266086| 207754| -1| 1.0E8|| 513942| 745| 8401| 86243| -1|8.8888888E7|| 201060| 745| 8401| 86243| -1|5.5555556E7|| 289563| 685| 37665| 120847| 278301| 1.5E7|| 35156| 527| 417722| 72273| 278301| 1.0E7|| 33756| 527| 416333| 70894| -1| 9900000.0|| 335495| 739| 170121| 148946| 326126| 9600000.0|| 218306| 206| 162394| 4339| 221720| 8888888.0|| 213567| 7213| 239302| 205612| 406125| 5888888.0|| 375920| 527| 217512| 148946| 326126| 4760000.0|| 262215| 527| 132721| 11947| 417898| 3980000.0|| 154623| 739| 170121| 148946| 326126| 3900000.0|| 152414| 739| 170121| 148946| 326126| 3900000.0|| 448651| 527| 422260| 41289| 209959| 3800000.0|+---------+------+----------+----------+-------+-----------+only showing top 20 rows价格高于1w的条目个数： 6527价格低于1的条目个数 5762 特征选择 品牌有些缺失，我们并没有建立用户和品牌的关系（召回时，我只算了类目）。不考虑 类别：在召回时已经考虑了，虽说会影响点击结果，但是我们仍不考虑。 淘客无法从数据体现区别，所以用 price来作为具体特征。 cateId：脱敏过的商品类目ID； campaignId：脱敏过的广告计划ID； customerId:脱敏过的广告主ID； brandId：脱敏过的品牌ID； 以上四个特征均属于分类特征，但由于分类值个数均过于庞大，如果去做热独编码处理，会导致数据过于稀疏 且当前我们缺少对这些特征更加具体的信息，（如商品类目具体信息、品牌具体信息等），从而无法对这些特征的数据做聚类、降维处理 因此这里不选取它们作为特征 而只选取price作为特征数据，因为价格本身是一个统计类型连续数值型数据，且能很好的体现广告的价值属性特征，通常也不需要做其他处理(离散化、归一化、标准化等)，所以这里直接将当做特征数据来使用 4. 分析并预处理user_profile数据集12345# 从HDFS加载用户基本信息数据df = spark.read.csv(&quot;hdfs://localhost:8020/csv/user_profile.csv&quot;, header=True)# 发现pvalue_level和new_user_class_level存在空值：（注意此处的null表示空值，而如果是NULL，则往往表示是一个字符串）# 因此直接利用schema就可以加载进该数据，无需替换null值df.show() 显示结果: 123456789101112131415161718192021222324+------+---------+------------+-----------------+---------+------------+--------------+----------+---------------------+|userid|cms_segid|cms_group_id|final_gender_code|age_level|pvalue_level|shopping_level|occupation|new_user_class_level |+------+---------+------------+-----------------+---------+------------+--------------+----------+---------------------+| 234| 0| 5| 2| 5| null| 3| 0| 3|| 523| 5| 2| 2| 2| 1| 3| 1| 2|| 612| 0| 8| 1| 2| 2| 3| 0| null|| 1670| 0| 4| 2| 4| null| 1| 0| null|| 2545| 0| 10| 1| 4| null| 3| 0| null|| 3644| 49| 6| 2| 6| 2| 3| 0| 2|| 5777| 44| 5| 2| 5| 2| 3| 0| 2|| 6211| 0| 9| 1| 3| null| 3| 0| 2|| 6355| 2| 1| 2| 1| 1| 3| 0| 4|| 6823| 43| 5| 2| 5| 2| 3| 0| 1|| 6972| 5| 2| 2| 2| 2| 3| 1| 2|| 9293| 0| 5| 2| 5| null| 3| 0| 4|| 9510| 55| 8| 1| 2| 2| 2| 0| 2|| 10122| 33| 4| 2| 4| 2| 3| 0| 2|| 10549| 0| 4| 2| 4| 2| 3| 0| null|| 10812| 0| 4| 2| 4| null| 2| 0| null|| 10912| 0| 4| 2| 4| 2| 3| 0| null|| 10996| 0| 5| 2| 5| null| 3| 0| 4|| 11256| 8| 2| 2| 2| 1| 3| 0| 3|| 11310| 31| 4| 2| 4| 1| 3| 0| 4|+------+---------+------------+-----------------+---------+------------+--------------+----------+---------------------+ 1234567891011121314151617181920# 注意：这里的null会直接被pyspark识别为None数据，也就是na数据，所以这里可以直接利用schema导入数据from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType# 构建表结构schema对象schema = StructType([ StructField(&quot;userId&quot;, IntegerType()), StructField(&quot;cms_segid&quot;, IntegerType()), StructField(&quot;cms_group_id&quot;, IntegerType()), StructField(&quot;final_gender_code&quot;, IntegerType()), # 性别 StructField(&quot;age_level&quot;, IntegerType()), # 年龄范围 StructField(&quot;pvalue_level&quot;, IntegerType()), # 消费档次 StructField(&quot;shopping_level&quot;, IntegerType()), # 购物频繁程度 StructField(&quot;occupation&quot;, IntegerType()), # 是否是大学生 StructField(&quot;new_user_class_level&quot;, IntegerType()) # 居住城市水平])# 利用schema从hdfs加载user_profile_df = spark.read.csv(&quot;hdfs://localhost:8020/csv/user_profile.csv&quot;, header=True, schema=schema)user_profile_df.printSchema()user_profile_df.show() 显示结果: 123456789101112131415161718192021222324252627282930313233343536root |-- userId: integer (nullable = true) |-- cms_segid: integer (nullable = true) |-- cms_group_id: integer (nullable = true) |-- final_gender_code: integer (nullable = true) |-- age_level: integer (nullable = true) |-- pvalue_level: integer (nullable = true) |-- shopping_level: integer (nullable = true) |-- occupation: integer (nullable = true) |-- new_user_class_level: integer (nullable = true)+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+|userId|cms_segid|cms_group_id|final_gender_code|age_level|pvalue_level|shopping_level|occupation|new_user_class_level|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+| 234| 0| 5| 2| 5| null| 3| 0| 3|| 523| 5| 2| 2| 2| 1| 3| 1| 2|| 612| 0| 8| 1| 2| 2| 3| 0| null|| 1670| 0| 4| 2| 4| null| 1| 0| null|| 2545| 0| 10| 1| 4| null| 3| 0| null|| 3644| 49| 6| 2| 6| 2| 3| 0| 2|| 5777| 44| 5| 2| 5| 2| 3| 0| 2|| 6211| 0| 9| 1| 3| null| 3| 0| 2|| 6355| 2| 1| 2| 1| 1| 3| 0| 4|| 6823| 43| 5| 2| 5| 2| 3| 0| 1|| 6972| 5| 2| 2| 2| 2| 3| 1| 2|| 9293| 0| 5| 2| 5| null| 3| 0| 4|| 9510| 55| 8| 1| 2| 2| 2| 0| 2|| 10122| 33| 4| 2| 4| 2| 3| 0| 2|| 10549| 0| 4| 2| 4| 2| 3| 0| null|| 10812| 0| 4| 2| 4| null| 2| 0| null|| 10912| 0| 4| 2| 4| 2| 3| 0| null|| 10996| 0| 5| 2| 5| null| 3| 0| 4|| 11256| 8| 2| 2| 2| 1| 3| 0| 3|| 11310| 31| 4| 2| 4| 1| 3| 0| 4|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+only showing top 20 rows 显示特征情况 1234567891011121314151617print(&quot;分类特征值个数情况: &quot;)print(&quot;cms_segid: &quot;, user_profile_df.groupBy(&quot;cms_segid&quot;).count().count())print(&quot;cms_group_id: &quot;, user_profile_df.groupBy(&quot;cms_group_id&quot;).count().count())print(&quot;final_gender_code: &quot;, user_profile_df.groupBy(&quot;final_gender_code&quot;).count().count())print(&quot;age_level: &quot;, user_profile_df.groupBy(&quot;age_level&quot;).count().count())print(&quot;shopping_level: &quot;, user_profile_df.groupBy(&quot;shopping_level&quot;).count().count())print(&quot;occupation: &quot;, user_profile_df.groupBy(&quot;occupation&quot;).count().count())print(&quot;含缺失值的特征情况: &quot;)user_profile_df.groupBy(&quot;pvalue_level&quot;).count().show()user_profile_df.groupBy(&quot;new_user_class_level&quot;).count().show()t_count = user_profile_df.count()pl_na_count = t_count - user_profile_df.dropna(subset=[&quot;pvalue_level&quot;]).count()print(&quot;pvalue_level的空值情况：&quot;, pl_na_count, &quot;空值占比：%0.2f%%&quot;%(pl_na_count/t_count*100))nul_na_count = t_count - user_profile_df.dropna(subset=[&quot;new_user_class_level&quot;]).count()print(&quot;new_user_class_level的空值情况：&quot;, nul_na_count, &quot;空值占比：%0.2f%%&quot;%(nul_na_count/t_count*100)) 显示内容: 1234567891011121314151617181920212223242526272829分类特征值个数情况: cms_segid: 97cms_group_id: 13final_gender_code: 2age_level: 7shopping_level: 3occupation: 2含缺失值的特征情况: +------------+------+|pvalue_level| count|+------------+------+| null|575917|| 1|154436|| 3| 37759|| 2|293656|+------------+------++--------------------+------+|new_user_class_level| count|+--------------------+------+| null|344920|| 1| 80548|| 3|173047|| 4|138833|| 2|324420|+--------------------+------+pvalue_level的空值情况： 575917 空值占比：54.24%new_user_class_level的空值情况： 344920 空值占比：32.49% 缺失值处理 注意，一般情况下： 缺失率低于10%：可直接进行相应的填充，如默认值、均值、算法拟合等等； 高于10%：往往会考虑舍弃该特征 特征处理，如1维转多维 但根据我们的经验，我们的广告推荐其实和用户的消费水平、用户所在城市等级都有比较大的关联，因此在这里pvalue_level、new_user_class_level都是比较重要的特征，我们不考虑舍弃 缺失值处理方案： 填充方案：结合用户的其他特征值，利用随机森林算法进行预测；但产生了大量人为构建的数据，一定程度上增加了数据的噪音 把变量映射到高维空间：如pvalue_level的1维数据，转换成是否1、是否2、是否3、是否缺失的4维数据；这样保证了所有原始数据不变，同时能提高精确度，但这样会导致数据变得比较稀疏，如果样本量很小，反而会导致样本效果较差，因此也不能滥用 填充方案 利用随机森林对pvalue_level的缺失值进行预测 12345678910111213from pyspark.mllib.regression import LabeledPoint# pyspark.mllib是基于RDD的，需要将数据先转化为RDD,转化时需要有特定的格式.# 剔除掉缺失值数据，将余下的数据作为训练数据# user_profile_df.dropna(subset=[&quot;pvalue_level&quot;])： 将pvalue_level中的空值所在行数据剔除后的数据，作为训练样本train_data = user_profile_df.dropna(subset=[&quot;pvalue_level&quot;]).rdd.map( lambda r:LabeledPoint(r.pvalue_level-1, [r.cms_segid, r.cms_group_id, r.final_gender_code, r.age_level, r.shopping_level, r.occupation]) # 目标值是从1开始。要求是从零开始，所以减一。)# 注意随机森林输入数据时，由于label的分类数是从0开始的，但pvalue_level的目前只分别是1，2，3，所以需要对应分别-1来作为目标值# 自然那么最终得出预测值后，需要对应+1才能还原回来# 我们使用cms_segid, cms_group_id, final_gender_code, age_level, shopping_level, occupation作为特征值，pvalue_level作为目标值 Labeled point A labeled point is a local vector, either dense or sparse, associated with a label/response. In MLlib, labeled points are used in supervised learning algorithms. We use a double to store a label, so we can use labeled points in both regression and classification. For binary classification, a label should be either 0 (negative) or 1 (positive). For multiclass classification, labels should be class indices starting from zero: 0, 1, 2, ….标记点是与标签/响应相关联的密集或稀疏的局部矢量。在MLlib中，标记点用于监督学习算法。我们使用double来存储标签，因此我们可以在回归和分类中使用标记点。对于二分类，标签应为0（负）或1（正）。对于多类分类，标签应该是从零开始的类索引：0, 1, 2, …。 PythonA labeled point is represented by LabeledPoint.标记点表示为 LabeledPoint。Refer to the LabeledPoint Python docs for more details on the API.有关API的更多详细信息，请参阅LabeledPointPython文档。 如何创建LabeledPoint的数据格式呢？这里给个例子！ 每个样本都可以准备一个LabeledPoint，把一堆LabeledPoint交给模型进行处理。 12345678from pyspark.mllib.linalg import SparseVectorfrom pyspark.mllib.regression import LabeledPoint# Create a labeled point with a positive label and a dense feature vector.pos = LabeledPoint(1.0, [1.0, 0.0, 3.0]) # 前一个1.0为目标值，后边的列表为特征——此处为稠密表示# Create a labeled point with a negative label and a sparse feature vector.neg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0])) # 此处为稀疏的表示 随机森林：pyspark.mllib.tree.RandomForest 1234567from pyspark.mllib.tree import RandomForest# 训练分类模型# 参数1 训练的数据# 参数2 目标值的分类个数 0,1,2# 参数3 特征中是否包含分类的特征 &#123;2:2,3:7&#125; &#123;2:2&#125; 表示 在特征中 第三个特征是分类的: 有两个分类# 参数4 随机森林中 树的棵数model = RandomForest.trainClassifier(train_data, 3, &#123;&#125;, 5) 随机森林模型：pyspark.mllib.tree.RandomForestModel 123# 预测单个数据# 注意用法：https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=tree%20random#pyspark.mllib.tree.RandomForestModel.predictmodel.predict([0.0, 4.0 ,2.0 , 4.0, 1.0, 0.0]) 显示结果: 11.0 接下来进行有缺失值的预测。 筛选出缺失值条目 12345678910111213141516pl_na_df = user_profile_df.na.fill(-1).where(&quot;pvalue_level=-1&quot;)pl_na_df.show(10)def row(r): return r.cms_segid, r.cms_group_id, r.final_gender_code, r.age_level, r.shopping_level, r.occupation# 转换为普通的rdd类型 MLlib都是基于 RDD 的、rdd = pl_na_df.rdd.map(row)# 预测全部的pvalue_level值:predicts = model.predict(rdd)# 查看前20条print(predicts.take(20))print(&quot;预测值总数&quot;, predicts.count())# 这里注意predict参数，如果是预测多个，那么参数必须是直接由列表构成的rdd参数，而不能是dataframe.rdd类型# 因此这里经过map函数处理，将每一行数据转换为普通的列表数据 显示结果: 123456789101112131415161718+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+|userId|cms_segid|cms_group_id|final_gender_code|age_level|pvalue_level|shopping_level|occupation|new_user_class_level|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+| 234| 0| 5| 2| 5| -1| 3| 0| 3|| 1670| 0| 4| 2| 4| -1| 1| 0| -1|| 2545| 0| 10| 1| 4| -1| 3| 0| -1|| 6211| 0| 9| 1| 3| -1| 3| 0| 2|| 9293| 0| 5| 2| 5| -1| 3| 0| 4|| 10812| 0| 4| 2| 4| -1| 2| 0| -1|| 10996| 0| 5| 2| 5| -1| 3| 0| 4|| 11602| 0| 5| 2| 5| -1| 3| 0| 2|| 11727| 0| 3| 2| 3| -1| 3| 0| 1|| 12195| 0| 10| 1| 4| -1| 3| 0| 2|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+only showing top 10 rows[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]预测值总数 575917 转换为pandas dataframe 1234567# 这里数据量比较小，直接转换为pandas dataframe来处理，因为方便，但注意如果数据量较大不推荐，因为这样会把全部数据加载到内存中temp = predicts.map(lambda x:int(x)).collect()pdf = pl_na_df.toPandas()import numpy as np # 在pandas df的基础上直接替换掉列数据pdf[&quot;pvalue_level&quot;] = np.array(temp) + 1 # 注意+1 还原预测值（前面减1了。）pdf 与非缺失数据进行拼接，完成pvalue_level的缺失值预测 12345new_user_profile_df = user_profile_df.dropna(subset=[&quot;pvalue_level&quot;]).unionAll(spark.createDataFrame(pdf, schema=schema))new_user_profile_df.show()# 注意：unionAll的使用，两个df的表结构必须完全一样 显示结果: 12345678910111213141516171819202122232425+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+|userId|cms_segid|cms_group_id|final_gender_code|age_level|pvalue_level|shopping_level|occupation|new_user_class_level|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+| 523| 5| 2| 2| 2| 1| 3| 1| 2|| 612| 0| 8| 1| 2| 2| 3| 0| null|| 3644| 49| 6| 2| 6| 2| 3| 0| 2|| 5777| 44| 5| 2| 5| 2| 3| 0| 2|| 6355| 2| 1| 2| 1| 1| 3| 0| 4|| 6823| 43| 5| 2| 5| 2| 3| 0| 1|| 6972| 5| 2| 2| 2| 2| 3| 1| 2|| 9510| 55| 8| 1| 2| 2| 2| 0| 2|| 10122| 33| 4| 2| 4| 2| 3| 0| 2|| 10549| 0| 4| 2| 4| 2| 3| 0| null|| 10912| 0| 4| 2| 4| 2| 3| 0| null|| 11256| 8| 2| 2| 2| 1| 3| 0| 3|| 11310| 31| 4| 2| 4| 1| 3| 0| 4|| 11739| 20| 3| 2| 3| 2| 3| 0| 4|| 12549| 33| 4| 2| 4| 2| 3| 0| 2|| 15155| 36| 5| 2| 5| 2| 1| 0| null|| 15347| 20| 3| 2| 3| 2| 3| 0| 3|| 15455| 8| 2| 2| 2| 2| 3| 0| 3|| 15783| 0| 4| 2| 4| 2| 3| 0| null|| 16749| 5| 2| 2| 2| 1| 3| 1| 4|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+only showing top 20 rows 利用随机森林对new_user_class_level的缺失值进行预测 12345678910from pyspark.mllib.regression import LabeledPoint# 选出new_user_class_level全部的train_data2 = user_profile_df.dropna(subset=[&quot;new_user_class_level&quot;]).rdd.map( lambda r:LabeledPoint(r.new_user_class_level - 1, [r.cms_segid, r.cms_group_id, r.final_gender_code, r.age_level, r.shopping_level, r.occupation]))from pyspark.mllib.tree import RandomForestmodel2 = RandomForest.trainClassifier(train_data2, 4, &#123;&#125;, 5)model2.predict([0.0, 4.0 ,2.0 , 4.0, 1.0, 0.0])# 预测值实际应该为2 显示结果: 11.0 123456789nul_na_df = user_profile_df.na.fill(-1).where(&quot;new_user_class_level=-1&quot;)nul_na_df.show(10)def row(r): return r.cms_segid, r.cms_group_id, r.final_gender_code, r.age_level, r.shopping_level, r.occupationrdd2 = nul_na_df.rdd.map(row)predicts2 = model.predict(rdd2)predicts2.take(20) 显示结果: 12345678910111213141516171819202122232425262728293031323334353637+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+|userId|cms_segid|cms_group_id|final_gender_code|age_level|pvalue_level|shopping_level|occupation|new_user_class_level|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+| 612| 0| 8| 1| 2| 2| 3| 0| -1|| 1670| 0| 4| 2| 4| -1| 1| 0| -1|| 2545| 0| 10| 1| 4| -1| 3| 0| -1|| 10549| 0| 4| 2| 4| 2| 3| 0| -1|| 10812| 0| 4| 2| 4| -1| 2| 0| -1|| 10912| 0| 4| 2| 4| 2| 3| 0| -1|| 12620| 0| 4| 2| 4| -1| 2| 0| -1|| 14437| 0| 5| 2| 5| -1| 3| 0| -1|| 14574| 0| 1| 2| 1| -1| 2| 0| -1|| 14985| 0| 11| 1| 5| -1| 2| 0| -1|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+only showing top 10 rows[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0] 总结：可以发现由于这两个字段的缺失过多，所以预测出来的值已经大大失真，但如果缺失率在10%以下，这种方法是比较有效的一种 1234user_profile_df = user_profile_df.na.fill(-1)user_profile_df.show()# new_df = new_df.withColumn(&quot;pvalue_level&quot;, new_df.pvalue_level.cast(StringType()))\\# .withColumn(&quot;new_user_class_level&quot;, new_df.new_user_class_level.cast(StringType())) 显示结果: 12345678910111213141516171819202122232425+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+|userId|cms_segid|cms_group_id|final_gender_code|age_level|pvalue_level|shopping_level|occupation|new_user_class_level|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+| 234| 0| 5| 2| 5| -1| 3| 0| 3|| 523| 5| 2| 2| 2| 1| 3| 1| 2|| 612| 0| 8| 1| 2| 2| 3| 0| -1|| 1670| 0| 4| 2| 4| -1| 1| 0| -1|| 2545| 0| 10| 1| 4| -1| 3| 0| -1|| 3644| 49| 6| 2| 6| 2| 3| 0| 2|| 5777| 44| 5| 2| 5| 2| 3| 0| 2|| 6211| 0| 9| 1| 3| -1| 3| 0| 2|| 6355| 2| 1| 2| 1| 1| 3| 0| 4|| 6823| 43| 5| 2| 5| 2| 3| 0| 1|| 6972| 5| 2| 2| 2| 2| 3| 1| 2|| 9293| 0| 5| 2| 5| -1| 3| 0| 4|| 9510| 55| 8| 1| 2| 2| 2| 0| 2|| 10122| 33| 4| 2| 4| 2| 3| 0| 2|| 10549| 0| 4| 2| 4| 2| 3| 0| -1|| 10812| 0| 4| 2| 4| -1| 2| 0| -1|| 10912| 0| 4| 2| 4| 2| 3| 0| -1|| 10996| 0| 5| 2| 5| -1| 3| 0| 4|| 11256| 8| 2| 2| 2| 1| 3| 0| 3|| 11310| 31| 4| 2| 4| 1| 3| 0| 4|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+only showing top 20 rows 低维转高维方式 我们接下来采用将变量映射到高维空间的方法来处理数据，即将缺失项也当做一个单独的特征来对待，保证数据的原始性由于该思想正好和热独编码实现方法一样，因此这里直接使用热独编码方式处理数据 12345678910111213141516171819202122232425from pyspark.ml.feature import OneHotEncoderfrom pyspark.ml.feature import StringIndexerfrom pyspark.ml import Pipeline# 使用热独编码转换pvalue_level的一维数据为多维，其中缺失值单独作为一个特征值# 需要先将缺失值全部替换为数值，与原有特征一起处理from pyspark.sql.types import StringTypeuser_profile_df = user_profile_df.na.fill(-1)user_profile_df.show()# 热独编码时，必须先将待处理字段转为字符串类型才可处理user_profile_df = user_profile_df.withColumn(&quot;pvalue_level&quot;, user_profile_df.pvalue_level.cast(StringType()))\\ .withColumn(&quot;new_user_class_level&quot;, user_profile_df.new_user_class_level.cast(StringType()))user_profile_df.printSchema()# 对pvalue_level进行热独编码，求值stringindexer = StringIndexer(inputCol=&#x27;pvalue_level&#x27;, outputCol=&#x27;pl_onehot_feature&#x27;)encoder = OneHotEncoder(dropLast=False, inputCol=&#x27;pl_onehot_feature&#x27;, outputCol=&#x27;pl_onehot_value&#x27;)pipeline = Pipeline(stages=[stringindexer, encoder])pipeline_fit = pipeline.fit(user_profile_df)user_profile_df2 = pipeline_fit.transform(user_profile_df)# pl_onehot_value列的值为稀疏向量，存储热独编码的结果user_profile_df2.printSchema()user_profile_df2.show() 显示结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+|userId|cms_segid|cms_group_id|final_gender_code|age_level|pvalue_level|shopping_level|occupation|new_user_class_level|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+| 234| 0| 5| 2| 5| -1| 3| 0| 3|| 523| 5| 2| 2| 2| 1| 3| 1| 2|| 612| 0| 8| 1| 2| 2| 3| 0| -1|| 1670| 0| 4| 2| 4| -1| 1| 0| -1|| 2545| 0| 10| 1| 4| -1| 3| 0| -1|| 3644| 49| 6| 2| 6| 2| 3| 0| 2|| 5777| 44| 5| 2| 5| 2| 3| 0| 2|| 6211| 0| 9| 1| 3| -1| 3| 0| 2|| 6355| 2| 1| 2| 1| 1| 3| 0| 4|| 6823| 43| 5| 2| 5| 2| 3| 0| 1|| 6972| 5| 2| 2| 2| 2| 3| 1| 2|| 9293| 0| 5| 2| 5| -1| 3| 0| 4|| 9510| 55| 8| 1| 2| 2| 2| 0| 2|| 10122| 33| 4| 2| 4| 2| 3| 0| 2|| 10549| 0| 4| 2| 4| 2| 3| 0| -1|| 10812| 0| 4| 2| 4| -1| 2| 0| -1|| 10912| 0| 4| 2| 4| 2| 3| 0| -1|| 10996| 0| 5| 2| 5| -1| 3| 0| 4|| 11256| 8| 2| 2| 2| 1| 3| 0| 3|| 11310| 31| 4| 2| 4| 1| 3| 0| 4|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+only showing top 20 rowsroot |-- userId: integer (nullable = true) |-- cms_segid: integer (nullable = true) |-- cms_group_id: integer (nullable = true) |-- final_gender_code: integer (nullable = true) |-- age_level: integer (nullable = true) |-- pvalue_level: string (nullable = true) |-- shopping_level: integer (nullable = true) |-- occupation: integer (nullable = true) |-- new_user_class_level: string (nullable = true)root |-- userId: integer (nullable = true) |-- cms_segid: integer (nullable = true) |-- cms_group_id: integer (nullable = true) |-- final_gender_code: integer (nullable = true) |-- age_level: integer (nullable = true) |-- pvalue_level: string (nullable = true) |-- shopping_level: integer (nullable = true) |-- occupation: integer (nullable = true) |-- new_user_class_level: string (nullable = true) |-- pl_onehot_feature: double (nullable = false) |-- pl_onehot_value: vector (nullable = true)+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+|userId|cms_segid|cms_group_id|final_gender_code|age_level|pvalue_level|shopping_level|occupation|new_user_class_level|pl_onehot_feature|pl_onehot_value|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+| 234| 0| 5| 2| 5| -1| 3| 0| 3| 0.0| (4,[0],[1.0])|| 523| 5| 2| 2| 2| 1| 3| 1| 2| 2.0| (4,[2],[1.0])|| 612| 0| 8| 1| 2| 2| 3| 0| -1| 1.0| (4,[1],[1.0])|| 1670| 0| 4| 2| 4| -1| 1| 0| -1| 0.0| (4,[0],[1.0])|| 2545| 0| 10| 1| 4| -1| 3| 0| -1| 0.0| (4,[0],[1.0])|| 3644| 49| 6| 2| 6| 2| 3| 0| 2| 1.0| (4,[1],[1.0])|| 5777| 44| 5| 2| 5| 2| 3| 0| 2| 1.0| (4,[1],[1.0])|| 6211| 0| 9| 1| 3| -1| 3| 0| 2| 0.0| (4,[0],[1.0])|| 6355| 2| 1| 2| 1| 1| 3| 0| 4| 2.0| (4,[2],[1.0])|| 6823| 43| 5| 2| 5| 2| 3| 0| 1| 1.0| (4,[1],[1.0])|| 6972| 5| 2| 2| 2| 2| 3| 1| 2| 1.0| (4,[1],[1.0])|| 9293| 0| 5| 2| 5| -1| 3| 0| 4| 0.0| (4,[0],[1.0])|| 9510| 55| 8| 1| 2| 2| 2| 0| 2| 1.0| (4,[1],[1.0])|| 10122| 33| 4| 2| 4| 2| 3| 0| 2| 1.0| (4,[1],[1.0])|| 10549| 0| 4| 2| 4| 2| 3| 0| -1| 1.0| (4,[1],[1.0])|| 10812| 0| 4| 2| 4| -1| 2| 0| -1| 0.0| (4,[0],[1.0])|| 10912| 0| 4| 2| 4| 2| 3| 0| -1| 1.0| (4,[1],[1.0])|| 10996| 0| 5| 2| 5| -1| 3| 0| 4| 0.0| (4,[0],[1.0])|| 11256| 8| 2| 2| 2| 1| 3| 0| 3| 2.0| (4,[2],[1.0])|| 11310| 31| 4| 2| 4| 1| 3| 0| 4| 2.0| (4,[2],[1.0])|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+only showing top 20 rows 使用热编码转换new_user_class_level的一维数据为多维 123456stringindexer = StringIndexer(inputCol=&#x27;new_user_class_level&#x27;, outputCol=&#x27;nucl_onehot_feature&#x27;)encoder = OneHotEncoder(dropLast=False, inputCol=&#x27;nucl_onehot_feature&#x27;, outputCol=&#x27;nucl_onehot_value&#x27;)pipeline = Pipeline(stages=[stringindexer, encoder])pipeline_fit = pipeline.fit(user_profile_df2)user_profile_df3 = pipeline_fit.transform(user_profile_df2)user_profile_df3.show() 显示结果: 12345678910111213141516171819202122232425+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+-------------------+-----------------+|userId|cms_segid|cms_group_id|final_gender_code|age_level|pvalue_level|shopping_level|occupation|new_user_class_level|pl_onehot_feature|pl_onehot_value|nucl_onehot_feature|nucl_onehot_value|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+-------------------+-----------------+| 234| 0| 5| 2| 5| -1| 3| 0| 3| 0.0| (4,[0],[1.0])| 2.0| (5,[2],[1.0])|| 523| 5| 2| 2| 2| 1| 3| 1| 2| 2.0| (4,[2],[1.0])| 1.0| (5,[1],[1.0])|| 612| 0| 8| 1| 2| 2| 3| 0| -1| 1.0| (4,[1],[1.0])| 0.0| (5,[0],[1.0])|| 1670| 0| 4| 2| 4| -1| 1| 0| -1| 0.0| (4,[0],[1.0])| 0.0| (5,[0],[1.0])|| 2545| 0| 10| 1| 4| -1| 3| 0| -1| 0.0| (4,[0],[1.0])| 0.0| (5,[0],[1.0])|| 3644| 49| 6| 2| 6| 2| 3| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|| 5777| 44| 5| 2| 5| 2| 3| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|| 6211| 0| 9| 1| 3| -1| 3| 0| 2| 0.0| (4,[0],[1.0])| 1.0| (5,[1],[1.0])|| 6355| 2| 1| 2| 1| 1| 3| 0| 4| 2.0| (4,[2],[1.0])| 3.0| (5,[3],[1.0])|| 6823| 43| 5| 2| 5| 2| 3| 0| 1| 1.0| (4,[1],[1.0])| 4.0| (5,[4],[1.0])|| 6972| 5| 2| 2| 2| 2| 3| 1| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|| 9293| 0| 5| 2| 5| -1| 3| 0| 4| 0.0| (4,[0],[1.0])| 3.0| (5,[3],[1.0])|| 9510| 55| 8| 1| 2| 2| 2| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|| 10122| 33| 4| 2| 4| 2| 3| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|| 10549| 0| 4| 2| 4| 2| 3| 0| -1| 1.0| (4,[1],[1.0])| 0.0| (5,[0],[1.0])|| 10812| 0| 4| 2| 4| -1| 2| 0| -1| 0.0| (4,[0],[1.0])| 0.0| (5,[0],[1.0])|| 10912| 0| 4| 2| 4| 2| 3| 0| -1| 1.0| (4,[1],[1.0])| 0.0| (5,[0],[1.0])|| 10996| 0| 5| 2| 5| -1| 3| 0| 4| 0.0| (4,[0],[1.0])| 3.0| (5,[3],[1.0])|| 11256| 8| 2| 2| 2| 1| 3| 0| 3| 2.0| (4,[2],[1.0])| 2.0| (5,[2],[1.0])|| 11310| 31| 4| 2| 4| 1| 3| 0| 4| 2.0| (4,[2],[1.0])| 3.0| (5,[3],[1.0])|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+-------------------+-----------------+only showing top 20 rows 用户特征合并 LR回归用的是 pyspark.ml它要求所有的特征放在一个向量里面。 123from pyspark.ml.feature import VectorAssemblerfeature_df = VectorAssembler().setInputCols([&quot;age_level&quot;, &quot;pl_onehot_value&quot;, &quot;nucl_onehot_value&quot;]).setOutputCol(&quot;features&quot;).transform(user_profile_df3)feature_df.show() 显示结果: 12345678910111213141516171819202122232425+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+-------------------+-----------------+--------------------+|userId|cms_segid|cms_group_id|final_gender_code|age_level|pvalue_level|shopping_level|occupation|new_user_class_level|pl_onehot_feature|pl_onehot_value|nucl_onehot_feature|nucl_onehot_value| features|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+-------------------+-----------------+--------------------+| 234| 0| 5| 2| 5| -1| 3| 0| 3| 0.0| (4,[0],[1.0])| 2.0| (5,[2],[1.0])|(10,[0,1,7],[5.0,...|| 523| 5| 2| 2| 2| 1| 3| 1| 2| 2.0| (4,[2],[1.0])| 1.0| (5,[1],[1.0])|(10,[0,3,6],[2.0,...|| 612| 0| 8| 1| 2| 2| 3| 0| -1| 1.0| (4,[1],[1.0])| 0.0| (5,[0],[1.0])|(10,[0,2,5],[2.0,...|| 1670| 0| 4| 2| 4| -1| 1| 0| -1| 0.0| (4,[0],[1.0])| 0.0| (5,[0],[1.0])|(10,[0,1,5],[4.0,...|| 2545| 0| 10| 1| 4| -1| 3| 0| -1| 0.0| (4,[0],[1.0])| 0.0| (5,[0],[1.0])|(10,[0,1,5],[4.0,...|| 3644| 49| 6| 2| 6| 2| 3| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|(10,[0,2,6],[6.0,...|| 5777| 44| 5| 2| 5| 2| 3| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|(10,[0,2,6],[5.0,...|| 6211| 0| 9| 1| 3| -1| 3| 0| 2| 0.0| (4,[0],[1.0])| 1.0| (5,[1],[1.0])|(10,[0,1,6],[3.0,...|| 6355| 2| 1| 2| 1| 1| 3| 0| 4| 2.0| (4,[2],[1.0])| 3.0| (5,[3],[1.0])|(10,[0,3,8],[1.0,...|| 6823| 43| 5| 2| 5| 2| 3| 0| 1| 1.0| (4,[1],[1.0])| 4.0| (5,[4],[1.0])|(10,[0,2,9],[5.0,...|| 6972| 5| 2| 2| 2| 2| 3| 1| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|(10,[0,2,6],[2.0,...|| 9293| 0| 5| 2| 5| -1| 3| 0| 4| 0.0| (4,[0],[1.0])| 3.0| (5,[3],[1.0])|(10,[0,1,8],[5.0,...|| 9510| 55| 8| 1| 2| 2| 2| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|(10,[0,2,6],[2.0,...|| 10122| 33| 4| 2| 4| 2| 3| 0| 2| 1.0| (4,[1],[1.0])| 1.0| (5,[1],[1.0])|(10,[0,2,6],[4.0,...|| 10549| 0| 4| 2| 4| 2| 3| 0| -1| 1.0| (4,[1],[1.0])| 0.0| (5,[0],[1.0])|(10,[0,2,5],[4.0,...|| 10812| 0| 4| 2| 4| -1| 2| 0| -1| 0.0| (4,[0],[1.0])| 0.0| (5,[0],[1.0])|(10,[0,1,5],[4.0,...|| 10912| 0| 4| 2| 4| 2| 3| 0| -1| 1.0| (4,[1],[1.0])| 0.0| (5,[0],[1.0])|(10,[0,2,5],[4.0,...|| 10996| 0| 5| 2| 5| -1| 3| 0| 4| 0.0| (4,[0],[1.0])| 3.0| (5,[3],[1.0])|(10,[0,1,8],[5.0,...|| 11256| 8| 2| 2| 2| 1| 3| 0| 3| 2.0| (4,[2],[1.0])| 2.0| (5,[2],[1.0])|(10,[0,3,7],[2.0,...|| 11310| 31| 4| 2| 4| 1| 3| 0| 4| 2.0| (4,[2],[1.0])| 3.0| (5,[3],[1.0])|(10,[0,3,8],[4.0,...|+------+---------+------------+-----------------+---------+------------+--------------+----------+--------------------+-----------------+---------------+-------------------+-----------------+--------------------+only showing top 20 rows 1feature_df.select(&quot;features&quot;).show() 显示结果: 12345678910111213141516171819202122232425+--------------------+| features|+--------------------+|(10,[0,1,7],[5.0,...||(10,[0,3,6],[2.0,...||(10,[0,2,5],[2.0,...||(10,[0,1,5],[4.0,...||(10,[0,1,5],[4.0,...||(10,[0,2,6],[6.0,...||(10,[0,2,6],[5.0,...||(10,[0,1,6],[3.0,...||(10,[0,3,8],[1.0,...||(10,[0,2,9],[5.0,...||(10,[0,2,6],[2.0,...||(10,[0,1,8],[5.0,...||(10,[0,2,6],[2.0,...||(10,[0,2,6],[4.0,...||(10,[0,2,5],[4.0,...||(10,[0,1,5],[4.0,...||(10,[0,2,5],[4.0,...||(10,[0,1,8],[5.0,...||(10,[0,3,7],[2.0,...||(10,[0,3,8],[4.0,...|+--------------------+only showing top 20 rows 特征选取 除了前面处理的pvalue_level和new_user_class_level需要作为特征以外，(能体现出用户的购买力特征)，还有： 前面分析的以下几个分类特征值个数情况: 123456789- cms_segid: 97- cms_group_id: 13- final_gender_code: 2- age_level: 7- shopping_level: 3- occupation: 2-pvalue_level-new_user_class_level-price 根据经验，以上几个分类特征都一定程度能体现用户在购物方面的特征，且类别都较少，都可以用来作为用户特征 总结缺失值处理 连续的特征 缺失比例比较严重 可以考虑舍弃 可以考虑使用平均值 中位数 分位数填充 算法预测 （利用样本中的其它特征作为 特征值，有缺失的特征作为目标值） 分类的特征 缺失比例比较严重 可以考虑舍弃 把缺失作为单独的分类， 如果之前的数据只有两个分类，那么把缺失考虑进来就变成3个分类 算法预测 利用算法预测缺失值 其它特征和要预测的特征之间是否有联系 样本数据是否足够 利用算法预测缺失值会引入噪声 利用随机森林预测缺失值 pyspark MLlib 基于RDD的 监督学习的样本数据要创建成LabeledPoint对象，MLlib通过LabeledPoint来训练模型 pos = LabeledPoint(目标, [特征list]) 目标值是分类情况 分类值从0开始连续增加 所有特征是double类型","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"},{"name":"推荐系统实战","slug":"机器学习/大数据的lambda架构/推荐系统实战","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"12-二叉树的路径问题汇总","slug":"12-二叉树的路径问题汇总","date":"2021-07-09T13:51:32.000Z","updated":"2021-07-09T14:01:05.938Z","comments":true,"path":"20210709/12-二叉树的路径问题汇总.html","link":"","permalink":"https://xxren8218.github.io/20210709/12-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB.html","excerpt":"","text":"二叉树的路径问题对于刚刚接触树的问题的新手而言，路径问题是一个比较棘手的问题。题解中关于二叉树路径问题的总结还偏少，今天我用一篇文章总结一下二叉树的路径问题。学透这篇文章，二叉树路径题可以秒杀 1.问题分类二叉树路径的问题大致可以分为两类： 自顶向下：顾名思义，就是从某一个节点(不一定是根节点)，从上向下寻找路径，到某一个节点(不一定是叶节点)结束具体题目如下： 257.二叉树的所有路径 面试题 04.12. 求和路径 112.路径总和 113.路径总和 II 437.路径总和 III 988.从叶结点开始的最小字符串 而继续细分的话还可以分成一般路径与给定和的路径 非自顶向下：就是从任意节点到任意节点的路径，不需要自顶向下124.二叉树中的最大路径和125.最长同值路径126.二叉树的直径 2.解题模板这类题通常用深度优先搜索(DFS)和广度优先搜索(BFS)解决，BFS较DFS繁琐，这里为了简洁只展现DFS代码下面是我对两类题目的分析与模板 一、自顶而下：DFS 12345678910111213141516171819202122232425262728###########一般路径：###########res = []def dfs(root, path)： if not root: return # 根节点为空直接返回 path.append(root.val) # 作出选择 if not root.left and not root.right: # 如果到叶节点 res.append(path) return dfs(root.left,path) # 继续递归 dfs(root.right,path) ############### 给定和的路径：###############def dfs(root, Sum, path): if not root: return Sum -= root.val path.append(root.val) if not root.left and not root.right and Sum == 0: res.append(path) return dfs(root.left, Sum, path) dfs(root.right, Sum, path) 这类题型DFS注意点： 如果是找路径和等于给定target的路径的，那么可以不用新增一个临时变量cursum来判断当前路径和，只需要用给定和target减去节点值，最终结束条件判断 target==0 即可 是否要回溯：二叉树的问题大部分是不需要回溯的，原因如下：二叉树的递归部分：dfs(root.left),dfs(root.right)已经把可能的路径穷尽了,因此到任意叶节点的路径只可能有一条，绝对不可能出现另外的路径也到这个满足条件的叶节点的; 而对比二维数组(例如迷宫问题)的DFS,for循环向四个方向查找每次只能朝向一个方向，并没有穷尽路径，因此某一个满足条件的点可能是有多条路径到该点的 并且visited数组标记已经走过的路径是会受到另外路径是否访问的影响，这时候必须回溯 找到路径后是否要return:取决于题目是否要求找到叶节点满足条件的路径,如果必须到叶节点,那么就要return;但如果是到任意节点都可以，那么必不能return,因为这条路径下面还可能有更深的路径满足条件，还要在此基础上继续递归 是否要双重递归(即调用根节点的dfs函数后，继续调用根左右节点的pathsum函数)：看题目要不要求从根节点开始的，还是从任意节点开始 二、非自顶而下：这类题目一般解题思路如下：设计一个辅助函数maxpath，调用自身求出以一个节点为根节点的左侧最长路径left和右侧最长路径right，那么经过该节点的最长路径就是left+right接着只需要从根节点开始dfs,不断比较更新全局变量即可 1234567res = 0def maxPath(root) # 以root为路径起始点的最长路径 if not root: return 0 left = maxPath(root.left) right = maxPath(root.right) res = max(res, left + right + root.val) # 更新全局变量 return max(left, right) # 返回左右路径较长者 这类题型DFS注意点： left,right代表的含义要根据题目所求设置，比如最长路径、最大路径和等等 全局变量res的初值设置是0还是INT_MIN要看题目节点是否存在负值,如果存在就用INT_MIN，否则就是0 注意两点之间路径为1，因此一个点是不能构成路径的 题目分析下面是对具体题目的分析和代码呈现 一、自顶向下257.二叉树的所有路径 直接套用模板1即可，注意把”-&gt;”放在递归调用中 12345678910111213res = &#x27;&#x27;def binaryTreePaths(root): dfs(root, &quot;&quot;) return resdef dfs(root, path): if not root: return path += str(root.val) if not root.left and not root.right: res.append(path) return dfs(root.left, path+&quot;-&gt;&quot;) dfs(root.right, path+&quot;-&gt;&quot;) 答题代码如下： 123456789101112131415161718192021222324252627# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def __init__(self): self.res = [] def binaryTreePaths(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[str] &quot;&quot;&quot; if not root: return [] path = &#x27;&#x27; self.dfs(root, path) return self.res def dfs(self, root, path): if not root: return path += str(root.val) if not root.left and not root.right: self.res.append(path) return self.dfs(root.left, path + &#x27;-&gt;&#x27;) self.dfs(root.right, path + &#x27;-&gt;&#x27;) 113.路径总和 II 直接套用模板2 123456789101112131415161718192021res = []def pathSum(root, targetSum): path = [] dfs(root, path, targetSum) return resdef dfs(root, path, Sum) if not root: return Sum -= root.val # path.append(root.val) # 注意此处传递的是引用，用append方法path的地址不会变，所以，出栈以后的函数的path值并不会减小，这里使用一个赋值语句来存储新增加的值。 # ＋相当于extend方法。 path = path + [root.val] if not root.left and not root.right and Sum == 0: res.append(path) return dfs(root.left, path, Sum) dfs(root.right, path, Sum) 完成代码： 12345678910111213141516171819202122232425262728293031# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def __init__(self): self.res = [] def pathSum(self, root, targetSum): &quot;&quot;&quot; :type root: TreeNode :type targetSum: int :rtype: List[List[int]] &quot;&quot;&quot; if not root: return [] path = [] self.dfs(root, path, targetSum) return self.res def dfs(self, root, path, Sum): if not root: return Sum -= root.val path = path + [root.val] # path.append(root.val) if not root.left and not root.right and Sum == 0: self.res.append(path) return self.dfs(root.left, path, Sum) self.dfs(root.right, path, Sum) 437.路径总和 III 双重递归：先调用dfs函数从root开始查找路径，再调用pathsum函数到root左右子树开始查找套用模板2 123456789101112131415count = 0def pathSum(root, targetSum) if not root: return 0 dfs1(root, targetSum) # 以root为起始点查找路径 pathSum(root.left, targetSum) # 左子树递归 pathSum(root.right, targetSum) # 右子树递归 return countdef dfs(root, Sum): if not root: return Sum -= root.val if Sum == 0: # 注意不要return,因为不要求到叶节点结束,所以一条路径下面还可能有另一条 count += 1 # 如果找到了一个路径全局变量就+1 dfs1(root.left, Sum) dfs1(root.right, Sum) 完成代码如下： 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def __init__(self): self.count = 0 def pathSum(self, root, targetSum): &quot;&quot;&quot; :type root: TreeNode :type targetSum: int :rtype: int &quot;&quot;&quot; if not root: return 0 self.dfs1(root, targetSum) self.pathSum(root.left, targetSum) self.pathSum(root.right, targetSum) return self.count def dfs1(self, root, sum): if not root: return sum -= root.val if sum == 0: self.count += 1 self.dfs1(root.left, sum) self.dfs1(root.right, sum) 988.从叶结点开始的最小字符串 换汤不换药，套用模板1 1234567891011121314151617181920path = []def smallestFromLeaf(root): dfs(root, &quot;&quot;) path = path.sort() # 升序排序 return path[0]def dfs(root, s): if not root: return s += chr(ord(&#x27;a&#x27;) + root.val) if not root.left and not root.right: s = s[::-1] # 题目要求从叶子节点到根节点，因此反转 path.append(s) return dfs(root.left, s) dfs(root.right, s) 整体代码如下： 1234567891011121314151617181920212223242526272829303132# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def __init__(self): self.path = [] def smallestFromLeaf(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: str &quot;&quot;&quot; if not root: return None self.dfs(root, &quot;&quot;) self.path.sort() # 升序排序 return self.path[0] def dfs(self, root, s): if not root: return s += chr(ord(&#x27;a&#x27;) + root.val) if not root.left and not root.right: s = s[::-1] # 题目要求从叶子节点到根节点，因此反转 self.path.append(s) return self.dfs(root.left, s) self.dfs(root.right, s) 二、非自顶向下124.二叉树中的最大路径和right分别为根节点左右子树最大路径和, 注意：如果最大路径和&lt;0,意味着该路径和对总路径和做负贡献，因此不要计入到总路径中，将它设置为0 123456789101112res = -float(&#x27;inf&#x27;) # 注意节点值可能为负数，因此要设置为最小值def maxPathSum(root): maxPath(root) return resdef maxPath(root): # 以root为路径起始点的最长路径 if not root: return 0 left = max(maxPath(root.left), 0) right = max(maxPath(root.right), 0) res = max(res, left + right + root.val) # 比较当前最大路径和与左右子树最长路径加上根节点值的较大值，更新全局变量 return max(left + root.val, right + root.val) # 返回左右子树较大的路径和加上根节点值 完整代码如下： 123456789101112131415161718192021222324# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def __init__(self): self.res = -float(&#x27;inf&#x27;) def maxPathSum(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return 0 self.maxPath(root) return self.res def maxPath(self, root): if not root: return 0 left = max(self.maxPath(root.left), 0) right = max(self.maxPath(root.right), 0) self.res = max(self.res, left + right + root.val) return max(left + root.val, right + root.val) 687.最长同值路径 123456789101112131415161718192021res = 0def longestUnivaluePath(root): if not root: return 0 longestPath(root) return resdef longestPath(root): if not root: return 0 left = longestPath(root.left) right = longestPath(root.right) # 如果存在左子节点和根节点同值，更新左最长路径;否则左最长路径为0 if root.left and root.val == root.left.val: left += 1 else: left = 0 if root.right and root.val == root.right.val: right += 1 else: right = 0 res = max(res, left + right) return max(left, right) 完整代码如下： 1234567891011121314151617181920212223242526272829303132# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def __init__(self): self.res = 0 def longestUnivaluePath(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return 0 self.longestPath(root) return self.res def longestPath(self, root): if not root: return 0 left = self.longestPath(root.left) right = self.longestPath(root.right) if root.left and root.left.val == root.val: left += 1 else: left = 0 if root.right and root.right.val == root.val: right += 1 else: right = 0 self.res = max(self.res, left + right) return max(left, right) 543.二叉树的直径 1234567891011121314151617181920212223res = 0def diameterOfBinaryTree(root): maxPath(root) return resdef maxPath(root):# 这里递归结束条件要特别注意：不能是not root(而且不需要判断root为空,因为只有非空才会进入递归)，因为单个节点路径长也是0 if not root.left and not root.right: return 0 # 判断左子节点是否为空，从而更新左边最长路径 if root.left: left = maxPath(root.left) + 1 else: left = 0 if root.right: right = maxPath(root.right) + 1 else: right = 0 res = max(res, left + right) # 更新全局变量 return max(left, right) # 返回左右路径较大者 完整代码如下： 123456789101112131415161718192021222324252627282930313233# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def __init__(self): self.res = 0 def diameterOfBinaryTree(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; self.maxPath(root) return self.res def maxPath(self, root): if not root.left and not root.right: return 0 # 判断左子节点是否为空，从而更新左边最长路径 if root.left: left = self.maxPath(root.left) + 1 else: left = 0 if root.right: right = self.maxPath(root.right) + 1 else: right = 0 self.res = max(self.res, left + right) # 更新全局变量 return max(left, right) # 返回左右路径较大者","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"02-推荐系统实战之根据用户行为数据创建ALS模型并召回商品","slug":"02-推荐系统实战之根据用户行为数据创建ALS模型并召回商品","date":"2021-07-08T17:00:30.000Z","updated":"2021-07-19T09:39:18.362Z","comments":true,"path":"20210709/02-推荐系统实战之根据用户行为数据创建ALS模型并召回商品.html","link":"","permalink":"https://xxren8218.github.io/20210709/02-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98%E4%B9%8B%E6%A0%B9%E6%8D%AE%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE%E5%88%9B%E5%BB%BAALS%E6%A8%A1%E5%9E%8B%E5%B9%B6%E5%8F%AC%E5%9B%9E%E5%95%86%E5%93%81.html","excerpt":"","text":"根据用户行为数据创建ALS模型并召回商品 打开HDFS，Hadoop下的sbin目录下的./start-dfs.sh 打开Spark, Spark下的sbin目录下的./start-master.sh -h 192.168.19.2 Spark下的sbin目录下的./start-slave.sh spark://192.168.19.2:7077 可以使用192.168.19.2:8080进行可视化查看 进入虚拟环境 workon 虚拟环境名字（有所需的工具包：如jupyter notebook） jupyter notebook —ip 0.0.0.0 0. 用户行为数据拆分 海量数据处理应该怎么办？2T数据的处理，不至于在Excel中处理吧。 这里说一个面试题：给你2T的邮箱数据，如何去重排序？ 外排序：分成多块，去重排序，然后再合并。 方便练习可以对数据做拆分处理 pandas的数据分批读取 chunk 厚厚的一块 相当大的数量或部分 123456789101112131415161718import pandas as pdreader = pd.read_csv(&#x27;behavior_log.csv&#x27;,chunksize=100,iterator=True) &quot;&quot;&quot;chunksize 一次数据读多少条。iterator 是否返回可迭代对象。&quot;&quot;&quot;count = 0for chunk in reader: count += 1 if count == 1: chunk.to_csv(&#x27;test4.csv&#x27;,index = False) # index = False 去掉自动添加行索引。保留列索引 elif count &gt; 1 and count &lt; 1000: chunk.to_csv(&#x27;test4.csv&#x27;,index = False, mode = &#x27;a&#x27;, header = False) # mode = ‘a’ 表示追加模式，去掉自动添加行索引，去掉列索引。 else: breakpd.read_csv(&#x27;test4.csv&#x27;) 1. 预处理behavior_log数据集创建Spark的连接，通过SparkSQL将数据加载进来，进行简单分析。 创建spark session 123456789101112131415161718192021222324252627282930313233import os# 配置spark driver和pyspark运行时，所使用的python解释器路径PYSPARK_PYTHON = &quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;JAVA_HOME=&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;# 当存在多个版本时，不指定很可能会导致出错os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHONos.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHONos.environ[&#x27;JAVA_HOME&#x27;]=JAVA_HOME# spark配置信息from pyspark import SparkConffrom pyspark.sql import SparkSessionSPARK_APP_NAME = &quot;preprocessingBehaviorLog&quot;SPARK_URL = &quot;spark://192.168.19.2:7077&quot;conf = SparkConf() # 创建spark config对象config = ( (&quot;spark.app.name&quot;, SPARK_APP_NAME), # 设置启动的spark的app名称，没有提供，将随机产生一个名称 (&quot;spark.executor.memory&quot;, &quot;6g&quot;), # 设置该app启动时占用的内存用量，默认1g (&quot;spark.master&quot;, SPARK_URL), # spark master的地址 (&quot;spark.executor.cores&quot;, &quot;4&quot;), # 设置spark executor使用的CPU核心数 # 以下三项配置，可以控制执行器数量# (&quot;spark.dynamicAllocation.enabled&quot;, True),# (&quot;spark.dynamicAllocation.initialExecutors&quot;, 1), # 1个执行器# (&quot;spark.shuffle.service.enabled&quot;, True)# (&#x27;spark.sql.pivotMaxValues&#x27;, &#x27;99999&#x27;), # 当需要pivot DF，且值很多时，需要修改，默认是10000)# 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.htmlconf.setAll(config)# 利用config对象，创建spark sessionspark = SparkSession.builder.config(conf=conf).getOrCreate() 从hdfs中加载csv文件为DataFrame 12345# 从hdfs加载CSV文件为DataFramedf = spark.read.csv(&quot;hdfs://localhost:9000/datasets/behavior_log.csv&quot;, header=True)df.show() # 查看dataframe，默认显示前20条# 大致查看一下数据类型df.printSchema() # 打印当前dataframe的结构 显示结果: 1234567891011121314151617181920212223242526272829303132+------+----------+----+-----+------+| user|time_stamp|btag| cate| brand|+------+----------+----+-----+------+|558157|1493741625| pv| 6250| 91286||558157|1493741626| pv| 6250| 91286||558157|1493741627| pv| 6250| 91286||728690|1493776998| pv|11800| 62353||332634|1493809895| pv| 1101|365477||857237|1493816945| pv| 1043|110616||619381|1493774638| pv| 385|428950||467042|1493772641| pv| 8237|301299||467042|1493772644| pv| 8237|301299||991528|1493780710| pv| 7270|274795||991528|1493780712| pv| 7270|274795||991528|1493780712| pv| 7270|274795||991528|1493780712| pv| 7270|274795||991528|1493780714| pv| 7270|274795||991528|1493780765| pv| 7270|274795||991528|1493780714| pv| 7270|274795||991528|1493780765| pv| 7270|274795||991528|1493780764| pv| 7270|274795||991528|1493780633| pv| 7270|274795||991528|1493780764| pv| 7270|274795|+------+----------+----+-----+------+only showing top 20 rowsroot |-- user: string (nullable = true) |-- time_stamp: string (nullable = true) |-- btag: string (nullable = true) |-- cate: string (nullable = true) |-- brand: string (nullable = true) 从hdfs加载数据为dataframe，并设置结构—schema 12345678910111213from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType# 构建结构对象schema = StructType([ StructField(&quot;userId&quot;, IntegerType()), StructField(&quot;timestamp&quot;, LongType()), StructField(&quot;btag&quot;, StringType()), StructField(&quot;cateId&quot;, IntegerType()), StructField(&quot;brandId&quot;, IntegerType())])# 从hdfs加载数据为dataframe，并设置结构behavior_log_df = spark.read.csv(&quot;hdfs://localhost:8020/datasets/behavior_log.csv&quot;, header=True, schema=schema)behavior_log_df.show()behavior_log_df.count() 显示结果: 1234567891011121314151617181920212223242526272829303132+------+----------+----+------+-------+|userId| timestamp|btag|cateId|brandId|+------+----------+----+------+-------+|558157|1493741625| pv| 6250| 91286||558157|1493741626| pv| 6250| 91286||558157|1493741627| pv| 6250| 91286||728690|1493776998| pv| 11800| 62353||332634|1493809895| pv| 1101| 365477||857237|1493816945| pv| 1043| 110616||619381|1493774638| pv| 385| 428950||467042|1493772641| pv| 8237| 301299||467042|1493772644| pv| 8237| 301299||991528|1493780710| pv| 7270| 274795||991528|1493780712| pv| 7270| 274795||991528|1493780712| pv| 7270| 274795||991528|1493780712| pv| 7270| 274795||991528|1493780714| pv| 7270| 274795||991528|1493780765| pv| 7270| 274795||991528|1493780714| pv| 7270| 274795||991528|1493780765| pv| 7270| 274795||991528|1493780764| pv| 7270| 274795||991528|1493780633| pv| 7270| 274795||991528|1493780764| pv| 7270| 274795|+------+----------+----+------+-------+only showing top 20 rowsroot |-- userId: integer (nullable = true) |-- timestamp: long (nullable = true) |-- btag: string (nullable = true) |-- cateId: integer (nullable = true) |-- brandId: integer (nullable = true) 分析数据集字段的类型和格式 查看是否有空值 查看每列数据的类型 查看每列数据的类别情况 1234print(&quot;查看userId的数据情况：&quot;, behavior_log_df.groupBy(&quot;userId&quot;).count().count()) # 第一个count是将相同的用户放在同一组内。，再count数数。# 约113w用户#注意：behavior_log_df.groupBy(&quot;userId&quot;).count() 返回的是一个dataframe，这里的count计算的是每一个分组的个数，但当前还没有进行计算# 当调用df.count()时才开始进行计算，这里的count计算的是dataframe的条目数，也就是共有多少个分组 1查看user的数据情况： 1136340 123print(&quot;查看btag的数据情况：&quot;, behavior_log_df.groupBy(&quot;btag&quot;).count().collect()) # collect会把计算结果全部加载到内存，谨慎使用# 只有四种类型数据：pv、fav、cart、buy# 这里由于类型只有四个，所以直接使用collect，把数据全部加载出来 1查看btag的数据情况： [Row(btag=&#x27;buy&#x27;, count=9115919), Row(btag=&#x27;fav&#x27;, count=9301837), Row(btag=&#x27;cart&#x27;, count=15946033), Row(btag=&#x27;pv&#x27;, count=688904345)] 12print(&quot;查看cateId的数据情况：&quot;, behavior_log_df.groupBy(&quot;cateId&quot;).count().count())# 约12968类别id 1查看cateId的数据情况： 12968 12print(&quot;查看brandId的数据情况：&quot;, behavior_log_df.groupBy(&quot;brandId&quot;).count().count())# 约460561品牌id 1查看brandId的数据情况： 460561 123print(&quot;判断数据是否有空值：&quot;, behavior_log_df.count(), behavior_log_df.dropna().count())# 约7亿条目723268134 723268134# 本数据集无空值条目，可放心处理 1判断数据是否有空值： 723268134 723268134 pivot透视操作，把某列里的字段值转换成行并进行聚合运算(pyspark.sql.GroupedData.pivot) 如果透视的字段中的不同属性值超过10000个，则需要设置spark.sql.pivotMaxValues，否则计算过程中会出现错误。文档介绍。 123# 统计每个用户对各类商品的pv、fav、cart、buy数量cate_count_df = behavior_log_df.groupBy(behavior_log_df.userId, behavior_log_df.cateId).pivot(&quot;btag&quot;,[&quot;pv&quot;,&quot;fav&quot;,&quot;cart&quot;,&quot;buy&quot;]).count() # 默认按照字典排序的，想要按重要程度排序，在里面穿值。此处已经传了。[&quot;pv&quot;,&quot;fav&quot;,&quot;cart&quot;,&quot;buy&quot;]cate_count_df.printSchema() # 此时还没有开始计算 显示效果: 1234567root |-- userId: integer (nullable = true) |-- cateId: integer (nullable = true) |-- pv: long (nullable = true) |-- fav: long (nullable = true) |-- cart: long (nullable = true) |-- buy: long (nullable = true) 统计每个用户对各个品牌的pv、fav、cart、buy数量并保存结果 12345678# 统计每个用户对各个品牌的pv、fav、cart、buy数量brand_count_df = behavior_log_df.groupBy(behavior_log_df.userId, behavior_log_df.brandId).pivot(&quot;btag&quot;,[&quot;pv&quot;,&quot;fav&quot;,&quot;cart&quot;,&quot;buy&quot;]).count()# brand_count_df.show() # 同上# 113w * 46w# 由于运算时间比较长，所以这里先将结果存储起来，供后续其他操作使用# 写入数据时才开始计算cate_count_df.write.csv(&quot;hdfs://localhost:9000/preprocessing_dataset/cate_count.csv&quot;, header=True)brand_count_df.write.csv(&quot;hdfs://localhost:9000/preprocessing_dataset/brand_count.csv&quot;, header=True) 2. 根据用户对类目偏好打分训练ALS模型 根据您统计的次数 + 打分规则 ==&gt; 偏好打分数据集 ==&gt; ALS模型 123456789101112131415161718192021# spark ml的模型训练是基于内存的，如果数据过大，内存空间小，迭代次数过多的化，可能会造成内存溢出，报错# 设置Checkpoint的话，会把所有数据落盘，这样如果异常退出，下次重启后，可以接着上次的训练节点继续运行# 但该方法其实指标不治本，因为无法防止内存溢出，所以还是会报错# 如果数据量大，应考虑的是增加内存、或限制迭代次数和训练数据量级等spark.sparkContext.setCheckpointDir(&quot;/checkPoint/&quot;)from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType# 构建结构对象schema = StructType([ StructField(&quot;userId&quot;, IntegerType()), StructField(&quot;cateId&quot;, IntegerType()), StructField(&quot;pv&quot;, IntegerType()), StructField(&quot;fav&quot;, IntegerType()), StructField(&quot;cart&quot;, IntegerType()), StructField(&quot;buy&quot;, IntegerType())])# 从hdfs加载CSV文件cate_count_df = spark.read.csv(&quot;hdfs://localhost:9000/preprocessing_dataset/cate_count.csv&quot;, header=True, schema=schema)cate_count_df.printSchema()cate_count_df.first() # 第一行数据 显示结果: 123456789root |-- userId: integer (nullable = true) |-- cateId: integer (nullable = true) |-- pv: integer (nullable = true) |-- fav: integer (nullable = true) |-- cart: integer (nullable = true) |-- buy: integer (nullable = true)Row(userId=1061650, cateId=4520, pv=2326, fav=None, cart=53, buy=None) 处理每一行数据：r表示row对象 12345678910111213141516171819202122232425def process_row(r): # 处理每一行数据：r表示row对象 # 偏好评分规则： # m: 用户对应的行为次数 # 该偏好权重比例，次数上限仅供参考，具体数值应根据产品业务场景权衡 # pv: if m&lt;=20: score=0.2*m; else score=4 # fav: if m&lt;=20: score=0.4*m; else score=8 # cart: if m&lt;=20: score=0.6*m; else score=12 # buy: if m&lt;=20: score=1*m; else score=20 # 注意这里要全部设为浮点数，spark运算时对类型比较敏感，要保持数据类型都一致 pv_count = r.pv if r.pv else 0.0 fav_count = r.fav if r.fav else 0.0 cart_count = r.cart if r.cart else 0.0 buy_count = r.buy if r.buy else 0.0 pv_score = 0.2*pv_count if pv_count&lt;=20 else 4.0 fav_score = 0.4*fav_count if fav_count&lt;=20 else 8.0 cart_score = 0.6*cart_count if cart_count&lt;=20 else 12.0 buy_score = 1.0*buy_count if buy_count&lt;=20 else 20.0 rating = pv_score + fav_score + cart_score + buy_score # 返回用户ID、分类ID、用户对分类的偏好打分 return r.userId, r.cateId, rating 返回一个PythonRDD类型 1234# 返回一个PythonRDD类型，此时还没开始计算# 先转化为RDD再进行map，一条数据一条数据算。虽然DF也可以用UDF，但是麻烦！处理好好再转为DF# 并不是所有的RDD都能转为DF，必须有结构的才行。Schema才可以。此处可以是因为，他就是DF转过去的。cate_count_df.rdd.map(process_row).toDF([&quot;userId&quot;, &quot;cateId&quot;, &quot;rating&quot;]) 显示结果: 1DataFrame[userId: bigint, cateId: bigint, rating: double] 用户对商品类别的打分数据 123456789101112# 用户对商品类别的打分数据# map返回的结果是rdd类型，需要调用toDF方法转换为Dataframecate_rating_df = cate_count_df.rdd.map(process_row).toDF([&quot;userId&quot;, &quot;cateId&quot;, &quot;rating&quot;])# 注意：toDF不是每个rdd都有的方法，仅局限于此处的rdd# 可通过该方法获得 user-cate-matrix# 但由于cateId字段过多，这里运算量比很大，机器内存要求很高才能执行，否则无法完成任务# 请谨慎使用# 但好在我们训练ALS模型时，不需要转换为user-cate-matrix，所以这里可以不用运行# cate_rating_df.groupBy(&quot;userId&quot;).povit(&quot;cateId&quot;).min(&quot;rating&quot;)# 用户对类别的偏好打分数据cate_rating_df 显示结果: 1DataFrame[userId: bigint, cateId: bigint, rating: double] 通常如果USER-ITEM打分数据应该是通过一下方式进行处理转换为USER-ITEM-MATRIX 但这里我们将使用的Spark的ALS模型进行CF推荐，因此注意这里数据输入不需要提前转换为矩阵，直接是 USER-ITEM-RATE的数据 基于Spark的ALS隐因子模型进行CF评分预测 ALS的意思是交替最小二乘法（Alternating Least Squares），是Spark2.*中加入的进行基于模型的协同过滤（model-based CF）的推荐系统算法。 同SVD，它也是一种矩阵分解技术，对数据进行降维处理。 详细使用方法：pyspark.ml.recommendation.ALS 注意：由于数据量巨大，因此这里也不考虑基于内存的CF算法 参考：为什么Spark中只有ALS 123456789# 使用pyspark中的ALS矩阵分解方法实现CF评分预测# 文档地址：https://spark.apache.org/docs/2.2.2/api/python/pyspark.ml.html?highlight=vectors#module-pyspark.ml.recommendationfrom pyspark.ml.recommendation import ALS # ml：dataframe， mllib：rdd# 利用打分数据，训练ALS模型als = ALS(userCol=&#x27;userId&#x27;, itemCol=&#x27;cateId&#x27;, ratingCol=&#x27;rating&#x27;, checkpointInterval=5) # 训练五步缓存一次。# 此处训练时间较长model = als.fit(cate_rating_df) 模型训练好后，调用方法进行使用，具体API查看 123456# model.recommendForAllUsers(N) 给所有用户推荐TOP-N个物品ret = model.recommendForAllUsers(3)# 由于是给所有用户进行推荐，此处运算时间也较长ret.show()# 推荐结果存放在recommendations列中，ret.select(&quot;recommendations&quot;).show() 显示结果: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051+------+--------------------+|userId| recommendations|+------+--------------------+| 148|[[3347, 12.547271...|| 463|[[1610, 9.250818]...|| 471|[[1610, 10.246621...|| 496|[[1610, 5.162216]...|| 833|[[5607, 9.065482]...|| 1088|[[104, 6.886987],...|| 1238|[[5631, 14.51981]...|| 1342|[[5720, 10.89842]...|| 1580|[[5731, 8.466453]...|| 1591|[[1610, 12.835257...|| 1645|[[1610, 11.968531...|| 1829|[[1610, 17.576496...|| 1959|[[1610, 8.353473]...|| 2122|[[1610, 12.652732...|| 2142|[[1610, 12.48068]...|| 2366|[[1610, 11.904813...|| 2659|[[5607, 11.699315...|| 2866|[[1610, 7.752719]...|| 3175|[[3347, 2.3429515...|| 3749|[[1610, 3.641833]...|+------+--------------------+only showing top 20 rows+--------------------+| recommendations|+--------------------+|[[3347, 12.547271...||[[1610, 9.250818]...||[[1610, 10.246621...||[[1610, 5.162216]...||[[5607, 9.065482]...||[[104, 6.886987],...||[[5631, 14.51981]...||[[5720, 10.89842]...||[[5731, 8.466453]...||[[1610, 12.835257...||[[1610, 11.968531...||[[1610, 17.576496...||[[1610, 8.353473]...||[[1610, 12.652732...||[[1610, 12.48068]...||[[1610, 11.904813...||[[5607, 11.699315...||[[1610, 7.752719]...||[[3347, 2.3429515...||[[1610, 3.641833]...|+--------------------+only showing top 20 rows model.recommendForUserSubset 给部分用户推荐TOP-N个物品 12345678910# 注意：recommendForUserSubset API，2.2.2版本中无法使用dataset = spark.createDataFrame([[1],[2],[3]])# 若不指定行索引，会有个默认的&#x27;_1&#x27;,将其改为&#x27;userId&#x27;dataset = dataset.withColumnRenamed(&quot;_1&quot;, &quot;userId&quot;) # 指定用户 推荐物品 参数1 要给哪些用户推荐（用户id的dataframe） 参数2 给这些用户推荐几个物品ret = model.recommendForUserSubset(dataset, 3)# 只给部分用推荐，运算时间短ret.show()ret.collect() # 注意： collect会将所有数据加载到内存，慎用 显示结果: 1234567891011+------+--------------------+|userId| recommendations|+------+--------------------+| 1|[[1610, 25.4989],...|| 3|[[5607, 13.665942...|| 2|[[5579, 5.9051886...|+------+--------------------+[Row(userId=1, recommendations=[Row(cateId=1610, rating=25.498899459838867), Row(cateId=5737, rating=24.901548385620117), Row(cateId=3347, rating=20.736785888671875)]), Row(userId=3, recommendations=[Row(cateId=5607, rating=13.665942192077637), Row(cateId=1610, rating=11.770171165466309), Row(cateId=3347, rating=10.35690689086914)]), Row(userId=2, recommendations=[Row(cateId=5579, rating=5.90518856048584), Row(cateId=2447, rating=5.624575138092041), Row(cateId=5690, rating=5.2555742263793945)])] transform中提供userId和cateId可以对打分进行预测，利用打分结果排序后 1234567891011# transform中提供userId和cateId可以对打分进行预测，利用打分结果排序后，同样可以实现TOP-N的推荐model.transform# 将模型进行存储model.save(&quot;hdfs://localhost:8020/models/userCateRatingALSModel.obj&quot;)# 测试存储的模型from pyspark.ml.recommendation import ALSModel# 从hdfs加载之前存储的模型als_model = ALSModel.load(&quot;hdfs://localhost:8020/models/userCateRatingALSModel.obj&quot;)# model.recommendForAllUsers(N) 给用户推荐TOP-N个物品result = als_model.recommendForAllUsers(3)result.show() 显示结果: 12345678910111213141516171819202122232425+------+--------------------+|userId| recommendations|+------+--------------------+| 148|[[3347, 12.547271...|| 463|[[1610, 9.250818]...|| 471|[[1610, 10.246621...|| 496|[[1610, 5.162216]...|| 833|[[5607, 9.065482]...|| 1088|[[104, 6.886987],...|| 1238|[[5631, 14.51981]...|| 1342|[[5720, 10.89842]...|| 1580|[[5731, 8.466453]...|| 1591|[[1610, 12.835257...|| 1645|[[1610, 11.968531...|| 1829|[[1610, 17.576496...|| 1959|[[1610, 8.353473]...|| 2122|[[1610, 12.652732...|| 2142|[[1610, 12.48068]...|| 2366|[[1610, 11.904813...|| 2659|[[5607, 11.699315...|| 2866|[[1610, 7.752719]...|| 3175|[[3347, 2.3429515...|| 3749|[[1610, 3.641833]...|+------+--------------------+only showing top 20 rows 召回到redis 123456789101112131415161718import redishost = &quot;192.168.19.8&quot;port = 6379 # 召回到redisdef recall_cate_by_cf(partition): # 建立redis 连接池 pool = redis.ConnectionPool(host=host, port=port) # 建立redis客户端 client = redis.Redis(connection_pool=pool) for row in partition: client.hset(&quot;recall_cate&quot;, row.userId, [i.cateId for i in row.recommendations])# 对每个分片的数据进行处理 #mapPartition Transformation map（一条一条走，和数据库建立链接耗时间） 而此处的partation是多块走，transformer的操作（）# foreachPartition Action操作 foreachRDD 一块一块的走。是action的操作（一块召回一次）result.foreachPartition(recall_cate_by_cf)# 注意：这里这是召回的是用户最感兴趣的n个类别# 总的条目数，查看redis中总的条目数是否一致result.count() 显示结果: 11136340 3. 根据用户对品牌偏好打分训练ALS模型(与上面的套路一样)1234567891011121314151617181920212223242526272829303132333435363738394041from pyspark.sql.types import StructType, StructField, StringType, IntegerTypeschema = StructType([ StructField(&quot;userId&quot;, IntegerType()), StructField(&quot;brandId&quot;, IntegerType()), StructField(&quot;pv&quot;, IntegerType()), StructField(&quot;fav&quot;, IntegerType()), StructField(&quot;cart&quot;, IntegerType()), StructField(&quot;buy&quot;, IntegerType())])# 从hdfs加载预处理好的品牌的统计数据brand_count_df = spark.read.csv(&quot;hdfs://localhost:8020/preprocessing_dataset/brand_count.csv&quot;, header=True, schema=schema)# brand_count_df.show()def process_row(r): # 处理每一行数据：r表示row对象 # 偏好评分规则： # m: 用户对应的行为次数 # 该偏好权重比例，次数上限仅供参考，具体数值应根据产品业务场景权衡 # pv: if m&lt;=20: score=0.2*m; else score=4 # fav: if m&lt;=20: score=0.4*m; else score=8 # cart: if m&lt;=20: score=0.6*m; else score=12 # buy: if m&lt;=20: score=1*m; else score=20 # 注意这里要全部设为浮点数，spark运算时对类型比较敏感，要保持数据类型都一致 pv_count = r.pv if r.pv else 0.0 fav_count = r.fav if r.fav else 0.0 cart_count = r.cart if r.cart else 0.0 buy_count = r.buy if r.buy else 0.0 pv_score = 0.2*pv_count if pv_count&lt;=20 else 4.0 fav_score = 0.4*fav_count if fav_count&lt;=20 else 8.0 cart_score = 0.6*cart_count if cart_count&lt;=20 else 12.0 buy_score = 1.0*buy_count if buy_count&lt;=20 else 20.0 rating = pv_score + fav_score + cart_score + buy_score # 返回用户ID、品牌ID、用户对品牌的偏好打分 return r.userId, r.brandId, rating# 用户对品牌的打分数据brand_rating_df = brand_count_df.rdd.map(process_row).toDF([&quot;userId&quot;, &quot;brandId&quot;, &quot;rating&quot;])# brand_rating_df.show() 基于Spark的ALS隐因子模型进行CF评分预测 ALS的意思是交替最小二乘法（Alternating Least Squares），是Spark中进行基于模型的协同过滤（model-based CF）的推荐系统算法，也是目前Spark内唯一一个推荐算法。 同SVD，它也是一种矩阵分解技术，但理论上，ALS在海量数据的处理上要优于SVD。 更多了解：pyspark.ml.recommendation.ALS 注意：由于数据量巨大，因此这里不考虑基于内存的CF算法 参考：为什么Spark中只有ALS 使用pyspark中的ALS矩阵分解方法实现CF评分预测 12345678910111213141516171819# 使用pyspark中的ALS矩阵分解方法实现CF评分预测# 文档地址：https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=vectors#module-pyspark.ml.recommendationfrom pyspark.ml.recommendation import ALSals = ALS(userCol=&#x27;userId&#x27;, itemCol=&#x27;brandId&#x27;, ratingCol=&#x27;rating&#x27;, checkpointInterval=2)# 利用打分数据，训练ALS模型# 此处训练时间较长model = als.fit(brand_rating_df)# model.recommendForAllUsers(N) 给用户推荐TOP-N个物品model.recommendForAllUsers(3).show()# 将模型进行存储model.save(&quot;hdfs://localhost:9000/models/userBrandRatingModel.obj&quot;)# 测试存储的模型from pyspark.ml.recommendation import ALSModel# 从hdfs加载模型my_model = ALSModel.load(&quot;hdfs://localhost:9000/models/userBrandRatingModel.obj&quot;)my_model# model.recommendForAllUsers(N) 给用户推荐TOP-N个物品my_model.recommendForAllUsers(3).first() 4.小结spark 训练 ALS 模型 spark 机器学习相关的库 spark MLlib 最早开发的 基于RDD 的api 目前已经停止维护了 （从2.3开始停止维护） 还可以使用 spark ML 目前在更新的是这个库 基于dataframe ALS 模型训练 spark ML的库中封装了 协同过滤的 ALS模型 from pyspark.ml.recommendation import ALS 需要准备一个dataframe 包含 用户id 物品id 用户-物品评分 这三列，利用这三列数据就可以使用spark ALS模块训练ALS模型 123from pyspark.ml.recommendation import ALSals = ALS(userCol = &#x27;userId&#x27;,itemCol=&#x27;cateId&#x27;,ratingCol = &#x27;rating&#x27;,checkpointInterval = 5)model = als.fit(dataframe) 训练出模型之后就可以为用户召回物品 12345model.recommendForAllUsers(3)#为指定用户推荐物品dataset = spark.createDataFrame([[1],[2],[3]])dataset = dataset.withColumnRenamed(&quot;_1&quot;, &quot;userId&quot;)ret = model.recommendForUserSubset(dataset, 3)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"},{"name":"推荐系统实战","slug":"机器学习/大数据的lambda架构/推荐系统实战","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"01-推荐系统实战之个性化电商广告推荐系统介绍","slug":"01—推荐系统实战之个性化电商广告推荐系统介绍","date":"2021-07-07T17:10:24.000Z","updated":"2021-07-19T09:39:01.480Z","comments":true,"path":"20210708/01—推荐系统实战之个性化电商广告推荐系统介绍.html","link":"","permalink":"https://xxren8218.github.io/20210708/01%E2%80%94%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98%E4%B9%8B%E4%B8%AA%E6%80%A7%E5%8C%96%E7%94%B5%E5%95%86%E5%B9%BF%E5%91%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D.html","excerpt":"","text":"一 个性化电商广告推荐系统介绍1 数据集介绍 Ali_Display_Ad_Click是阿里巴巴提供的一个淘宝展示广告点击率预估数据集 数据集来源：天池竞赛 数据集-阿里云天池 (aliyun.com) 原始样本骨架raw_sample 淘宝网站中随机抽样了114万用户8天内的广告展示/点击日志（2600万条记录），构成原始的样本骨架。 字段说明如下： user_id：脱敏过的用户ID； adgroup_id：脱敏过的广告单元ID； time_stamp：时间戳； pid：资源位； noclk：为1代表没有点击；为0代表点击； clk：为0代表没有点击；为1代表点击； 此处的点与没点通过埋点来实现（JS代码）。 有两个点和没点数据，是记录展示了什么数据，他没点。（看了没点与压根没看到区别） 得通过两个埋点对比得到结果，一个记录曝光，一个记录点击的。 用前面7天的做训练样本（20170506-20170512），用第8天的做测试样本（20170513） 广告基本信息表ad_feature 本数据集涵盖了raw_sample中全部广告的基本信息(约80万条目)。字段说明如下： adgroup_id：脱敏过的广告ID； cate_id：脱敏过的商品类目ID； campaign_id：脱敏过的广告计划ID； customer_id: 脱敏过的广告主ID； brand_id：脱敏过的品牌ID； price: 宝贝的价格 其中一个广告ID对应一个商品（宝贝），一个宝贝属于一个类目，一个宝贝属于一个品牌。 用户基本信息表user_profile 本数据集涵盖了raw_sample中全部用户的基本信息(约100多万用户)。字段说明如下： userid：脱敏过的用户ID； cms_segid：微群ID； cms_group_id：cms_group_id； final_gender_code：性别 1:男,2:女； age_level：年龄层次； 1234 pvalue_level：消费档次，1:低档，2:中档，3:高档； shopping_level：购物深度，1:浅层用户,2:中度用户,3:深度用户 occupation：是否大学生 ，1:是,0:否 new_user_class_level：城市层级 用户的行为日志behavior_log 本数据集涵盖了raw_sample中全部用户22天内的购物行为(共七亿条记录)。字段说明如下： user：脱敏过的用户ID；time_stamp：时间戳；btag：行为类型, 包括以下四种：​ 类型 | 说明​ pv | 浏览​ cart | 加入购物车​ fav | 收藏​ buy | 购买cate_id：脱敏过的商品类目id；brand_id: 脱敏过的品牌id；这里以user + time_stamp为key，会有很多重复的记录；这是因为我们的不同的类型的行为数据是不同部门记录的，在打包到一起的时候，实际上会有小的偏差（即两个一样的time_stamp实际上是差异比较小的两个时间） 2. 项目效果展示 3. 项目实现分析 主要包括 一份广告点击的样本数据raw_sample.csv：体现的是用户对不同位置广告点击、没点击的情况 一份广告基本信息数据ad_feature.csv：体现的是每个广告的类目(id)、品牌(id)、价格特征 一份用户基本信息数据user_profile.csv：体现的是用户群组、性别、年龄、消费购物档次、所在城市级别等特征 一份用户行为日志数据behavior_log.csv：体现用户对商品类目(id)、品牌(id)的浏览、加购物车、收藏、购买等信息 我们是在对非搜索类型的广告进行点击率预测和推荐(没有搜索词、没有广告的内容特征信息) 推荐业务处理主要流程： 召回 ===&gt; 排序 ===&gt; 过滤 离线处理业务流（①训练逻辑回归模型，②为每个用户召回感兴趣的广告） raw_sample.csv ==&gt; 历史样本数据 ad_feature.csv ==&gt; 广告特征数据 user_profile.csv ==&gt; 用户特征数据 raw_sample.csv + ad_feature.csv + user_profile.csv ==&gt; CTR点击率预测模型 behavior_log.csv ==&gt; 评分数据 ==&gt; user-cate/brand评分数据 ==&gt; 协同过滤 ==&gt; top-N cate/brand ==&gt; 关联广告（找到感兴趣的广告） 协同过滤召回 ==&gt; top-N cate/brand ==&gt; 关联对应的广告完成召回 在线处理业务流 数据处理部分： 实时行为日志 ==&gt; 实时特征 ==&gt; 缓存 实时行为日志 ==&gt; 实时商品类别/品牌 ==&gt; 实时广告召回集 ==&gt; 缓存 推荐任务部分： CTR点击率预测模型 + 广告/用户特征(缓存) + 对应的召回集(缓存) ==&gt; 点击率排序 ==&gt; top-N 广告推荐结果 涉及技术：Flume、Kafka、Spark-streming\\HDFS、Spark SQL、Spark ML、Redis Flume：日志数据收集 Kafka：实时日志数据处理队列 HDFS：存储数据 Spark SQL：离线处理 Spark ML：模型训练 Redis：缓存 4. 点击率预测(CTR—Click-Through-Rate)概念 电商广告推荐通常使用广告点击率(CTR—Click-Through-Rate)预测来实现 点击率预测 VS 推荐算法 点击率预测需要给出精准的点击概率，比如广告A点击率0.5%、广告B的点击率0.12%等；而推荐算法很多时候只需要得出一个最优的次序A&gt;B&gt;C即可。 点击率预测使用的算法通常是如逻辑回归(Logic Regression)这样的机器学习算法，而推荐算法则是一些基于协同过滤推荐、基于内容的推荐等思想实现的算法 点击率 VS 转化率 点击率预测是对每次广告的点击情况做出预测，可以判定这次为点击或不点击，也可以给出点击或不点击的概率 转化率指的是从状态A进入到状态B的概率，电商的转化率通常是指到达网站后，进而有成交记录的用户比率，如用户成交量/用户访问量 搜索和非搜索广告点击率预测的区别 搜索中有很强的搜索信号-“查询词(Query)”，查询词和广告内容的匹配程度很大程度影响了点击概率，搜索广告的点击率普遍较高 非搜索广告（例如展示广告，信息流广告）的点击率的计算很多就来源于用户的兴趣和广告自身的特征，以及上下文环境。通常好位置能达到百分之几的点击率（5%左右就很不错了）。对于很多底部的广告，点击率非常低，常常是千分之几，甚至更低。 5.小结：数据集分析 召回 采用用户的行为日志 behavior_log 创建召回模型 协同过滤 ALS——Spark有封装 协同过滤需需要用户对物品的评分。 用户-物品 评分 这里没有。只有对品类、品牌的数据。 用户-品类 评分 用户-品牌 评分 pv cart fav buy通过四种行为转化成 -&gt; 评分 评分的设定是根据具体的业务来的。如买给的评分最高，看最低。以及不同行为的上限，看了100次，那最多评分50. 排序 LR 逻辑回归 以 raw_sample 为骨架 把ad_feature 广告信息和user_profile用户信息拼接过来 训练逻辑回归模型 训练逻辑回归模型时 ： 用到user_profile中会影响到用户是否会点击广告的用户特征 用到ad_feature会影响到用户是否会点击广告的特征 点/不点作为目标值 预测的是点击的概率 若预测是0,1的话全部都是0.（实际广告100个人，有5个人点，正负样本极度不均衡，那我模型直接预测成全部不点也很好。所以需要调整阈值，概率不能以0.5划分了。得到结果处理：如果几个物品不点的概率分别为，0.87 0.88 0.91），那我先推荐的是点的概率大的0.13那个物品。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"},{"name":"推荐系统实战","slug":"机器学习/大数据的lambda架构/推荐系统实战","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"33-Spark Streaming的状态操作","slug":"33-Spark-Streaming的状态操作","date":"2021-07-06T16:39:56.000Z","updated":"2021-07-06T16:42:23.690Z","comments":true,"path":"20210707/33-Spark-Streaming的状态操作.html","link":"","permalink":"https://xxren8218.github.io/20210707/33-Spark-Streaming%E7%9A%84%E7%8A%B6%E6%80%81%E6%93%8D%E4%BD%9C.html","excerpt":"","text":"1、Spark Streaming的状态操作在Spark Streaming中存在两种状态操作 UpdateStateByKey Windows操作 使用有状态的transformation，需要开启Checkpoint spark streaming 的容错机制 它将足够多的信息checkpoint到某些具备容错性的存储系统如hdfs上，以便出错时能够迅速恢复 1.1 updateStateByKeySpark Streaming实现的是一个实时批处理操作，每隔一段时间将数据进行打包，封装成RDD，是无状态的。 无状态：指的是每个时间片段的数据之间是没有关联的。 需求：想要将一个大时间段（1天），即多个小时间段的数据内的数据持续进行累积操作 一般超过一天都是用RDD或Spark SQL来进行离线批处理 如果没有UpdateStateByKey，我们需要将每一秒的数据计算好放入mysql中取，再用mysql来进行统计计算 Spark Streaming中提供这种状态保护机制，即updateStateByKey 步骤： 首先，要定义一个state，可以是任意的数据类型 其次，要定义state更新函数—指定一个函数如何使用之前的state和新值来更新state 对于每个batch，Spark都会为每个之前已经存在的key去应用一次state更新函数，无论这个key在batch中是否有新的数据。如果state更新函数返回none，那么key对应的state就会被删除 对于每个新出现的key，也会执行state更新函数 举例：词统计。 案例：updateStateByKey需求：监听网络端口的数据，获取到每个批次的出现的单词数量，并且需要把每个批次的信息保留下来 代码 1234567891011121314151617181920212223242526272829303132333435import os# 配置spark driver和pyspark运行时，所使用的python解释器路径PYSPARK_PYTHON = &quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;JAVA_HOME=&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;SPARK_HOME = &quot;/home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0&quot;# 当存在多个版本时，不指定很可能会导致出错os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHONos.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHONos.environ[&#x27;JAVA_HOME&#x27;]=JAVA_HOMEos.environ[&quot;SPARK_HOME&quot;] = SPARK_HOMEfrom pyspark.streaming import StreamingContextfrom pyspark.sql.session import SparkSession# 创建SparkContextspark = SparkSession.builder.master(&quot;local[2]&quot;).getOrCreate()sc = spark.sparkContextssc = StreamingContext(sc, 3)##### 开启检查点 #####ssc.checkpoint(&quot;checkpoint&quot;) # 默认会在Hadoop的/user/root下有个Chekpoint# 定义state更新函数def updateFunc(new_values, last_sum): return sum(new_values) + (last_sum or 0)lines = ssc.socketTextStream(&quot;localhost&quot;, 9999)# 对数据以空格进行拆分，分为多个单词counts = lines.flatMap(lambda line: line.split(&quot; &quot;)) \\ .map(lambda word: (word, 1)) \\ .updateStateByKey(updateFunc=updateFunc) # 应用updateStateByKey函数 counts.pprint()ssc.start()ssc.awaitTermination() 1.2 Windows 实时热搜 窗口长度L：运算的数据量 滑动间隔G：控制每隔多长时间做一次运算 两个函数： 删除的数据怎么处理 新加入的数据怎么处理 每隔G秒，统计最近L秒的数据 操作细节 Window操作是基于窗口长度和滑动间隔来工作的 窗口的长度控制考虑前几批次数据量 默认为批处理的滑动间隔来确定计算结果的频率 相关函数 Smart computation invAddFunc reduceByKeyAndWindow(func,invFunc,windowLength,slideInterval,[num,Tasks]) func:正向操作，类似于updateStateByKey invFunc：反向操作，移除的数据如何处理 windowLength 窗口长度，统计一小时的热搜关键词，窗口长度就是1h。 例如在热词时，在上一个窗口中可能是热词，这个一个窗口中可能不是热词，就需要在这个窗口中把该次剔除掉 典型案例：热点搜索词滑动统计，每隔10秒，统计最近60秒钟的搜索词的搜索频次，并打印出最靠前的3个搜索词出现次数。 案例 监听网络端口的数据，每隔3秒统计前6秒出现的单词数量 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import os# 配置spark driver和pyspark运行时，所使用的python解释器路径PYSPARK_PYTHON = &quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;JAVA_HOME=&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;SPARK_HOME = &quot;/home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0&quot;# 当存在多个版本时，不指定很可能会导致出错os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHONos.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHONos.environ[&#x27;JAVA_HOME&#x27;]=JAVA_HOMEos.environ[&quot;SPARK_HOME&quot;] = SPARK_HOMEfrom pyspark import SparkContextfrom pyspark.streaming import StreamingContextfrom pyspark.sql.session import SparkSessiondef get_countryname(line): country_name = line.strip() if country_name == &#x27;usa&#x27;: output = &#x27;USA&#x27; elif country_name == &#x27;ind&#x27;: output = &#x27;India&#x27; elif country_name == &#x27;aus&#x27;: output = &#x27;Australia&#x27; else: output = &#x27;Unknown&#x27; return (output, 1)if __name__ == &quot;__main__&quot;: # 定义处理的时间间隔 batch_interval = 1 # base time unit (in seconds) # 定义窗口长度 window_length = 6 * batch_interval # 定义滑动时间间隔 frequency = 3 * batch_interval # 获取StreamingContext spark = SparkSession.builder.master(&quot;local[2]&quot;).getOrCreate() sc = spark.sparkContext ssc = StreamingContext(sc, batch_interval) # 需要设置检查点 ssc.checkpoint(&quot;checkpoint&quot;) lines = ssc.socketTextStream(&#x27;localhost&#x27;, 9999) addFunc = lambda x, y: x + y invAddFunc = lambda x, y: x - y # 调用reduceByKeyAndWindow，来进行窗口函数的调用 window_counts = lines.map(get_countryname) \\ .reduceByKeyAndWindow(addFunc, invAddFunc, window_length, frequency) # 输出处理结果信息 window_counts.pprint() ssc.start() ssc.awaitTermination()","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"11-二叉树的左下角的值","slug":"11-二叉树的左下角的值","date":"2021-07-06T14:01:31.000Z","updated":"2021-07-06T14:02:16.681Z","comments":true,"path":"20210706/11-二叉树的左下角的值.html","link":"","permalink":"https://xxren8218.github.io/20210706/11-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%B7%A6%E4%B8%8B%E8%A7%92%E7%9A%84%E5%80%BC.html","excerpt":"","text":"二叉树的左下角的值 513.找树左下角的值 思路所要求的的是 1.递归法可以用一个字典来保护每行第一个节点，使用前序遍历。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def __init__(self): self.dic = &#123;&#125; def findBottomLeftValue(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return None return self.leftValue(root, 1) def leftValue(self, node, index): if not node: return None if index not in self.dic: self.dic[index] = node.val ########################################## # 只需要把下面的两行代码互换，就可以找右边的元素了# ########################################## self.leftValue(node.left, index + 1) self.leftValue(node.right, index + 1) # 由于是数字hash，所以其实是有序的，直接values.pop()即可。 return list(self.dic.values()).pop() 2.迭代法可以套用框架。在每层判断时，只讲第一个值赋值给结果即可。 123456789101112131415161718192021222324252627282930# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def findBottomLeftValue(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return None queue = [root] result = None while queue: for i in range(len(queue)): cur_node = queue.pop(0) # 保存这行第一个元素即可 ##################################################### #只需要把i == 0，换成 i == len(queue),就可以找右边的元素了# ##################################################### if i == 0: result = cur_node.val if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) return result 总结本题同样采用了递归和迭代的方式进行求解。 迭代法直接套框架。用一个临时变量存值。 递归法可以用字典对每一层的第一个值进行保护。而字典的键可以用数字（每层的层数），使得无序的字典有序，直接取最后一个值即可。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"10-二叉树的左叶子之和","slug":"10-二叉树的左叶子之和","date":"2021-07-06T14:00:10.000Z","updated":"2021-07-06T14:01:14.305Z","comments":true,"path":"20210706/10-二叉树的左叶子之和.html","link":"","permalink":"https://xxren8218.github.io/20210706/10-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%B7%A6%E5%8F%B6%E5%AD%90%E4%B9%8B%E5%92%8C.html","excerpt":"","text":"二叉树的左叶子之和 404 左叶子之和 1.思路「首先要注意是判断左叶子，不是二叉树左侧节点，所以不要上来想着层序遍历。」 其实题目说的也很清晰了，左和叶子我们都知道表示什么，那么左叶子也应该知道了，但为了大家不会疑惑，我还是来给出左叶子的明确定义：「如果左节点不为空，且左节点没有左右孩子，那么这个节点就是左叶子」 「判断当前节点是不是左叶子是无法判断的，必须要通过节点的父节点来判断其左孩子是不是左叶子。」 思考一下如下图中二叉树，左叶子之和究竟是多少？ 「其实是0，因为这棵树根本没有左叶子！」 那么「判断当前节点是不是左叶子是无法判断的，必须要通过节点的父节点来判断其左孩子是不是左叶子。」 如果该节点的左节点不为空，该节点的左节点的左节点为空，该节点的左节点的右节点为空，则找到了一个左叶子，判断代码如下： 12if node.left and not node.left.left and not node.left.right: 左叶子节点处理逻辑 1.1递归法递归的遍历顺序为后序遍历（左右中），是因为要通过递归函数的返回值来累加求取左叶子数值之和。 递归三部曲： 确定递归函数的参数和返回值 判断一个树的左叶子节点之和，那么一定要传入树的根节点，递归函数的返回值为数值之和。 确定终止条件 依然是 1if not root: return 0 确定单层递归的逻辑 当遇到左叶子节点的时候，记录数值，然后通过递归求取左子树左叶子之和，和 右子树左叶子之和，相加便是整个树的左叶子之和。 代码如下： 12345678910111213141516171819202122# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def sumOfLeftLeaves(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return 0 leftValue = self.sumOfLeftLeaves(root.left) # 左 rightValue = self.sumOfLeftLeaves(root.right) # 右 midValue = 0 if root.left and not root.left.left and not root.left.right: # 中 midValue = root.left.val return leftValue + rightValue + midValue 1.2 迭代法我们可以使用一个辅助函数来判断此节点是否是叶子节点。然后按照层序遍历的框架进行判断即可。 123456789101112131415161718192021222324252627282930313233343536# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def sumOfLeftLeaves(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return 0 # 来判断此节点是否是叶子结点 isLeafNode = lambda node: not node.left and not node.right queue = [root] result = 0 while queue: cur_node = queue.pop(0) if cur_node.left: # 若此节点为叶子结点，加入结果 if isLeafNode(cur_node.left): result += cur_node.left.val else: queue.append(cur_node.left) if cur_node.right: # 若此节点为叶子结点，不用管它，若不是，则加入队列中。 if not isLeafNode(cur_node.right): queue.append(cur_node.right) return result 总结我觉得这道题还是有一定难度的，不知道为什么给划分为简单，","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"09-二叉树的所有路径","slug":"09-二叉树的所有路径","date":"2021-07-06T13:58:48.000Z","updated":"2021-07-06T13:59:43.736Z","comments":true,"path":"20210706/09-二叉树的所有路径.html","link":"","permalink":"https://xxren8218.github.io/20210706/09-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%89%80%E6%9C%89%E8%B7%AF%E5%BE%84.html","excerpt":"","text":"二叉树的所有路径 257.二叉树的所有路径 1. 思路1.1递归法最直观的方法是使用深度优先搜索。在深度优先搜索遍历二叉树时，我们需要考虑当前的节点以及它的孩子节点。 如果当前节点不是叶子节点，则在当前的路径末尾添加该节点，并继续递归遍历该节点的每一个孩子节点。 如果当前节点是叶子节点，则在当前路径末尾添加该节点后我们就得到了一条从根节点到叶子节点的路径，将 该路径加入到答案即可。 如此，当遍历完整棵二叉树以后我们就得到了所有从根节点到叶子节点的路径。当然，深度优先搜索也可以使用非递归的方式实现，这里不再赘述。 注意字符串的拼接是 += （公共方法） 列表的extend和append的区别。+= 与 extend类似。 123456789101112131415161718192021222324252627282930313233343536# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def binaryTreePaths(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[str] &quot;&quot;&quot; if not root: return [] result = [] return self.traversal(root, &#x27;&#x27;, result) def traversal(self, node, path, result): # 如果当前节点不存在 if not node: return # 否则当前节点存在 path += str(node.val) # 判断是否到达叶子节点 # 到达叶子节点 if not node.left and not node.right: # 将路径增加到答案中 result.append(path) # 未到达叶子节点 else: path += &quot;-&gt;&quot; self.traversal(node.left, path, result) self.traversal(node.right, path, result) return result 时间复杂度：O(N^2) 空间复杂度：O(N^2) 1.2 迭代法这里的迭代法采用广度优先搜索来实现。我们维护一个队列，存储节点以及根到该节点的路径。一开始这个队列里只有根节点。在每一步迭代中，我们取出队列中的首节点，如果它是叶子节点，则将它对应的路径加入到答案中。如果它不是叶子节点，则将它的所有孩子节点加入到队列的末尾。当队列为空时广度优先搜索结束，我们即能得到答案。 这种方式不能保证从左到右的输出。 12345678910111213141516171819202122232425262728293031# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def binaryTreePaths(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[str] &quot;&quot;&quot; if not root: return [] result = [] node_queue = [root] path_queue = [str(root.val)] while node_queue: node = node_queue.pop(0) path = path_queue.pop(0) if not node.left and not node.right: result.append(path) if node.left: node_queue.append(node.left) path_queue.append(path + &quot;-&gt;&quot; + str(node.left.val)) if node.right: node_queue.append(node.right) path_queue.append(path + &quot;-&gt;&quot; + str(node.right.val)) return result 2. 总结可以看出来，不管是递归的前序遍历还是广度的迭代遍历，都有我们模板的影子。实际上还是在模板上的一些小改进而已。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"32-Spark Streaming概述及编码实战","slug":"32-Spark-Streaming概述及编码实战","date":"2021-07-05T17:04:53.000Z","updated":"2021-07-05T17:06:32.231Z","comments":true,"path":"20210706/32-Spark-Streaming概述及编码实战.html","link":"","permalink":"https://xxren8218.github.io/20210706/32-Spark-Streaming%E6%A6%82%E8%BF%B0%E5%8F%8A%E7%BC%96%E7%A0%81%E5%AE%9E%E6%88%98.html","excerpt":"","text":"掌握目标 说出Spark Streaming的特点 说出DStreaming的常见操作api 能够应用Spark Streaming实现实时数据处理 能够应用Spark Streaming的状态操作解决实际问题 独立实现foreachRDD向mysql数据库的数据写入 独立实现Spark Streaming对接kafka实现实时数据处理 1、sparkStreaming概述1.1 SparkStreaming是什么 它是一个可扩展，高吞吐具有容错性的流式计算框架 吞吐量：单位时间内成功传输数据的数量 之前我们接触的spark-core和spark-sql都是处理属于离线批处理任务，数据一般都是在固定位置上，通常我们写好一个脚本，每天定时去处理数据，计算，保存数据结果。这类任务通常是T+1(一天一个任务)，对实时性要求不高。 但在企业中存在很多实时性处理的需求，例如：双十一的京东阿里，通常会做一个实时的数据大屏，显示实时订单。这种情况下，对数据实时性要求较高，仅仅能够容忍到延迟1分钟或几秒钟。 实时计算框架对比 Storm 流式计算框架 以record为单位处理数据 也支持micro-batch方式（Trident） 没有处理机器学习的生态 没有离线计算的框架 对python不友好 Spark 批处理计算框架 以RDD为单位处理数据 支持micro-batch流式处理数据（Spark Streaming） 有机器学习相关的库 对比： 吞吐量：Spark Streaming优于Storm 延迟：Spark Streaming差于Storm 1.2 Spark Streaming的组件 Streaming Context 一旦一个Context已经启动(调用了Streaming Context的start()),就不能有新的流算子(Dstream)建立或者是添加到context中 一旦一个context已经停止,不能重新启动(Streaming Context调用了stop方法之后 就不能再次调 start()) 在JVM(java虚拟机)中, 同一时间只能有一个Streaming Context处于活跃状态, 一个SparkContext创建一个Streaming Context 在Streaming Context上调用Stop方法, 也会关闭SparkContext对象, 如果只想仅关闭Streaming Context对象,设置stop()的可选参数为false 一个SparkContext对象可以重复利用去创建多个Streaming Context对象(不关闭SparkContext前提下), 但是需要关一个再开下一个 DStream (离散流) 代表一个连续的数据流 在内部, DStream由一系列连续的RDD组成 DStreams中的每个RDD都包含确定时间间隔内的数据 任何对DStreams的操作都转换成了对DStreams隐含的RDD的操作 数据源 基本源 TCP/IP Socket FileSystem 高级源 Kafka Flume 2、Spark Streaming编码实践Spark Streaming编码步骤： 1，创建一个StreamingContext 2，从StreamingContext中创建一个数据对象 3，对数据对象进行Transformations操作 4，输出结果 5，开始和停止 利用Spark Streaming实现WordCount 需求：监听某个端口上的网络数据，实时统计出现的不同单词个数。 1，需要安装一个nc工具：sudo yum install -y nc 2，执行指令：nc -lk 9999 -v 12345678910111213141516171819202122232425262728293031323334import os# 配置spark driver和pyspark运行时，所使用的python解释器路径PYSPARK_PYTHON = &quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;JAVA_HOME=&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;SPARK_HOME = &quot;/home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0&quot;# 当存在多个版本时，不指定很可能会导致出错os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHONos.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHONos.environ[&#x27;JAVA_HOME&#x27;]=JAVA_HOMEos.environ[&quot;SPARK_HOME&quot;] = SPARK_HOMEfrom pyspark import SparkContextfrom pyspark.streaming import StreamingContextif __name__ == &quot;__main__&quot;: sc = SparkContext(&quot;local[2]&quot;,appName=&quot;NetworkWordCount&quot;) # 参数2：指定执行计算的时间间隔 ssc = StreamingContext(sc, 1) # 监听ip，端口上的上的数据 （需要打开端口）【nc -lk 9999 -v】 lines = ssc.socketTextStream(&#x27;localhost&#x27;,9999) # 将数据按空格进行拆分为多个单词 words = lines.flatMap(lambda line: line.split(&quot; &quot;)) # 将单词转换为(单词，1)的形式 pairs = words.map(lambda word:(word,1)) # 统计单词个数 wordCounts = pairs.reduceByKey(lambda x,y:x+y) # 打印结果信息，会使得前面的transformation操作执行 wordCounts.pprint() # pprint() 对RDD的操作 # 启动StreamingContext ssc.start() # 等待计算结束(不是交互式环境的话需要加这个参数，不然很快就停了) ssc.awaitTermination() 可视化查看效果：http://192.168.199.188:4040 点击streaming，查看效果","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"31-Spark JSON数据处理 & 数据清洗","slug":"31-Spark-JSON数据处理-数据清洗","date":"2021-07-05T17:03:02.000Z","updated":"2021-07-05T17:04:23.493Z","comments":true,"path":"20210706/31-Spark-JSON数据处理-数据清洗.html","link":"","permalink":"https://xxren8218.github.io/20210706/31-Spark-JSON%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97.html","excerpt":"","text":"1、JSON数据的处理1.1 介绍JSON数据 网页和后端数据交互所用我的格式。 在Spark中能自动化的把结构加载进来，并且能推断数据类型。（CSV将所有都处理成 String） Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame Spark SQL能够自动将JSON数据集以结构化的形式加载为一个DataFrame This conversion can be done using SparkSession.read.json on a JSON file 读取一个JSON文件可以用SparkSession.read.json方法 从JSON到DataFrame 指定DataFrame的schema 1，通过反射自动推断，适合静态数据 2，程序指定，适合程序运行中动态生成的数据 加载json数据 123456# 使用内部的schemajsonDF = spark.read.json(&quot;xxx.json&quot;)jsonDF = spark.read.format(&#x27;json&#x27;).load(&#x27;xxx.json&#x27;)# 指定schemajsonDF = spark.read.schema(jsonSchema).json(&#x27;xxx.json&#x27;) 嵌套结构的JSON 重要的方法 1，get_json_object 2，get_json 3，explode 1.2 实践1.2.1 静态json数据的读取和操作无嵌套结构的json数据 1234567891011from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&#x27;json_demo&#x27;).getOrCreate()sc = spark.sparkContext# ==========================================# 无嵌套结构的json# ==========================================jsonString = [&quot;&quot;&quot;&#123; &quot;id&quot; : &quot;01001&quot;, &quot;city&quot; : &quot;AGAWAM&quot;, &quot;pop&quot; : 15338, &quot;state&quot; : &quot;MA&quot; &#125;&quot;&quot;&quot;,&quot;&quot;&quot;&#123; &quot;id&quot; : &quot;01002&quot;, &quot;city&quot; : &quot;CUSHMAN&quot;, &quot;pop&quot; : 36963, &quot;state&quot; : &quot;MA&quot; &#125;&quot;&quot;&quot;] 从json字符串数组得到DataFrame 12345678# 从json字符串数组得到rdd有两种方法# 1. 转换为rdd，再从rdd到DataFrame# 2. 直接利用spark.createDataFrame()，见后面例子jsonRDD = sc.parallelize(jsonString) # stringJSONRDDjsonDF = spark.read.json(jsonRDD) # convert RDD into DataFramejsonDF.printSchema()jsonDF.show() 直接从文件生成DataFrame 12345678910111213141516# -- 直接从文件生成DataFrame# 只有被压缩后的json文件内容，才能被spark-sql正确读取，否则格式化后的数据读取会出现问题jsonDF = spark.read.json(&quot;xxx.json&quot;)# or# jsonDF = spark.read.format(&#x27;json&#x27;).load(&#x27;xxx.json&#x27;)jsonDF.printSchema()jsonDF.show(truncate=False) # truncate=False 数据较长的时候不会...进行省略。默认会以...替换行内过长的数据jsonDF.filter(jsonDF.pop&gt;4000).show(10)# 依照已有的DataFrame，创建一个临时的表(相当于mysql数据库中的一个表)，这样就可以用纯sql语句进行数据操作jsonDF.createOrReplaceTempView(&quot;tmp_table&quot;)resultDF = spark.sql(&quot;select * from tmp_table where pop&gt;4000&quot;)resultDF.show(10) 1.2.1 动态json数据的读取和操作指定DataFrame的Schema 上面的例子为通过反射自动推断schema，适合静态数据 下面我们来讲解如何进行程序指定schema 没有嵌套结构的json 1234567891011121314151617181920212223242526272829jsonString = [&quot;&quot;&quot;&#123; &quot;id&quot; : &quot;01001&quot;, &quot;city&quot; : &quot;AGAWAM&quot;, &quot;pop&quot; : 15338, &quot;state&quot; : &quot;MA&quot; &#125;&quot;&quot;&quot;,&quot;&quot;&quot;&#123; &quot;id&quot; : &quot;01002&quot;, &quot;city&quot; : &quot;CUSHMAN&quot;, &quot;pop&quot; : 36963, &quot;state&quot; : &quot;MA&quot; &#125;&quot;&quot;&quot;]jsonRDD = sc.parallelize(jsonString)from pyspark.sql.types import *# 定义结构类型# StructType：schema的整体结构，表示JSON的对象结构# XXXStype:指的是某一列的数据类型jsonSchema = StructType() \\ .add(&quot;id&quot;, StringType(),True) \\ .add(&quot;city&quot;, StringType()) \\ .add(&quot;pop&quot; , LongType()) \\ .add(&quot;state&quot;,StringType())jsonSchema = StructType() \\ .add(&quot;id&quot;, LongType(),True) \\ .add(&quot;city&quot;, StringType()) \\ .add(&quot;pop&quot; , DoubleType()) \\ .add(&quot;state&quot;,StringType())reader = spark.read.schema(jsonSchema)jsonDF = reader.json(jsonRDD)jsonDF.printSchema()jsonDF.show() 带有嵌套结构的json 1234567891011121314from pyspark.sql.types import *jsonSchema = StructType([ StructField(&quot;id&quot;, StringType(), True), StructField(&quot;city&quot;, StringType(), True), StructField(&quot;loc&quot; , ArrayType(DoubleType())), StructField(&quot;pop&quot;, LongType(), True), StructField(&quot;state&quot;, StringType(), True)])reader = spark.read.schema(jsonSchema)jsonDF = reader.json(&#x27;data/nest.json&#x27;)jsonDF.printSchema()jsonDF.show(2)jsonDF.filter(jsonDF.pop&gt;4000).show(10) 2、数据清洗 处理重复数据 处理缺失情况 处理异常值 前面我们处理的数据实际上都是已经被处理好的规整数据，但是在大数据整个生产过程中，需要先对数据进行数据清洗，将杂乱无章的数据整理为符合后面处理要求的规整数据。 2.1 数据去重123456789101112131415161718192021222324252627282930313233&#x27;&#x27;&#x27;1.删除重复数据groupby().count()：可以看到数据的重复情况&#x27;&#x27;&#x27;df = spark.createDataFrame([ (1, 144.5, 5.9, 33, &#x27;M&#x27;), (2, 167.2, 5.4, 45, &#x27;M&#x27;), (3, 124.1, 5.2, 23, &#x27;F&#x27;), (4, 144.5, 5.9, 33, &#x27;M&#x27;), (5, 133.2, 5.7, 54, &#x27;F&#x27;), (3, 124.1, 5.2, 23, &#x27;F&#x27;), (5, 129.2, 5.3, 42, &#x27;M&#x27;),], [&#x27;id&#x27;, &#x27;weight&#x27;, &#x27;height&#x27;, &#x27;age&#x27;, &#x27;gender&#x27;])# 查看重复记录# 无意义重复数据去重：数据中行与行完全重复# 1.首先删除完全一样的记录df2 = df.dropDuplicates()# 有意义去重：删除除去无意义字段之外的完全重复的行数据# 2.其次，关键字段值完全一模一样的记录（在这个例子中，是指除了id之外的列一模一样）# 删除某些字段值完全一样的重复记录，subset参数定义这些字段df3 = df2.dropDuplicates(subset = [c for c in df2.columns if c!=&#x27;id&#x27;])# 3.有意义的重复记录去重之后，再看某个无意义字段的值是否有重复（在这个例子中，是看id是否重复）# 查看某一列是否有重复值import pyspark.sql.functions as fndf3.agg(fn.count(&#x27;id&#x27;).alias(&#x27;id_count&#x27;),fn.countDistinct(&#x27;id&#x27;).alias(&#x27;distinct_id_count&#x27;)).collect()# 4.对于id这种无意义的列重复，添加另外一列自增id——不连续。df3.withColumn(&#x27;new_id&#x27;,fn.monotonically_increasing_id()).show() 2.2 缺失值处理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#x27;&#x27;&#x27;2.处理缺失值2.1 对缺失值进行删除操作(行，列)2.2 对缺失值进行填充操作(列的均值)2.3 对缺失值对应的行或列进行标记&#x27;&#x27;&#x27;df_miss = spark.createDataFrame([(1, 143.5, 5.6, 28,&#x27;M&#x27;, 100000),(2, 167.2, 5.4, 45,&#x27;M&#x27;, None),(3, None , 5.2, None, None, None),(4, 144.5, 5.9, 33, &#x27;M&#x27;, None),(5, 133.2, 5.7, 54, &#x27;F&#x27;, None),(6, 124.1, 5.2, None, &#x27;F&#x27;, None),(7, 129.2, 5.3, 42, &#x27;M&#x27;, 76000),], [&#x27;id&#x27;, &#x27;weight&#x27;, &#x27;height&#x27;, &#x27;age&#x27;, &#x27;gender&#x27;, &#x27;income&#x27;])# 1.计算每条记录的缺失值情况# 将DataFrame转换成RDD，这样写自定义函数方便些。# 从行的角度统计缺失情况.df_miss.rdd.map(lambda row:(row[&#x27;id&#x27;], sum([c==None for c in row]))).collect()[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]# 2.计算各列的缺失情况百分比df_miss.agg(*[(1 - (fn.count(c) / fn.count(&#x27;*&#x27;))).alias(c + &#x27;_missing&#x27;) for c in df_miss.columns]).show()# 3、删除缺失值过于严重的列# 其实是先建一个DF，不要缺失值的列df_miss_no_income = df_miss.select([c for c in df_miss.columns if c != &#x27;income&#x27;])# 4、按照缺失值删除行（threshold是根据一行记录中，缺失字段的百分比的定义）df_miss_no_income.dropna(thresh=3).show()# 5、填充缺失值，可以用fillna来填充缺失值，# 对于bool类型、或者分类类型，可以为缺失值单独设置一个类型，missing# 对于数值类型，可以用均值或者中位数等填充# fillna可以接收两种类型的参数：# 一个数字、字符串，这时整个DataSet中所有的缺失值都会被填充为相同的值。# 也可以接收一个字典｛列名：值｝这样# 先计算均值，并组织成一个字典(除去性别这一列。)means = df_miss_no_income.agg( *[fn.mean(c).alias(c) for c in df_miss_no_income.columns if c != &#x27;gender&#x27;]).toPandas().to_dict(&#x27;records&#x27;)[0]# 然后添加其它的列——非数值型means[&#x27;gender&#x27;] = &#x27;missing&#x27;df_miss_no_income.fillna(means).show() 2.3 异常值处理——年龄等。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&#x27;&#x27;&#x27;3、异常值处理异常值：不属于正常的值 包含：缺失值，超过正常范围内的较大值或较小值分位数去极值中位数绝对偏差去极值正态分布去极值上述三种操作的核心都是：通过原始数据设定一个正常的范围，超过此范围的就是一个异常值&#x27;&#x27;&#x27;df_outliers = spark.createDataFrame([(1, 143.5, 5.3, 28),(2, 154.2, 5.5, 45),(3, 342.3, 5.1, 99),(4, 144.5, 5.5, 33),(5, 133.2, 5.4, 54),(6, 124.1, 5.1, 21),(7, 129.2, 5.3, 42),], [&#x27;id&#x27;, &#x27;weight&#x27;, &#x27;height&#x27;, &#x27;age&#x27;])# 设定范围 超出这个范围的 用边界值替换# approxQuantile方法接收三个参数：参数1，列名；参数2：想要计算的分位点，可以是一个点，也可以是一个列表（0和1之间的小数），第三个参数是能容忍的误差，如果是0，代表百分百精确计算。cols = [&#x27;weight&#x27;, &#x27;height&#x27;, &#x27;age&#x27;]bounds = &#123;&#125;for col in cols: quantiles = df_outliers.approxQuantile(col, [0.25, 0.75], 0.05) IQR = quantiles[1] - quantiles[0] bounds[col] = [ quantiles[0] - 1.5 * IQR, quantiles[1] + 1.5 * IQR ]&gt;&gt;&gt; bounds&#123;&#x27;age&#x27;: [-11.0, 93.0], &#x27;height&#x27;: [4.499999999999999, 6.1000000000000005], &#x27;weight&#x27;: [91.69999999999999, 191.7]&#125;# 为异常值字段打标志outliers = df_outliers.select(*[&#x27;id&#x27;] + [( (df_outliers[c] &lt; bounds[c][0]) | (df_outliers[c] &gt; bounds[c][1]) ).alias(c + &#x27;_o&#x27;) for c in cols ])outliers.show()## +---+--------+--------+-----+# | id|weight_o|height_o|age_o|# +---+--------+--------+-----+# | 1| false| false|false|# | 2| false| false|false|# | 3| true| false| true|# | 4| false| false|false|# | 5| false| false|false|# | 6| false| false|false|# | 7| false| false|false|# +---+--------+--------+-----+# 再回头看看这些异常值的值，重新和原始数据关联df_outliers = df_outliers.join(outliers, on=&#x27;id&#x27;)df_outliers.filter(&#x27;weight_o&#x27;).select(&#x27;id&#x27;, &#x27;weight&#x27;).show()# +---+------+# | id|weight|# +---+------+# | 3| 342.3|# +---+------+df_outliers.filter(&#x27;age_o&#x27;).select(&#x27;id&#x27;, &#x27;age&#x27;).show()# +---+---+# | id|age|# +---+---+# | 3| 99|# +---+---+","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"08-平衡二叉树","slug":"08-平衡二叉树","date":"2021-07-05T10:36:20.000Z","updated":"2021-07-05T10:37:28.881Z","comments":true,"path":"20210705/08-平衡二叉树.html","link":"","permalink":"https://xxren8218.github.io/20210705/08-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91.html","excerpt":"","text":"平衡二叉树 110.平衡二叉树 咋眼一看这道题目和[二叉树的最大深度]很像，其实有很大区别。 这里强调一波概念： 二叉树节点的深度：指从根节点到该节点的最长简单路径边的条数。 二叉树节点的高度：指从该节点到叶子节点的最长简单路径边的条数。 前言这道题中的平衡二叉树的定义是：二叉树的每个节点的左右子树的高度差的绝对值不超过 1，则二叉树是平衡二叉树。根据定义，一棵二叉树是平衡二叉树，当且仅当其所有子树也都是平衡二叉树，因此可以使用递归的方式判断二叉树是不是平衡二叉树，递归的顺序可以是自顶向下或者自底向上。 1.思路1.1递归法——自上而下（前序遍历）定义函数 height，用于计算二叉树中的任意一个节点 p 的高度： 有了计算节点高度的函数，即可判断二叉树是否平衡。具体做法类似于二叉树的前序遍历，即对于当前遍历到的节点，首先计算左右子树的高度，如果左右子树的高度差是否不超过 1，再分别递归地遍历左右子节点，并判断左子树和右子树是否平衡。这是一个自顶向下的递归的过程。也就是前序遍历。 实际上就是写找最大深度即可(后序遍历写出)。再前序递归进行计算。 递归三步曲分析： 明确递归函数的参数和返回值 参数的话为传入的节点指针，就没有其他参数需要传递了，返回值要返回传入节点为根节点树的高度。 那么如何标记左右子树是否差值大于1呢。 只需要单独判断 左右子树是否为平衡树， 左子树是否为平衡树 右子树是否为平衡树即可 代码如下： 12# 定义函数找深度当前节点为根节点的树的深度。def Height(node): 明确终止条件 递归的过程中依然是遇到空节点了为终止，返回0，表示当前节点为根节点的高度为0 代码如下： 1if not node: return 0 明确单层递归的逻辑 代码如下： 123# 只要一个不满足就不是平衡树了。return abs(leftHeight - rightHeight) &lt;= 1 and isBalanced(root.left) and isBalanced(root.right) 整体代码如下： 12345678910111213141516171819202122232425# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def isBalanced(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: bool &quot;&quot;&quot; if not root: return True ###################### ## 前序在这里体现 ## ###################### return abs(self.Height(root.left) - self.Height(root.right)) &lt;= 1 and \\ self.isBalanced(root.left) and \\ self.isBalanced(root.right) def Height(self, node): if not node: return 0 leftHeight = self.Height(node.left) rightHeight = self.Height(node.right) return 1 + max(leftHeight, rightHeight) 时间复杂度：O(n^2)，其中 n 是二叉树中的节点个数。 空间复杂度：O(n)。 1.2 递归法——自下而上（后序遍历）方法一由于是自顶向下递归，因此对于同一个节点，函数 height 会被重复调用，导致时间复杂度较高。如果使用自底向上的做法，则对于每个节点，函数 height 只会被调用一次。 自底向上递归的做法类似于后序遍历，对于当前遍历到的节点，先递归地判断其左右子树是否平衡，再判断以当前节点为根的子树是否平衡。如果一棵子树是平衡的，则返回其高度（高度一定是非负整数），否则返回 -1−1。如果存在一棵子树不平衡，则整个二叉树一定不平衡。 只需修改后序遍历的代码，相当于在子函数（Height()）递归时进行判断了。 12345678910111213141516171819202122232425262728293031# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def isBalanced(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: bool &quot;&quot;&quot; if not root: return True return self.Height(root) &gt;= 0 def Height(self, node): &quot;&quot;&quot; 此处的单层递归的逻辑是： 若是平衡二叉树，直接返回深度 若不是平衡二叉树，返回 -1。 &quot;&quot;&quot; if not node: return 0 leftHeight = self.Height(node.left) rightHeight = self.Height(node.right) #################### ## 后序在这里体现 ## #################### if leftHeight == -1 or rightHeight == -1 or abs(leftHeight - rightHeight) &gt; 1: return -1 else: return max(leftHeight, rightHeight) + 1 时间复杂度：O(n)，其中 nn 是二叉树中的节点个数。使用自底向上的递归，每个节点的计算高度和判断是否平衡都只需要处理一次，最坏情况下需要遍历二叉树中的所有节点，因此时间复杂度是 O(n)。 空间复杂度：O(n)，其中 nn 是二叉树中的节点个数。空间复杂度主要取决于递归调用的层数，递归调用的层数不会超过 n。 总结本题没有给迭代的解法，说实话我觉得有点难，懒得想了！","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"07-完全二叉树的节点个数","slug":"07-完全二叉树的节点个数","date":"2021-07-05T10:34:49.000Z","updated":"2021-07-05T10:35:49.427Z","comments":true,"path":"20210705/07-完全二叉树的节点个数.html","link":"","permalink":"https://xxren8218.github.io/20210705/07-%E5%AE%8C%E5%85%A8%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E8%8A%82%E7%82%B9%E4%B8%AA%E6%95%B0.html","excerpt":"","text":"完全二叉树的节点个数 222.完全二叉树的节点个数 1.思路这道题目其实没有必要强调是完全二叉树，就是求二叉树节点的个数。 依然可以使用递归法和迭代法来解决。 这道题目的递归法和求二叉树的深度写法类似， 而迭代法：二叉树层序遍历模板稍稍修改一下，记录遍历的节点数量就可以了。 递归遍历的顺序依然是后序（左右中）。 2.递归法 确定递归函数的参数和返回值：参数就是传入树的根节点，返回就返回以该节点为根节点二叉树的节点数量。 代码如下： 1def getTreeNumber(node): 确定终止条件：如果为空节点的话，就返回0，表示节点数为0 代码如下： 1if not root: return 0 确定单层递归的逻辑：先求它的左子树的节点数量，再求的右子树的节点数量，最后取总和再加一 （加1是因为算上当前中间节点）就是目前节点为根节点的节点数量。 整体代码如下： 1234567891011121314151617181920# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def countNodes(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return 0 return self.getTreeNumber(root) def getTreeNumber(self, node): if not node: return 0 leftnum = self.getTreeNumber(node.left) rightnum = self.getTreeNumber(node.right) return 1 + leftnum + rightnum 3.迭代法层序迭代法也很简单。只要模板少做改动，加一个变量result，统计节点数量就可以了 12345678910111213141516171819202122232425# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def countNodes(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return 0 queue = [root] res = 0 while queue: cur_node = queue.pop(0) res += 1 if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) return res 4.总结一样的分析套路，代码也差不多，估计此时大家最这一类求二叉树节点数量以及求深度应该非常熟练了。 没有做过这道题目的同学可以愉快的刷了它。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"30-Spark SQL概述 & Spark DataFrame","slug":"30-Spark-SQL概述-Spark-DataFrame","date":"2021-07-04T17:00:41.000Z","updated":"2021-07-04T17:04:20.780Z","comments":true,"path":"20210705/30-Spark-SQL概述-Spark-DataFrame.html","link":"","permalink":"https://xxren8218.github.io/20210705/30-Spark-SQL%E6%A6%82%E8%BF%B0-Spark-DataFrame.html","excerpt":"","text":"掌握目标 说出Spark Sql的相关概念 说出DataFrame与RDD的联系 独立实现Spark Sql对JSON数据的处理 独立实现Spark Sql进行数据清洗 1、Spark SQL 概述Spark SQL概念 Spark SQL is Apache Spark’s module for working with structured data. 它是spark中用于处理结构化数据的一个模块 Spark SQL历史 Hive是目前大数据领域，事实上的数据仓库标准。 Shark：shark底层使用spark的基于内存的计算模型，从而让性能比Hive提升了数倍到上百倍。 底层很多东西还是依赖于Hive，修改了内存管理、物理计划、执行三个模块 2014年6月1日的时候，Spark宣布了不再开发Shark，全面转向Spark SQL的开发 Spark SQL优势 Write Less Code Performance python操作RDD，转换为可执行代码，运行在java虚拟机，涉及两个不同语言引擎之间的切换，进行进程间 通信很耗费性能。 DataFrame 是RDD为基础的分布式数据集，类似于传统关系型数据库的二维表，dataframe记录了对应列的名称和类型 dataFrame引入schema和off-heap(使用操作系统层面上的内存) 1、解决了RDD的缺点 序列化和反序列化开销大 频繁的创建和销毁对象造成大量的GC 2、丢失了RDD的优点 RDD编译时进行类型检查 RDD具有面向对象编程的特性 用scala/python编写的RDD比Spark SQL编写转换的RDD慢，涉及到执行计划 CatalystOptimizer：Catalyst优化器 ProjectTungsten：钨丝计划，为了提高RDD的效率而制定的计划 Code gen：代码生成器 直接编写RDD也可以自实现优化代码，但是远不及SparkSQL前面的优化操作后转换的RDD效率高，快1倍左右 优化引擎：类似mysql等关系型数据库基于成本的优化器 首先执行逻辑执行计划，然后转换为物理执行计划(选择成本最小的)，通过Code Generation最终生成为RDD Language-independent API 用任何语言编写生成的RDD都一样，而使用spark-core编写的RDD，不同的语言生成不同的RDD Schema 结构化数据，可以直接看出数据的详情 在RDD中无法看出，解释性不强，无法告诉引擎信息，没法详细优化。 为什么要学习sparksql sparksql特性 1、易整合 2、统一的数据源访问 3、兼容hive 4、提供了标准的数据库连接（jdbc/odbc） 2、DataFrame2.1 介绍在Spark语义中，DataFrame是一个分布式的行集合，可以想象为一个关系型数据库的表，或者一个带有列名的Excel表格。它和RDD一样，有这样一些特点： Immuatable：一旦RDD、DataFrame被创建，就不能更改，只能通过transformation生成新的RDD、DataFrame Lazy Evaluations：只有action才会触发Transformation的执行 Distributed：DataFrame和RDD一样都是分布式的 dataframe和dataset（没有python版本,没法做类型校验。python是若类型语言）统一，dataframe只是dataset[ROW]的类型别名。由于Python是弱类型语言，只能使用DataFrame DataFrame vs RDD RDD：分布式的对象的集合，Spark并不知道对象的详细模式信息 DataFrame：分布式的Row对象的集合，其提供了由列组成的详细模式信息，使得Spark SQL可以进行某些形式的执行优化。 DataFrame和普通的RDD的逻辑框架区别如下所示： 左侧的RDD Spark框架本身不了解 Person类的内部结构。 右侧的DataFrame提供了详细的结构信息（schema——每列的名称，类型） DataFrame还配套了新的操作数据的方法，DataFrame API（如df.select())和SQL(select id, name from xx_table where …)。 DataFrame还引入了off-heap,意味着JVM堆以外的内存, 这些内存直接受操作系统管理（而不是JVM）。 RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化。 DataFrame的抽象后，我们处理数据更加简单了，甚至可以用SQL来处理数据了 通过DataFrame API或SQL处理数据，会自动经过Spark 优化器（Catalyst）的优化，即使你写的程序或SQL不仅高效，也可以运行的很快。 DataFrame相当于是一个带着schema的RDD Pandas DataFrame vs Spark DataFrame Cluster Parallel：集群并行执行 Lazy Evaluations: 只有action才会触发Transformation的执行 Immutable：不可更改 Pandas rich API：比Spark SQL api丰富 2.2 创建DataFrame0.创建之前必须有一个SparkSession. 1，创建dataFrame的步骤 ​ 调用方法例如：spark.read.xxx方法 2，其他方式创建dataframe createDataFrame：pandas dataframe、list、RDD 数据源：RDD、csv、json、parquet、orc、jdbc 1234567jsonDF = spark.read.json(&quot;xxx.json&quot;)jsonDF = spark.read.format(&#x27;json&#x27;).load(&#x27;xxx.json&#x27;)parquetDF = spark.read.parquet(&quot;xxx.parquet&quot;)jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;,&quot;jdbc:mysql://localhost:3306/db_name&quot;).option(&quot;dbtable&quot;,&quot;table_name&quot;).option(&quot;user&quot;,&quot;xxx&quot;).option(&quot;password&quot;,&quot;xxx&quot;).load() Transformation:延迟性操作 action：立即操作 2.3 DataFrame API实现基于RDD创建 12345678910111213141516from pyspark.sql import SparkSessionfrom pyspark.sql import Row# 创建DatFrame需要有 Spark Session# 创建RDD需要有SparkContextspark = SparkSession.builder.appName(&#x27;test&#x27;).getOrCreate()sc = spark.sparkContext# spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, 6)# ================直接创建==========================l = [(&#x27;Ankit&#x27;,25),(&#x27;Jalfaizy&#x27;,22),(&#x27;saurabh&#x27;,20),(&#x27;Bala&#x27;,26)]rdd = sc.parallelize(l)# 为数据添加列名people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))# 通过SparkSession来创建DataFrameschemaPeople = spark.createDataFrame(people) 从csv中读取数据 12345678910111213# ==================从csv读取======================# 加载csv类型的数据并转换为DataFrame,默认是从hadoop的/user/root//下查找，目前我们用户是rootdf = spark.read.format(&quot;csv&quot;). \\ option(&quot;header&quot;, &quot;true&quot;) \\ # header True,把最前面的展示出来。 .load(&quot;iris.csv&quot;)# 显示数据结构df.printSchema()# 显示前10条数据df.show(10)# 统计总量df.count()# 列名df.columns 增加一列 1234# ===============增加一列(或者替换) withColumn===========# 定义一个新的列，数据为其他某列数据的两倍# 如果操作的是原有列，可以替换原有列的数据df.withColumn(&#x27;newWidth&#x27;,df.SepalWidth * 2).show() 删除一列 123# ==========删除一列 drop=========================# 删除一列df.drop(&#x27;newWidth&#x27;).show() 统计信息 1234# ================ 统计信息 describe================df.describe().show()# 计算某一列的描述信息df.describe(&#x27;newWidth&#x27;).show() 提取部分列 12# ===============提取部分列 select==============df.select(&#x27;SepalLength&#x27;,&#x27;SepalWidth&#x27;).show() 基本统计功能 12# ==================基本统计功能 distinct count=====df.select(&#x27;cls&#x27;).distinct().count() 分组统计 123456# 分组统计 groupby(colname).agg(&#123;&#x27;col&#x27;:&#x27;fun&#x27;,&#x27;col2&#x27;:&#x27;fun2&#x27;&#125;)df.groupby(&#x27;cls&#x27;).agg(&#123;&#x27;SepalWidth&#x27;:&#x27;mean&#x27;,&#x27;SepalLength&#x27;:&#x27;max&#x27;&#125;).show()# avg(), count(), countDistinct(), first(), kurtosis(),# max(), mean(), min(), skewness(), stddev(), stddev_pop(),# stddev_samp(), sum(), sumDistinct(), var_pop(), var_samp() and variance() 自定义的汇总方法 1234# 自定义的汇总方法import pyspark.sql.functions as fn# 调用函数并起一个别名df.agg(fn.count(&#x27;SepalWidth&#x27;).alias(&#x27;width_count&#x27;),fn.countDistinct(&#x27;cls&#x27;).alias(&#x27;distinct_cls_count&#x27;)).show() 拆分数据集 123# ====================数据集拆成两部分 randomSplit ===========# 设置数据比例将数据划分为两部分trainDF, testDF = df.randomSplit([0.6, 0.4]) 采样数据 12345# ================采样数据 sample===========# withReplacement：是否有放回的采样# fraction：采样比例# seed：随机种子sdf = df.sample(False,0.2,100) 查看两个数据集在类别上的差异 123# 查看两个数据集在类别上的差异 subtract，确保训练数据集覆盖了所有分类diff_in_train_test = testDF.select(&#x27;cls&#x27;).subtract(trainDF.select(&#x27;cls&#x27;))diff_in_train_test.distinct().count() 交叉表 12# ================交叉表 crosstab=============df.crosstab(&#x27;cls&#x27;,&#x27;SepalLength&#x27;).show() udf udf：自定义函数 1234567891011121314151617181920212223242526272829303132# ================== 综合案例 + udf================# 测试数据集中有些类别在训练集中是不存在的，找到这些数据集做后续处理trainDF,testDF = df.randomSplit([0.99,0.01])diff_in_train_test = trainDF.select(&#x27;cls&#x27;).subtract(testDF.select(&#x27;cls&#x27;)).distinct().show()# 首先找到这些类，整理到一个列表not_exist_cls = trainDF.select(&#x27;cls&#x27;).subtract(testDF.select(&#x27;cls&#x27;)).distinct().rdd.map(lambda x :x[0]).collect()# 定义一个方法，用于检测def should_remove(x): if x in not_exist_cls: return -1 else : return x# 创建udf，udf函数需要两个参数：# Function# Return type (in my case StringType())# 在RDD中可以直接定义函数，交给rdd的transformatioins方法进行执行# 在DataFrame中 需要通过udf将自定义函数封装成udf函数，创建一个UDF对象，只有UDF对象 才可以DataFrame进行调用执行from pyspark.sql.types import StringTypefrom pyspark.sql.functions import udf# 先封装成UDF函数。check = udf(should_remove,StringType())resultDF = trainDF.withColumn(&#x27;New_cls&#x27;,check(trainDF[&#x27;cls&#x27;])).filter(&#x27;New_cls &lt;&gt; -1&#x27;)resultDF.show()","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"06-二叉树的最小深度","slug":"06-二叉树的最小深度","date":"2021-07-04T13:36:56.000Z","updated":"2021-07-04T13:38:02.041Z","comments":true,"path":"20210704/06-二叉树的最小深度.html","link":"","permalink":"https://xxren8218.github.io/20210704/06-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%9C%80%E5%B0%8F%E6%B7%B1%E5%BA%A6.html","excerpt":"","text":"二叉树的最小深度 111.二叉树的最小深度 直觉上好像和求最大深度差不多，其实还是差不少的。 遍历顺序上依然是后序遍历（因为要比较递归返回之后的结果），但在处理中间节点的逻辑上，最大深度很容易理解，最小深度可有一个误区，如图： 这就重新审题了，题目中说的是：「最小深度是从根节点到最近叶子节点的最短路径上的节点数量。」，注意是「叶子节点」。 什么是叶子节点，左右孩子都为空的节点才是叶子节点！ 1.递归法来来来，一起递归三部曲： 确定递归函数的参数和返回值 参数为要传入的二叉树根节点。 代码如下： 1def getDepth(node): 确定终止条件 终止条件也是遇到空节点返回0，表示当前节点的高度为0。 代码如下： 1if not node: return 0 确定单层递归的逻辑 这块和求最大深度可就不一样了，一些同学可能会写如下代码： 1234leftDepth = getDepth(node.left)rightDepth = getDepth(node.right)result = 1 + min(leftDepth, rightDepth)return result 这个代码就犯了此图中的误区： 如果这么求的话，没有左孩子的分支会算为最短深度。 所以，如果左子树为空，右子树不为空，说明最小深度是 1 + 右子树的深度。 反之，右子树为空，左子树不为空，最小深度是 1 + 左子树的深度。最后如果左右子树都不为空，返回左右子树深度最小值 + 1 。 代码如下： 123456789101112leftDepth = getDepth(node.left) # 左rightDepth = getDepth(node.right) # 右 # 中# 当一个左子树为空，右不为空，这时并不是最低点if not node.left and node.right: return 1 + rightDepth# 当一个右子树为空，左不为空，这时并不是最低点if node.left and not node.right: return 1 + leftDepthresult = 1 + min(leftDepth, rightDepth) return result 遍历的顺序为后序（左右中），可以看出：「求二叉树的最小深度和求二叉树的最大深度的差别主要在于处理左右孩子不为空的逻辑。」 整体递归代码如下： 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def minDepth(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return 0 return self.getDepth(root) def getDepth(self, node): if not node: return 0 leftDpeth = self.getDepth(node.left) rightDepth = self.getDepth(node.right) if not node.left and node.right: return 1 + rightDepth if node.left and not node.right: return 1 + leftDpeth result = 1 + min(leftDpeth, rightDepth) return result 2. 迭代法按照层序遍历的方法很好解决：直接套模板。用res来记录结果，每进入一层res加1，当左右孩子都为空的时候，说明为最小深度，直接返回结果即可。 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def minDepth(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return 0 queue = [root] res = 0 while queue: res += 1 for _ in range(len(queue)): cur_node = queue.pop(0) if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) if not cur_node.left and not cur_node.right: return res 总结 最大深度与最小深度也不过如此嘛！可以用【递归法】和【迭代法分别实现】！","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"05-二（N）叉树的最大深度","slug":"05-二（N）叉树的最大深度","date":"2021-07-04T13:35:05.000Z","updated":"2021-07-04T17:04:51.355Z","comments":true,"path":"20210704/05-二（N）叉树的最大深度.html","link":"","permalink":"https://xxren8218.github.io/20210704/05-%E4%BA%8C%EF%BC%88N%EF%BC%89%E5%8F%89%E6%A0%91%E7%9A%84%E6%9C%80%E5%A4%A7%E6%B7%B1%E5%BA%A6.html","excerpt":"","text":"二（N）叉树的最大深度 1. 二叉树的最大深度 104.二叉树的最大深度 559.N叉树的最大深度 给定一个二叉树，找出其最大深度。 二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。 说明: 叶子节点是指没有子节点的节点。 示例：给定二叉树 [3,9,20,null,null,15,7]， 返回它的最大深度 3 。 1. 思路1.1 递归法本题其实也要后序遍历（左右中），依然是因为要通过递归函数的返回值做计算树的高度。 按照递归三部曲，来看看如何来写。 确定递归函数的参数和返回值：参数就是传入树的根节点，返回就返回这棵树的深度。 代码如下： 1def getDepth(self, node): 确定终止条件：如果为空节点的话，就返回0，表示高度为0。 代码如下： 1if node == None: return 0 确定单层递归的逻辑：先求它的左子树的深度，再求的右子树的深度，最后取左右深度最大的数值 再+1 （加1是因为算上当前中间节点）就是目前节点为根节点的树的深度。 代码如下：后序遍历，按照左右中的顺序） 1234leftDepth = getDepth(node.left); # 左rightDepth = getDepth(node.right); # 右depth = 1 + max(leftDepth, rightDepth); # 中return depth 所以整体代码如下： 1234567891011121314151617181920# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def maxDepth(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return 0 return self.getDepth(root) def getDepth(self, node): if not node: return 0 leftDepth = self.getDepth(node.left) rightDepth = self.getDepth(node.right) return 1 + max(leftDepth, rightDepth) 1.2 迭代法使用迭代法的话，使用层序遍历是比较合适的，也是比较容易理解的。因为最大的深度就是二叉树的层数，和层序遍历的方式极其吻合。 在二叉树中，一层一层的来遍历二叉树，记录一下遍历的层数就是二叉树的深度，如图所示： 所以这道题的迭代法就是一道【模板题】，可以使用二叉树层序遍历的模板来解决的。 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def maxDepth(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return 0 queue = [root] res = 0 while queue: # 框架上改一行 res += 1 n = len(queue) for _ in range(n): cur_node = queue.pop(0) if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) return res 2. N叉树的最大深度 给定一个 N 叉树，找到其最大深度。 最大深度是指从根节点到最远叶子节点的最长路径上的节点总数。 例如，给定一个 3叉树 : 我们应返回其最大深度，3。 1. 思路：依然可以提供递归法和迭代法，来解决这个问题，思路是和二叉树思路一样的，直接给出代码如下： 2.1 递归法1234567891011121314151617181920212223242526&quot;&quot;&quot;# Definition for a Node.class Node(object): def __init__(self, val=None, children=None): self.val = val self.children = children&quot;&quot;&quot;class Solution(object): def maxDepth(self, root): &quot;&quot;&quot; :type root: Node :rtype: int &quot;&quot;&quot; if not root: return 0 return self.getDepth(root) def getDepth(self, node): if not node: return 0 depth = 0 # leftDepth = self.getDepth(node.left) # rightDepth = self.getDepth(node.right) # return 1 + max(leftDepth, rightDepth) for i in range(len(node.children)): depth = max(depth, self.getDepth(node.children[i])) return 1 + depth 2.2 迭代法同样的也是套用模板。 1234567891011121314151617181920212223242526272829&quot;&quot;&quot;# Definition for a Node.class Node(object): def __init__(self, val=None, children=None): self.val = val self.children = children&quot;&quot;&quot;class Solution(object): def maxDepth(self, root): &quot;&quot;&quot; :type root: Node :rtype: int &quot;&quot;&quot; if not root: return 0 res = 0 queue = [root] while queue: n = len(queue) res += 1 for i in range(n): cur_node = queue.pop(0) # 只需要改变这里就可以了。 while cur_node.children: queue.append(cur_node.children.pop(0)) return res 总结 至此，二（N）叉树的最大深度就完成了，分别采用了递归法和迭代法进行求解，递归法可以看到递归三部曲的身影，迭代法套用模板即可！","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"04-对称二叉树","slug":"04-对称二叉树","date":"2021-07-04T13:32:41.000Z","updated":"2021-07-04T13:34:15.917Z","comments":true,"path":"20210704/04-对称二叉树.html","link":"","permalink":"https://xxren8218.github.io/20210704/04-%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91.html","excerpt":"","text":"对称二叉树 101.对称二叉树 给定一个二叉树，检查它是否是镜像对称的。 一、思路「首先想清楚，判断对称二叉树要比较的是哪两个节点，要比较的可不是左右节点！」 对于二叉树是否对称，要比较的是根节点的左子树与右子树是不是相互翻转的，理解这一点就知道了「其实我们要比较的是两个树（这两个树是根节点的左右子树）」，所以在递归遍历的过程中，也是要同时遍历两棵树。 那么如果比较呢？ 比较的是两个子树的里侧和外侧的元素是否相等。如图所示： 那么遍历的顺序应该是什么样的呢？ 本题遍历只能是“后序遍历”，因为我们要通过递归函数的返回值来判断两个子树的内侧节点和外侧节点是否相等。 「正是因为要遍历两棵树而且要比较内侧和外侧节点，所以准确的来说是一个树的遍历顺序是左右中，一个树的遍历顺序是右左中。」 但都可以理解算是后序遍历，尽管已经不是严格上在一个树上进行遍历的后序遍历了。 说到这大家可能感觉我有点啰嗦，哪有这么多道理，上来就干就完事了。别急，我说的这些在下面的代码讲解中都有身影。 那么我们先来看看递归法的代码应该怎么写。 二、递归法递归三部曲 确定递归函数的参数和返回值 因为我们要比较的是根节点的两个子树是否是相互翻转的，进而判断这个树是不是对称树，所以要比较的是两个树，参数自然也是左子树节点和右子树节点。 返回值自然是bool类型。 代码如下： 1def compare(left, right): -&gt; bool 确定终止条件 要比较两个节点数值相不相同，首先要把两个节点为空的情况弄清楚！否则后面比较数值的时候就会操作空指针了。 节点为空的情况有：（「注意我们比较的其实不是左孩子和右孩子，所以如下我称之为左节点右节点」） 左节点为空，右节点不为空，不对称，return false 左不为空，右为空，不对称 return false 左右都为空，对称，返回true 此时已经排除掉了节点为空的情况，那么剩下的就是左右节点不为空： 左右都不为空，比较节点数值，不相同就return false 此时左右节点不为空，且数值也不相同的情况我们也处理了。 代码如下： 1234if left == None and right == None: return Trueif left != None and right == None: return Falseif left == None and right != None: return Falseif left.val != right.val: return False 确定单层递归的逻辑 此时才进入单层递归的逻辑，单层递归的逻辑就是处理 右节点都不为空，且数值相同的情况。 比较二叉树外侧是否对称：传入的是左节点的左孩子，右节点的右孩子。 比较内测是否对称，传入左节点的右孩子，右节点的左孩子。 如果左右都对称就返回true ，有一侧不对称就返回false 。 代码如下： 123outside = compare(left.left, right.right)inside = compare(left.right, right.left)isSame = outside and inside 如上代码中，我们可以看出使用的遍历方式，左子树左右中，右子树右左中，所以我把这个遍历顺序也称之为“后序遍历”（尽管不是严格的后序遍历）. 最后看一下完整的代码： 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def isSymmetric(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: bool &quot;&quot;&quot; if not root: return None return self.compare(root.left, root.right) def compare(self, left, right): if left == None and right == None: return True if left != None and right == None: return False if left == None and right != None: return False if left.val != right.val: return False outside = self.compare(left.left, right.right) inside = self.compare(left.right, right.left) isSame = outside and inside return isSame 「建议大家做题的时候，一定要想清楚逻辑，每一步做什么。把道题目所有情况想到位，相应的代码写出来之后，再去追求简洁代码的效果。」 三、迭代法这道题目我们也可以使用迭代法，但要注意，这里的迭代法可不是前中后序的迭代写法，因为本题的本质是判断两个树是否是相互翻转的，其实已经不是所谓二叉树遍历的前中后序的关系了。 这里我们可以使用队列来比较两个树（根节点的左右子树）是否相互翻转，（「注意这不是层序遍历」） 1.使用队列通过队列来判断根节点的左子树和右子树的内侧和外侧是否相等，如动画所示： 如下的条件判断和递归的逻辑是一样的。 12345678910111213141516171819202122232425262728293031323334353637383940# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def isSymmetric(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: bool &quot;&quot;&quot; if not root: return None queue = [] queue.append(root.left) queue.append(root.right) while queue: # 将左右节点分别加入队列 left_node = queue.pop(0) right_node = queue.pop(0) # 两个都为空，则对称，继续。 if not left_node and not right_node: continue # 一个为空，另一个不为空，返回False if (not left_node and right_node) or \\ (left_node and not right_node): return False # 两个都不为空，值不相等，返回False if left_node.val != right_node.val: return False queue.append(left_node.left) queue.append(right_node.right) queue.append(left_node.right) queue.append(right_node.left) return True 2.使用栈细心的话，其实可以发现，这个迭代法，其实是把左右两个子树要比较的元素顺序放进一个容器，然后成对成对的取出来进行比较，那么其实使用栈也是可以的。——先判断里侧和先判断外侧的顺序不影响。 只要把队列原封不动的改成栈就可以了，我下面也给出了代码。 12345678910111213141516171819202122232425262728293031323334353637383940# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def isSymmetric(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: bool &quot;&quot;&quot; if not root: return None stack = [] stack.append(root.left) stack.append(root.right) while stack: # 将左右节点分别加入队列 left_node = stack.pop() right_node = stack.pop() # 两个都为空，则对称，继续。 if not left_node and not right_node: continue # 一个为空，另一个不为空，返回False if (not left_node and right_node) or \\ (left_node and not right_node): return False # 两个都不为空，值不相等，返回False if left_node.val != right_node.val: return False stack.append(left_node.left) stack.append(right_node.right) stack.append(left_node.right) stack.append(right_node.left) return True 四、总结 这次我们又深度剖析了一道二叉树的“简单题”，大家会发现，真正的把题目搞清楚其实并不简单，leetcode上accept了和真正掌握了还是有距离的。 我们介绍了递归法和迭代法，递归依然通过递归三部曲来解决了这道题目。 在迭代法中我们使用了队列，需要注意的是这不是层序遍历，而且仅仅通过一个容器来成对的存放我们要比较的元素，知道这一本质之后就发现：用队列，用栈，甚至用数组，都是可以的。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"29-spark_core补充","slug":"29-spark-core补充","date":"2021-07-03T17:58:13.000Z","updated":"2021-07-03T18:01:52.543Z","comments":true,"path":"20210704/29-spark-core补充.html","link":"","permalink":"https://xxren8218.github.io/20210704/29-spark-core%E8%A1%A5%E5%85%85.html","excerpt":"","text":"spark 相关概念补充掌握目标 了解spark的安装部署 知道spark作业提交集群的过程 1. spark的安装部署 1、下载spark安装包 http://spark.apache.org/downloads.html 高版本不存在cdh的编译版本，可以从官网下载源码版本，指定高版本hadoop进行编译 编译步骤： 1，安装java(JDK 1.7及以上) 1234export JAVA_HOME&#x3D;&#x2F;xxxexport JRE_HOME&#x3D;&#x2F;xxxexport CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar:$JRE_HOME&#x2F;lib:$CLASSPATHexport PATH&#x3D;$JAVA_HOME&#x2F;bin:$PATH 2，安装Maven， 版本为3.3.9或者以上 下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache//maven/maven-3/3.3.9/binaries 配置MAVEN_HOME 12export MAVEN_HOME&#x3D;&#x2F;xxxexport PATH&#x3D;$MAVEN_HOME&#x2F;bin:$PATH 3，下载spark源码 4，增加cdh的repository 解压spark的源码包，编辑pom.xml文件， 在repositories节点 加入如下配置： 123&lt;repository&gt; &lt;id&gt;cloudera&lt;&#x2F;id&gt; &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&#x2F;&lt;&#x2F;url&gt;&lt;&#x2F;repository&gt; 5，编译 设置内存： export MAVEN_OPTS=”-Xmx2g -XX:ReservedCodeCacheSize=512m” 开始编译： 1.&#x2F;dev&#x2F;make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version&#x3D;2.6.0-cdh5.7.0 -DskipTests clean package 源码编译后，bin目录下的文件可能不存在可执行权限，需要通过chmod指令添加可执行权限 chmod +x xxx 2、规划spark安装目录 3、解压安装包 4、重命名安装目录 5、修改配置文件 spark-env.sh(需要将spark-env.sh.template重命名) 配置java环境变量 export JAVA_HOME=java_home_path 配置PYTHON环境 export PYSPARK_PYTHON=/xx/pythonx_home/bin/pythonx 配置master的地址 export SPARK_MASTER_HOST=node-teach 配置master的端口 export SPARK_MASTER_PORT=7077 6、配置spark环境变量 export SPARK_HOME=/xxx/spark2.x export PATH=\\$PATH:\\$SPARK_HOME/bin 2. 启动Spark集群 进入到$SPARK_HOME/sbin目录 启动Master 1./start-master.sh -h 192.168.19.137 启动Slave 1./start-slave.sh spark://192.168.19.137:7077 jps查看进程 1227073 Master27151 Worker 关闭防火墙 1systemctl stop firewalld 通过SPARK WEB UI查看Spark集群及Spark http://192.168.199.188:8080/ 监控Spark集群 http://192.168.199.188:4040/ 监控Spark Job 3. spark 集群相关概念 spark集群架构(Standalone模式) Application 用户自己写的Spark应用程序，批处理作业的集合。Application的main方法为应用程序的入口，用户通过Spark的API，定义了RDD和对RDD的操作。 Master和Worker 整个集群分为 Master 节点和 Worker 节点，相当于 Hadoop 的 Master 和 Slave 节点。 Master：Standalone模式中主控节点，负责接收Client提交的作业，管理Worker，并命令Worker启动Driver和Executor。 Worker：Standalone模式中slave节点上的守护进程，负责管理本节点的资源，定期向Master汇报心跳，接收Master的命令，启动Driver和Executor。 Client：客户端进程，负责提交作业到Master。 Driver： 一个Spark作业运行时包括一个Driver进程，也是作业的主进程，负责作业的解析、生成Stage并调度Task到Executor上。包括DAGSchedule（负责作业的拆解），TaskScheduler（负责把对应的Task发到对应的Worker上，交给Executor求执行）。 Executor：即真正执行作业的地方，一个集群一般包含多个Executor，每个Executor接收Driver的命令Launch Task，一个Executor可以执行一到多个Task。 Spark作业相关概念 Stage：一个Spark作业一般包含一到多个Stage。 Task：一个Stage包含一到多个Task，通过多个Task实现并行运行的功能（可能算的结果不一样，前面有讲）。 DAGScheduler： 实现将Spark作业分解成一到多个Stage，每个Stage根据RDD的Partition个数决定Task的个数，然后生成相应的Task set放到TaskScheduler中。 TaskScheduler：实现Task分配到Executor上执行。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"28-spark_core 实战案例","slug":"28-spark-core-实战案例","date":"2021-07-02T17:03:57.000Z","updated":"2021-07-02T17:07:57.328Z","comments":true,"path":"20210703/28-spark-core-实战案例.html","link":"","permalink":"https://xxren8218.github.io/20210703/28-spark-core-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B.html","excerpt":"","text":"spark-core 实战案例掌握目标： 独立实现Spark RDD的word count案例 独立实现spark RDD的PV UV统计案例 说出广播变量的概念 对于数据在shell里面写，没有交互，对于数据分析而言不好，将其用PyCharm编写可进行交互，便于分析。 即在PyCharm上编写，然后数据同步到Centos上面运行，运行结束后，还在PyCharm上面显示。 1.0 Pycharm编写spark代码环境配置准备pycharm环境 1.PyCharm的配置 file-&gt;new-project-&gt;pure Python-&gt;exiting interpreter-&gt;add remote-&gt;SSH Crederitals-&gt;填写Host(192.168.19.137)-&gt;Username(root)-&gt;密码-&gt;选择python的解释器路径为(/miniconda2/envs/py365/bin/python)-&gt;Remote project location(/root/bigdata/code) 代码如下： 12345678910from pyspark import SparkContext# 第一个是链接用的哪种环境，若几个集群的话，传递的应该是master节点的RL地址sc = SparkContext(&#x27;local[2]&#x27;,&#x27;wordcount&#x27;)rdd1 = sc.textFile(&quot;file:///root/code/test.txt&quot;).\\ flatMap(lambda x: x.split()).map(lambda x: (x, 1)).\\ reduceByKey(lambda a,b: a+b)print(rdd1.collect()) 运行出现了错误： 解决方法： 在centos上面查找JAVA_HOME所在位置(vi ~/.bash_profile)，添加到环境变量中： 1234567891011121314151617####################################import osJAVA_HOME = &#x27;/root/bigdata/jdk&#x27;os.environ[&#x27;JAVA_HOME&#x27;] = JAVA_HOME####################################from pyspark import SparkContext# 第一个master的位置，第二个是spark作业的名字。sc = SparkContext(&#x27;local[2]&#x27;,&#x27;wordcount&#x27;) # 第一个是链接用的哪种环境，若几个集群的话，传递的应该是master节点的RL地址rdd1 = sc.textFile(&quot;file:///root/code/test.txt&quot;).\\ flatMap(lambda x: x.split()).map(lambda x: (x, 1)).\\ reduceByKey(lambda a,b: a+b)print(rdd1.collect()) 还会出现python的版本不一致的问题，再添加python的环境。以及pyspark的版本问题。 1234567891011121314151617####################################import osJAVA_HOME = &#x27;/root/bigdata/jdk&#x27;PYSPARK_PYTHON = &quot;/miniconda2/envs/py365/bin/python&quot;os.environ[&#x27;JAVA_HOME&#x27;] = JAVA_HOMEos.environ[&#x27;PYSPARK_PYTHON&#x27;] = PYSPARK_PYTHONos.environ[&#x27;PYSPARK_DRIVER_PYTHON&#x27;] = PYSPARK_PYTHON####################################from pyspark import SparkContextsc = SparkContext(&#x27;local[2]&#x27;,&#x27;wordcount&#x27;)rdd1 = sc.textFile(&quot;file:///root/code/test.txt&quot;).\\ flatMap(lambda x: x.split()).map(lambda x: (x,1)).\\ reduceByKey(lambda a,b: a+b)print(rdd1.collect()) 1.1利用PyCharm编写spark wordcount程序 代码 1234567891011121314151617import osJAVA_HOME = &#x27;/root/bigdata/jdk&#x27;PYSPARK_PYTHON = &quot;/miniconda2/envs/py365/bin/python&quot;os.environ[&quot;JAVA_HOME&quot;] = JAVA_HOMEos.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHONos.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHONfrom pyspark import SparkContextif __name__ == &#x27;__main__&#x27;: # 创建spark context sc = SparkContext(&#x27;local[2]&#x27;,&#x27;wordcount&#x27;) # 通过spark context 获取rdd rdd1 = sc.textFile(&#x27;file:///root/tmp/test.txt&#x27;) rdd2 = rdd1.flatMap(lambda line:line.split()) rdd3 = rdd2.map(lambda x:(x,1)) rdd4 = rdd3.reduceByKey(lambda x,y:x+y) print(rdd4.collect()) 1.2 通过spark实现点击流日志分析在新闻类网站中，经常要衡量一条网络新闻的页面访问量，最常见的就是uv和pv，如果在所有新闻中找到访问最多的前几条新闻，topN是最常见的指标。 数据示例 12345678910# 每条数据代表一次访问记录 包含了ip 访问时间 访问的请求方式 访问的地址...信息194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] &quot;GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/4.0 (compatible;)&quot;183.49.46.228 - - [18/Sep/2013:06:49:23 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot;163.177.71.12 - - [18/Sep/2013:06:49:33 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;163.177.71.12 - - [18/Sep/2013:06:49:36 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;101.226.68.137 - - [18/Sep/2013:06:49:42 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;101.226.68.137 - - [18/Sep/2013:06:49:45 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;60.208.6.156 - - [18/Sep/2013:06:49:48 +0000] &quot;GET /wp-content/uploads/2013/07/rcassandra.png HTTP/1.0&quot; 200 185524 &quot;http://cos.name/category/software/packages/&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;222.68.172.190 - - [18/Sep/2013:06:49:57 +0000] &quot;GET /images/my.jpg HTTP/1.1&quot; 200 19939 &quot;http://www.angularjs.cn/A00n&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;222.68.172.190 - - [18/Sep/2013:06:50:08 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot; 访问的pv pv：网站的总访问量 123456789from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&quot;pv&quot;).getOrCreate()sc = spark.sparkContextrdd1 = sc.textFile(&quot;file:///root/bigdata/data/access.log&quot;)# 把每一行数据记为(&quot;pv&quot;,1)rdd2 = rdd1.map(lambda x:(&quot;pv&quot;,1)).reduceByKey(lambda a,b:a+b)rdd2.collect()sc.stop() 访问的uv uv：网站的独立用户访问量 123456789101112from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&quot;pv&quot;).getOrCreate()sc = spark.sparkContextrdd1 = sc.textFile(&quot;file:///root/bigdata/data/access.log&quot;)# 对每一行按照空格拆分，将ip地址取出rdd2 = rdd1.map(lambda x:x.split(&quot; &quot;)).map(lambda x:x[0])# 把每个ur记为1rdd3 = rdd2.distinct().map(lambda x:(&quot;uv&quot;,1))rdd4 = rdd3.reduceByKey(lambda a,b:a+b)rdd4.saveAsTextFile(&quot;hdfs:///uv/result&quot;)sc.stop() 访问的topN 12345678910111213from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&quot;topN&quot;).getOrCreate()sc = spark.sparkContextrdd1 = sc.textFile(&quot;file:///root/bigdata/data/access.log&quot;)# 对每一行按照空格拆分，将url数据取出，把每个url记为1rdd2 = rdd1.map(lambda x:x.split(&quot; &quot;)).filter(lambda x:len(x)&gt;10).map(lambda x:(x[10],1))# 对数据进行累加，按照url出现次数的降序排列rdd3 = rdd2.reduceByKey(lambda a,b:a+b).sortBy(lambda x:x[1],ascending=False)# 取出序列数据中的前n个rdd4 = rdd3.take(5)rdd4.collect()sc.stop() 注意：这里不懂的可以去上篇看数据的具体格式！ 2. 通过spark实现ip地址查询需求 在互联网中，我们经常会见到城市热点图这样的报表数据，例如在百度统计中，会统计今年的热门旅游城市、热门报考学校等，会将这样的信息显示在热点图中。 因此，我们需要通过日志信息（运行商或者网站自己生成）和城市ip段信息来判断用户的ip段，统计热点经纬度。 ip日志信息 在ip日志信息中，我们只需要关心ip这一个维度就可以了，其他的不做介绍 思路 1、 加载城市ip段信息，获取ip起始数字和结束数字，经度，纬度 2、 加载日志数据，获取ip信息，然后转换为数字，和ip段比较 3、 比较的时候采用二分法查找，找到对应的经度和纬度 4，对相同的经度和纬度做累计求和 5， 取出最终的topN的经纬度 启动Spark集群 进入到$SPARK_HOME/sbin目录 启动Master 1./start-master.sh -h 192.168.199.188 启动Slave 1./start-slave.sh spark://192.168.199.188:7077 jps查看进程 1227073 Master27151 Worker 关闭防火墙 1systemctl stop firewalld 通过SPARK WEB UI查看Spark集群及Spark http://192.168.199.188:8080/ 监控Spark集群 http://192.168.199.188:4040/ 监控Spark Job 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from pyspark.sql import SparkSession# 255.255.255.255 0~255 256个数 2^8 是8位2进制数 ——&gt;转化成32位的二进制数#将ip转换为特殊的数字形式 223.243.0.0|223.243.191.255| 255 2^8#‭11011111‬#00000000#1101111100000000#‭ 11110011‬#11011111111100110000000000000000def ip_transform(ip): ips = ip.split(&quot;.&quot;) # [223,243,0,0] 32位二进制数 ip_num = 0 for i in ips: ip_num = int(i) | ip_num &lt;&lt; 8 return ip_num# 二分法查找ip对应的行的索引def binary_search(ip_num, broadcast_value): start = 0 end = len(broadcast_value) - 1 while (start &lt;= end): mid = int((start + end) / 2) if ip_num &gt;= int(broadcast_value[mid][0]) and ip_num &lt;= int(broadcast_value[mid][1]): return mid if ip_num &lt; int(broadcast_value[mid][0]): end = mid if ip_num &gt; int(broadcast_value[mid][1]): start = middef main(): spark = SparkSession.builder.appName(&quot;test&quot;).getOrCreate() sc = spark.sparkContext city_id_rdd = sc.textFile(&quot;file:///home/hadoop/app/tmp/data/ip.txt&quot;).map(lambda x:x.split(&quot;|&quot;)).map(lambda x: (x[2], x[3], x[13], x[14])) # 创建一个广播变量 city_broadcast = sc.broadcast(city_id_rdd.collect()) dest_data = sc.textFile(&quot;file:///home/hadoop/app/tmp/data/20090121000132.394251.http.format&quot;).map( lambda x: x.split(&quot;|&quot;)[1]) # 根据取出对应的位置信息 def get_pos(x): # 从广播变量中获取ip地址库 city_broadcast_value = city_broadcast.value # 根据单个ip获取对应经纬度信息 def get_result(ip): ip_num = ip_transform(ip) index = binary_search(ip_num, city_broadcast_value) # ((纬度,精度),1) return ((city_broadcast_value[index][2], city_broadcast_value[index][3]), 1) x = map(tuple,[get_result(ip) for ip in x]) return x dest_rdd = dest_data.mapPartitions(lambda x: get_pos(x)) # ((纬度,精度),1) result_rdd = dest_rdd.reduceByKey(lambda a, b: a + b).sortBy(lambda x:x[1],ascending=False) print(result_rdd.collect()) sc.stop()if __name__ == &#x27;__main__&#x27;: main() 广播变量的使用 要统计Ip所对应的经纬度, 每一条数据都会去查询ip表 每一个task 都需要这一个ip表, 默认情况下, 所有task都会去复制ip表 实际上 每一个Worker上会有多个task, 数据也是只需要进行查询操作的, 所以这份数据可以共享,没必要每个task复制一份 可以通过广播变量, 通知当前worker上所有的task, 来共享这个数据,避免数据的多次复制,可以大大降低内存的开销 sparkContext.broadcast(要共享的数据) mapPartitions transformation操作 类似map 但是map是一条一条传给里面函数的 mapPartitions 数据是一部分一部分传给函数的 应用场景 数据处理的时候 需要连接其它资源 如果一条一条处理 会处理一条连一次， 一份一份处理可以很多条数据连一次其它资源 可以提高效率 二分法查找 ip_transform 把223.243.0.0 转换成10进制的数字——位运算。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"03-翻转二叉树--递归和迭代的应用","slug":"03-翻转二叉树-递归和迭代的应用","date":"2021-07-02T10:16:14.000Z","updated":"2021-07-04T13:29:55.300Z","comments":true,"path":"20210702/03-翻转二叉树-递归和迭代的应用.html","link":"","permalink":"https://xxren8218.github.io/20210702/03-%E7%BF%BB%E8%BD%AC%E4%BA%8C%E5%8F%89%E6%A0%91-%E9%80%92%E5%BD%92%E5%92%8C%E8%BF%AD%E4%BB%A3%E7%9A%84%E5%BA%94%E7%94%A8.html","excerpt":"","text":"03_翻转二叉树—递归和迭代的应用 226.翻转二叉树 这道题目是非常经典的题目，也是比较简单的题目（至少一看就会）。 但正是因为这道题太简单，一看就会，一些同学都没有抓住其本质，稀里糊涂的就把这道题目过了。 如果做过这道题的同学也建议认真看完，相信一定有所收获！ 一、思路我们之前介绍的都是各种方式遍历二叉树，这次要翻转了，感觉还是有点懵逼。 这得怎么翻转呢？ 如果要从整个树来看，翻转还真的挺复杂，整个树以中间线进行翻转，如图： 可以发现想要翻转它，其实就把每一个节点的左右孩子交换一下（注意孩子下面的节点是一起交换的）就可以了。 关键在于遍历顺序，前中后序应该选哪一种遍历顺序？（一些同学这道题都过了，但是不知道自己用的是什么顺序） 遍历的过程中去翻转每一个节点的左右孩子就可以达到整体翻转的效果。 「注意只要把每一个节点的左右孩子翻转一下，就可以达到整体翻转的效果」 「这道题目使用前序遍历和后序遍历都可以，唯独中序遍历不行，因为中序遍历会把某些节点的左右孩子翻转了两次！建议拿纸画一画，就理解了」 那么层序遍历可以不可以呢？「依然可以的！只要把每一个节点的左右孩子翻转一下的遍历方式都是可以的！」 二、递归法我们下文以前序遍历为例，通过动画来看一下翻转的过程: 我们来看一下递归三部曲： 1.确定递归函数的参数和返回值 参数就是要传入节点的指针，不需要其他参数了，通常此时定下来主要参数，如果在写递归的逻辑中发现还需要其他参数的时候，随时补充。 返回值的话其实也不需要，但是题目中给出的要返回root节点的指针，可以直接使用题目定义好的函数，所以就函数的返回类型为TreeNode。 1def invertTree(self, root: TreeNode) -&gt; TreeNode: 2.确定终止条件: 当前节点为空的时候，就返回 1if not root: return None 3.确定单层递归的逻辑 因为是先前序遍历，所以先进行交换左右孩子节点，然后反转左子树，反转右子树。 12345678root.left, root.right = root.right, root.left# tmp = root.left# root.left = root.right# root.right = tmpinvertTree(root.left)invertTree(root.right) 基于这递归三步法，代码基本写完，代码如下（前序）： 123456789101112131415161718192021# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def invertTree(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: TreeNode &quot;&quot;&quot; if root == None: return None # 前序遍历 root.left, root.right = root.right, root.left self.invertTree(root.left) self.invertTree(root.right) return root 三、迭代法1.深度优先遍历可以很轻松的切出如下迭代法（前序）的代码：只需稍作修改即可。将添加值的操作改为变化左右 123456789101112131415161718192021222324252627282930# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def invertTree(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: TreeNode &quot;&quot;&quot; if not root: return None stack = [root] while stack: node = stack.pop() if node: ### 若不为空，将node的左右孩子调换。 node.left, node.right = node.right, node.left else: # 若为空，结束本次循环（他没有左右孩子） continue # 将右左孩子分别添加进stack中，因为是栈,所以先处理的是左 stack.append(node.right) stack.append(node.left) return root 2.广度优先遍历广度优先遍历也是比较容易的：只需要添加一行——处理队列的节点的代码改变即可。直接上代码！ 123456789101112131415161718192021222324252627# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def invertTree(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: TreeNode &quot;&quot;&quot; if not root: return None queue = [root] while queue: cur_node = queue.pop(0) ## 只需要添加这一行就行了！ cur_node.left, cur_node.right = cur_node.right, cur_node.left if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) return root 四、总结 针对二叉树的问题，解题之前一定要想清楚究竟是前中后序遍历，还是层序遍历。 「二叉树解题的大忌就是自己稀里糊涂的过了（因为这道题相对简单），但是也不知道自己是怎么遍历的。」 这也是造成了二叉树的题目“一看就会，一写就废”的原因。 针对翻转二叉树，我给出了一种递归，三种迭代（一种深度优先遍历，一种层序遍历）的写法，都是之前我们讲过的写法，融汇贯通一下而已。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"00-二叉树的层序遍历2","slug":"00-二叉树的层序遍历2","date":"2021-07-02T10:13:20.000Z","updated":"2021-07-04T13:30:05.787Z","comments":true,"path":"20210702/00-二叉树的层序遍历2.html","link":"","permalink":"https://xxren8218.github.io/20210702/00-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%B1%82%E5%BA%8F%E9%81%8D%E5%8E%862.html","excerpt":"","text":"二(N)叉树的层序遍历2上文学习了二叉树的层序遍历，还有四道题没有做完，今天就给做了吧！ 637.⼆叉树的层平均值 515.在每个树行中找最⼤值 116.填充每个节点的下⼀个右侧节点指针 117.填充每个节点的下⼀个右侧节点指针II 一、二叉树的层平均值 思路也是比较简单的，只需要在原来的基础上修改一行即可。对tmp列表的元素求和，然后除以列表的长度。——注意，python的两个整数相除仍是整数，需要把一个转化为浮点型。 123456789101112131415161718192021222324252627282930# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def averageOfLevels(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[float] &quot;&quot;&quot; if not root: return [] queue = [root] res = [] while queue: tmp = [] for _ in range(len(queue)): cur_node = queue.pop(0) tmp.append(cur_node.val) if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) # 仅仅修改这一行即可。 res.append(sum(tmp)/float(len(tmp))) return res 二、在每个树行中找最⼤值 这个思路不需多说吧，也是改变那一行的事情！ 123456789101112131415161718192021222324252627282930# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def largestValues(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[int] &quot;&quot;&quot; if not root: return [] queue = [root] res = [] while queue: tmp = [] for _ in range(len(queue)): cur_node = queue.pop(0) tmp.append(cur_node.val) if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) # 仅需修改这一行 res.append(max(tmp)) return res 三、填充每个节点的下⼀个右侧节点指针 看着这道题像层序遍历，那么我们就先写下我们的框架，然后看怎么处理！ 123456789101112131415161718192021222324252627282930&quot;&quot;&quot;# Definition for a Node.class Node(object): def __init__(self, val=0, left=None, right=None, next=None): self.val = val self.left = left self.right = right self.next = next&quot;&quot;&quot;class Solution(object): def connect(self, root): &quot;&quot;&quot; :type root: Node :rtype: Node &quot;&quot;&quot; if not root: return None queue = [root] while queue: for i in range(len(queue)): cur_node = queue.pop(0) if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) return root 这道题目的关键是返回Node，并不是list，所以，我写了上面的框架。 在框架上每层处理节点之间的“连线”即可。题目的难点在于: 如何知道下一个节点是啥？很容易那就是队列的第一个元素。 如何找到这层的最后一个节点。可以在进入for遍历之后取一下本层的长度n。这样“连线”时，n-1这个节点就是本层的最后了，不需要“连线”了。这样就解决了。 123456789101112131415161718192021222324252627282930313233343536&quot;&quot;&quot;# Definition for a Node.class Node(object): def __init__(self, val=0, left=None, right=None, next=None): self.val = val self.left = left self.right = right self.next = next&quot;&quot;&quot;class Solution(object): def connect(self, root): &quot;&quot;&quot; :type root: Node :rtype: Node &quot;&quot;&quot; if not root: return None queue = [root] while queue: # 记录本层的长度 n = len(queue) for i in range(n): cur_node = queue.pop(0) # 判断是否是本层的最后一个节点 if i &lt; n - 1: cur_node.next = queue[0] if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) return root 四、填充每个节点的下⼀个右侧节点指针II 代码思路完全一样，不过一个是完全二叉树、一个不是而已。上题写的代码具有鲁棒性。 123456789101112131415161718192021222324252627282930313233&quot;&quot;&quot;# Definition for a Node.class Node(object): def __init__(self, val=0, left=None, right=None, next=None): self.val = val self.left = left self.right = right self.next = next&quot;&quot;&quot;class Solution(object): def connect(self, root): &quot;&quot;&quot; :type root: Node :rtype: Node &quot;&quot;&quot; if not root: return None queue = [root] while queue: n = len(queue) for i in range(n): cur_node = queue.pop(0) if i &lt; n - 1: cur_node.next = queue[0] if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) return root 五、总结 至此层序遍历的题目做完了。归根结底就两个框架。 返回Node 1234567891011121314151617181920class Solution(object): def connect(self, root): &quot;&quot;&quot; :type root: Node :rtype: Node &quot;&quot;&quot; if not root: return None queue = [root] while queue: for i in range(len(queue)): cur_node = queue.pop(0) if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) return root 返回列表 12345678910111213141516171819202122232425class Solution(object): def connect(self, root): &quot;&quot;&quot; :type root: Node :rtype: Node &quot;&quot;&quot; if not root: return None queue = [root] res = [] while queue: tmp = [] for i in range(len(queue)): cur_node = queue.pop(0) tmp.append(cur_node.val) if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) res.append(tmp) return res","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"27-Spark_core概述","slug":"27-Spark-core概述","date":"2021-07-01T17:32:25.000Z","updated":"2021-07-01T17:36:09.544Z","comments":true,"path":"20210702/27-Spark-core概述.html","link":"","permalink":"https://xxren8218.github.io/20210702/27-Spark-core%E6%A6%82%E8%BF%B0.html","excerpt":"","text":"1. spark-core概述掌握目标： 知道RDD的概念 独立实现RDD的创建 1.1 什么是RDD RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合. Dataset:一个数据集，简单的理解为集合，用于存放数据的 Distributed：它的数据是分布式存储，并且可以做分布式的计算 Resilient：弹性的 它表示的是数据可以保存在磁盘，也可以保存在内存中——弹性的一种表现 数据分布式也是弹性的 弹性:并不是指他可以动态扩展，而是容错机制。 RDD会在多个节点上存储，就和hdfs的分布式道理是一样的。hdfs文件被切分为多个block存储在各个节点上，而RDD是被切分为多个partition。不同的partition可能在不同的节点上 spark读取hdfs的场景下，spark把hdfs的block读到内存就会抽象为spark的partition。 spark计算结束，一般会把数据做持久化到hive，hbase，hdfs等等。我们就拿hdfs举例，将RDD持久化到hdfs上，RDD的每个partition就会存成一个文件，如果文件小于128M，就可以理解为一个partition对应hdfs的一个block。反之，如果大于128M，就会被且分为多个block，这样，一个partition就会对应多个block。 不可变 可分区 并行计算 1.2 RDD的创建 第一步 创建sparkContext SparkContext, Spark程序的入口. SparkContext代表了和Spark集群的链接, 在Spark集群中通过SparkContext来创建RDD SparkConf 创建SparkContext的时候需要一个SparkConf， 用来传递Spark应用的基本信息 12conf = SparkConf().setAppName(appName).setMaster(master)sc = SparkContext(conf=conf) 创建RDD 进入pyspark环境 123456789101112131415161718[hadoop@hadoop000 ~]$ pysparkPython 3.5.0 (default, Nov 13 2018, 15:43:53)[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.19/03/08 12:19:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &#x27;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.3.0 /_/Using Python version 3.5.0 (default, Nov 13 2018 15:43:53)SparkSession available as &#x27;spark&#x27;.&gt;&gt;&gt; sc&lt;SparkContext master=local[*] appName=PySparkShell&gt; 在spark shell中 已经为我们创建好了 SparkContext 通过sc直接使用 可以在spark UI中看到当前的Spark作业 在浏览器访问当前centos的4040端口 Parallelized Collections方式创建RDD 调用SparkContext的 parallelize 方法并且传入已有的可迭代对象或者集合 12data = [1, 2, 3, 4, 5]distData = sc.parallelize(data) 123456&gt;&gt;&gt; data = [1, 2, 3, 4, 5]&gt;&gt;&gt; distData = sc.parallelize(data)&gt;&gt;&gt; data[1, 2, 3, 4, 5]&gt;&gt;&gt; distDataParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175 在spark ui中观察执行情况 在通过parallelize方法创建RDD 的时候可以指定分区数量 123&gt;&gt;&gt; distData = sc.parallelize(data,5) # 5表示数据分区的数量，最终一个分区对应一个task&gt;&gt;&gt; distData.reduce(lambda a, b: a + b)15 在spark ui中观察执行情况 Spark将为群集的每个分区（partition）运行一个任务（task）。 通常，可以根据CPU核心数量指定分区数量（每个CPU有2-4个分区）如未指定分区数量，Spark会自动设置分区数。 通过外部数据创建RDD PySpark可以从Hadoop支持的任何存储源创建RDD，包括本地文件系统，HDFS，Cassandra，HBase，Amazon S3等 支持整个目录、多文件、通配符 支持压缩文件 123&gt;&gt;&gt; rdd1 = sc.textFile(&#x27;file:///home/hadoop/tmp/word.txt&#x27;)&gt;&gt;&gt; rdd1.collect()[&#x27;foo foo quux labs foo bar quux abc bar see you by test welcome test&#x27;, &#x27;abc labs foo me python hadoop ab ac bc bec python&#x27;] 加载了数据以后接下来就是计算了。 2. spark-core RDD常用算子练习掌握目标 说出RDD的三类算子 掌握transformation和action算子的基本使用 2.1 RDD 常用操作 RDD 支持两种类型的操作： transformation 从一个已经存在的数据集创建一个新的数据集 rdd a ——-&gt;transformation ——&gt; rdd b 比如， map就是一个transformation 操作，把数据集中的每一个元素传给一个函数并返回一个新的RDD，代表transformation操作的结果 action 获取对数据进行运算操作之后的结果 比如， reduce 就是一个action操作，使用某个函数聚合RDD所有元素的操作，并返回最终计算结果 所有的transformation操作都是惰性的（lazy） 不会立即计算结果 只记下应用于数据集的transformation操作 只有调用action一类的操作之后才会计算所有transformation 这种设计使Spark运行效率更高 例如map reduce 操作，map创建的数据集将用于reduce，map阶段的结果不会返回，仅会返回reduce结果。 persist 操作 persist操作用于将数据缓存 可以缓存在内存中 也可以缓存到磁盘上， 也可以复制到磁盘的其它节点上 2.2 RDD Transformation算子 map: map(func)——进来几个元素，出去几个元素。 将func函数作用到数据集的每一个元素上，生成一个新的RDD返回 1234&gt;&gt;&gt; rdd1 = sc.parallelize([1,2,3,4,5,6,7,8,9],3)&gt;&gt;&gt; rdd2 = rdd1.map(lambda x: x+1)&gt;&gt;&gt; rdd2.collect() # collect就属于 action。若不调用拿不到结果。[2, 3, 4, 5, 6, 7, 8, 9, 10] 1234567&gt;&gt;&gt; rdd1 = sc.parallelize([1,2,3,4,5,6,7,8,9],3)&gt;&gt;&gt; def add(x):... return x+1...&gt;&gt;&gt; rdd2 = rdd1.map(add)&gt;&gt;&gt; rdd2.collect()[2, 3, 4, 5, 6, 7, 8, 9, 10] filter filter(func) 选出所有func返回值为true的元素，生成一个新的RDD返回 12345&gt;&gt;&gt; rdd1 = sc.parallelize([1,2,3,4,5,6,7,8,9],3)&gt;&gt;&gt; rdd2 = rdd1.map(lambda x:x*2)&gt;&gt;&gt; rdd3 = rdd2.filter(lambda x:x&gt;4)&gt;&gt;&gt; rdd3.collect()[6, 8, 10, 12, 14, 16, 18] flatmap flatMap会先执行map的操作，再将所有对象合并为一个对象 1234&gt;&gt;&gt; rdd1 = sc.parallelize([&quot;a b c&quot;,&quot;d e f&quot;,&quot;h i j&quot;])&gt;&gt;&gt; rdd2 = rdd1.flatMap(lambda x:x.split(&quot; &quot;))&gt;&gt;&gt; rdd2.collect()[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;j&#x27;] flatMap和map的区别：flatMap在map的基础上将结果合并到一个list中 1234&gt;&gt;&gt; rdd1 = sc.parallelize([&quot;a b c&quot;,&quot;d e f&quot;,&quot;h i j&quot;])&gt;&gt;&gt; rdd2 = rdd1.map(lambda x:x.split(&quot; &quot;))&gt;&gt;&gt; rdd2.collect()[[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;], [&#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;], [&#x27;h&#x27;, &#x27;i&#x27;, &#x27;j&#x27;]] union 对两个RDD求并集 12345&gt;&gt;&gt; rdd1 = sc.parallelize([(&quot;a&quot;,1),(&quot;b&quot;,2)])&gt;&gt;&gt; rdd2 = sc.parallelize([(&quot;c&quot;,1),(&quot;b&quot;,3)])&gt;&gt;&gt; rdd3 = rdd1.union(rdd2)&gt;&gt;&gt; rdd3.collect()[(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;c&#x27;, 1), (&#x27;b&#x27;, 3)] intersection 对两个RDD求交集 123456&gt;&gt;&gt; rdd1 = sc.parallelize([(&quot;a&quot;,1),(&quot;b&quot;,2)])&gt;&gt;&gt; rdd2 = sc.parallelize([(&quot;c&quot;,1),(&quot;b&quot;,3)])&gt;&gt;&gt; rdd3 = rdd1.union(rdd2)&gt;&gt;&gt; rdd4 = rdd3.intersection(rdd2)&gt;&gt;&gt; rdd4.collect()[(&#x27;c&#x27;, 1), (&#x27;b&#x27;, 3)] groupByKey 以元组中的第0个元素作为key，进行分组，返回一个新的RDD 1234567&gt;&gt;&gt; rdd1 = sc.parallelize([(&quot;a&quot;,1),(&quot;b&quot;,2)])&gt;&gt;&gt; rdd2 = sc.parallelize([(&quot;c&quot;,1),(&quot;b&quot;,3)])&gt;&gt;&gt; rdd3 = rdd1.union(rdd2)&gt;&gt;&gt; rdd4 = rdd3.groupByKey()&gt;&gt;&gt; rdd4.collect()[(&#x27;a&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fba6a5e5898&gt;), (&#x27;c&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fba6a5e5518&gt;), (&#x27;b&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fba6a5e5f28&gt;)] groupByKey之后的结果中 value是一个Iterable 123456&gt;&gt;&gt; result[2](&#x27;b&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fba6c18e518&gt;)&gt;&gt;&gt; result[2][1]&lt;pyspark.resultiterable.ResultIterable object at 0x7fba6c18e518&gt;&gt;&gt;&gt; list(result[2][1])[2, 3] reduceByKey 将key相同的键值对，按照Function进行计算 123&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])&gt;&gt;&gt; rdd.reduceByKey(lambda x,y:x+y).collect()[(&#x27;b&#x27;, 1), (&#x27;a&#x27;, 2)] sortByKey sortByKey(ascending=True, numPartitions=None, keyfunc=","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"00-二(N)叉树的层序遍历——训练","slug":"00-二-N-叉树的层序遍历——训练","date":"2021-07-01T11:02:04.000Z","updated":"2021-07-01T11:04:38.797Z","comments":true,"path":"20210701/00-二-N-叉树的层序遍历——训练.html","link":"","permalink":"https://xxren8218.github.io/20210701/00-%E4%BA%8C-N-%E5%8F%89%E6%A0%91%E7%9A%84%E5%B1%82%E5%BA%8F%E9%81%8D%E5%8E%86%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83.html","excerpt":"","text":"二(N)叉树的层序遍历——训练学会⼆叉树的层序遍历，可以⼀⼝⽓撸完leetcode上⼋道题⽬： 102.⼆叉树的层序遍历 429.N叉树的前序遍历 107.⼆叉树的层次遍历II 199.⼆叉树的右视图 637.⼆叉树的层平均值 515.在每个树⾏中找最⼤值 116.填充每个节点的下⼀个右侧节点指针 117.填充每个节点的下⼀个右侧节点指针II 这次先做四道题吧！，找找感觉！ 给你⼀个⼆叉树，请你返回其按 层序遍历 得到的节点值。 （即逐层地，从左到右访问所有节点）。 层序遍历是利用队列实现的： 如下图： 可以看出来，队列实现层序遍历是这样的顺序，现将节点出队-&gt;然后取值-&gt;将其左右孩子分别入队-&gt;出队-&gt;取值-&gt;将其左右孩子入队。。。一直到队列为空，此时输出结果。符合这样的逻辑，可以写出这样的代码： 1234567891011121314def breadth_travel(self): &quot;&quot;&quot;广度遍历&quot;&quot;&quot; if self.root is None: return queue = [root] res = [] while queue: cur_node = queue.pop(0) # print(cur_node.elem, end=&quot; &quot;) res.append(cur_node.val) if cur_node.lchild: queue.append(cur_node.lchild) if cur_node.rchild: queue.append(cur_node.rchild) 有了这样的框架我们就可以做题了。 一、二叉树的层序遍历 可以看出这个与我们框架不同的地方是，每一层都用一个列表包裹，那么我们可以使用一个循环来遍历当前层的节点，并在每层使用一个临时列表存储值不就解决了吗？ 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def levelOrder(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[List[int]] &quot;&quot;&quot; if not root: return [] queue = [root] res = [] while queue: tmp = [] # 用其来处理每一层的节点 for _ in range(len(queue)): cur_node = queue.pop(0) tmp.append(cur_node.val) if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) res.append(tmp) return res 二、N叉树的层序遍历 会了二叉树相信你N叉树的层序遍历也一定会了。 分析一下，我们把之前的框架稍作修改，之前不是只有左右孩子嘛！现在有多个孩子，其实比较简单了，当前出队的节点添加孩子时，将所有的孩子都入队即可。 123456789101112131415161718192021222324252627282930&quot;&quot;&quot;# Definition for a Node.class Node(object): def __init__(self, val=None, children=None): self.val = val self.children = children&quot;&quot;&quot;class Solution(object): def levelOrder(self, root): &quot;&quot;&quot; :type root: Node :rtype: List[List[int]] &quot;&quot;&quot; if not root: return [] queue = [root] res = [] while queue: tmp = [] for _ in range(len(queue)): cur_node = queue.pop(0) tmp.append(cur_node.val) # 如果当前弹出的节点有孩子，将所有的孩子入队。 if cur_node.children: queue.extend(cur_node.children[::1]) res.append(tmp) return res 三、⼆叉树的层次遍历II 题目如下： 这道题很简单，看输出结果不就是之前层序结果的反转嘛！ 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def levelOrder(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[List[int]] &quot;&quot;&quot; if not root: return [] queue = [root] res = [] while queue: tmp = [] # 用其来处理每一层的节点 for _ in range(len(queue)): cur_node = queue.pop(0) tmp.append(cur_node.val) if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) res.append(tmp) return res[::-1] 四、二叉树的右视图 这道题也是比较简单的：上面不是有了包裹式的层序遍历嘛，里面有个临时列表不知道还记得不？添加时将临时列表的最后一个加入结果列表即可。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def rightSideView(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[int] &quot;&quot;&quot; if not root: return [] res = [] queue = [root] while queue: tmp = [] for _ in range(len(queue)): cur_node = queue.pop(0) tmp.append(cur_node.val) if cur_node.left: queue.append(cur_node.left) if cur_node.right: queue.append(cur_node.right) res.append(tmp[-1]) return res 五、总结 对于层序遍历、理解其队列的执行过程很重要！","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"26-HBase概述","slug":"26-HBase概述","date":"2021-06-30T16:54:44.000Z","updated":"2021-06-30T17:01:15.347Z","comments":true,"path":"20210701/26-HBase概述.html","link":"","permalink":"https://xxren8218.github.io/20210701/26-HBase%E6%A6%82%E8%BF%B0.html","excerpt":"","text":"HBase简介与环境部署1. HBase简介&amp;在Hadoop生态中的地位1.1. 什么是HBase HBase是一个分布式的、面向列的开源数据库 HBase是Google BigTable的开源实现 HBase不同于一般的关系数据库, 适合非结构化数据存储 1.2 BigTable BigTable是Google设计的分布式数据存储系统，用来处理海量的数据的一种非关系型的数据库。 适合大规模海量数据，PB级数据； 分布式、并发数据处理，效率极高； 易于扩展，支持动态伸缩 适用于廉价设备； 不适用于传统关系型数据的存储； 1.3 什么是非结构化数据存储 结构化数据 适合用二维表来展示的数据 非结构化数据 非结构化数据是数据结构不规则或不完整 如名人词条：科学家：成果；演员：电影；其中如演员有50个字段，但是只有5个与科学家是公用的。如年龄、性别等。导致二维表出现很多数据稀疏。 或者处理业务时数据一直变，删一行，当达到100000行时，此时处理时间很长，需要把数据库锁起来，这样对线上的业务就有影响了。 没有预定义的数据模型 开始没想好字段是什么，随着业务逻辑增加。 不方便用数据库二维逻辑表来表现 办公文档、文本、图片、XML, HTML、各类报表、图像和音频/视频信息等 1.4 HBase在Hadoop生态中的地位 HBase是Apache基金会顶级项目 HBase基于HDFS进行数据存储 HBase可以存储超大数据并适合用来进行大数据的实时查询 1.5 HBase与HDFS HBase建立在Hadoop文件系统上, 利用了HDFS的容错能力 HBase提供对数据的随机实时读/写访问功能 HBase内部使用哈希表, 并存储索引, 可以快速查找HDFS中数据 1.6 HBase使用场景 瞬间写入量很大 大量数据需要长期保存, 且数量会持续增长 HBase不适合有join, 多级索引, 表关系复杂的数据模型 2 HBase的数据模型2.1 ACID定义 指数据库事务正确执行的四个基本要素的缩写 原子性 A 要么都完成，要么都失败，事务过程不能分割。 整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性 C 状态改变，无论并发的事务有多少，必须保持同一个状态。 一个事务可以封装状态改变（除非它是一个只读的）。事务必须始终保持系统处于一致的状态，不管在任何给定的时间并发事务有多少。 隔离性 I 两个事务同时运行必须是一个事务运行完了，再运行另一个事务。不能同时执行。 隔离状态执行事务，使它们好像是系统在给定时间内执行的唯一操作。如果有两个事务，运行在相同的时间内，执行相同的功能，事务的隔离性将确保每一事务在系统中认为只有该事务在使用系统。这种属性有时称为串行化，为了防止事务操作间的混淆，必须串行化或序列化请求，使得在同一时间仅有一个请求用于同一数据。 持久性 D 事务一旦完成不会回滚 在事务完成以后，该事务对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。 HBase 不同于Hive，Hive只是涉及到查询操作，并不涉及事务的概念。但是HBase就不是了。 HBase 支持特定场景下的 ACID，即对行级别的事务 操作保证完全的 ACID 2.2 cap定理 分布式系统的最大难点，就是各个节点的状态如何同步。CAP 定理是这方面的基本定理，也是理解分布式系统的起点。 一致性(所有节点在同一时间具有相同的数据) 可用性(保证每个请求不管成功或失败都有响应,但不保证获取的数据的正确性) 分区容错性(系统中任意信息的丢失或失败不会影响系统的运行,系统如果不能在某个时限内达成数据一致性,就必须在上面两个操作之间做出选择)——任何时候都要保证的！其余两个就要做取舍了。 hbase是CAP中的CP系统,即hbase是强一致性的——用牺牲可用性的代价。 2.3 HBase表结构 NameSpace: 关系型数据库的”数据库”(database) 表(table)：用于存储管理数据，具有稀疏的、面向列的特点。HBase中的每一张表，就是所谓的大表(Bigtable)，可以有上亿行，上百万列。对于为值为空的列，并不占用存储空间，因此表可以设计的非常稀疏。 行(Row)：在表里面,每一行代表着一个数据对象,每一行都是以一个行键(Row Key)来进行唯一标识的, 行键并没有什么特定的数据类型, 以二进制的字节来存储 列(Column): HBase的列由 Column family 和 Column qualifier 组成, 由冒号: 进行行间隔, 如 family: qualifier 行键(RowKey)：类似于MySQL中的主键，HBase根据行键来快速检索数据，一个行键对应一条记录。与MySQL主键不同的是，HBase的行键是天然固有的，每一行数据都存在行键。 列族(ColumnFamily)：是列的集合。列族在表定义时需要指定，而列在插入数据时动态指定。列中的数据都是以二进制形式存在，没有数据类型。在物理存储结构上，每个表中的每个列族单独以一个文件存储。一个表可以有多个列簇。 列修饰符(Column Qualifier) : 列族中的数据通过列标识来进行映射, 可以理解为一个键值对(key-value), 列修饰符(Column Qualifier) 就是key 对应关系型数据库的列 时间戳(TimeStamp)：是列的一个属性，是一个64位整数。由行键和列确定的单元格，可以存储多个数据，每个数据含有时间戳属性，数据具有版本特性。可根据版本(VERSIONS)或时间戳来指定查询历史版本数据，如果都不指定，则默认返回最新版本的数据。 区域(Region)：HBase自动把表水平划分成的多个区域，划分的区域随着数据的增大而增多。 HBase 支持特定场景下的 ACID，即对行级别的 操作保证完全的 ACID 2.4 面向列的数据库HBase 与 传统关系数据库的区别 HBase 关系型数据库 数据库大小 PB级别 GB TB 数据类型 Bytes 丰富的数据类型 事务支持 ACID只支持单个Row级别 全面的ACID支持, 对Row和表 索引 只支持Row-key 支持 吞吐量 百万写入/秒 数千写入/秒 关系型数据库中数据示例 ID FILE NAME FILE PATH FILE TYPE FILE SIZE CREATOR 1 file1.txt /home txt 1024 tom 2 file2.txt /home/pics jpg 5032 jerry 同样数据保存到列式数据库中 RowKey FILE INFO（列族：列标识符[列名]：值） SAVE INFO 1 file_info:name:file1.txtfile_info:type:txtfile_info:size:1024 path:/home/pics creator:Jerry 2 file_info:name:file2.jpg file_info:type:jpg file_info:size:5032 path:/home creator:Tom 行数据库&amp;列数据库存储方式比较 3 HBase组件3.1 HBase 基础架构 3.1.1 Client ①与zookeeper通信, 找到数据入口地址 ②使用HBase RPC机制与HMaster和HRegionServer进行通信； ③Client与HMaster进行通信进行管理类操作； ④Client与HRegionServer进行数据读写类操作。 3.1.2 Zookeeper ①保证任何时候，集群中只有一个running master，避免单点问题； ②存贮所有Region的寻址入口，包括-ROOT-表地址、HMaster地址； ③实时监控Region Server的状态，将Region server的上线和下线信息，实时通知给Master； ④存储Hbase的schema，包括有哪些table，每个table有哪些column family。 3.1.3 HMaster（主）可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。 角色功能： ①为Region server分配region； ②负责region server的负载均衡； ③发现失效的region serve并重新分配其上的region； ④HDFS上的垃圾文件回收； ⑤处理用户对表的增删改查操作。 3.1.4 HRegionServer（从）HBase中最核心的模块，主要负责响应用户I/O请求，向HDFS文件系统中读写数据。 作用： ①维护Master分配给它的region，处理对这些region的IO请求； ②负责切分在运行过程中变得过大的region。 此外，HRegionServer管理一系列HRegion对象，每个HRegion对应Table中一个Region，HRegion由多个HStore组成，每个HStore对应Table中一个Column Family的存储，Column Family就是一个集中的存储单元，故将具有相同IO特性的Column放在一个Column Family会更高效。 3.1.5 HStore HBase存储的核心，由MemStore和StoreFile组成。 用户写入数据的流程为：client访问ZK, ZK返回RegionServer地址-&gt; client访问RegionServer写入数据 -&gt; 数据存入MemStore，一直到MemStore满 -&gt; Flush成StoreFile 写也是一样的操作，先看内存有没，没有才去Storefile中读取。 3.1.6 HRegion 一个表最开始存储的时候，是一个region。 一个Region中会有个多个store，每个store用来存储一个列簇。如果只有一个column family，就只有一个store。 region会随着插入的数据越来越多，会进行拆分。默认大小是10G一个。 3.1.7 HLog 在分布式系统环境中，无法避免系统出错或者宕机，一旦HRegionServer意外退出，MemStore中的内存数据就会丢失，引入HLog就是防止这种情况，其在磁盘上不会像内存那样出大问题，内存出问题，将其写进内存即可。 而且一旦MemStore的数据flush到Hstore中，HLog中的数据就会抹掉。——持久化后抹除。避免其过大 3.2 HBase模块协作 HBase启动 HMaster启动, 注册到Zookeeper, 等待RegionServer汇报 RegionServer注册到Zookeeper, 并向HMaster汇报 对各个RegionServer(包括失效的)的数据进行整理, 分配Region和meta信息 RegionServer失效 HMaster将失效RegionServer上的Region分配到其他节点 HMaster更新hbase: meta 表以保证数据正常访问 HMaster失效 处于Backup状态的其他HMaster节点推选出一个转为Active状态 数据能正常读写, 但是不能创建删除表, 也不能更改表结构 4 HBase 的安装与实战4.1 HBase的安装 下载安装包 http://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.7.0.tar.gz 配置伪分布式环境 环境变量配置 12export HBASE_HOME=/usr/local/development/hbase-1.2.4export PATH=$HBASE_HOME/bin:$PATH 配置hbase-env.sh 12export JAVA_HOME=/usr/local/development/jdk1.7.0_15export HBASE_MANAGES_ZK=false --如果你是使用hbase自带的zk就是true，如果使用自己的zk就是false 配置hbase-site.xml 123456789101112131415&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; --hbase持久保存的目录 &lt;value&gt;hdfs://hadoop001:8020/opt/hbase&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; --是否是分布式 &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; --指定要连接zk的端口 &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hbase/zkData&lt;/value&gt; &lt;/property&gt; 启动hbase（启动的hbase的时候要保证hadoop集群已经启动） 1/hbase/bin/start-hbase.sh 输入hbase shell（进入shell命令行） 4.2 HBase shell HBase DDL 和 DML 命令 名称 命令表达式 创建表 create '表名', '列族名1','列族名2','列族名n' 添加记录 put '表名','行名','列名:','值 查看记录 get '表名','行名' 查看表中的记录总数 count '表名' 删除记录 delete '表名', '行名','列名' 删除一张表 第一步 disable '表名' 第二步 drop '表名' 查看所有记录 scan \"表名称\" 查看指定表指定列所有数据 scan '表名' ,{COLUMNS=>'列族名:列名'} 更新记录 重写覆盖 连接集群 1hbase shell 创建表 1create &#x27;user&#x27;,&#x27;base_info&#x27; 删除表 12disable &#x27;user&#x27;drop &#x27;user&#x27; 创建名称空间 1create_namespace &#x27;test&#x27; 展示现有名称空间 1list_namespace 创建表的时候添加namespace 1create &#x27;test:user&#x27;,&#x27;base_info&#x27; 显示某个名称空间下有哪些表 1list_namespace_tables &#39;test&#39; 插入数据 put ‘表名’，‘rowkey的值’，’列族：列标识符‘，’值‘ 123456789101112131415161718192021222324put &#39;user&#39;,&#39;rowkey_10&#39;,&#39;base_info:username&#39;,&#39;Tom&#39;put &#39;user&#39;,&#39;rowkey_10&#39;,&#39;base_info:birthday&#39;,&#39;2014-07-10&#39;put &#39;user&#39;,&#39;rowkey_10&#39;,&#39;base_info:sex&#39;,&#39;1&#39;put &#39;user&#39;,&#39;rowkey_10&#39;,&#39;base_info:address&#39;,&#39;Tokyo&#39;put &#39;user&#39;,&#39;rowkey_16&#39;,&#39;base_info:username&#39;,&#39;Mike&#39;put &#39;user&#39;,&#39;rowkey_16&#39;,&#39;base_info:birthday&#39;,&#39;2014-07-10&#39;put &#39;user&#39;,&#39;rowkey_16&#39;,&#39;base_info:sex&#39;,&#39;1&#39;put &#39;user&#39;,&#39;rowkey_16&#39;,&#39;base_info:address&#39;,&#39;beijing&#39;put &#39;user&#39;,&#39;rowkey_22&#39;,&#39;base_info:username&#39;,&#39;Jerry&#39;put &#39;user&#39;,&#39;rowkey_22&#39;,&#39;base_info:birthday&#39;,&#39;2014-07-10&#39;put &#39;user&#39;,&#39;rowkey_22&#39;,&#39;base_info:sex&#39;,&#39;1&#39;put &#39;user&#39;,&#39;rowkey_22&#39;,&#39;base_info:address&#39;,&#39;Newyork&#39;put &#39;user&#39;,&#39;rowkey_24&#39;,&#39;base_info:username&#39;,&#39;Nico&#39;put &#39;user&#39;,&#39;rowkey_24&#39;,&#39;base_info:birthday&#39;,&#39;2014-07-10&#39;put &#39;user&#39;,&#39;rowkey_24&#39;,&#39;base_info:sex&#39;,&#39;1&#39;put &#39;user&#39;,&#39;rowkey_24&#39;,&#39;base_info:address&#39;,&#39;shanghai&#39;put &#39;user&#39;,&#39;rowkey_25&#39;,&#39;base_info:username&#39;,&#39;Rose&#39;put &#39;user&#39;,&#39;rowkey_25&#39;,&#39;base_info:birthday&#39;,&#39;2014-07-10&#39;put &#39;user&#39;,&#39;rowkey_25&#39;,&#39;base_info:sex&#39;,&#39;1&#39;put &#39;user&#39;,&#39;rowkey_25&#39;,&#39;base_info:address&#39;,&#39;Soul&#39; 查询表中的所有数据 1scan &#39;user&#39; 查询某个rowkey的数据 1get &#39;user&#39;,&#39;rowkey_16&#39; 查询某个列簇的数据 123get &#x27;user&#x27;,&#x27;rowkey_16&#x27;,&#x27;base_info&#x27;get &#x27;user&#x27;,&#x27;rowkey_16&#x27;,&#x27;base_info:username&#x27;get &#x27;user&#x27;, &#x27;rowkey_16&#x27;, &#123;COLUMN =&gt; [&#x27;base_info:username&#x27;,&#x27;base_info:sex&#x27;]&#125; 删除表中的数据 1delete &#39;user&#39;, &#39;rowkey_16&#39;, &#39;base_info:username&#39; 清空数据 1truncate &#39;user&#39; 操作列簇 12alter &#39;user&#39;, NAME &#x3D;&gt; &#39;f2&#39;alter &#39;user&#39;, &#39;delete&#39; &#x3D;&gt; &#39;f2&#39; HBase 追加型数据库 会保留多个版本数据 12345678desc &#x27;user&#x27;Table user is ENABLEDuserCOLUMN FAMILIES DESCRIPTION&#123;NAME =&gt; &#x27;base_info&#x27;, VERSIONS =&gt; &#x27;1&#x27;, EVICT_BLOCKS_ON_CLOSE =&gt; &#x27;false&#x27;, NEW_VERSION_BHE_DATA_ON_WRITE =&gt; &#x27;false&#x27;, DATA_BLOCK_ENCODING =&gt; &#x27;NONE&#x27;, TTL =&gt; &#x27;FOREVER&#x27;, MIER =&gt; &#x27;NONE&#x27;, CACHE_INDEX_ON_WRITE =&gt; &#x27;false&#x27;, IN_MEMORY =&gt; &#x27;false&#x27;, CACHE_BLOOMse&#x27;, COMPRESSION =&gt; &#x27;NONE&#x27;, BLOCKCACHE =&gt; &#x27;false&#x27;, BLOCKSIZE =&gt; &#x27;65536&#x27;&#125; VERSIONS=&gt;’1’说明最多可以显示一个版本 修改数据 1put &#x27;user&#x27;,&#x27;rowkey_10&#x27;,&#x27;base_info:username&#x27;,&#x27;Tom&#x27; 指定显示多个版本 1get &#x27;user&#x27;,&#x27;rowkey_10&#x27;,&#123;COLUMN=&gt;&#x27;base_info:username&#x27;,VERSIONS=&gt;2&#125; 修改可以显示的版本数量 1alter &#x27;user&#x27;,NAME=&gt;&#x27;base_info&#x27;,VERSIONS=&gt;10 命令表 可以通过HbaseUi界面查看表的信息 端口60010打不开的情况，是因为hbase 1.0 以后的版本，需要自己手动配置，在文件 hbase-site 1234&lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;&#x2F;name&gt; &lt;value&gt;60010&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; 4.3 HappyBase操作Hbase 什么是HappyBase HappyBase is a developer-friendly Python library to interact with Apache HBase. HappyBase is designed for use in standard HBase setups, and offers application developers a Pythonic API to interact with HBase. Below the surface, HappyBase uses the Python Thrift library to connect to HBase using its Thrift gateway, which is included in the standard HBase 0.9x releases. HappyBase 是FaceBook员工开发的操作HBase的python库, 其基于Python Thrift, 但使用方式比Thrift简单, 已被广泛应用 启动hbase thrift server : hbase-daemon.sh start thrift 安装happy base pip install happybase 使用happy base时可能出现的问题(windows系统) happybase1.0在win下不支持绝对路径 解决方案：将488行的url_scheme == ”改为url_scheme in (‘代码盘符’, ”) 如何使用HappyBase 建立连接 12import happybaseconnection = happybase.Connection(&#x27;somehost&#x27;) 当连接建立时, 会自动创建一个与 HBase Thrift server的socket链接. 可以通过参数禁止自动链接, 然后再需要连接是调用 Connection.open(): 123connection = happybase.Connection(&#x27;somehost&#x27;, autoconnect=False)# before first use:connection.open() Connection 这个类提供了一个与HBase交互的入口, 比如获取HBase中所有的表: Connection.tables(): 1print(connection.tables()) 操作表 Table类提供了大量API, 这些API用于检索和操作HBase中的数据。 在上面的示例中，我们已经使用Connection.tables（）方法查询HBase中的表。 如果还没有任何表，可使用Connection.create_table（）创建一个新表： 1connection.create_table(&#x27;users&#x27;,&#123;&#x27;cf1&#x27;: dict()&#125;) 创建表之后可以传入表名获取到Table类的实例: 1table &#x3D; connection.table(&#39;mytable&#39;) 查询操作 12345678910111213141516# apitable.scan() #全表查询table.row(row_keys[0]) # 查询一行table.rows(row_keys) # 查询多行#封装函数def show_rows(table, row_keys=None): if row_keys: print(&#x27;show value of row named %s&#x27; % row_keys) if len(row_keys) == 1: print(table.row(row_keys[0])) else: print(table.rows(row_keys)) else: print(&#x27;show all row values of table named %s&#x27; % table.name) for key, value in table.scan(): print(key, value) 插入数据 123456789101112#apitable.put(row_key, &#123;cf:cq:value&#125;)def put_row(table, column_family, row_key, value): print(&#x27;insert one row to hbase&#x27;) #put &#x27;user&#x27;,&#x27;rowkey_10&#x27;,&#x27;base_info:username&#x27;,&#x27;Tom&#x27; #&#123;&#x27;cf:cq&#x27;:’数据‘&#125; table.put(row_key, &#123;&#x27;%s:name&#x27; % column_family:&#x27;name_%s&#x27; % value&#125;)def put_rows(table, column_family, row_lines=30): print(&#x27;insert rows to hbase now&#x27;) for i in range(row_lines): put_row(table, column_family, &#x27;row_%s&#x27; % i, i) 删除数据 123456789101112#apitable.delete(row_key, cf_list) #函数封装 def delete_row(table, row_key, column_family=None, keys=None): if keys: print(&#x27;delete keys:%s from row_key:%s&#x27; % (keys, row_key)) key_list = [&#x27;%s:%s&#x27; % (column_family, key) for key in keys] table.delete(row_key, key_list) else: print(&#x27;delete row(column_family:) from hbase&#x27;) table.delete(row_key) 删除表 123456#apiconn.delete_table(table_name, True)#函数封装def delete_table(table_name): pretty_print(&#x27;delete table %s now.&#x27; % table_name) conn.delete_table(table_name, True) 完整代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import happybasehostname = &#x27;192.168.199.188&#x27;table_name = &#x27;users&#x27;column_family = &#x27;cf&#x27;row_key = &#x27;row_1&#x27;conn = happybase.Connection(hostname)def show_tables(): print(&#x27;show all tables now&#x27;) tables = conn.tables() for t in tables: print tdef create_table(table_name, column_family): print(&#x27;create table %s&#x27; % table_name) conn.create_table(table_name, &#123;column_family:dict()&#125;)def show_rows(table, row_keys=None): if row_keys: print(&#x27;show value of row named %s&#x27; % row_keys) if len(row_keys) == 1: print table.row(row_keys[0]) else: print table.rows(row_keys) else: print(&#x27;show all row values of table named %s&#x27; % table.name) for key, value in table.scan(): print key, valuedef put_row(table, column_family, row_key, value): print(&#x27;insert one row to hbase&#x27;) table.put(row_key, &#123;&#x27;%s:name&#x27; % column_family:&#x27;name_%s&#x27; % value&#125;)def put_rows(table, column_family, row_lines=30): print(&#x27;insert rows to hbase now&#x27;) for i in range(row_lines): put_row(table, column_family, &#x27;row_%s&#x27; % i, i)def delete_row(table, row_key, column_family=None, keys=None): if keys: print(&#x27;delete keys:%s from row_key:%s&#x27; % (keys, row_key)) key_list = [&#x27;%s:%s&#x27; % (column_family, key) for key in keys] table.delete(row_key, key_list) else: print(&#x27;delete row(column_family:) from hbase&#x27;) table.delete(row_key)def delete_table(table_name): pretty_print(&#x27;delete table %s now.&#x27; % table_name) conn.delete_table(table_name, True)def pool(): pretty_print(&#x27;test pool connection now.&#x27;) pool = happybase.ConnectionPool(size=3, host=hostname) with pool.connection() as connection: print connection.tables()def main(): # show_tables() # create_table(table_name, column_family) # show_tables() table = conn.table(table_name) show_rows(table) put_rows(table, column_family) show_rows(table) # # # 更新操作 # put_row(table, column_family, row_key, &#x27;xiaoh.me&#x27;) # show_rows(table, [row_key]) # # # 删除数据 # delete_row(table, row_key) # show_rows(table, [row_key]) # # delete_row(table, row_key, column_family, [&#x27;name&#x27;]) # show_rows(table, [row_key]) # # counter(table, row_key, column_family) # # delete_table(table_name)if __name__ == &quot;__main__&quot;: main() 5 HBase表设计 设计HBase表时需要注意的特点 HBase中表的索引是通过rowkey实现的 在表中是通过Row key的字典顺序来对数据进行排序的, 表中Region的划分通过起始Rowkey和结束Rowkey来决定的 所有存储在HBase中的数据都是二进制字节, 没有数据类型 原子性只在行内保证, HBase表中没有多行事务 列族(Column Family)在表创建之前就要定义好 列族中的列标识(Column Qualifier)可以在表创建后动态插入数据的时候添加 不同的column family保存在不同的文件中。 如何设计HBase表 Row key的结构该如何设置, Row key中又该包含什么样的信息 表中应该有多少的列族 列族中应该存储什么样的数据 每个列族中存储多少列数据 列的名字分别是什么 cell中应该存储什么样的信息 每个cell中存储多少个版本信息 DDI 目的是为了克服HBase架构上的缺陷(join繁琐 只有row key索引等) Denormalization (反规范化, 解决join麻烦的问题) Duplication (数据冗余) Intelligent keys(通过row key设计实现 索引 排序对读写优化) 5.1 HBase表设计案例: 社交应用互粉信息表 设计表保存应用中用户互粉的信息 读场景: 某用户都关注了哪些用户 用户A有没有关注用户B 谁关注了用户A 写场景 用户关注了某个用户 用户取消关注了某个用户 设计1: colunm qulifier(列名) 1: 2: 设计2 添加了一个 count 记录当前的最后一个记录的列名 设计3 列名 user_id 最终设计(DDI) 解决谁关注了用户A问题 ① 设计一张新表, 里面保存某个用户和他的粉丝 ② 在同一张表中同时记录粉丝列表的和用户关注的列表, 并通过Rowkey来区分 01_userid: 用户关注列表 02_userid: 粉丝列表 上两种设计方案的问题(事务) 案例总结 Rowkey是HBase表结构设计中很重要的环节, 直接影响到HBase的效率和性能 HBase的表结构比传统关系型数据库更灵活, 能存储任何二进制数据,无需考虑数据类型 利用列标识(Column Qualifier)来存储数据 衡量设计好坏的简单标准 是否会全表查询","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"25-Sqoop概述","slug":"25-Sqoop概述","date":"2021-06-29T16:51:29.000Z","updated":"2021-06-29T16:53:26.362Z","comments":true,"path":"20210630/25-Sqoop概述.html","link":"","permalink":"https://xxren8218.github.io/20210630/25-Sqoop%E6%A6%82%E8%BF%B0.html","excerpt":"","text":"Sqoop1 Sqoop概述 什么是Sqoop Sqoop 是一款进行数据传输的工具, 可在hadoop 的 hdfs 和关系型数据库之间传输数据 可以使用Sqoop把数据从MySQL 或 Oracle导入到hdfs中, 也可以把数据从hdfs导入到MySQL或Oracle中 Sqoop可自动执行数据传输的大部分过程, 使用MapReduce导入和导出数据，提供并行操作和容错 为什么要使用sqoop? 快速实现Hadoop(HDFS/hive/hbase)与mysql/Oracle等关系型数据库之间的数据传递 Sqoop提供多种数据传输方式 Sqoop原理 2 Sqoop安装 下载安装包url 解压到centos中 1tar -zxvf &#x2F;home&#x2F;hadoop&#x2F;software&#x2F;sqoop-1.4.6-cdh5.7.0.tar.gz -C ~&#x2F;app&#x2F; 配置环境变量 123vi ~/.bash_profileexport SQOOP_HOME=/home/hadoop/app/sqoop-1.4.6-cdh5.7.0export PATH=$SQOOP_HOME/bin:$PATH 激活环境变量 1source ~&#x2F;.bash_profile 到 $SQOOP_HOME/conf 目录下 配置sqoop_env.sh 123456cp sqoop-env-template.sh sqoop-env.shvi sqoop-env.sh#在sqoop_env.sh中export HADOOP_COMMON_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/export HADOOP_MAPRED_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0/ 拷贝 mysql驱动到$SQOOP_HOME/lib目录下 1cp /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib/mysql-connector-java-5.1.47.jar /home/hadoop/app/sqoop-1.4.6-cdh5.7.0/lib/ 测试sqoop环境 1sqoop-version 看到如下输出 说明sqoop安装成功 123Sqoop 1.4.6-cdh5.7.0git commit idCompiled by jenkins on ****** 然后进入到MySQL的Docker环境中 1docker exec -ti mysql bash 3 使用Sqoop导入数据到hdfs中 准备mysql数据 建表语句 1CREATE table u(id int PRIMARY KEY AUTO_INCREMENT,fname varchar(20),lname varchar(20)); 插入数据 1234insert into u3 (fname, lname) values(&#x27;George&#x27;,&#x27;washington&#x27;);insert into u3 (fname, lname) values(&#x27;George&#x27;,&#x27;bush&#x27;);insert into u3 (fname, lname) values(&#x27;Bill&#x27;,&#x27;clinton&#x27;);insert into u3 (fname, lname) values(&#x27;Bill&#x27;,&#x27;gates&#x27;); Sqoop导入命令介绍 命令语法: sqoop import (控制参数) (导入参数) 命令元素: 导入操作, 数据源, 访问方式, 导入控制, 目标地址 命令理解: 数据从哪里来, 有什么控制, 到哪里去 12sqoop import --connect jdbc:mysql://127.0.0.1:3306/test --username root --password root\\!123A --table u -m 1 # -m 表示用几个MR任务执行，前提是文件小于128M，否则拆成多个block 添加—target-dir 指定hdfs上数据存放的目录 1sqoop import --connect jdbc:mysql://localhost:3306/test --username root --password root!123A --table u --target-dir /tmp/u1 -m 1 导入可能出现的问题 ​ 解决 上传java-json.jar到$SQOOP_HOME/lib目录下 默认数据上传到hdfs中如下路径 1&#x2F;user&#x2F;当前linux用户名&#x2F;mysql表名&#x2F; 通过hive 建立外表导入数据到hive 1234567CREATE EXTERNAL TABLE u4( id INT, fname STRING, lname STRING)ROW FORMAT delimited fields terminated by &#x27;,&#x27; LOCATION &#x27;/user/hadoop/u/&#x27;; 也可能出现断开连接的情况 1hive --service metastore&amp; # 加上&amp;表示在后台跑。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"02-二叉树之深度遍历思想训练","slug":"02-二叉树之深度遍历思想训练","date":"2021-06-29T10:19:29.000Z","updated":"2021-07-04T13:30:25.795Z","comments":true,"path":"20210629/02-二叉树之深度遍历思想训练.html","link":"","permalink":"https://xxren8218.github.io/20210629/02-%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B9%8B%E6%B7%B1%E5%BA%A6%E9%81%8D%E5%8E%86%E6%80%9D%E6%83%B3%E8%AE%AD%E7%BB%83.html","excerpt":"","text":"二叉树之深度遍历思想训练读完本文，你能去力扣解决如下题目： 654.最大二叉树（难度 Medium） 105.从前序与中序遍历序列构造二叉树（难度 Medium） 106.从中序与后序遍历序列构造二叉树（难度 Medium） 先来复习一下，我们说过写树的算法，关键思路如下： 把题目的要求细化，搞清楚根节点应该做什么，然后剩下的事情抛给前/中/后序的遍历框架就行了，我们千万不要跳进递归的细节里，你的脑袋才能压几个栈呀。 也许你还不太理解这句话，我们下面来看例子。 一、构造最大二叉树 先来道简单的，这是力扣第 654 题，题目如下： 12输出：[6,3,5,null,2,0,null,null,1]返回: TreeNode 函数签名如下： 1def constructMaximumBinaryTree(self, nums: List[int]) -&gt; TreeNode: 按照我们刚才说的，先明确根节点做什么？对于构造二叉树的问题，根节点要做的就是把想办法把自己构造出来。 我们肯定要遍历数组把找到最大值maxVal，把根节点root做出来，然后对maxVal左边的数组和右边的数组进行递归调用，作为root的左右子树。 按照题目给出的例子，输入的数组为[3,2,1,6,0,5]，对于整棵树的根节点来说，其实在做这件事： 1234567def constructMaximumBinaryTree([3,2,1,6,0,5]): # 找到数组中的最大值 root = TreeNode(6) # 递归调用构造左右子树 root.left = constructMaximumBinaryTree([3,2,1]) root.right = constructMaximumBinaryTree([0,5]) return root 再详细一点，就是如下伪码: 1234567891011def constructMaximumBinaryTree(nums): if not nums: return None # 找到数组中的最大值 maxVal = max(nums) index = nums.index(maxVal) root = TreeNode(maxVal) # 递归调用构造左右子树 root.left = constructMaximumBinaryTree(nums[:index]) root.right = constructMaximumBinaryTree(nums[index+1:]) return root 看懂了吗？对于每个根节点，只需要找到当前nums中的最大值和对应的索引，然后递归调用左右数组构造左右子树即可。 将其拆分成两个函数如下： 1234567891011121314151617def constructMaximumBinaryTree(nums): if not nums: return None return build(nums, 0, len(nums) - 1)def build(nums. lo, hi): if lo &gt; hi: return None # 找到数组中的最大值 maxVal = max(nums[lo:hi + 1]) index = nums.index(maxVal) root = TreeNode(maxVal) # 递归调用构造左右子树 root.left = build(nums,lo, index - 1) root.right = build(nums, index, hi) return root 至此，这道题就做完了，还是挺简单的对吧，下面看两道更困难一些的。 二、通过前序和中序遍历结果构造二叉树 经典问题了，面试/笔试中常考，力扣第 105 题就是这个问题： 函数签名如下： 1def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; TreeNode: 废话不多说，直接来想思路，首先思考，根节点应该做什么。 类似上一题，我们肯定要想办法确定根节点的值，把根节点做出来，然后递归构造左右子树即可。 我们先来回顾一下，前序遍历和中序遍历的结果有什么特点？ 1234567891011def traverse(root): # 前序遍历 preorder.append(root.val) traverse(root.left) traverse(root.right)def traverse(root): traverse(root.left) # 中序遍历 inorder.append(root.val) traverse(root.right) 这样的遍历顺序差异，导致了preorder和inorder数组中的元素分布有如下特点： 找到根节点是很简单的，前序遍历的第一个值preorder[0]就是根节点的值，关键在于如何通过根节点的值，将preorder和postorder数组划分成两半，构造根节点的左右子树？ 换句话说，对于以下代码中的?部分应该填入什么： 123456789101112131415161718192021222324252627# 主函数def buildTree(preorder, inorder): return build(preorder, 0, len(preorder) - 1, inorder, 0, len(inorder) - 1)&quot;&quot;&quot; 若前序遍历数组为 preorder[preStart..preEnd]， 后续遍历数组为 postorder[postStart..postEnd]， 构造二叉树，返回该二叉树的根节点 &quot;&quot;&quot;def build(preorder, preStart, preEnd, inorder, inStart, inEnd): # root 节点对应的值就是前序遍历数组的第一个元素 rootVal = preorder[preStart] # rootVal 在中序遍历数组中的索引 index = inorder.index(rootVal) root = TreeNode(rootVal) # 递归构造左右子树 root.left = build(preorder, ?, ?, inorder, ?, ?) root.right = build(preorder, ?, ?, inorder, ?, ?) return root 对于代码中的rootVal和index变量，就是下图这种情况： 现在我们来看图做填空题，下面这几个问号处应该填什么： 12345root.left = build(preorder, ?, ?, inorder, ?, ?)root.right = build(preorder, ?, ?, inorder, ?, ?) 对于左右子树对应的inorder数组的起始索引和终止索引比较容易确定： 对于preorder数组呢？如何确定左右数组对应的起始索引和终止索引？ 这个可以通过左子树的节点数推导出来，假设左子树的节点数为leftSize，那么preorder数组上的索引情况是这样的： 看着这个图就可以把preorder对应的索引写进去了： 1234567leftSize = index - inStartroot.left = build(preorder, preStart + 1, preStart + leftSize, inorder, inStart, index - 1)root.right = build(preorder, preStart + leftSize + 1, preEnd, inorder, index + 1, inEnd) 至此，整个算法思路就完成了，我们再补一补 base case 即可写出解法代码： 1234567891011121314151617181920212223242526272829303132# 主函数def buildTree(preorder, inorder): return build(preorder, 0, len(preorder) - 1, inorder, 0, len(inorder) - 1)&quot;&quot;&quot; 若前序遍历数组为 preorder[preStart..preEnd]， 后续遍历数组为 postorder[postStart..postEnd]， 构造二叉树，返回该二叉树的根节点 &quot;&quot;&quot;def build(preorder, preStart, preEnd, inorder, inStart, inEnd): # base case if preStart &gt; preEnd: return # root 节点对应的值就是前序遍历数组的第一个元素 rootVal = preorder[preStart] # rootVal 在中序遍历数组中的索引 index = inorder.index(rootVal) leftSize = index - inStart # 先构造出当前根节点 root = TreeNode(rootVal) # 递归构造左右子树 root.left = build(preorder, preStart + 1, preStart + leftSize, inorder, inStart, index - 1) root.right = build(preorder, preStart + leftSize + 1, preEnd, inorder, index + 1, inEnd) return root 我们的主函数只要调用build函数即可，你看着函数这么多参数，解法这么多代码，似乎比我们上面讲的那道题难很多，让人望而生畏，实际上呢，这些参数无非就是控制数组起止位置的，画个图就能解决了。 三、通过后序和中序遍历结果构造二叉树 类似上一题，这次我们利用后序和中序遍历的结果数组来还原二叉树，这是力扣第 106 题： 函数签名如下： 1def buildTree(self, inorder: List[int], postorder: List[int]) -&gt; TreeNode: 类似的，看下后序和中序遍历的特点： 1234567891011def traverse(TreeNode root): traverse(root.left) traverse(root.right) # 前序遍历 postorder.append(root.val)def traverse(TreeNode root): traverse(root.left) # 中序遍历 inorder.append(root.val) traverse(root.right) 这样的遍历顺序差异，导致了preorder和inorder数组中的元素分布有如下特点： 这道题和上一题的关键区别是，后序遍历和前序遍历相反，根节点对应的值为postorder的最后一个元素。 整体的算法框架和上一题非常类似，我们依然写一个辅助函数build： 12345678910111213141516171819202122# 主函数def buildTree(inorder, postorder): return build(inorder, 0, len(inorder) - 1, postorder, 0, len(postorder) - 1)def build(inorder, inStart, inEnd, postorder, postStart, postEnd): # root 节点对应的值就是前序遍历数组的第一个元素 rootVal = postorder[preEnd] # rootVal 在中序遍历数组中的索引 index = inorder.index(rootVal) root = TreeNode(rootVal) # 递归构造左右子树 root.left = build(inorder, ?, ?, postorder, ?, ?) root.right = build(inorder, ?, ?, postorder, ?, ?) return root 现在postoder和inorder对应的状态如下： 我们可以按照上图将问号处的索引正确填入： 1234567leftSize = index - inStartroot.left = build(inorder, inStart, index - 1, postorder, postStart, postStart + leftSize - 1)root.right = build(inorder, index + 1, inEnd, postorder, postStart + leftSize, postEnd - 1) 综上，可以写出完整的解法代码： 1234567891011121314151617181920212223242526# 主函数def buildTree(inorder, postorder): return build(inorder, 0, len(inorder) - 1, postorder, 0, len(postorder) - 1)def build(inorder, inStart, inEnd, postorder, postStart, postEnd): if inStart &gt; inEnd: return # root 节点对应的值就是前序遍历数组的第一个元素 rootVal = postorder[postEnd] # rootVal 在中序遍历数组中的索引 index = inorder.index(rootVal) leftSize = index - inStart root = TreeNode(rootVal) # 递归构造左右子树 root.left = build(inorder, inStart, index - 1, postorder, postStart, postStart + leftSize - 1) root.right = build(inorder, index + 1, inEnd, postorder, postStart + leftSize, postEnd - 1) return root 有了前一题的铺垫，这道题很快就解决了，无非就是rootVal变成了最后一个元素，再改改递归函数的参数而已，只要明白二叉树的特性，也不难写出来。 最后呼应下前文，做二叉树的问题，关键是把题目的要求细化，搞清楚根节点应该做什么，然后剩下的事情抛给前/中/后序的遍历框架就行了。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"24-Hive概述","slug":"24-Hive概述","date":"2021-06-28T16:47:32.000Z","updated":"2021-06-28T16:52:13.914Z","comments":true,"path":"20210629/24-Hive概述.html","link":"","permalink":"https://xxren8218.github.io/20210629/24-Hive%E6%A6%82%E8%BF%B0.html","excerpt":"","text":"Hive一 Hive基本概念1 Hive简介 学习目标 了解什么是Hive 了解为什么使用Hive 1.1 什么是 Hive Hive 由 Facebook 实现并开源，是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据映射为一张数据库表，并提供 HQL(Hive SQL)查询功能，底层数据是存储在 HDFS 上。 Hive 本质: 将 SQL 语句转换为 MapReduce 任务运行，使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据,是一款基于 HDFS 的 MapReduce 计算框架——相当于给MR套了壳子。 主要用途：用来做离线数据分析，比直接用 MapReduce 开发效率更高。 1.2 为什么使用 Hive 直接使用 Hadoop MapReduce 处理数据所面临的问题： 人员学习成本太高 MapReduce 实现复杂查询逻辑开发难度太大——得适应Map和Reduce的思维。 使用 Hive 操作接口采用类 SQL 语法，提供快速开发的能力 避免了去写 MapReduce，减少开发人员的学习成本 功能扩展很方便 缺点：非结构化数据不能用Hive，还是用MR来算。 2 Hive 架构2.1 Hive 架构图 2.2 Hive 组件 用户接口：包括 CLI、JDBC/ODBC、WebGUI。 CLI(command line interface)为 shell 命令行 JDBC/ODBC 是 Hive 的 JAVA 实现，与传统数据库JDBC 类似 WebGUI 是通过浏览器访问 Hive。 HiveServer2基于Thrift, 允许远程客户端使用多种编程语言如Java、Python向Hive提交请求 元数据存储：通常是存储在关系数据库如 mysql/derby 中。 Hive 将元数据存储在数据库中。 Hive 中的元数据包括 表的名字 表的列 分区及其属性 表的属性（是否为外部表等） 表的数据所在目录等。 解释器、编译器、优化器、执行器:完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行 2.3 Hive 与 Hadoop 的关系Hive 利用 HDFS 存储数据，利用 MapReduce 查询分析数据。 Hive是数据仓库工具，没有集群的概念，如果想提交Hive作业只需要在hadoop集群 Master节点上装Hive就可以了 3 Hive 与传统数据库对比 hive 用于海量数据的离线数据分析。 Hive 关系型数据库 ANSI SQL 不完全支持 支持 更新 INSERT OVERWRITE\\INTO TABLE(默认) UPDATE\\INSERT\\DELETE 事务 不支持(默认) 支持 模式 读模式 写模式 查询语言 HQL SQL 数据存储 HDFS Raw Device or Local FS 执行 MapReduce Executor 执行延迟 高 低 子查询 只能用在From子句中 完全支持 处理数据规模 大 小 可扩展性 高 低 索引 0.8版本后加入位图索引 有复杂的索引 hive支持的数据类型 原子数据类型 TINYINT SMALLINT INT BIGINT BOOLEAN FLOAT DOUBLE STRING BINARY TIMESTAMP DECIMAL CHAR VARCHAR DATE 复杂数据类型 ARRAY MAP STRUCT hive中表的类型 托管表 (managed table) (内部表) 外部表 4 Hive 数据模型 Hive 中所有的数据都存储在 HDFS 中，没有专门的数据存储格式 在创建表时指定数据中的分隔符，Hive 就可以映射成功，解析数据。 Hive 中包含以下数据模型： db：在 hdfs 中表现为 hive.metastore.warehouse.dir 目录下一个文件夹 table：在 hdfs 中表现所属 db 目录下一个文件夹 external table：数据存放位置可以在 HDFS 任意指定路径 partition：在 hdfs 中表现为 table 目录下的子目录 bucket：在 hdfs 中表现为同一个表目录下根据 hash 散列之后的多个文件 5 Hive 安装部署 Hive 安装前需要安装好 JDK 和 Hadoop。配置好环境变量。 下载Hive的安装包 http://archive.cloudera.com/cdh5/cdh/5/ 并解压 1tar -zxvf hive-1.1.0-cdh5.7.0.tar.gz -C ~/app/ 进入到 解压后的hive目录 找到 conf目录, 修改配置文件 12cp hive-env.sh.template hive-env.shvi hive-env.sh 在hive-env.sh中指定hadoop的路径 1HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 配置环境变量 ```shellvi ~/.bash_profile 1234- &#96;&#96;&#96;shell export HIVE_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hive-1.1.0-cdh5.7.0 export PATH&#x3D;$HIVE_HOME&#x2F;bin:$PATH ```shellsource ~/.bash_profile 1234567891011121314151617181920212223242526272829303132333435363738394041424344- 根据元数据存储的介质不同，分为下面两个版本，其中 derby 属于内嵌模式。实际生产环境中则使用 mysql 来进行元数据的存储。 - 内置 derby 版： bin&#x2F;hive 启动即可使用 缺点：不同路径启动 hive，每一个 hive 拥有一套自己的元数据，无法共享 - mysql 版： - 上传 mysql驱动到 hive安装目录的lib目录下 mysql-connector-java-5.*.jar - vi conf&#x2F;hive-site.xml 配置 Mysql 元数据库信息(MySql安装见文档) &#96;&#96;&#96;xml-dtd &lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; standalone&#x3D;&quot;no&quot;?&gt; &lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!-- 插入以下代码 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt; &lt;value&gt;hive&lt;&#x2F;value&gt;&lt;!-- 指定mysql用户名 --&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt; &lt;value&gt;hive&lt;&#x2F;value&gt;&lt;!-- 指定mysql密码 --&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;mysql &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;hive&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;!-- 指定mysql数据库地址 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;&lt;!-- 指定mysql驱动 --&gt; &lt;&#x2F;property&gt; &lt;!-- 到此结束代码 --&gt; &lt;property&gt; &lt;name&gt;hive.exec.script.wrapper&lt;&#x2F;name&gt; &lt;value&#x2F;&gt; &lt;description&#x2F;&gt; &lt;&#x2F;property&gt; &lt;&#x2F;configuration&gt; hive启动 启动docker service docker start 通过docker 启动mysql docker start mysql 启动 hive的metastore元数据服务 hive --service metastore 启动hive hive MySQL root 密码 password hive用户 密码 hive 二 Hive 基本操作2.1 Hive HQL操作初体验 创建数据库 1CREATE DATABASE test; 显示所有数据库 1SHOW DATABASES; 创建表 1234CREATE TABLE student(classNo string, stuNo string, score int) row format delimited fields terminated by &#x27;,&#x27;;-- 在text数据库下面创建学生表CREATE TABLE text.student(classNo string, stuNo string, score int) row format delimited fields terminated by &#x27;,&#x27;; row format delimited fields terminated by ‘,’ 指定了字段的分隔符为逗号，所以load数据的时候，load的文本也要为逗号，否则加载后为NULL。hive只支持单个字符的分隔符，hive默认的分隔符是\\001 将数据load到表中 在本地文件系统创建一个如下的文本文件：/home/hadoop/tmp/student.txt 123456789C01,N0101,82C01,N0102,59C01,N0103,65C02,N0201,81C02,N0202,82C02,N0203,79C03,N0301,56C03,N0302,92C03,N0306,72 ``` sql load data local inpath ‘/home/hadoop/tmp/student.txt’overwrite into table student; — 导入default.student 12load data local inpath &#39;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;student.txt&#39;overwrite into table text.student; &#x2F;&#x2F;导入text下的studen 这个命令将student.txt文件复制到hive的warehouse目录中，这个目录由hive.metastore.warehouse.dir配置项设置，默认值为/user/hive/warehouse。Overwrite选项将导致Hive事先删除student目录下所有的文件, 并将文件内容映射到表中。 Hive不会对student.txt做任何格式处理，因为Hive本身并不强调数据的存储格式。 12345- 查询表中的数据 跟SQL类似——响应快（不涉及MR） &#96;&#96;&#96; sql hive&gt;select * from student; 分组查询group by和统计 count——响应慢（会启动一个MR作业，涉及MR） 1hive&gt;select classNo,count(score) from student where score&gt;=60 group by classNo; 从执行结果可以看出 hive把查询的结果变成了MapReduce作业通过hadoop执行 2.2 Hive的内部表和外部表 内部表(managed table) 外部表(external table) 概念 创建表时无external修饰 创建表时被external修饰 数据管理 由Hive自身管理 由HDFS管理 数据保存位置 hive.metastore.warehouse.dir （默认：/user/hive/warehouse） hdfs中任意位置 删除时影响 直接删除元数据（metadata）及存储数据 仅会删除元数据，HDFS上的文件并不会被删除 表结构修改时影响 修改会将修改直接同步给元数据 表结构和分区进行修改，则需要修复（MSCK REPAIR TABLE table_name;） 案例 创建一个外部表student2 1CREATE EXTERNAL TABLE student2 (classNo string, stuNo string, score int) row format delimited fields terminated by &#x27;,&#x27; location &#x27;/tmp/student&#x27;; 装载数据 123load data local inpath &#x27;/home/hadoop/tmp/student.txt&#x27; overwrite into table student2;-- 用...put 也可以放到/user/hive/warehouse中,不过上面的load是加载到了默认的路径/user/hive/warehouse中 显示表信息 1desc formatted table_name; 删除表查看结果 1drop table student; 再次创建外部表 student2 不插入数据直接查询查看结果 1select * from student2; 2.3 分区表——实际上是表目录下的子目录 什么是分区表 随着表的不断增大，对于新纪录的增加，查找，删除等(DML)的维护也更加困难。对于数据库中的超大型表，可以通过把它的数据分成若干个小表，从而简化数据库的管理活动，对于每一个简化后的小表，我们称为一个单个的分区。 hive中分区表实际就是对应hdfs文件系统上独立的文件夹，该文件夹内的文件是该分区所有数据文件。 分区可以理解为分类，通过分类把不同类型的数据放到不同的目录下。 分类的标准就是分区字段，可以一个，也可以多个。 分区表的意义在于优化查询。查询时尽量利用分区字段。如果不使用分区字段，就会全部扫描。 创建分区表 12345tom,4300jerry,12000mike,13000jake,11000rob,10000 1create table employee (name string,salary bigint) partitioned by (date1 string) row format delimited fields terminated by &#x27;,&#x27; lines terminated by &#x27;\\n&#x27; stored as textfile; 查看表的分区 1show partitions employee; 添加分区 1alter table employee add if not exists partition(date1&#x3D;&#39;2018-12-01&#39;); 加载数据到分区 12load data local inpath &#x27;/home/hadoop/tmp/employee.txt&#x27; into table employee partition(date1=&#x27;2018-12-01&#x27;);-- 也可以用put 如果重复加载同名文件，不会报错，会自动创建一个*_copy_1.txt 外部分区表即使有分区的目录结构, 也必须要通过HQL添加分区, 才能看到相应的数据 12hadoop fs -mkdir /user/hive/warehouse/emp/dt=2018-12-04hadoop fs -copyFromLocal /tmp/employee.txt /user/hive/warehouse/test.db/emp/dt=2018-12-04/employee.txt 此时查看表中数据发现数据并没有变化, 需要通过hql添加分区 1alter table emp add if not exists partition(dt&#x3D;&#39;2018-12-04&#39;); 此时再次查看才能看到新加入的数据 总结 利用分区表方式减少查询时需要扫描的数据量 分区字段不是表中的列, 数据文件中没有对应的列 分区仅仅是一个目录名 查看数据时, hive会自动添加分区列 支持多级分区, 多级子目录 2.4 动态分区 在写入数据时自动创建分区(包括目录结构) 创建表 1create table employee2 (name string,salary bigint) partitioned by (date1 string) row format delimited fields terminated by &#39;,&#39; lines terminated by &#39;\\n&#39; stored as textfile; 导入数据——没有reduce,只有map。每条数据都丢过去。 1insert into table employee2 partition(date1) select name,salary,date1 from employee; 使用动态分区需要设置参数 1set hive.exec.dynamic.partition.mode=nonstrict; 三 Hive 函数3.1 内置运算符在 Hive 有四种类型的运算符： 关系运算符 算术运算符 逻辑运算符 复杂运算 (内容较多，见《Hive 官方文档》》) 3.2 内置函数https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF 简单函数: 日期函数 字符串函数 类型转换 统计函数: sum avg distinct 集合函数 分析函数 show functions; 显示所有函数 desc function 函数名; desc function extended 函数名; 3.3 Hive 自定义函数和 Transform UDF 当 Hive 提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。 TRANSFORM,and UDF and UDAF it is possible to plug in your own custom mappers and reducers A UDF is basically only a transformation done by a mapper meaning that each row should be mapped to exactly one row. A UDAF on the other hand allows us to transform a group of rows into one or more rows, meaning that we can reduce the number of input rows to a single output row by some custom aggregation. UDF：就是做一个mapper，对每一条输入数据，映射为一条输出数据。（输入一个，输出一个） UDAF:就是一个reducer，把一组输入数据映射为一条(或多条)输出数据。（输入100个，输出10个） 一个脚本至于是做mapper还是做reducer，又或者是做udf还是做udaf，取决于我们把它放在什么样的hive操作符中。放在select中的基本就是udf，放在distribute by和cluster by中的就是reducer。 We can control if the script is run in a mapper or reducer step by the way we formulate our HiveQL query. The statements DISTRIBUTE BY and CLUSTER BY allow us to indicate that we want to actually perform an aggregation. User-Defined Functions (UDFs) for transformations and even aggregations which are therefore called User-Defined Aggregation Functions (UDAFs) UDF示例(运行java已经编写好的UDF) 在hdfs中创建 /user/hive/lib目录 1hadoop fs -mkdir /user/hive/lib 把 hive目录下 lib/hive-contrib-hive-contrib-1.1.0-cdh5.7.0.jar 放到hdfs中 1hadoop fs -put hive-contrib-1.1.0-cdh5.7.0.jar /user/hive/lib/ 把集群中jar包的位置添加到hive中 1hive&gt; add jar hdfs:///user/hive/lib/hive-contrib-1.1.0-cdh2.3.4.jar; 在hive中创建临时UDF 1hive&gt; CREATE TEMPORARY FUNCTION row_sequence as &#x27;org.apache.hadoop.hive.contrib.udf.UDFRowSequence&#x27; -- 相当于给这个jar包重起名字叫row_FUNCTION了。 在之前的案例中使用临时自定义函数(函数功能: 添加自增长的行号) 1Select row_sequence(),* from employee; 创建非临时自定义函数 1CREATE FUNCTION row_sequence as &#x27;org.apache.hadoop.hive.contrib.udf.UDFRowSequence&#x27; using jar &#x27;hdfs:///user/hive/lib/hive-contrib-1.1.0-cdh2.3.4.jar&#x27;; Python UDF 准备案例环境 创建表 1CREATE table u(fname STRING,lname STRING); 向表中插入数据(不要一条一条放，应该load或put) 1234insert into table u2 values(&#x27;George&#x27;,&#x27;washington&#x27;);insert into table u2 values(&#x27;George&#x27;,&#x27;bush&#x27;);insert into table u2 values(&#x27;Bill&#x27;,&#x27;clinton&#x27;);insert into table u2 values(&#x27;Bill&#x27;,&#x27;gates&#x27;); 编写map风格脚本 123456import sysfor line in sys.stdin: line = line.strip() fname , lname = line.split(&#x27;\\t&#x27;) l_name = lname.upper() print &#x27;\\t&#x27;.join([fname, str(l_name)]) 通过hdfs向hive中ADD file 加载文件到hdfs 1hadoop fs -put udf.py /user/hive/lib/ hive从hdfs中加载python脚本——自己写的必须有这一步。 12ADD FILE hdfs:///user/hive/lib/udf.py;ADD FILE /root/tmp/udf1.py; Transform 1SELECT TRANSFORM(fname, lname) USING &#x27;python udf1.py&#x27; AS (fname, l_name) FROM u; Python UDAF 四 hive综合案例 内容推荐数据处理 需求 根据用户行为以及文章标签筛选出用户最感兴趣(阅读最多)的标签 相关数据 ​ user_id article_id event_time 12345678910111213141516171811,101,2018-12-01 06:01:1022,102,2018-12-01 07:28:1233,103,2018-12-01 07:50:1411,104,2018-12-01 09:08:1222,103,2018-12-01 13:37:1233,102,2018-12-02 07:09:1211,101,2018-12-02 18:42:1235,105,2018-12-03 09:21:1222,104,2018-12-03 16:42:1277,103,2018-12-03 18:31:1299,102,2018-12-04 00:04:1233,101,2018-12-04 19:10:1211,101,2018-12-05 09:07:1235,102,2018-12-05 11:00:1222,103,2018-12-05 12:11:1277,104,2018-12-05 18:02:0299,105,2018-12-05 20:09:11 文章数据 123456artical_id,artical_url,artical_keywords101,http:&#x2F;&#x2F;www.itcast.cn&#x2F;1.html,kw8|kw1102,http:&#x2F;&#x2F;www.itcast.cn&#x2F;2.html,kw6|kw3103,http:&#x2F;&#x2F;www.itcast.cn&#x2F;3.html,kw7104,http:&#x2F;&#x2F;www.itcast.cn&#x2F;4.html,kw5|kw1|kw4|kw9105,http:&#x2F;&#x2F;www.itcast.cn&#x2F;5.html, 要做的事情：将文章的关键词和用户联系起来。拿到任何一个用户可以知道他看这个标签文章的频率。 数据上传hdfs 12hadoop fs -mkdir /tmp/demohadoop fs -mkdir /tmp/demo/user_action 创建外部表 用户行为表 12345678drop table if exists user_actions;CREATE EXTERNAL TABLE user_actions( user_id STRING, article_id STRING, time_stamp STRING)ROW FORMAT delimited fields terminated by &#x27;,&#x27;LOCATION &#x27;/tmp/demo/user_action&#x27;; 文章表 12345678910111213drop table if exists articles;CREATE EXTERNAL TABLE articles( article_id STRING, url STRING, key_words array&lt;STRING&gt;)ROW FORMAT delimited fields terminated by &#x27;,&#x27; COLLECTION ITEMS terminated BY &#x27;|&#x27; -- key1|key2LOCATION &#x27;/tmp/demo/article_keywords&#x27;;/*key_words array&lt;STRING&gt; 数组的数据类型COLLECTION ITEMS terminated BY &#x27;|&#x27; 数组的元素之间用&#x27;|&#x27;分割*/ 查看数据 12select * from user_actions;select * from articles; 分组查询每个用户的浏览记录 collect_set/collect_list作用: 将group by中的某列转为一个数组返回 collect_list不去重而collect_set去重 collect_set 12select user_id,collect_set(article_id) from user_actions group by user_id; 12345611 [&quot;101&quot;,&quot;104&quot;]22 [&quot;102&quot;,&quot;103&quot;,&quot;104&quot;]33 [&quot;103&quot;,&quot;102&quot;,&quot;101&quot;]35 [&quot;105&quot;,&quot;102&quot;]77 [&quot;103&quot;,&quot;104&quot;]99 [&quot;102&quot;,&quot;105&quot;] collect_list 12select user_id,collect_list(article_id) from user_actions group by user_id; 123456711 [&quot;101&quot;,&quot;104&quot;,&quot;101&quot;,&quot;101&quot;]22 [&quot;102&quot;,&quot;103&quot;,&quot;104&quot;,&quot;103&quot;]33 [&quot;103&quot;,&quot;102&quot;,&quot;101&quot;]35 [&quot;105&quot;,&quot;102&quot;]77 [&quot;103&quot;,&quot;104&quot;]99 [&quot;102&quot;,&quot;105&quot;] sort_array: 对数组排序 12select user_id,sort_array(collect_list(article_id)) as contents from user_actions group by user_id; 12345611 [&quot;101&quot;,&quot;101&quot;,&quot;101&quot;,&quot;104&quot;]22 [&quot;102&quot;,&quot;103&quot;,&quot;103&quot;,&quot;104&quot;]33 [&quot;101&quot;,&quot;102&quot;,&quot;103&quot;]35 [&quot;102&quot;,&quot;105&quot;]77 [&quot;103&quot;,&quot;104&quot;]99 [&quot;102&quot;,&quot;105&quot;] 如上所示：11与77都含有”104”,在Array里面不好统计次数。所以将其展开——拆分。 查看每一篇文章的关键字 lateral view explode explode函数 将array 拆分 1select explode(key_words) from articles; 输出为： 123456789kw8kw1kw6kw3kw7kw5kw1kw4kw9 - lateral view 和 explode 配合使用,将一行数据拆分成多行数据，在此基础上可以对拆分的数据进行聚合。 1select article_id,kw from articles lateral view explode(key_words) t as kw; 123456789101 kw8101 kw1102 kw6102 kw3103 kw7104 kw5104 kw1104 kw4104 kw9 1select article_id,kw from articles lateral view outer explode(key_words) t as kw; 12345678910101 kw8101 kw1102 kw6102 kw3103 kw7104 kw5104 kw1104 kw4104 kw9105 NULL 含有outer——将105留下 根据文章id找到用户查看文章的关键字 原始数据 12345101 http://www.itcast.cn/1.html [&quot;kw8&quot;,&quot;kw1&quot;]102 http://www.itcast.cn/2.html [&quot;kw6&quot;,&quot;kw3&quot;]103 http://www.itcast.cn/3.html [&quot;kw7&quot;]104 http://www.itcast.cn/4.html [&quot;kw5&quot;,&quot;kw1&quot;,&quot;kw4&quot;,&quot;kw9&quot;]105 http://www.itcast.cn/5.html [] 12345select a.user_id, b.kw from user_actions as a left outer JOIN (select article_id,kw from articleslateral view outer explode(key_words) t as kw) bon (a.article_id = b.article_id)order by a.user_id; 1234567891011121314151617181920212223242526272829303132333411 kw111 kw811 kw511 kw111 kw411 kw111 kw911 kw811 kw111 kw822 kw122 kw722 kw922 kw422 kw522 kw722 kw322 kw633 kw833 kw133 kw333 kw633 kw735 NULL35 kw635 kw377 kw977 kw177 kw777 kw477 kw599 kw399 kw699 NULL 根据文章id找到用户查看文章的关键字并统计频率 1234567 select a.user_id, b.kw,count(1) as weight from user_actions as a left outer JOIN (select article_id,kw from articles lateral view outer explode(key_words) t as kw) b on (a.article_id = b.article_id) group by a.user_id,b.kw order by a.user_id,weight desc; 1234567891011121314151617181920212223242526272811 kw1 411 kw8 311 kw5 111 kw9 111 kw4 122 kw7 222 kw9 122 kw1 122 kw3 122 kw4 122 kw5 122 kw6 133 kw3 133 kw8 133 kw7 133 kw6 133 kw1 135 NULL 135 kw3 135 kw6 177 kw1 177 kw4 177 kw5 177 kw7 177 kw9 199 NULL 199 kw3 199 kw6 1 CONCAT：——将其链接在一起。 CONCAT(str1,str2,…) 返回结果为连接参数产生的字符串。如有任何一个参数为NULL ，则返回值为 NULL。 1select concat(user_id,article_id) from user_actions; CONCAT_WS: 使用语法为：CONCAT_WS(separator,str1,str2,…) CONCAT_WS() 代表 CONCAT With Separator ，是CONCAT()的特殊形式。第一个参数是其它参数的分隔符。分隔符的位置放在要连接的两个字符串之间。分隔符可以是一个字符串，也可以是其它参数。如果分隔符为 NULL，则结果为 NULL。 1select concat_ws(&#x27;:&#x27;,user_id,article_id) from user_actions; 将用户查看的关键字和频率合并成 key:value形式 123456 select a.user_id, concat_ws(&#x27;:&#x27;,b.kw,cast (count(1) as string)) as kw_w --cast类型转化。 from user_actions as a left outer JOIN (select article_id,kw from articles lateral view outer explode(key_words) t as kw) b on (a.article_id = b.article_id)group by a.user_id,b.kw; 1234567891011121314151617181920212223242526272811 kw1:411 kw4:111 kw5:111 kw8:311 kw9:122 kw1:122 kw3:122 kw4:122 kw5:122 kw6:122 kw7:222 kw9:133 kw1:133 kw3:133 kw6:133 kw7:133 kw8:135 135 kw3:135 kw6:177 kw1:177 kw4:177 kw5:177 kw7:177 kw9:199 199 kw3:199 kw6:1 将用户查看的关键字和频率合并成 key:value形式并按用户聚合 12345678910 select cc.user_id,concat_ws(&#x27;,&#x27;,collect_set(cc.kw_w)) from( select a.user_id, concat_ws(&#x27;:&#x27;,b.kw,cast (count(1) as string)) as kw_w from user_actions as a left outer JOIN (select article_id,kw from articles lateral view outer explode(key_words) t as kw) b on (a.article_id = b.article_id) group by a.user_id,b.kw ) as cc group by cc.user_id; 12345611 kw1:4,kw4:1,kw5:1,kw8:3,kw9:122 kw1:1,kw3:1,kw4:1,kw5:1,kw6:1,kw7:2,kw9:133 kw1:1,kw3:1,kw6:1,kw7:1,kw8:135 1,kw3:1,kw6:177 kw1:1,kw4:1,kw5:1,kw7:1,kw9:199 1,kw3:1,kw6:1 将上面聚合结果转换成map 12345678910select cc.user_id,str_to_map(concat_ws(&#x27;,&#x27;,collect_set(cc.kw_w))) as wmfrom(select a.user_id, concat_ws(&#x27;:&#x27;,b.kw,cast (count(1) as string)) as kw_w from user_actions as a left outer JOIN (select article_id,kw from articleslateral view outer explode(key_words) t as kw) bon (a.article_id = b.article_id)group by a.user_id,b.kw) as cc group by cc.user_id; 123456 11 &#123;&quot;kw1&quot;:&quot;4&quot;,&quot;kw4&quot;:&quot;1&quot;,&quot;kw5&quot;:&quot;1&quot;,&quot;kw8&quot;:&quot;3&quot;,&quot;kw9&quot;:&quot;1&quot;&#125; 22 &#123;&quot;kw1&quot;:&quot;1&quot;,&quot;kw3&quot;:&quot;1&quot;,&quot;kw4&quot;:&quot;1&quot;,&quot;kw5&quot;:&quot;1&quot;,&quot;kw6&quot;:&quot;1&quot;,&quot;kw7&quot;:&quot;2&quot;,&quot;kw9&quot;:&quot;1&quot;&#125; 33 &#123;&quot;kw1&quot;:&quot;1&quot;,&quot;kw3&quot;:&quot;1&quot;,&quot;kw6&quot;:&quot;1&quot;,&quot;kw7&quot;:&quot;1&quot;,&quot;kw8&quot;:&quot;1&quot;&#125; 35 &#123;&quot;1&quot;:null,&quot;kw3&quot;:&quot;1&quot;,&quot;kw6&quot;:&quot;1&quot;&#125; 77 &#123;&quot;kw1&quot;:&quot;1&quot;,&quot;kw4&quot;:&quot;1&quot;,&quot;kw5&quot;:&quot;1&quot;,&quot;kw7&quot;:&quot;1&quot;,&quot;kw9&quot;:&quot;1&quot;&#125;99 &#123;&quot;1&quot;:null,&quot;kw3&quot;:&quot;1&quot;,&quot;kw6&quot;:&quot;1&quot;&#125; 将用户的阅读偏好结果保存到表中 1234567891011 create table user_kws as select cc.user_id,str_to_map(concat_ws(&#x27;,&#x27;,collect_set(cc.kw_w))) as wm from( select a.user_id, concat_ws(&#x27;:&#x27;,b.kw,cast (count(1) as string)) as kw_w from user_actions as a left outer JOIN (select article_id,kw from articles lateral view outer explode(key_words) t as kw) b on (a.article_id = b.article_id) group by a.user_id,b.kw ) as cc group by cc.user_id; 从表中通过key查询map中的值 1select user_id, wm[&#x27;kw1&#x27;] from user_kws; 12345611 422 133 135 NULL77 199 NULL 从表中获取map中所有的key 和 所有的value 1select user_id,map_keys(wm),map_values(wm) from user_kws; 12345611 [&quot;kw1&quot;,&quot;kw4&quot;,&quot;kw5&quot;,&quot;kw8&quot;,&quot;kw9&quot;] [&quot;4&quot;,&quot;1&quot;,&quot;1&quot;,&quot;3&quot;,&quot;1&quot;]22 [&quot;kw1&quot;,&quot;kw3&quot;,&quot;kw4&quot;,&quot;kw5&quot;,&quot;kw6&quot;,&quot;kw7&quot;,&quot;kw9&quot;] [&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;2&quot;,&quot;1&quot;]33 [&quot;kw1&quot;,&quot;kw3&quot;,&quot;kw6&quot;,&quot;kw7&quot;,&quot;kw8&quot;] [&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;]35 [&quot;1&quot;,&quot;kw3&quot;,&quot;kw6&quot;] [null,&quot;1&quot;,&quot;1&quot;]77 [&quot;kw1&quot;,&quot;kw4&quot;,&quot;kw5&quot;,&quot;kw7&quot;,&quot;kw9&quot;] [&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;]99 [&quot;1&quot;,&quot;kw3&quot;,&quot;kw6&quot;] [null,&quot;1&quot;,&quot;1&quot;] 用lateral view explode把map中的数据转换成多列 1select user_id,keyword,weight from user_kws lateral view explode(wm) t as keyword,weight; 1234567891011121314151617181920212223242526272811 kw1 411 kw4 111 kw5 111 kw8 311 kw9 122 kw1 122 kw3 122 kw4 122 kw5 122 kw6 122 kw7 222 kw9 133 kw1 133 kw3 133 kw6 133 kw7 133 kw8 135 1 NULL35 kw3 135 kw6 177 kw1 177 kw4 177 kw5 177 kw7 177 kw9 199 1 NULL99 kw3 199 kw6 1","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"01-二叉树之常用框架","slug":"01-二叉树之常用框架","date":"2021-06-28T14:07:14.000Z","updated":"2021-06-28T14:13:54.285Z","comments":true,"path":"20210628/01-二叉树之常用框架.html","link":"","permalink":"https://xxren8218.github.io/20210628/01-%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%A1%86%E6%9E%B6.html","excerpt":"","text":"二叉树之常用框架读完本文，你可以去力扣拿下： 226.翻转二叉树，难度 Easy 116.将二叉树展开为链表，难度 Medium 114.填充二叉树节点的右侧指针，难度 Medium 先刷二叉树的题目，先刷二叉树的题目，先刷二叉树的题目，因为很多经典算法，回溯、动归、分治算法，其实都是树的问题，而树的问题就永远逃不开树的递归遍历框架这几行破代码： 1234567# 二叉树遍历框架def traverse(root): # 前序遍历 traverse(root.left) # 中序遍历 traverse(root.right) # 后序遍历 递归解法应该是最简单，最容易理解的才对，行云流水地写递归代码是学好算法的基本功，而二叉树相关的题目就是最练习递归基本功，最练习框架思维的。 一、二叉树的重要性 举个例子，比如说我们的经典算法「快速排序」和「归并排序」，对于这两个算法，你有什么理解？如果你告诉我，快速排序就是个二叉树的前序遍历，归并排序就是个二叉树的后续遍历，那么我就知道你是个算法高手了。 为什么快速排序和归并排序能和二叉树扯上关系？我们来简单分析一下他们的算法思想和代码框架： 快速排序的逻辑是，若要对nums[lo..hi]进行排序，我们先找一个分界点p，通过交换元素使得nums[lo..p-1]都小于等于nums[p]，且nums[p+1..hi]都大于nums[p]，然后递归地去nums[lo..p-1]和nums[p+1..hi]中寻找新的分界点，最后整个数组就被排序了。 快速排序的代码框架如下： 1234567def sort(nums, lo, hi): ##### 前序遍历位置 ##### # 通过交换元素构建分界点 p p = partition(nums, lo, hi) sort(nums, lo, p - 1) sort(nums, p + 1, hi) 先构造分界点，然后去左右子数组构造分界点，你看这不就是一个二叉树的前序遍历吗？ 再说说归并排序的逻辑，若要对nums[lo..hi]进行排序，我们先对nums[lo..mid]排序，再对nums[mid+1..hi]排序，最后把这两个有序的子数组合并，整个数组就排好序了。 归并排序的代码框架如下： 123456789def sort(nums, lo, hi): mid = (lo + hi) / 2 sort(nums, lo, mid) sort(nums, mid + 1, hi) ###### 后序遍历位置 ##### # 合并两个排好序的子数组 merge(nums, lo, mid, hi) ######################## 先对左右子数组排序，然后合并（类似合并有序链表的逻辑），你看这是不是二叉树的后序遍历框架？另外，这不就是传说中的分治算法嘛，不过如此呀。 如果你一眼就识破这些排序算法的底细，还需要背这些算法代码吗？这不是手到擒来，从框架慢慢扩展就能写出算法了。 说了这么多，旨在说明，二叉树的算法思想的运用广泛，甚至可以说，只要涉及递归，都可以抽象成二叉树的问题。 所以接下来，我们直接上几道比较有意思，且能体现出递归算法精妙的二叉树题目。 二、写递归算法的秘诀 写递归算法的关键是要明确函数的「定义」是什么，然后相信这个定义，利用这个定义推导最终结果，绝不要试图跳入递归。 怎么理解呢，我们用一个具体的例子来说，比如说让你计算一棵二叉树共有几个节点： 123456# 定义：count(root) 返回以 root 为根的树有多少节点def count(root): # base case if root == None: return 0 # 自己加上子树的节点数就是整棵树的节点数 return 1 + count(root.left) + count(root.right) 这个问题非常简单，大家应该都会写这段代码，root本身就是一个节点，加上左右子树的节点数就是以root为根的树的节点总数。 左右子树的节点数怎么算？其实就是计算根为root.left和root.right两棵树的节点数呗，按照定义，递归调用count函数即可算出来。 写树相关的算法，简单说就是，先搞清楚当前root节点该做什么，然后根据函数定义递归调用子节点，递归调用会让孩子节点做相同的事情。 我们接下来看几道算法题目实操一下。 三、算法实践 第一题、翻转二叉树我们先从简单的题开始，看看力扣第 226 题「翻转二叉树」，输入一个二叉树根节点root，让你把整棵树镜像翻转，比如输入的二叉树如下： 12345 4 &#x2F; \\ 2 7 &#x2F; \\ &#x2F; \\1 3 6 9 算法原地翻转二叉树，使得以root为根的树变成： 12345 4 &#x2F; \\ 7 2 &#x2F; \\ &#x2F; \\9 6 3 1 通过观察，我们发现只要把二叉树上的每一个节点的左右子节点进行交换，最后的结果就是完全翻转之后的二叉树。 可以直接写出解法代码： 12345678910111213141516# 将整棵树的节点翻转def invertTree(root): # base case if root == None:return None # 前序遍历位置 # root 节点需要交换它的左右子节点 tmp = root.left root.left = root.right root.right = tmp # 让左右子节点继续翻转它们的子节点 invertTree(root.left) invertTree(root.right) return root 这道题目比较简单，关键思路在于我们发现翻转整棵树就是交换每个节点的左右子节点，于是我们把交换左右子节点的代码放在了前序遍历的位置。 值得一提的是，如果把交换左右子节点的代码放在后序遍历的位置也是可以的，但是放在中序遍历的位置是不行的，请你想一想为什么？这个应该不难想到。 中序遍历换节点 根据左根右的遍历顺序 相当于左侧节点交换了两次 右侧节点没换 因为遍历根的时候交换了左右节点 遍历右侧的时候还是之前那个左节点 首先讲这道题目是想告诉你，二叉树题目的一个难点就是，如何把题目的要求细化成每个节点需要做的事情。 这种洞察力需要多刷题训练，我们看下一道题 第二题、填充二叉树节点的右侧指针这是力扣第 116 题，看下题目： 函数签名如下： 1def connect(root): 题目的意思就是把二叉树的每一层节点都用next指针连接起来： 而且题目说了，输入是一棵「完美二叉树」，形象地说整棵二叉树是一个正三角形，除了最右侧的节点next指针会指向null，其他节点的右侧一定有相邻的节点。 这道题怎么做呢？把每一层的节点穿起来，是不是只要把每个节点的左右子节点都穿起来就行了？ 我们可以模仿上一道题，写出如下代码： 12345678910def connect(root): if root == None and root.left == None: return root root.left.next = root.right connect(root.left) connect(root.right) return root 这样其实有很大问题，再看看这张图： 节点 5 和节点 6 不属于同一个父节点，那么按照这段代码的逻辑，它俩就没办法被穿起来，这是不符合题意的。 回想刚才说的，二叉树的问题难点在于，如何把题目的要求细化成每个节点需要做的事情，但是如果只依赖一个节点的话，肯定是没办法连接「跨父节点」的两个相邻节点的。 那么，我们的做法就是增加函数参数，一个节点做不到，我们就给他安排两个节点，「将每一层二叉树节点连接起来」可以细化成「将每两个相邻节点都连接起来」： 1234567891011121314151617181920# 主函数def connect(root): if root == None: return None connectTwoNode(root.left, root.right) return root# 定义：输入两个节点，将它俩连接起来def connectTwoNode(Node node1, Node node2): if node1 == None and node2 == None: return #### 前序遍历位置 #### # 将传入的两个节点连接 node1.next = node2 # 连接相同父节点的两个子节点 connectTwoNode(node1.left, node1.right) connectTwoNode(node2.left, node2.right) # 连接跨越父节点的两个子节点 connectTwoNode(node1.right, node2.left) 这样，connectTwoNode函数不断递归，可以无死角覆盖整棵二叉树，将所有相邻节点都连接起来，也就避免了我们之前出现的问题，这道题就解决了。 第三题、将二叉树展开为链表这是力扣第 114 题，看下题目： 函数签名如下： 1def flatten(root): 我们尝试给出这个函数的定义： 给flatten函数输入一个节点root，那么以root为根的二叉树就会被拉平为一条链表。 我们再梳理一下，如何按题目要求把一棵树拉平成一条链表？很简单，以下流程： 1、将root的左子树和右子树拉平。 2、将root的右子树接到左子树下方，然后将整个左子树作为右子树。 上面三步看起来最难的应该是第一步对吧，如何把root的左右子树拉平？其实很简单，按照flatten函数的定义，对root的左右子树递归调用flatten函数即可： 1234567891011121314151617181920212223# 定义：将以 root 为根的树拉平为链表def flatten(root): # base case if root == None: return flatten(root.left) flatten(root.right) ##### 后序遍历位置 ##### # 1、左右子树已经被拉平成一条链表 left = root.left right = root.right # 2、将左子树作为右子树 root.left = None root.right = left # 3、将原先的右子树接到当前右子树的末端 p = root while p.right != None: p = p.right p.right = right 你看，这就是递归的魅力，你说flatten函数是怎么把左右子树拉平的？不容易说清楚，但是只要知道flatten的定义如此，相信这个定义，让root做它该做的事情，然后flatten函数就会按照定义工作。 另外注意递归框架是后序遍历，因为我们要先拉平左右子树才能进行后续操作。 至此，这道题也解决了，与 [k 个一组反转链表] 的递归思路和本题也有一些类似。 四、最后总结 递归算法的关键要明确函数的定义，相信这个定义，而不要跳进递归细节。 写二叉树的算法题，都是基于递归框架的，我们先要搞清楚root节点它自己要做什么，然后根据题目要求选择使用前序，中序，后续的递归框架。 二叉树题目的难点在于如何通过题目的要求思考出每一个节点需要做什么，这个只能通过多刷题进行练习了。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"08-动态规划之详解最长公共子序列系列问题","slug":"08-动态规划之详解最长公共子序列系列问题","date":"2021-06-28T14:02:58.000Z","updated":"2021-06-28T16:46:17.614Z","comments":true,"path":"20210628/08-动态规划之详解最长公共子序列系列问题.html","link":"","permalink":"https://xxren8218.github.io/20210628/08-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B9%8B%E8%AF%A6%E8%A7%A3%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97%E7%B3%BB%E5%88%97%E9%97%AE%E9%A2%98.html","excerpt":"","text":"动态规划之详解最长公共子序列本文从「最长公共子序列问题」展开，总结三道子序列问题，解这道题仔细讲讲这种子序列问题的套路，你就能感受到这种思维方式了。 一、最长公共子序列 计算最长公共子序列（Longest Common Subsequence，简称 LCS）是一道经典的动态规划题目，大家应该都见过： 给你输入两个字符串s1和s2，请你找出他们俩的最长公共子序列，返回这个子序列的长度。 力扣第 1143 题就是这道题，函数签名如下： 计算最长公共子序列（Longest Common Subsequence，简称 LCS）是一道经典的动态规划题目，大家应该都见过： 给你输入两个字符串s1和s2，请你找出他们俩的最长公共子序列，返回这个子序列的长度。 力扣第 1143 题就是这道题，函数签名如下： 1def longestCommonSubsequence(s1, s2): 比如说输入s1 = &quot;zabcde&quot;, s2 = &quot;acez&quot;，它俩的最长公共子序列是lcs = &quot;ace&quot;，长度为 3，所以算法返回 3。 如果没有做过这道题，一个最简单的暴力算法就是，把s1和s2的所有子序列都穷举出来，然后看看有没有公共的，然后在所有公共子序列里面再寻找一个长度最大的。 显然，这种思路的复杂度非常高，你要穷举出所有子序列，这个复杂度就是指数级的，肯定不实际。 正确的思路是不要考虑整个字符串，而是细化到s1和s2的每个字符。前文 子序列解题模板 中总结的一个规律： 对于两个字符串求子序列的问题，都是用两个指针i和j分别在两个字符串上移动，大概率是动态规划思路。 最长公共子序列的问题也可以遵循这个规律，我们可以先写一个dp函数： 12# 定义：计算 s1[i..] 和 s2[j..] 的最长公共子序列长度dp(s1, i, s2, j) 这个dp函数的定义是：dp(s1, i, s2, j)计算s1[i..]和s2[j..]的最长公共子序列长度。 根据这个定义，那么我们想要的答案就是dp(s1, 0, s2, 0)，且 base case 就是i == len(s1)或j == len(s2)时，因为这时候s1[i..]或s2[j..]就相当于空串了，最长公共子序列的长度显然是 0： 12345678910def longestCommonSubsequence(s1, s2): return dp(s1, 0, s2, 0);# 主函数 def dp(s1, i, s2, j): # base case if (i == len(s1) or j == len(s2): return 0 # ... 接下来，咱不要看s1和s2两个字符串，而是要具体到每一个字符，思考每个字符该做什么。 我们只看s1[i]和s2[j]，如果s1[i] == s2[j]，说明这个字符一定在lcs中： 这样，就找到了一个lcs中的字符，根据dp函数的定义，我们可以完善一下代码： 12345678# 定义：计算 s1[i..] 和 s2[j..] 的最长公共子序列长度def dp(s1, i, s2, j): if si[i] == s2[j]: # s1[i] 和 s2[j] 必然在 lcs 中， # 加上 s1[i+1..] 和 s2[j+1..] 中的 lcs 长度，就是答案 return 1 + dp(s1, i + 1, s2, j + 1) else: # ... 刚才说的s1[i] == s2[j]的情况，但如果s1[i] != s2[j]，应该怎么办呢？ s1[i] != s2[j]意味着，s1[i]和s2[j]中至少有一个字符不在lcs中： 如上图，总共可能有三种情况，我怎么知道具体是那种情况呢？ 其实我们也不知道，那就把这三种情况的答案都算出来，取其中结果最大的那个呗，因为题目让我们算「最长」公共子序列的长度嘛。 这三种情况的答案怎么算？回想一下我们的dp函数定义，不就是专门为了计算它们而设计的嘛！ 代码可以再进一步： 123456789101112131415# 定义：计算 s1[i..] 和 s2[j..] 的最长公共子序列长度def dp(s1, i, s2, j): if si[i] == s2[j]: return 1 + dp(s1, i + 1, s2, j + 1) else: # s1[i] 和 s2[j] 中至少有一个字符不在 lcs 中， # 穷举三种情况的结果，取其中的最大结果 return max( # 情况一、s1[i] 不在 lcs 中 dp(s1, i + 1, s2, j), # 情况二、s2[j] 不在 lcs 中 dp(s1, i, s2, j + 1), # 情况三、都不在 lcs 中 dp(s1, i + 1, s2, j + 1) ) 这里就已经非常接近我们的最终答案了，还有一个小的优化，情况三「s1[i]和s2[j]都不在 lcs 中」其实可以直接忽略。 因为我们在求最大值嘛，情况三在计算s1[i+1..]和s2[j+1..]的lcs长度，这个长度肯定是小于等于情况二s1[i..]和s2[j+1..]中的lcs长度的，因为s1[i+1..]比s1[i..]短嘛，那从这里面算出的lcs当然也不可能更长嘛。 同理，情况三的结果肯定也小于等于情况一。说白了，情况三被情况一和情况二包含了，所以我们可以直接忽略掉情况三，完整代码如下： 123456789101112131415161718192021222324252627282930def longestCommonSubsequence(s1, s2): m, n = len(s1), len(s2) # 备忘录值为 -1 代表未曾计算 memo = [[-1] * n for _ in range(m)] # 计算 s1[0..] 和 s2[0..] 的 lcs 长度 return dp(s1, 0, s2, 0)# 定义：计算 s1[i..] 和 s2[j..] 的最长公共子序列长度def dp(s1, i, s2, j): # base case if i == len(s1) or j == len(s2): return 0 # 如果之前计算过，则直接返回备忘录中的答案 if memo[i][j] != -1: return memo[i][j] # 根据 s1[i] 和 s2[j] 的情况做选择 if s1[i] == s2[j]: # s1[i] 和 s2[j] 必然在 lcs 中 memo[i][j] = 1 + dp(s1, i + 1, s2, j + 1) else: # s1[i] 和 s2[j] 至少有一个不在 lcs 中 memo[i][j] = max( dp(s1, i + 1, s2, j), dp(s1, i, s2, j + 1) ) return memo[i][j] 以上思路完全就是按照我们之前的 动态规划详解 的框架来的，应该是很容易理解的。至于为什么要加memo备忘录，，这里再简单重复一下，首先抽象出我们核心dp函数的递归框架： 12345def dp(int i, int j): dp(i + 1, j + 1) #1 dp(i, j + 1) #2 dp(i + 1, j) #3 你看，假设我想从dp(i, j)转移到dp(i+1, j+1)，有不止一种方式，可以直接走#1，也可以走#2 -&gt; #3，也可以走#3 -&gt; #2。 这就是重叠子问题，如果我们不用memo备忘录消除子问题，那么dp(i+1, j+1)就会被多次计算，这是没有必要的。 至此，最长公共子序列问题就完全解决了，用的是自顶向下带备忘录的动态规划思路。 下面，来看两道和最长公共子序列相似的两道题目。 二、字符串的删除操作 这是力扣第 583 题「两个字符串的删除操作」，看下题目： 函数签名如下： 1def minDistance(s1, s2): 题目让我们计算将两个字符串变得相同的最少删除次数，那我们可以思考一下，最后这两个字符串会被删成什么样子？ 删除的结果不就是它俩的最长公共子序列嘛！ 那么，要计算删除的次数，就可以通过最长公共子序列的长度推导出来： 123456def minDistance(s1, s2): m, n = len(s1), len(s2) # 复用前文计算 lcs 长度的函数 lcs = longestCommonSubsequence(s1, s2) return m - lcs + n - lcs; 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839class Solution(object): def __init__(self): self.memo = None def minDistance(self, s1, s2): &quot;&quot;&quot; :type s1: str :type s2: str :rtype: int &quot;&quot;&quot; m, n = len(s1), len(s2) # 备忘录值为 -1 代表未曾计算 self.memo = [[-1] * n for _ in range(m)] # 计算 s1[0..] 和 s2[0..] 的 lcs 长度 lcs = self.dp(s1, 0, s2, 0) return m - lcs + n - lcs # 定义：计算 s1[i..] 和 s2[j..] 的最长公共子序列长度 def dp(self, s1, i, s2, j): # base case if i == len(s1) or j == len(s2): return 0 # 如果之前计算过，则直接返回备忘录中的答案 if self.memo[i][j] != -1: return self.memo[i][j] # 根据 s1[i] 和 s2[j] 的情况做选择 if s1[i] == s2[j]: # s1[i] 和 s2[j] 必然在 lcs 中 self.memo[i][j] = 1 + self.dp(s1, i + 1, s2, j + 1) else: # s1[i] 和 s2[j] 至少有一个不在 lcs 中 self.memo[i][j] = max( self.dp(s1, i + 1, s2, j), self.dp(s1, i, s2, j + 1) ) return self.memo[i][j] 这道题就解决了！ 三、最小 ASCII 删除和 这是力扣第 712 题，看下题目： 这道题，和上一道题非常类似，这回不问我们删除的字符个数了，问我们删除的字符的 ASCII 码加起来是多少。 那就不能直接复用计算最长公共子序列的函数了，但是可以依照之前的思路，稍微修改 base case 和状态转移部分即可直接写出解法代码： 123456789101112131415161718192021222324252627282930313233343536373839def minimumDeleteSum(s1, s2): m, n = len(s1), len(s2) # 备忘录值为 -1 代表未曾计算 memo = [[-1] * n for _ in range(m)] return dp(s1, 0, s2, 0)# 定义：将 s1[i..] 和 s2[j..] 删除成相同字符串，# 最小的 ASCII 码之和为 dp(s1, i, s2, j)。def dp(s1, i, s2, j): res = 0 # base case if i == len(s1): # 如果 s1 到头了，那么 s2 剩下的都得删除 for j in range(j, len(s2)): res += ord(s2[j]) return res if j == len(s2): # 如果 s2 到头了，那么 s1 剩下的都得删除 for i in range(i, len(s1)): res += ord(s1[i]) return res if memo[i][j] != -1: return memo[i][j] if s1[i] == s2[j]: # s1[i] 和 s2[j] 都是在 lcs 中的，不用删除 memo[i][j] = dp(s1, i + 1, s2, j + 1) else: # s1[i] 和 s2[j] 至少有一个不在 lcs 中，删一个 memo[i][j] = min( ord(s1[i]) + dp(s1, i + 1, s2, j), ord(s2[j]) + dp(s1, i, s2, j + 1) ) return memo[i][j] base case 有一定区别，计算lcs长度时，如果一个字符串为空，那么lcs长度必然是 0；但是这道题如果一个字符串为空，另一个字符串必然要被全部删除，所以需要计算另一个字符串所有字符的 ASCII 码之和。 关于状态转移，当s1[i]和s2[j]相同时不需要删除，不同时需要删除，所以可以利用dp函数计算两种情况，得出最优的结果。其他的大同小异，就不具体展开了。 也可以这样：思路一样： 12345678910111213141516171819class Solution(object): def minimumDeleteSum(self, s1, s2): dp = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)] # 确定table的初始条件。 for i in range(len(s1) - 1, -1, -1): dp[i][len(s2)] = dp[i+1][len(s2)] + ord(s1[i]) for j in range(len(s2) - 1, -1, -1): dp[len(s1)][j] = dp[len(s1)][j+1] + ord(s2[j]) for i in range(len(s1) - 1, -1, -1): for j in range(len(s2) - 1, -1, -1): if s1[i] == s2[j]: dp[i][j] = dp[i+1][j+1] else: dp[i][j] = min(dp[i+1][j] + ord(s1[i]), dp[i][j+1] + ord(s2[j])) return dp[0][0] 至此，三道子序列问题就解决完了，关键在于将问题细化到字符，根据每两个字符是否相同来判断他们是否在结果子序列中，从而避免了对所有子序列进行穷举。","categories":[{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"00-二(N)叉树的深度遍历——递归&迭代","slug":"00-二叉树的深度遍历——递归-迭代","date":"2021-06-27T06:22:25.000Z","updated":"2021-07-01T11:01:32.376Z","comments":true,"path":"20210627/00-二叉树的深度遍历——递归-迭代.html","link":"","permalink":"https://xxren8218.github.io/20210627/00-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%81%8D%E5%8E%86%E2%80%94%E2%80%94%E9%80%92%E5%BD%92-%E8%BF%AD%E4%BB%A3.html","excerpt":"","text":"二叉树的深度遍历——递归&amp;迭代读完本文，你可以去力扣拿下： 144.二叉树的前序遍历：难度 Easy 145.二叉树的后序遍历：难度 Easy 94.二叉树的中序遍历 ：难度 Easy 本文所讲的是：二叉树的深度遍历逻辑。一般分为递归版本和非递归的版本。说到递归，很多人不禁会想到一个词：‘谈归色变’，不知道进去后怎么处理。甚至不知道怎么进去的。递归其实没那么困难。这里介绍递归三部曲： 确定递归的参数和返回值——参数一般头结点指针（cur）,返回值一般为列表 确定递归终止条件——if cur == None: return 单层递归的逻辑 前序：中左右 中：res.append(cur.val) 左：traversal(cur.left) 右：traversal(cur.right) 后序：左右中 左：traversal(cur.left) 右：traversal(cur.right) 中：res.append(cur.val) 中序：左中右 左：traversal(cur.left) 中：res.append(cur.val) 右：traversal(cur.right) 1.二叉树的遍历（递归法） 1.1 二叉树的前序遍历 函数签名如下： 1def preorderTraversal(self, root: TreeNode) -&gt; List[int]: 首先我们需要了解什么是二叉树的前序遍历：按照访问根节点——左子树——右子树的方式遍历这棵树，而在访问左子树或者右子树的时候，我们按照同样的方式遍历，直到遍历完整棵树。因此整个遍历过程天然具有递归的性质，我们可以直接用递归函数来模拟这一过程。 递归的解题思路很简单，这里直接写代码： 12345678910111213141516171819202122# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def preorderTraversal(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[int] &quot;&quot;&quot; res = [] def prev(root): if not root: return [] res.append(root.val) prev(root.left) prev(root.right) prev(root) return res 1.2 二叉树的后序遍历 函数签名如下： 1def postorderTraversal(self, root: TreeNode) -&gt; List[int]: 按照我们前面的框架直接可以写出来： 12345678910111213141516171819202122# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def postorderTraversal(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[int] &quot;&quot;&quot; res = [] def postorder(root): if not root: return [] postorder(root.left) postorder(root.right) res.append(root.val) postorder(root) return res 1.3 二叉树的中序遍历 按照我们前面的框架直接可以写出来： 12345678910111213141516171819202122# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def inorderTraversal(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[int] &quot;&quot;&quot; res = [] def inorder(root): if not root: return [] inorder(root.left) res.append(root.val) inorder(root.right) inorder(root) return res 2. 二叉树的遍历（迭代法） 我们也可以用迭代的方式实现方法一的递归函数，两种方式是等价的，区别在于递归的时候隐式地维护了一个栈，而我们在迭代的时候需要显式地将这个栈模拟出来。 因为函数本身调用就是基于栈实现的，原则上所有的递归都可以用栈来实现。所以这里用栈来实现。 2.1 二叉树的前序遍历我们先看⼀下前序遍历。前序遍历是中左右，每次先处理的是中间节点，那么先将跟节点放⼊栈中，然后将右孩⼦加⼊栈，再加⼊左孩⼦。为什么要先加⼊ 右孩⼦，再加⼊左孩⼦呢？ 因为这样出栈的时候才是中左右的顺序。 不难写出如下代码: （注意代码中空节点不⼊栈） 123456789101112131415161718192021222324252627282930313233# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def preorderTraversal(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[int] &quot;&quot;&quot; if not root: return[] stack = [root] res = [] while stack: # 从栈中弹出node节点 node = stack.pop() # 判断node节点是否为空 if node: # 若不为空，将node节点的值加入结果res列表中 res.append(node.val) else: # 若为空，结束本次循环（他没有左右孩子） continue # 将右左孩子分别添加进stack中，因为是栈,所以先处理的是左 stack.append(node.right) stack.append(node.left) return res 2.2 二叉树的后序遍历对于后序遍历而言，我们很容易看出来。 前序遍历为： 中左右 后序遍历为：左右中 发现他们的特点，我们可以执行这样的操作：将前序遍历的左右调换，成为中右左了，然后调用切片反转就可以了。 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def preorderTraversal(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[int] &quot;&quot;&quot; if not root: return[] stack = [root] res = [] while stack: node = stack.pop() if node: res.append(node.val) else: continue stack.append(node.left) ## 注意 stack.append(node.right) ## 注意 return res[::-1] ## 注意 2.3 二叉树的中序遍历此时是不是想改⼀点前序遍历代码顺序就把中序遍历搞出来了？其实还真不⾏！但接下来， 再⽤迭代法写中序遍历的时候，会发现套路⼜不⼀样了，⽬前的前序遍历的逻辑⽆法直接应⽤到中序遍历上。 为了解释清楚，我说明⼀下 刚刚在迭代的过程中，其实我们有两个操作： 处理：将元素放进result数组中 访问：遍历节点 分析⼀下为什么刚刚写的前序遍历的代码，不能和中序遍历通⽤呢，因为前序遍历的顺序是中左右，先访问的元素是中间节点，要处理的元素也是中间节点，所以刚刚才能写出相对简洁的代码， 因为要访问的元素和要处理的元素顺序是⼀致的，都是中间节点。 那么再看看中序遍历，中序遍历是左中右，先访问的是⼆叉树顶部的节点，然后⼀层⼀层向下访问，直到到达树左⾯的最底部，再开始处理节点（也就是在把节点的数值放进result数组中），这就造成了处理顺序和访问顺序是不⼀致的。 那么在使用迭代法写中序遍历，就需要借用指针的遍历来帮助访问节点，栈则用来处理节点上的元素。 动画如下： 代码如下： 1234567891011121314151617181920212223242526272829303132333435# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution(object): def inorderTraversal(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[int] &quot;&quot;&quot; if not root: return [] res = [] stack = [] cur = root # 当指针为空，且栈为空时停止 while stack or cur: # 若当前指针不为空，将当前访问元素加到栈内。 if cur: stack.append(cur) # 指针继续左走（左中右）,一路向左 cur = cur.left # 若zcur为空，说明到达最左边了. else: # 此时弹出栈顶元素——最左边的节点 cur = stack.pop() res.append(cur.val) # 遍历当前指针的右孩子 cur = cur.right return res 至此！迭代遍历的三种方法也已经得出了！ 3. N叉树的遍历（迭代法） 3.1 N叉树的前序遍历 思路和二叉树的差不多，比较简单直接写代码，注意点：写个for来遍历全部孩子即可。 123456789101112131415161718192021222324&quot;&quot;&quot;# Definition for a Node.class Node(object): def __init__(self, val=None, children=None): self.val = val self.children = children&quot;&quot;&quot;class Solution(object): def preorder(self, root): &quot;&quot;&quot; :type root: Node :rtype: List[int] &quot;&quot;&quot; res = [] def prev(root): if not root: return [] res.append(root.val) for child in root.children: prev(child) prev(root) return res 3.2 N叉树的后序遍历 123456789101112131415161718192021222324&quot;&quot;&quot;# Definition for a Node.class Node(object): def __init__(self, val=None, children=None): self.val = val self.children = children&quot;&quot;&quot;class Solution(object): def postorder(self, root): &quot;&quot;&quot; :type root: Node :rtype: List[int] &quot;&quot;&quot; res = [] def post(root): if not root: return [] for child in root.children: post(child) res.append(root.val) post(root) return res 4. N叉树的遍历（迭代法） 4.1 N叉树的前序遍历 我们使用栈来帮助我们得到前序遍历，需要保证栈顶的节点就是我们当前遍历到的节点。 我们首先把根节点入栈，因为根节点是前序遍历中的第一个节点。随后每次我们从栈顶取出一个节点 u，它是我们当前遍历到的节点，并把 u 的所有子节点逆序推入栈中。例如 u 的子节点从左到右为 v1, v2, v3，那么推入栈的顺序应当为 v3, v2, v1，这样就保证了下一个遍历到的节点（即 u 的第一个子节点 v1）出现在栈顶的位置。 12345678910111213141516171819202122232425262728&quot;&quot;&quot;# Definition for a Node.class Node(object): def __init__(self, val=None, children=None): self.val = val self.children = children&quot;&quot;&quot;class Solution(object): def preorder(self, root): &quot;&quot;&quot; :type root: Node :rtype: List[int] &quot;&quot;&quot; if not root: return [] res = [] stack = [root] while stack: cur = stack.pop() res.append(cur.val) # 注意append 和 extend 方法的区别： ## a = [] ## a.extend[1,2,3,4] -&gt; [1,2,3,4] ## a.append([1,2,3,4]) -&gt; [[1,2,3,4]] stack.extend(cur.children[::-1]) return res 4.2 N叉树的后序遍历 通过观察结果，我们可以看到若是将后序遍历的结果进行反转可以得到[1,4,2,3,6,5],即先遍历父节点，再从右往左遍历子节点。结果和前序遍历非常类似，只不过前序遍历中对子节点的遍历顺序是 v1, v2, v3，而这里是 v3, v2, v1。 因此我们可以使用和 N叉树的前序遍历 相同的方法，使用一个栈来得到后序遍历。我们首先把根节点入栈。 12345678910111213141516171819202122232425&quot;&quot;&quot;# Definition for a Node.class Node(object): def __init__(self, val=None, children=None): self.val = val self.children = children&quot;&quot;&quot;class Solution(object): def postorder(self, root): &quot;&quot;&quot; :type root: Node :rtype: List[int] &quot;&quot;&quot; if not root: return [] res = [] stack = [root] while stack: cur = stack.pop() res.append(cur.val) stack.extend(cur.children[::1]) return res[::-1] 5. 总结 我们用普通的递归的方法来实现二叉树的前序、后序和中序遍历。还用迭代的方式实现三种遍历方法。其实二叉树不仅仅有深度优先遍历（DFS）还有一种遍历方式：广度优先遍历（BFS）。后面我们会开始BFS的探索。","categories":[{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[]},{"title":"23-大数据产品与互联网产品结合","slug":"23-大数据产品与互联网产品结合","date":"2021-06-25T16:53:04.000Z","updated":"2021-06-25T16:56:01.393Z","comments":true,"path":"20210626/23-大数据产品与互联网产品结合.html","link":"","permalink":"https://xxren8218.github.io/20210626/23-%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%BA%A7%E5%93%81%E4%B8%8E%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%A7%E5%93%81%E7%BB%93%E5%90%88.html","excerpt":"","text":"1. 大数据产品与互联网产品结合 分布式系统执行任务瓶颈: 延迟高 MapReduce 几分钟 Spark几秒钟 互联网产品要求 毫秒级响应(1秒以内完成) 需要通过大数据实现 统计分析 数据挖掘 关联推荐 用户画像 如何将他们结合起来? 大数据平台 整合网站应用和大数据系统之间的差异, 将应用产生的数据导入到大数据系统, 经过处理计算后再导出给应用程序使用 互联网大数据平台架构: 数据采集 App/Web 产生的数据&amp;日志同步到大数据系统 数据库同步:Sqoop 日志同步:Flume 打点: Kafka 不同数据源产生的数据质量可能差别很大 数据库 也许可以直接用 日志 爬虫 大量的清洗,转化处理 ETL 数据处理 大数据存储与计算的核心 数据同步后导入HDFS MapReduce Hive Spark 读取数据进行计算 结果再保存到HDFS MapReduce Hive Spark 离线计算, HDFS 离线存储 离线计算通常针对(某一类别)全体数据, 比如 历史上所有订单 离线计算特点: 数据规模大, 运行时间长 流式计算 淘宝双11 每秒产生订单数 监控宣传 Storm(毫秒) SparkStreaming(秒) 数据输出与展示 HDFS需要把数据导出交给应用程序, 让用户实时展示 ECharts 淘宝卖家量子魔方 给运营和决策层提供各种统计报告, 数据需要写入数据库 很多运营管理人员, 上班后就会登陆后台数据系统 任务调度系统 将上面三个部分整合起来 2. 大数据应用—数据分析 通过数据分析指标监控企业运营状态, 及时调整运营和产品策略,是大数据技术的关键价值之一 大数据平台(互联网企业)运行的绝大多数大数据计算都是关于数据分析的 统计指标 关联分析, 汇总报告, 运营数据是公司管理的基础 了解公司目前发展的状况 数据驱动运营: 调节指标对公司进行管理 运营数据的获取需要大数据平台的支持 埋点采集数据 数据库,日志 三方采集数据 对数据清洗 转换 存储 利用SQL进行数据统计 汇总 分析 得到需要的运营数据报告 运营常用数据指标 新增用户数 UG user growth 用户增长 产品增长性的关键指标 新增访问网站(新下载APP)的用户数 用户留存率 用户留存率 = 留存用户数 / 当期新增用户数 3日留存 5日留存 7日留存 活跃用户数 打开使用产品的用户 日活 月活 提升活跃是网站运营的重要目标 PV Page View 打开产品就算活跃 打开以后是否频繁操作就用PV衡量, 每次点击, 页面跳转都记一次PV GMV 成交总金额(Gross Merchandise Volume) 电商网站统计营业额, 反应网站营收能力的重要指标 GMV相关的指标: 订单量 客单价 转化率 1转化率 = 有购买行为的用户数 / 总访问用户数 3. 数据分析案例 背景: 某电商网站, 垂直领域领头羊, 各项指标相对稳定 运营人员发现从 8 月 15 日开始，网站的订单量连续四天明显下跌 8 月 18 号早晨发现 8 月 17 号的订单量没有恢复正常，运营人员开始尝试寻找原因 是否有负面报道被扩散 是否竞争对手在做活动 是否某类商品缺货 价格异常 没有找到原因, 将问题交给数据分析团队 数据分析师分析可能性 新增用户出现问题 查看日活数据, 发现日活没有明显下降 基本判断, 用户在访问网站的过程中,转化出了问题 转化过程: 打开APP 搜索关键词 浏览搜索结果列表 点击商品访问详情 有购买意向开始咨询 放入购物车 支付 订单活跃转化率 = 日订单量 / 打开用户数 搜索打开转化率 = 搜索用户数 / 打开用户数 有明显降幅的是咨询详情转化率 对咨询信息分类统计后发现，新用户的咨询量几乎为 0 于是将问题提交给技术部门调查，工程师查看 8 月 15 日当天发布记录,发现有消息队列SDK更新 Hadoop企业应用案例之消费大数据 亚马逊提前发货系统 Hadoop企业案例之商业零售大数据 智能推荐","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"22-hadoop概念扩展——生态与HDFS的读写等","slug":"22-hadoop概念扩展——生态与HDFS的读写等","date":"2021-06-25T16:43:34.000Z","updated":"2021-06-25T16:49:17.698Z","comments":true,"path":"20210626/22-hadoop概念扩展——生态与HDFS的读写等.html","link":"","permalink":"https://xxren8218.github.io/20210626/22-hadoop%E6%A6%82%E5%BF%B5%E6%89%A9%E5%B1%95%E2%80%94%E2%80%94%E7%94%9F%E6%80%81%E4%B8%8EHDFS%E7%9A%84%E8%AF%BB%E5%86%99%E7%AD%89.html","excerpt":"","text":"hadoop概念扩展课程目标： 知道hadoop生态组成 了解hdfs读写流程 说出Hadoop发行版本的选择 1. Hadoop生态系统狭义的Hadoop VS 广义的Hadoop 狭义的Hadoop:HDFS、MapReduce、YARN。 广义的Hadoop：指的是Hadoop生态系统，Hadoop生态系统是一个很庞大的概念，hadoop是其中最重要最基础的一个部分，生态系统中每一子系统只解决某一个特定的问题域（甚至可能更窄），不搞统一型的全能系统，而是小而精的多个小系统； Hive:数据仓库——操作MapReuce来操作HDFS（我们的感觉是写SQL，Hive将SQL写成MapReduce的方式。） R:数据分析 Mahout:机器学习库 pig：脚本语言，跟Hive类似 Oozie:工作流引擎，管理作业执行顺序 Zookeeper:用户无感知，主节点挂掉选择从节点作为主的。分布式集群协调工具。数据改变的同步。 Flume:日志收集框架——将特定目录日志放到HDFS中去。 Sqoop:数据交换框架，例如：关系型数据库（MySQL、Oracle）与HDFSorHBase之间的数据交换介质。 Hbase : ——列式存储（MySQL为行式存储——连续存放）海量数据中的查询，相当于分布式文件系统中的数据库 Spark: 分布式的计算框架基于内存 ——有python的API：pyspark scala写的（java的虚拟机语言） spark core——对应MapReduce spark sql——对应Hive spark streaming 准实时 不算是一个标准的流式计算 对应——storm flink spark ML spark MLlib 机器学习的库 Kafka: 消息队列 Storm: 分布式的流式计算框架 不适合用python操作storm Flink: 分布式的流式计算框架 Hadoop生态系统的特点 开源、社区活跃 囊括了大数据处理的方方面面 成熟的生态圈 2. HDFS 读写流程 &amp; 高可用 HDFS读写流程 客户端向NameNode发出写文件请求。 检查是否已存在文件、检查权限。若通过检查，直接先将操作写入EditLog，并返回输出流对象。（注：WAL，write ahead log，先写Log，再写内存，因为EditLog记录的是最新的HDFS客户端执行所有的写操作。如果后续真实写操作失败了，由于在真实写操作之前，操作就被写入EditLog中了，故EditLog中仍会有记录，我们不用担心后续client读不到相应的数据块，因为在第5步中DataNode收到块后会有一返回确认信息，若没写成功，发送端没收到确认信息，会一直重试，直到成功） client端按128MB的块切分文件。 client将NameNode返回的分配的可写的DataNode列表和Data数据一同发送给最近的第一个DataNode节点，此后client端和NameNode分配的多个DataNode构成pipeline管道，client端向输出流对象中写数据。client每向第一个DataNode写入一个packet，这个packet便会直接在pipeline里传给第二个、第三个…DataNode。（注：并不是写好一个块或一整个文件后才向后分发） 每个DataNode写完一个块后，会返回确认信息。（注：并不是每写完一个packet后就返回确认信息，个人觉得因为packet中的每个chunk都携带校验信息，没必要每写一个就汇报一下，这样效率太慢。正确的做法是写完一个block块后，对校验信息进行汇总分析，就能得出是否有块写错的情况发生） 写完数据，关闭输输出流。 发送完成信号给NameNode。 （注：发送完成信号的时机取决于集群是强一致性还是最终一致性，强一致性则需要所有DataNode写完后才向NameNode汇报。最终一致性则其中任意一个DataNode写完后就能单独向NameNode汇报，HDFS一般情况下都是强调强一致性） HDFS如何实现高可用(HA) 数据存储故障容错 磁盘介质在存储过程中受环境或者老化影响,数据可能错乱 对于存储在 DataNode 上的数据块，计算并存储校验和（CheckSum) 读取数据的时候, 重新计算读取出来的数据校验和, 校验不正确抛出异常, 从其它DataNode上读取备份数据 磁盘故障容错 DataNode 监测到本机的某块磁盘损坏 将该块磁盘上存储的所有 BlockID 报告给 NameNode NameNode 检查这些数据块在哪些DataNode上有备份, 通知相应DataNode, 将数据复制到其他服务器上 DataNode故障容错 通过心跳和NameNode保持通讯 超时未发送心跳, NameNode会认为这个DataNode已经宕机 NameNode查找这个DataNode上有哪些数据块, 以及这些数据在其它DataNode服务器上的存储情况 从其它DataNode服务器上复制数据 NameNode故障容错 主从热备： 必须通过zookeeper secondary namenode，对namenode数据的备份 zookeeper配合： ①master节点选举， ②负责数据一致性的保证。（namenode变化，其余保证也要变化。） 3. Hadoop发行版的选择 Apache Hadoop 开源社区版 最新的Hadoop版本都是从Apache Hadoop发布的 Hadoop Hive Flume 版本不兼容的问题 jar包 spark scala Java-&gt;.class-&gt;.jar -&gt;JVM CDH: Cloudera Distributed Hadoop Cloudera 在社区版的基础上做了一些修改 http://archive.cloudera.com/cdh5/cdh/5/ hadoop-2.6.0-cdh-5.7.0 和 Flume*-cdh5.7.0 cdh版本一致 的各个组件配合是有不会有兼容性问题 CDH版本的这些组件 没有全部开源 HDP: Hortonworks Data Platform","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"07-动态规划之子序列解题模板——最长回文子序列","slug":"07-动态规划之子序列解题模板——最长回文子序列","date":"2021-06-25T16:38:51.000Z","updated":"2021-06-25T16:41:54.758Z","comments":true,"path":"20210626/07-动态规划之子序列解题模板——最长回文子序列.html","link":"","permalink":"https://xxren8218.github.io/20210626/07-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B9%8B%E5%AD%90%E5%BA%8F%E5%88%97%E8%A7%A3%E9%A2%98%E6%A8%A1%E6%9D%BF%E2%80%94%E2%80%94%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E5%BA%8F%E5%88%97.html","excerpt":"","text":"子序列解题模板：最长回文子序列子序列问题是常见的算法问题，而且并不好解决。 首先，子序列问题本身就相对子串、子数组更困难一些，因为前者是不连续的序列，而后两者是连续的，就算穷举都不容易，更别说求解相关的算法问题了。 而且，子序列问题很可能涉及到两个字符串，比如让你求两个字符串的 [最长公共子序列]，如果没有一定的处理经验，真的不容易想出来。所以本文就来扒一扒子序列问题的套路，其实就有两种模板，相关问题只要往这两种思路上想，十拿九稳。 一般来说，这类问题都是让你求一个最长子序列，因为最短子序列就是一个字符嘛，没啥可问的。一旦涉及到子序列和最值，那几乎可以肯定，考察的是动态规划技巧，时间复杂度一般都是 O(n^2)。 原因很简单，你想想一个字符串，它的子序列有多少种可能？起码是指数级的吧，这种情况下，不用动态规划技巧，还想怎么着呢？ 既然要用动态规划，那就要定义 dp 数组，找状态转移关系。我们说的两种思路模板，就是 dp 数组的定义思路。不同的问题可能需要不同的 dp 数组定义来解决。 一、两种思路 1、第一种思路模板是一个一维的 dp 数组：123456n = len(s1 + 1)dp = [0] * nfor i in range(1, n): for j in range(i): dp[i] = 最值(dp[i], dp[j] + ...) 举个我们写过的例子 [最长递增子序列]，在这个思路中 dp 数组的定义是： 在子数组array[0..i]中，以array[i]结尾的目标子序列（最长递增子序列）的长度是dp[i]。 为啥最长递增子序列需要这种思路呢？前文说得很清楚了，因为这样符合归纳法，可以找到状态转移的关系，这里就不具体展开了。 2、第二种思路模板是一个二维的 dp 数组：123456789n = len(s1 + 1)dp = [[0] * n for _ in range(n)]for i in range(n): for j in range(1, n) if (arr[i] == arr[j]) dp[i][j] = dp[i][j] + ... else: dp[i][j] = 最值(...) 这种思路运用相对更多一些，尤其是涉及两个字符串/数组的子序列。本思路中 dp 数组含义又分为「只涉及一个字符串」和「涉及两个字符串」两种情况。 涉及两个字符串/数组时（比如最长公共子序列），dp 数组的含义如下： 在子数组arr1[0..i]和子数组arr2[0..j]中，我们要求的子序列（最长公共子序列）长度为dp[i][j]。 只涉及一个字符串/数组时（比如本文要讲的最长回文子序列），dp 数组的含义如下： 在子数组array[i..j]中，我们要求的子序列（最长回文子序列）的长度为dp[i][j]。 下面就借最长回文子序列这个问题，详解一下第二种情况下如何使用动态规划。 二、最长回文子序列 我们说这个问题对 dp 数组的定义是：在子串s[i..j]中，最长回文子序列的长度为dp[i][j]。一定要记住这个定义才能理解算法。 为啥这个问题要这样定义二维的 dp 数组呢？我们前文多次提到，找状态转移需要归纳思维，说白了就是如何从已知的结果推出未知的部分，这样定义容易归纳，容易发现状态转移关系。 具体来说，如果我们想求dp[i][j]，假设你知道了子问题dp[i+1][j-1]的结果（s[i+1..j-1]中最长回文子序列的长度），你是否能想办法算出dp[i][j]的值（s[i..j]中，最长回文子序列的长度）呢？ 可以！这取决于s[i]和s[j]的字符： 如果它俩相等，那么它俩加上s[i+1..j-1]中的最长回文子序列就是s[i..j]的最长回文子序列： 如果它俩不相等，说明它俩不可能同时出现在s[i..j]的最长回文子序列中，那么把它俩分别加入s[i+1..j-1]中，看看哪个子串产生的回文子序列更长即可： 以上两种情况写成代码就是这样： 123456if s[i] == s[j]: # 它俩一定在最长回文子序列中 dp[i][j] = dp[i + 1][j - 1] + 2else: # s[i+1..j] 和 s[i..j-1] 谁的回文子序列更长？ dp[i][j] = max(dp[i + 1][j], dp[i][j - 1]) 三、代码实现 首先明确一下 base case，如果只有一个字符，显然最长回文子序列长度是 1，也就是dp[i][j] = 1,(i == j)。 因为i肯定小于等于j，所以对于那些i &gt; j的位置，根本不存在什么子序列，应该初始化为 0。 另外，看看刚才写的状态转移方程，想求dp[i][j]需要知道dp[i+1][j-1]，dp[i+1][j]，dp[i][j-1]这三个位置；再看看我们确定的 base case，填入 dp 数组之后是这样： 为了保证每次计算dp[i][j]，左、下、左下三个方向的位置已经被计算出来，只能斜着遍历或者反着遍历： 我选择反着遍历，代码如下： 123456789101112131415161718def longestPalindromeSubseq(s): n = len(s) # dp 数组全部初始化为 0 [[0] * n for _ in range(n)] # base case for i in range(n): dp[i][i] = 1 # 反着遍历保证正确的状态转移 for i in range(n - 1, -1, -1): for j in range(i + 1, n): # 状态转移方程 if s[i] == s[j]: dp[i][j] = dp[i + 1][j - 1] + 2 else: dp[i][j] = max(dp[i + 1][j], dp[i][j - 1]) # 整个 s 的最长回文子串长度 return dp[0][n - 1] 至此，最长回文子序列的问题就解决了。 主要还是正确定义 dp 数组的含义，遇到子序列问题，首先想到两种动态规划思路，然后根据实际问题看看哪种思路容易找到状态转移关系。 另外，找到状态转移和 base case 之后，一定要观察 DP table，看看怎么遍历才能保证通过已计算出来的结果解决新的问题 有了以上思路方向，子序列问题也不过如此嘛。","categories":[{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"06-动态规划之最长公共子序列","slug":"06-动态规划之最长公共子序列","date":"2021-06-25T16:35:11.000Z","updated":"2021-06-25T16:38:20.459Z","comments":true,"path":"20210626/06-动态规划之最长公共子序列.html","link":"","permalink":"https://xxren8218.github.io/20210626/06-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B9%8B%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97.html","excerpt":"","text":"动态规划之最长公共子序列最长公共子序列（Longest Common Subsequence，简称 LCS）是一道非常经典的面试题目，因为它的解法是典型的二维动态规划，大部分比较困难的字符串问题都和这个问题一个套路，比如说编辑距离。而且，这个算法稍加改造就可以用于解决其他问题，所以说 LCS 算法是值得掌握的。 题目就是让我们求两个字符串的 LCS 长度： 123输入: str1 = &quot;abcde&quot;, str2 = &quot;ace&quot; 输出: 3 解释: 最长公共子序列是 &quot;ace&quot;，它的长度是 3 为啥这个问题就是动态规划来解决呢？因为子序列类型的问题，穷举出所有可能的结果都不容易，而动态规划算法做的就是穷举 + 剪枝，它俩天生一对儿。所以可以说只要涉及子序列问题，十有八九都需要动态规划来解决，往这方面考虑就对了。 下面就来手把手分析一下，这道题目如何用动态规划技巧解决。 一、动态规划思路 第一步，一定要明确dp数组的含义。 对于两个字符串的动态规划问题，套路是通用的。比如说对于字符串s1和s2，一般来说都要构造一个这样的 DP table： 为了方便理解此表，我们暂时认为索引是从 1 开始的，待会的代码中只要稍作调整即可。其中，dp[i][j]的含义是：对于s1[1..i]和s2[1..j]，它们的 LCS 长度是dp[i][j]。 比如上图的例子，dp 的含义就是：对于&quot;ac&quot;和&quot;babc&quot;，它们的 LCS 长度是 2。我们最终想得到的答案应该是dp[3][5]。 第二步，定义 base case。 我们专门让索引为 0 的行和列表示空串，dp[0][..]和dp[..][0]都应该初始化为 0，这就是 base case。 比如说，按照刚才 dp 数组的定义，dp[0][3]=0的含义是：对于字符串&quot;&quot;和&quot;bab&quot;，其 LCS 的长度为 0。因为有一个字符串是空串，它们的最长公共子序列的长度显然应该是 0。 第三步，找状态转移方程。 这是动态规划最难的一步，不过好在这种字符串问题的套路都差不多，权且借这道题来聊聊处理这类问题的思路。 状态转移说简单些就是做选择，比如说这个问题，是求s1和s2的最长公共子序列，不妨称这个子序列为lcs。那么对于s1和s2中的每个字符，有什么选择？很简单，两种选择，要么在lcs中，要么不在。 这个「在」和「不在」就是选择，关键是，应该如何选择呢？这个需要动点脑筋：如果某个字符应该在lcs中，那么这个字符肯定同时存在于s1和s2中，因为lcs是最长公共子序列嘛。所以本题的思路是这样： 用两个指针i和j从后往前遍历s1和s2，如果s1[i]==s2[j]，那么这个字符一定在lcs中；否则的话，s1[i]和s2[j]这两个字符至少有一个不在lcs中，需要丢弃一个。先看一下递归解法，比较容易理解： 1234567891011121314def longestCommonSubsequence(str1, str2) -&gt; int: def dp(i, j): # 空的 base case if i == -1 or j == -1: return 0 if str1[i] == str2[j]: # 这边找到一个 lcs 的元素，继续往前找 return dp(i - 1, j - 1) + 1 else: # 谁能让 lcs 最长，就听谁的 return max(dp(i - 1, j), dp(i, j - 1)) # i 和 j 初始化为最后一个索引 return dp(len(str1) - 1, len(str2) - 1) 对于第一种情况，找到一个lcs中的字符，同时将i, j向前移动一位，并给lcs的长度加一；对于后者，则尝试两种情况，取更大的结果。 其实这段代码就是暴力解法，我们可以通过备忘录或者 DP table 来优化时间复杂度，比如通过前文描述的 DP table 来解决： 1234567891011121314def longestCommonSubsequence(str1, str2) -&gt; int: m, n = len(str1), len(str2) # 构建 DP table 和 base case dp = [[0] * (n + 1) for _ in range(m + 1) ] # 进行状态转移 for i in range(1, m + 1): for j in range(1, n + 1): if str1[i - 1] == str2[j - 1]: # 找到一个 lcs 中的字符 dp[i][j] = 1 + dp[i - 1][j - 1] else: dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]) return dp[-1][-1] 二、疑难解答 对于s1[i]和s2[j]不相等的情况，至少有一个字符不在lcs中，会不会两个字符都不在呢？比如下面这种情况 所以代码是不是应该考虑这种情况，改成这样： 123456if str1[i - 1] == str2[j - 1]: # ...else: dp[i][j] = max(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) 我一开始也有这种怀疑，其实可以这样改，也能得到正确答案，但是多此一举，因为dp[i-1][j-1]永远是三者中最小的，max 根本不可能取到它。 原因在于我们对 dp 数组的定义：对于s1[1..i]和s2[1..j]，它们的 LCS 长度是dp[i][j]。 这样一看，显然dp[i-1][j-1]对应的lcs长度不可能比前两种情况大，所以没有必要参与比较。 三、总结 对于两个字符串的动态规划问题，一般来说都是像本文一样定义 DP table，因为这样定义有一个好处，就是容易写出状态转移方程，dp[i][j]的状态可以通过之前的状态推导出来： 找状态转移方程的方法是，思考每个状态有哪些「选择」，只要我们能用正确的逻辑做出正确的选择，算法就能够正确运行。","categories":[{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"21-MapReduce原理详解","slug":"21-MapReduce原理详解","date":"2021-06-24T16:49:11.000Z","updated":"2021-06-24T16:54:48.314Z","comments":true,"path":"20210625/21-MapReduce原理详解.html","link":"","permalink":"https://xxren8218.github.io/20210625/21-MapReduce%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3.html","excerpt":"","text":"1.MapReduce原理详解单机程序计算流程 输入数据—-&gt;读取数据—-&gt;处理数据—-&gt;写入数据—-&gt;输出数据 Hadoop计算流程 input data：输入数据 InputFormat：对数据进行切分，格式化处理 map：将前面切分的数据做map处理(将数据进行分类，输出(k,v)键值对数据) shuffle&amp;sort:将相同的数据放在一起，并对数据进行排序处理 reduce：将map输出的数据进行hash计算，对每个map数据进行统计计算 hash的目的：如英文单词中y,z的单词开头比较少。再次词频统计时，少的统计完了，但是多的并没有进行统计完，少的需要等待。——hash能使得均匀的进行统计。 OutputFormat：格式化输出数据 map：将数据进行处理 buffer in memory：达到80%数据时，将数据锁在内存上，将这部分输出到磁盘上 partitions：在磁盘上有很多”小的数据”，将这些数据进行归并排序。 merge on disk：将所有的”小的数据”进行合并。 reduce：不同的reduce任务，会从map中对应的任务中copy数据 ​ 在reduce中同样要进行merge操作 MR慢的原因：内存和磁盘之间频繁的数据IO交换。基于当时限制，内存比较贵。 但是Spark是基于内存的计算，速度快很多。 2 MapReduce架构 MapReduce架构 1.X（没有YARN之前，计算与分配都在一起。） JobTracker:负责接收客户作业提交，负责任务到作业节点上运行，检查作业的状态 TaskTracker：由JobTracker指派任务，定期向JobTracker汇报状态，在每一个工作节点上永远只会有一个TaskTracker MapReduce2.X架构 ResourceManager：负责资源的管理，负责提交任务到NodeManager所在的节点运行，检查节点的状态 NodeManager：由ResourceManager指派任务，定期向ResourceManager汇报状态","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"20-MapReduce实战——基于MRJob","slug":"20-MapReduce实战——基于MRJob","date":"2021-06-24T16:41:37.000Z","updated":"2021-06-24T16:43:51.781Z","comments":true,"path":"20210625/20-MapReduce实战——基于MRJob.html","link":"","permalink":"https://xxren8218.github.io/20210625/20-MapReduce%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8EMRJob.html","excerpt":"","text":"MapReduce实战1 利用MRJob编写和运行MapReduce代码mrjob 简介 提出背景 若要写MapReduce，一般也不会用hadoop streaming,它没有任何封装，是自己写脚本，通过指令上传。 实际上有很多步骤有优化的余地。——出现了MRJob的库 使用python开发在Hadoop上运行的程序, mrjob是最简单的方式 mrjob程序可以在本地测试运行也可以部署到Hadoop集群上运行 如果不想成为hadoop专家, 但是需要利用Hadoop写MapReduce代码,mrJob是很好的选择 优点： 如果涉及多个map和多个reduce，或上个MR的输出作为下一个MR的输入的话。若用hadoop-streaming，需要写多个脚本。而MRJob可以通过一个类对其进行解决。——MRStep。——应用：TOPN统计 mrjob 安装 使用pip安装 pip install mrjob mrjob实现WordCount 1234567891011121314151617181920from mrjob.job import MRJobclass MRWordFrequencyCount(MRJob): def mapper(self, _, line): # 得到三个生成器，需next yield &quot;chars&quot;, len(line) yield &quot;words&quot;, len(line.split()) yield &quot;lines&quot;, 1 # key 相同的会走到同一个reducer中 def reducer(self, key, values): yield key, sum(values)if __name__ == &#x27;__main__&#x27;: &quot;&quot;&quot; 调用run以后，他会自己调用mapper,和reducer方法。 &quot;&quot;&quot; MRWordFrequencyCount.run() 每个单词词频的统计。 123456789101112131415from mrjob.job import MRJob class MRWordCount(MRJob): # 每一行从line中输入 def mapper(self, key, line): for word in line.split(): yield word,1 # word相同的 会走到同一个reduce def reducer(self, word, counts): yield word, sum(counts) if __name__ == &#x27;__main__&#x27;: MRWordCount.run() 运行WordCount代码 打开命令行, 找到一篇文本文档, 敲如下命令: 1python mr_word_count.py my_file.txt 2 运行MRJOB的不同方式1、内嵌(-r inline)方式 特点是调试方便，启动单一进程模拟任务执行状态和结果，默认(-r inline)可以省略，输出文件使用 &gt; output-file 或-o output-file，比如下面两种运行方式是等价的 python word_count.py -r inline input.txt &gt; output.txtpython word_count.py input.txt &gt; output.txt 2、本地(-r local)方式 用于本地模拟Hadoop调试，与内嵌(inline)方式的区别是启动了多进程执行每一个任务。如： python word_count.py -r local input.txt &gt; output1.txt 3、Hadoop(-r hadoop)方式 用于hadoop环境，支持Hadoop运行调度控制参数，如： 1)指定Hadoop任务调度优先级(VERY_HIGH|HIGH),如：—jobconf mapreduce.job.priority=VERY_HIGH。 2)Map及Reduce任务个数限制，如：—jobconf mapreduce.map.tasks=2 —jobconf mapreduce.reduce.tasks=5 python word_count.py -r hadoop hdfs:///test.txt -o hdfs:///output 要求输出的hadoop不能有内容——删掉output。 遇到的坑——code127错误 在后面加 -python-bin /miniconda2/envs/py365/bin/python就行。因为在虚拟机运行为py3.x，而本机环境为2.x！ 3 mrjob 实现 topN统计（实验） 上个MR的输出作为下一个MR的输入的话。MRJob.MRStep 统计数据中出现次数最多的前n个数据 123456789101112131415161718192021222324252627282930313233343536373839import sysfrom mrjob.job import MRJob,MRStepimport heapqclass TopNWords(MRJob): def mapper(self, _, line): if line.strip() != &quot;&quot;: for word in line.strip().split(): yield word,1 # 介于mapper和reducer之间，用于临时的将mapper输出的数据进行统计 def combiner(self, word, counts): yield word,sum(counts) def reducer_sum(self, word, counts): yield None,(sum(counts),word) # key为None，只有value有值。 # 调换位置原因——后面的取最大的N个值是按key进行取值的。 # 利用heapq将数据进行排序，将最大的2个取出 def top_n_reducer(self,_,word_cnts): for cnt,word in heapq.nlargest(2,word_cnts): yield word,cnt # 再调换一次。 # 实现steps方法用于指定自定义的mapper，comnbiner和reducer方法 def steps(self): # 这里有两个MR。不过第二个没有mapper return [ MRStep(mapper=self.mapper, combiner=self.combiner, reducer=self.reducer_sum), MRStep(reducer=self.top_n_reducer) ]def main(): TopNWords.run()if __name__==&#x27;__main__&#x27;: main() 本地运行实例： 4 MRJOB 文件合并需求描述 两个文件合并 类似于数据库中的两张表合并 123456789uid uname01 user1 02 user203 user3uid orderid order_price01 01 8001 02 9002 03 8202 04 95 mrjob 实现 实现对两个数据表进行join操作，显示效果为每个用户的所有订单信息 12&quot;01:user1&quot; &quot;01:80,02:90&quot;&quot;02:user2&quot; &quot;03:82,04:95&quot; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from mrjob.job import MRJobimport osimport sysclass UserOrderJoin(MRJob): SORT_VALUES = True # 二次排序参数：http://mrjob.readthedocs.io/en/latest/job.html def mapper(self, _, line): fields = line.strip().split(&#x27;\\t&#x27;) # 用制表符进行拆分 if len(fields) == 2: # user data source = &#x27;A&#x27; user_id = fields[0] user_name = fields[1] yield user_id,[source,user_name] # 01 [A,user1] elif len(fields) == 3: # order data source =&#x27;B&#x27; user_id = fields[0] order_id = fields[1] price = fields[2] yield user_id,[source,order_id,price] #01 [&#x27;B&#x27;,01,80][&#x27;B&#x27;,02,90] else : pass def reducer(self,user_id,values): &#x27;&#x27;&#x27; 每个用户的订单列表 &quot;01:user1&quot; &quot;01:80,02:90&quot; &quot;02:user2&quot; &quot;03:82,04:95&quot; :param user_id: :param values:[A,user1] [&#x27;B&#x27;,01,80] :return: &#x27;&#x27;&#x27; values = [v for v in values] # 加了 &quot;A&quot;&quot;&quot;B&quot;以后保证先过来的是两个元素值。 # 首行SORT_VALUES = True if len(values)&gt;1 : user_name = values[0][1] order_info = [&#x27;:&#x27;.join([v[1],v[2]]) for v in values[1:]] #[01:80,02:90] yield &#x27;:&#x27;.join([user_id,user_name]),&#x27;,&#x27;.join(order_info)def main(): UserOrderJoin.run()if __name__ == &#x27;__main__&#x27;: main() 实现对两个数据表进行join操作，显示效果为每个用户所下订单的订单总量和累计消费金额 12&quot;01:user1&quot; [2, 170]&quot;02:user2&quot; [2, 177] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from mrjob.job import MRJobimport osimport sysclass UserOrderJoin(MRJob): # 二次排序参数：http://mrjob.readthedocs.io/en/latest/job.html SORT_VALUES = True def mapper(self, _, line): fields = line.strip().split(&#x27;\\t&#x27;) if len(fields) == 2: # user data source = &#x27;A&#x27; user_id = fields[0] user_name = fields[1] yield user_id,[source,user_name] elif len(fields) == 3: # order data source =&#x27;B&#x27; user_id = fields[0] order_id = fields[1] price = fields[2] yield user_id,[source,order_id,price] else : pass def reducer(self,user_id,values): &#x27;&#x27;&#x27; 统计每个用户的订单数量和累计消费金额 :param user_id: :param values: :return: &#x27;&#x27;&#x27; values = [v for v in values] user_name = None order_cnt = 0 order_sum = 0 if len(values)&gt;1: for v in values: if len(v) == 2 : user_name = v[1] elif len(v) == 3: order_cnt += 1 order_sum += int(v[2]) yield &quot;:&quot;.join([user_id,user_name]),(order_cnt,order_sum)def main(): UserOrderJoin().run()if __name__ == &#x27;__main__&#x27;: main()","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"05-动态规划之高楼扔鸡蛋进阶版","slug":"05-动态规划之高楼扔鸡蛋进阶版","date":"2021-06-24T16:36:48.000Z","updated":"2021-06-24T16:40:09.855Z","comments":true,"path":"20210625/05-动态规划之高楼扔鸡蛋进阶版.html","link":"","permalink":"https://xxren8218.github.io/20210625/05-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B9%8B%E9%AB%98%E6%A5%BC%E6%89%94%E9%B8%A1%E8%9B%8B%E8%BF%9B%E9%98%B6%E7%89%88.html","excerpt":"","text":"动态规划之高楼扔鸡蛋（进阶）前面一篇文章的所讲的动态规划的效率不是饿很高，在力扣的用例通过率50%左右。但是是较为容易理解的动态规划解法。今天来讲两种思路，来优化一下这个问题。分别是 二分查找优化 和 重新定义状态转移 二分搜索的优化思路也许是我们可以尽力尝试写出的，而修改状态转移的解法可能是不容易想到的，可以借此见识一下动态规划算法设计的玄妙，当做思维拓展。 一、二分搜索优化 之前提到过这个解法，核心是因为状态转移方程的单调性，这里可以具体展开看看。 题目要求最坏情况下至少需要扔几次鸡蛋才能测出鸡蛋恰好摔不碎的楼层F。首先简述一下原始动态规划的思路： 1、暴力穷举尝试在所有楼层1 &lt;= i &lt;= N扔鸡蛋，每次选择尝试次数最少的那一层； 2、每次扔鸡蛋有两种可能，要么碎，要么没碎； 3、如果鸡蛋碎了，F应该在第i层下面，否则，F应该在第i层上面； 4、鸡蛋是碎了还是没碎，取决于哪种情况下尝试次数更多，因为我们想求的是最坏情况下的结果。 核心的状态转移代码是这段： 123456789101112# 当前状态为 K 个鸡蛋，面对 N 层楼# 返回这个状态下的最优结果def dp(K, N): for i in range(1, N): # 最坏情况下的最少扔鸡蛋次数 res = min(res, max( dp(K - 1, i - 1), # 碎 dp(K, N - i) # 没碎 ) + 1 # 在第 i 楼扔了一次 ) return res 这个 for 循环就是下面这个状态转移方程的具体代码实现： 如果能够理解这个状态转移方程，那么就很容易理解二分查找的优化思路。 首先我们根据dp(K, N)数组的定义（有K个鸡蛋面对N层楼，最少需要扔 dp(K, N) 次），很容易知道K固定时，这个函数随着N的增加一定是单调递增的，无论你策略多聪明，楼层增加的话，测试次数一定要增加。 那么注意dp(K - 1, i - 1)和dp(K, N - i)这两个函数，其中i是从 1 到N单增的，如果我们固定K和N，把这两个函数看做关于i的函数，前者随着i的增加应该也是单调递增的，而后者随着i的增加应该是单调递减的： 这时候求二者的较大值，再求这些最大值之中的最小值，其实就是求这两条直线交点，也就是红色折线的最低点嘛。 二分查找的运用很广泛，形如下面这种形式的 for 循环代码： 1234for i in range(n): if (isOK(i)): return i 都很有可能可以运用二分查找来优化线性搜索的复杂度，回顾这两个dp函数的曲线，我们要找的最低点其实就是这种情况： 123for i in range(1, N) if dp(K - 1, i - 1) == dp(K, N - i): return dp(K, N - i); 熟悉二分搜索的同学肯定敏感地想到了，这不就是相当于求 Valley（山谷）值嘛，可以用二分查找来快速寻找这个点的，直接看代码吧，整体的思路还是一样，只是加快了搜索速度： 123456789101112131415161718192021222324252627282930313233343536def superEggDrop(self, K: int, N: int) -&gt; int: memo = dict() def dp(K, N): if K == 1: return N if N == 0: return 0 if (K, N) in memo: return memo[(K, N)] # for 1 &lt;= i &lt;= N: # res = min(res, # max( # dp(K - 1, i - 1), # dp(K, N - i) # ) + 1 # ) res = float(&#x27;INF&#x27;) # 用二分搜索代替线性搜索 low, high = 1, N while low &lt;= high: mid = (low + high) // 2 broken = dp(K - 1, mid - 1) # 碎 not_broken = dp(K, N - mid) # 没碎 # res = min(max(碎，没碎) + 1) if broken &gt; not_broken: high = mid - 1 res = min(res, broken + 1) else: low = mid + 1 res = min(res, not_broken + 1) memo[(K, N)] = res return res return dp(K, N) 这个算法的时间复杂度是多少呢？动态规划算法的时间复杂度就是子问题个数 × 函数本身的复杂度。 函数本身的复杂度就是忽略递归部分的复杂度，这里dp函数中用了一个二分搜索，所以函数本身的复杂度是 O(logN)。 子问题个数也就是不同状态组合的总数，显然是两个状态的乘积，也就是 O(KN)。 所以算法的总时间复杂度是 O(KNlogN), 空间复杂度 O(KN)。效率上比之前的算法 O(KN^2) 要高效不少。 二、重写状态转移 找动态规划的状态转移本就是见仁见智，比较玄学的事情。不同的状态定义可以衍生出不同的解法，其解法和复杂程度都可能有巨大差异。这里就是一个很好的例子。 再回顾一下我们之前定义的dp数组含义： 123def dp(k, n) -&gt; int# 当前状态为 k 个鸡蛋，面对 n 层楼# 返回这个状态下最少的扔鸡蛋次数 用 dp 数组表示的话也是一样的： 123dp[k][n] = m# 当前状态为 k 个鸡蛋，面对 n 层楼# 这个状态下最少的扔鸡蛋次数为 m 按照这个定义，就是确定当前的鸡蛋个数和面对的楼层数，就知道最小扔鸡蛋次数。最终我们想要的答案就是dp(K, N)的结果。 这种思路下，肯定要穷举所有可能的扔法的，用二分搜索优化也只是做了「剪枝」，减小了搜索空间，但本质思路没有变，只不过是更聪明的穷举。 现在，我们稍微修改dp数组的定义，确定当前的鸡蛋个数和最多允许的扔鸡蛋次数，就知道能够确定F的最高楼层数。 有点绕口，具体来说是这个意思： 123456789dp[k][m] = n# 当前有 k 个鸡蛋，可以尝试扔 m 次鸡蛋# 这个状态下，最坏情况下最多能确切测试一栋 n 层的楼# 比如说 dp[1][7] = 7 表示：# 现在有 1 个鸡蛋，允许你扔 7 次;# 这个状态下最多给你 7 层楼，# 使得你可以确定楼层 F 使得鸡蛋恰好摔不碎# （一层一层线性探查嘛） 这其实就是我们原始思路的一个「反向」版本，我们先不管这种思路的状态转移怎么写，先来思考一下这种定义之下，最终想求的答案是什么？ 我们最终要求的其实是扔鸡蛋次数m，但是这时候m在状态之中而不是dp数组的结果，可以这样处理： 123456def superEggDrop(K, N): m = 0; while dp[K][m] &lt; N m += 1 # 状态转移... return m; 题目不是给你K鸡蛋，N层楼，让你求最坏情况下最少的测试次数m 吗？while循环结束的条件是dp[K][m] == N，也就是给你K个鸡蛋，允许测试m次，最坏情况下最多能测试N层楼。 注意看这两段描述，是完全一样的！所以说这样组织代码是正确的，关键就是状态转移方程怎么找呢？还得从我们原始的思路开始讲。之前的解法配了这样图帮助大家理解状态转移思路： 这个图描述的仅仅是某一个楼层i，原始解法还得线性或者二分扫描所有楼层，要求最大值、最小值。但是现在这种dp定义根本不需要这些了，基于下面两个事实： 1、无论你在哪层楼扔鸡蛋，鸡蛋只可能摔碎或者没摔碎，碎了的话就测楼下，没碎的话就测楼上。 2、无论你上楼还是下楼，总的楼层数 = 楼上的楼层数 + 楼下的楼层数 + 1（当前这层楼）。 根据这个特点，可以写出下面的状态转移方程： 1dp[k][m] = dp[k][m-1] + dp[k-1][m-1] + 1 dp[k][m - 1]就是楼上的楼层数，因为鸡蛋个数k不变，也就是鸡蛋没碎，扔鸡蛋次数m减一； dp[k - 1][m - 1]就是楼下的楼层数，因为鸡蛋个数k减一，也就是鸡蛋碎了，同时扔鸡蛋次数m减一。 PS：这个m为什么要减一而不是加一？之前定义得很清楚，这个m是一个允许的次数上界，而不是扔了几次。 至此，整个思路就完成了，只要把状态转移方程填进框架即可： 12345678910111213def superEggDrop(K, N): # m 最多不会超过 N 次（线性扫描） dp = [[0]*(N + 1) for i in range(K + 1)] # base case: # dp[0][..] = 0 # dp[..][0] = 0 m = 0 while dp[K][m] &lt; N: m += 1 for k in range(1, K): dp[k][m] = dp[k][m - 1] + dp[k - 1][m - 1] + 1 return m 如果你还觉得这段代码有点难以理解，其实它就等同于这样写： 123for m in range(1, dp[K][m] &lt; N): for k in range(1, K): dp[k][m] = dp[k][m - 1] + dp[k - 1][m - 1] + 1 看到这种代码形式就熟悉多了吧，因为我们要求的不是dp数组里的值，而是某个符合条件的索引m，所以用while循环来找到这个m而已。 这个算法的时间复杂度是多少？很明显就是两个嵌套循环的复杂度 O(KN)。 另外注意到dp[m][k]转移只和左边和左上的两个状态有关，所以很容易优化成一维dp数组，这里就不写了。 三、进一步思考 再往下就要用一些数学方法了，不具体展开，就简单提一下思路吧。 在刚才的思路之上，注意函数dp(m, k)是随着m单增的，因为鸡蛋个数k不变时，允许的测试次数越多，可测试的楼层就越高。 这里又可以借助二分搜索算法快速逼近dp[K][m] == N这个终止条件，时间复杂度进一步下降为 O(KlogN)，我们可以设g(k,m)等于…… 算了算了，打住吧。我觉得我们能够写出 O(KNlogN) 的二分优化算法就行了，后面的这些解法呢，听个响鼓个掌就行了，把欲望限制在能力的范围之内才能拥有快乐！ 不过可以肯定的是，根据二分搜索代替线性扫描 m 的取值，代码的大致框架肯定是修改穷举 m 的 for 循环： 123456789101112# 把线性搜索改成二分搜索# for m in range(1, dp[K][m] &lt; N):low, high = 1, Nwhile low &lt; high: mid = (low + high) / 2; if ... &lt; N: low = ... else: high = ... for k in range(1, K): # 状态转移方程 简单总结一下吧，第一个二分优化是利用了dp函数的单调性，用二分查找技巧快速搜索答案；第二种优化是巧妙地修改了状态转移方程，简化了求解了流程，但相应的，解题逻辑比较难以想到；","categories":[{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"19-hadoop之YARN & MapReduce","slug":"19-hadoop之YARN-MapReduce","date":"2021-06-23T04:50:39.000Z","updated":"2021-06-24T16:45:55.515Z","comments":true,"path":"20210623/19-hadoop之YARN-MapReduce.html","link":"","permalink":"https://xxren8218.github.io/20210623/19-hadoop%E4%B9%8BYARN-MapReduce.html","excerpt":"","text":"YARN&amp;MapReduce掌握目标： 了解YARN概念和产生背景 了解MapReduce概念 说出YARN执行流程 说出MapReduce原理 独立完成Mrjob实现wordcount 完成提交作业到YARN上执行 1.资源调度框架 YARN1.1 什么是YARN Yet Another Resource Negotiator, 另一种资源协调者 通用资源管理系统 为上层应用提供统一的资源管理和调度，为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处 1.2 YARN产生背景 通用资源管理系统 Hadoop数据分布式存储（数据分块，冗余存储） 当多个MapReduce任务要用到相同的hdfs数据， 需要进行资源调度管理 Hadoop1.x时并没有YARN，MapReduce 既负责进行计算作业又处理服务器集群资源调度管理 服务器集群资源调度管理和MapReduce执行过程耦合在一起带来的问题 Hadoop早期, 技术只有Hadoop, 这个问题不明显 随着大数据技术的发展，Spark Storm … 计算框架都要用到服务器集群资源 如果没有通用资源管理系统，只能为多个集群分别提供数据 资源利用率低 运维成本高 Yarn (Yet Another Resource Negotiator) 另一种资源调度器 Mesos 大数据资源管理产品 不同计算框架可以共享同一个HDFS集群上的数据，享受整体的资源调度 1.3 YARN的架构和执行流程 ResourceManager: RM 资源管理器​ 整个集群同一时间提供服务的RM只有一个，负责集群资源的统一管理和调度​ 处理客户端的请求： submit, kill​ 监控我们的NM，一旦某个NM挂了，那么该NM上运行的任务需要告诉我们的AM来如何进行处理 NodeManager: NM 节点管理器​ 整个集群中有多个，负责自己本身节点资源管理和使用​ 定时向RM汇报本节点的资源使用情况​ 接收并处理来自RM的各种命令：启动Container​ 处理来自AM的命令 ApplicationMaster: AM​ 每个应用程序对应一个：MR、Spark，负责应用程序的管理​ 为应用程序向RM申请资源（core、memory），分配给内部task​ 需要与NM通信：启动/停止task，task是运行在container里面，AM也是运行在container里面 Container 容器: 封装了CPU、Memory等资源的一个容器,是一个任务运行环境的抽象 Client: 提交作业 查询作业的运行进度,杀死作业 1，Client提交作业请求 2，ResourceManager 进程和 NodeManager 进程通信，根据集群资源，为用户程序分配第一个Container(容器)，并将 ApplicationMaster 分发到这个容器上面 3，在启动的Container中创建ApplicationMaster 4，ApplicationMaster启动后向ResourceManager注册进程,申请资源 5，ApplicationMaster申请到资源后，向对应的NodeManager申请启动Container,将要执行的程序分发到NodeManager上 6，Container启动后，执行对应的任务 7，Tast执行完毕之后，向ApplicationMaster返回结果 8，ApplicationMaster向ResourceManager汇报任务结束。 请求kill 1.4 YARN环境搭建1）mapred-site.xml 1234&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt; &lt;value&gt;yarn&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 2）yarn-site.xml 1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt; &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 3) 启动YARN相关的进程sbin/start-yarn.sh 4）验证​ jps​ ResourceManager​ NodeManager​ http://192,168.19.137:8088 5）停止YARN相关的进程​ sbin/stop-yarn.sh 2.分布式处理框架 MapReduce2.1 什么是MapReduce 源于Google的MapReduce论文(2004年12月) Hadoop的MapReduce是Google论文的开源实现 MapReduce优点: 海量数据离线处理&amp;易开发 MapReduce缺点: 不能实时流式计算 2.2 MapReduce编程模型 MapReduce分而治之的思想 数钱实例：一堆钞票，各种面值分别是多少 单点策略 一个人数所有的钞票，数出各种面值有多少张 分治策略 每个人分得一堆钞票，数出各种面值有多少张 汇总，每个人负责统计一种面值 解决数据可以切割进行计算的应用 MapReduce编程分Map和Reduce阶段——还是过于简单了（相比于Spark,不能进行求平均操作，得自己写。） 将作业拆分成Map阶段和Reduce阶段 Map阶段 Map Tasks 分：把复杂的问题分解为若干”简单的任务” Reduce阶段: Reduce Tasks 合：reduce MapReduce编程执行步骤 准备MapReduce的输入数据 准备Mapper数据，进行Mapper操作 Shuffle Reduce处理 结果输出 编程模型 借鉴函数式编程方式 用户只需要实现两个函数接口： Map(in_key,in_value) —-&gt;(out_key,intermediate_value) list Reduce(out_key,intermediate_value) list —-&gt;out_value list Word Count 词频统计案例 2.3 Hadoop Streaming 实现wordcount 提供了python的API，写完以后翻译成java去执行的。——此处用了虚拟环境（source activate py365） Mapper 1234567891011import sys# 输入为标准输入stdinfor line in sys.stdin: # 删除开头和结尾的空行 line = line.strip() # 以默认空格分隔单词到words列表 words = line.split() for word in words: # 输出所有单词，格式为“单词 1”以便作为Reduce的输入 print(&quot;%s %s&quot;%(word,1)) Reducer 123456789101112131415161718192021222324252627282930313233343536import syscurrent_word = Nonecurrent_count = 0word = None# 获取标准输入，即mapper.py的标准输出for line in sys.stdin: # 删除开头和结尾的空行 line = line.strip() # 解析mapper.py输出作为程序的输入，以tab作为分隔符 word, count = line.split() # 转换count从字符型到整型 try: count = int(count) except ValueError: # count非数字时，忽略此行 continue # 要求mapper.py的输出做排序（sort）操作，以便对连续的word做判断 if current_word == word: current_count += count else : # 出现了一个新词 # 输出当前word统计结果到标准输出 if current_word : print(&#x27;%s\\t%s&#x27; % (current_word, current_count)) # 开始对新词的统计 current_count = count current_word = word# 输出最后一个word统计if current_word == word: print(&quot;%s\\t%s&quot;% (current_word, current_count)) 本地实现 ​ cat xxx.txt|python3 map.py|sort|python3 red.py 得到最终的输出 注：hadoop-streaming会主动将map的输出数据进行字典排序 通过Hadoop Streaming 提交作业到Hadoop集群 12345678910111213STREAM_JAR_PATH=&quot;/root/bigdata/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.9.1.jar&quot; # hadoop streaming jar包所在位置INPUT_FILE_PATH_1=&quot;/The_Man_of_Property.txt&quot; #要进行词频统计的文档在hdfs中的路径OUTPUT_PATH=&quot;/output&quot; #MR作业后结果的存放路径hadoop fs -rm -r -skipTrash $OUTPUT_PATH #输出路径如果之前存在 先删掉否则会报错hadoop jar $STREAM_JAR_PATH \\ -input $INPUT_FILE_PATH_1 \\ # 指定输入文件位置 -output $OUTPUT_PATH \\ #指定输出结果位置 -mapper &quot;python map.py&quot; \\ #指定mapper执行的程序 -reducer &quot;python red.py&quot; \\ #指定reduce阶段执行的程序 -file ./map.py \\ #通过-file 把python源文件分发到集群的每一台机器上 -file ./red.py 到Hadoop集群查看运行结果 文档说明 对于java而言，.java编译-&gt;.class文件（多个打包）-&gt;.jar-&gt; 在JVM虚拟机上运行。对于JVM而言，.jar是其可执行文件。相当于windows的.exe。 通过hadoop-streaming-2.9.1.ja可执行文件将python的可执行文件翻译成java。 结果如图 注意！得开启YARN才可以 也可以去YARN去看：端口号8088","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"04-动态规划之高楼扔鸡蛋","slug":"04-动态规划之高楼扔鸡蛋","date":"2021-06-22T10:24:57.000Z","updated":"2021-06-22T11:26:11.752Z","comments":true,"path":"20210622/04-动态规划之高楼扔鸡蛋.html","link":"","permalink":"https://xxren8218.github.io/20210622/04-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B9%8B%E9%AB%98%E6%A5%BC%E6%89%94%E9%B8%A1%E8%9B%8B.html","excerpt":"","text":"动态规划之高楼扔鸡蛋今天要聊一个很经典的算法问题，若干层楼，若干个鸡蛋，让你算出最少的尝试次数，找到鸡蛋恰好摔不碎的那层楼。国内大厂以及谷歌脸书面试都经常考察这道题，只不过他们觉得扔鸡蛋太浪费，改成扔杯子，扔破碗什么的。 具体的问题等会再说，但是这道题的解法技巧很多，光动态规划就好几种效率不同的思路，最后还有一种极其高效数学解法。秉承我们一贯的作风，拒绝奇技淫巧，拒绝过于诡异的技巧，因为这些技巧无法举一反三，学了不太划算。 下面就来用我们一直强调的动态规划通用思路来研究一下这道题。 一、解析题目 题目是这样：你面前有一栋从 1 到N共N层的楼，然后给你K个鸡蛋（K至少为 1）。现在确定这栋楼存在楼层0 &lt;= F &lt;= N，在这层楼将鸡蛋扔下去，鸡蛋恰好没摔碎（高于F的楼层都会碎，低于F的楼层都不会碎）。现在问你，最坏情况下，你至少要扔几次鸡蛋，才能确定这个楼层F呢？ PS：F 可以为 0，比如说鸡蛋在 1 层都能摔碎，那么 F = 0。 也就是让你找摔不碎鸡蛋的最高楼层F，但什么叫「最坏情况」下「至少」要扔几次呢？我们分别举个例子就明白了。 比方说现在先不管鸡蛋个数的限制，有 7 层楼，你怎么去找鸡蛋恰好摔碎的那层楼？ 最原始的方式就是线性扫描：我先在 1 楼扔一下，没碎，我再去 2 楼扔一下，没碎，我再去 3 楼…… 以这种策略，最坏情况应该就是我试到第 7 层鸡蛋也没碎（F = 7），也就是我扔了 7 次鸡蛋。 现在你应该理解什么叫做「最坏情况」下了，鸡蛋破碎一定发生在搜索区间穷尽时，不会说你在第 1 层摔一下鸡蛋就碎了，这是你运气好，不是最坏情况。 现在再来理解一下什么叫「至少」要扔几次。依然不考虑鸡蛋个数限制，同样是 7 层楼，我们可以优化策略。 最好的策略是使用二分查找思路，我先去第(1 + 7) / 2 = 4层扔一下： 如果碎了说明F小于 4，我就去第(1 + 3) / 2 = 2层试…… 如果没碎说明F大于等于 4，我就去第(5 + 7) / 2 = 6层试…… 以这种策略，最坏情况应该是试到第 7 层鸡蛋还没碎（F = 7），或者鸡蛋一直碎到第 1 层（F = 0）。然而无论那种最坏情况，只需要试log7向上取整等于 3 次，比刚才的 7 次要少，这就是所谓的至少要扔几次。 实际上，如果不限制鸡蛋个数的话，二分思路显然可以得到最少尝试的次数，但问题是，现在给你了鸡蛋个数的限制K，直接使用二分思路就不行了。 比如说只给你 1 个鸡蛋，7 层楼，你敢用二分吗？你直接去第 4 层扔一下，如果鸡蛋没碎还好，但如果碎了你就没有鸡蛋继续测试了，无法确定鸡蛋恰好摔不碎的楼层F了。这种情况下只能用线性扫描的方法，算法返回结果应该是 7。 有的读者也许会有这种想法：二分查找排除楼层的速度无疑是最快的，那干脆先用二分查找，等到只剩 1 个鸡蛋的时候再执行线性扫描，这样得到的结果是不是就是最少的扔鸡蛋次数呢？ 很遗憾，并不是，比如说把楼层变高一些，100 层，给你 2 个鸡蛋，你在 50 层扔一下，碎了，那就只能线性扫描 1～49 层了，最坏情况下要扔 50 次。 如果不要「二分」，变成「五分」「十分」都会大幅减少最坏情况下的尝试次数。比方说第一个鸡蛋每隔十层楼扔，在哪里碎了第二个鸡蛋一个个线性扫描，总共不会超过 20 次。 最优解其实是 14 次。最优策略非常多，而且并没有什么规律可言。 说了这么多废话，就是确保大家理解了题目的意思，而且认识到这个题目确实复杂，就连我们手算都不容易，如何用算法解决呢？ 二、思路分析 对动态规划问题，直接套我们以前多次强调的框架即可：这个问题有什么「状态」，有什么「选择」，然后穷举。 「状态」很明显，就是当前拥有的鸡蛋数K和需要测试的楼层数N。随着测试的进行，鸡蛋个数可能减少，楼层的搜索范围会减小，这就是状态的变化。 「选择」其实就是去选择哪层楼扔鸡蛋。回顾刚才的线性扫描和二分思路，二分查找每次选择到楼层区间的中间去扔鸡蛋，而线性扫描选择一层层向上测试。不同的选择会造成状态的转移。 现在明确了「状态」和「选择」，动态规划的基本思路就形成了：肯定是个二维的dp数组或者带有两个状态参数的dp函数来表示状态转移；外加一个 for 循环来遍历所有选择，择最优的选择更新结果 ： dp（） 函数的含义：鸡蛋数目为k，可选楼层数为N时的最小扔鸡蛋次数。 12345678# 伪代码# 当前状态为 (K 个鸡蛋，N 层楼)# 返回这个状态下的最优结果def dp(K, N): res = float(&#x27;INF&#x27;) for 1 &lt;= i &lt;= N: res = min(res, 这次在第 i 层楼扔鸡蛋) return res 这段伪码还没有展示递归和状态转移，不过大致的算法框架已经完成了。 我们在第i层楼扔了鸡蛋之后，可能出现两种情况：鸡蛋碎了，鸡蛋没碎。注意，这时候状态转移就来了： 如果鸡蛋碎了，那么鸡蛋的个数K应该减一，搜索的楼层区间应该从[1..N]变为[1..i-1]共i-1层楼； 如果鸡蛋没碎，那么鸡蛋的个数K不变，搜索的楼层区间应该从 [1..N]变为[i+1..N]共N-i层楼。 PS：细心的读者可能会问，在第i层楼扔鸡蛋如果没碎，楼层的搜索区间缩小至上面的楼层，是不是应该包含第i层楼呀？不必，因为已经包含了。开头说了 F 是可以等于 0 的，向上递归后，第i层楼其实就相当于第 0 层，可以被取到，所以说并没有错误。 因为我们要求的是最坏情况下扔鸡蛋的次数，所以鸡蛋在第i层楼碎没碎，取决于那种情况的结果更大： 12345678910def dp(K, N): for 1 &lt;= i &lt;= N: # 最坏情况下的最少扔鸡蛋次数 res = min(res, max( dp(K - 1, i - 1), # 碎 dp(K, N - i) # 没碎 ) + 1 # 在第 i 楼扔了一次 ) return res 递归的 base case 很容易理解：当楼层数N等于 0 时，显然不需要扔鸡蛋；当鸡蛋数K为 1 时，显然只能线性扫描所有楼层： 1234def dp(K, N): if K == 1: return N if N == 0: return 0 ... 至此，其实这道题就解决了！只要添加一个备忘录消除重叠子问题即可： 12345678910111213141516171819202122232425def superEggDrop(K: int, N: int): memo = dict() def dp(K, N) -&gt; int: # base case if K == 1: return N if N == 0: return 0 # 避免重复计算 if (K, N) in memo: return memo[(K, N)] res = float(&#x27;INF&#x27;) # 穷举所有可能的选择 for i in range(1, N + 1): res = min(res, max( dp(K, N - i), dp(K - 1, i - 1) ) + 1 ) # 记入备忘录 memo[(K, N)] = res return res return dp(K, N) 这个算法的时间复杂度是多少呢？动态规划算法的时间复杂度就是子问题个数 × 函数本身的复杂度。 函数本身的复杂度就是忽略递归部分的复杂度，这里dp函数中有一个 for 循环，所以函数本身的复杂度是 O(N)。 子问题个数也就是不同状态组合的总数，显然是两个状态的乘积，也就是 O(KN)。 所以算法的总时间复杂度是 O(K*N^2), 空间复杂度为子问题个数，即 O(KN)。 三、疑难解答 这个问题很复杂，但是算法代码却十分简洁，这就是动态规划的特性，穷举加备忘录/DP table 优化，真的没啥新意。 首先，有读者可能不理解代码中为什么用一个 for 循环遍历楼层[1..N]，也许会把这个逻辑和之前探讨的线性扫描混为一谈。其实不是的，这只是在做一次「选择」。 比方说你有 2 个鸡蛋，面对 10 层楼，你得拿一个鸡蛋去某一层楼扔对吧？那选择去哪一层楼扔呢？不知道，那就把这 10 层楼全试一遍。至于鸡蛋碎没碎，下次怎么选择不用你操心，有正确的状态转移，递归会算出每个选择的代价，我们取最优的那个就是最优解。 其实，这个问题还有更好的解法，比如修改代码中的 for 循环为二分搜索，可以将时间复杂度降为 O(KNlogN)；再改进动态规划解法可以进一步降为 O(KN)；使用数学方法解决，时间复杂度达到最优 O(K*logN)，空间复杂度达到 O(1)。 二分的解法也有点误导性，你很可能以为它跟我们之前讨论的二分思路扔鸡蛋有关系，实际上没有半毛钱关系。能用二分搜索是因为状态转移方程的函数图像具有单调性，可以快速找到最小值。 我觉得吧，我们这种解法就够了：找状态，做选择，足够清晰易懂，可流程化，可举一反三。掌握这套框架学有余力的话，二分查找的优化应该可以看懂，之后的优化也就随缘吧。","categories":[{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"18-hadoop之分布式文件系统 HDFS","slug":"18-分布式文件系统-HDFS","date":"2021-06-21T16:38:13.000Z","updated":"2021-06-24T16:47:54.127Z","comments":true,"path":"20210622/18-分布式文件系统-HDFS.html","link":"","permalink":"https://xxren8218.github.io/20210622/18-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-HDFS.html","excerpt":"","text":"分布式文件系统 HDFS掌握目标： 知道什么是hdfs 说出hdfs的架构 能够掌握hdfs的环境搭建 能够掌握hdfs shell的基本使用 知道hdfs shell的优缺点 1 HDFS的使用 启动HDFS 来到$HADOOP_HOME/sbin目录下 执行start-dfs.sh 1[hadoop@hadoop00 sbin]$ ./start-dfs.sh 可以看到 namenode和 datanode启动的日志信息 12345Starting namenodes on [hadoop00]hadoop00: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop00.outlocalhost: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop00.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop00.out 通过jps命令查看当前运行的进程 12345[hadoop@hadoop00 sbin]$ jps4416 DataNode4770 Jps4631 SecondaryNameNode4251 NameNode 可以看到 NameNode DataNode 以及 SecondaryNameNode 说明启动成功 通过可视化界面查看HDFS的运行情况 通过浏览器查看 主机ip:50070端口 Overview界面查看整体情况 Datanodes界面查看datanode的情况 2 HDFS shell操作 调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式 ls使用方法：hadoop fs -ls 如果是文件，则按照如下格式返回文件信息：文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：目录名 修改日期 修改时间 权限 用户ID 组ID示例：hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile返回值：成功返回0，失败返回-1。 text使用方法：hadoop fs -text 将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。 mv使用方法：hadoop fs -mv URI [URI …] 将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。示例： hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1 返回值： 成功返回0，失败返回-1。 put使用方法：hadoop fs -put … 从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。 hadoop fs -put localfile /user/hadoop/hadoopfile hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile hadoop fs -put - hdfs://host:port/hadoop/hadoopfile从标准输入中读取输入。 返回值： 成功返回0，失败返回-1。 rm使用方法：hadoop fs -rm URI [URI …] 删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。示例： hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir 返回值： 成功返回0，失败返回-1。 http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html 3 HDFS shell操作练习 在centos 中创建 test.txt 1touch test.txt 在centos中为test.txt 添加文本内容 1vi test.txt 在HDFS中创建 hadoop001/test 文件夹 1hadoop fs -mkdir -p /hadoop001/test 把text.txt文件上传到HDFS中 1hadoop fs -put test.txt /hadoop001/test/ 查看hdfs中 hadoop001/test/test.txt 文件内容 1hadoop fs -cat /hadoop001/test/test.txt 将hdfs中 hadoop001/test/test.txt文件下载到centos 1hadoop fs -get /hadoop001/test/test.txt test.txt 删除HDFS中 hadoop001/test/ hadoop fs -rm -r /hadoop001 4 HDFS设计思路 分布式文件系统的设计思路： HDFS的设计目标 适合运行在通用硬件(commodity hardware)上的分布式文件系统 高度容错性的系统，适合部署在廉价的机器上 HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用 容易扩展，为用户提供性能不错的文件存储服务 5 HDFS架构 1个NameNode/NN(Master) 带 DataNode/DN(Slaves) (Master-Slave结构) 1个文件会被拆分成多个Block NameNode(NN) 负责客户端请求的响应 负责元数据（文件的名称、副本系数、Block存放的DN）的管理 元数据 MetaData 描述数据的数据 监控DataNode健康状况 10分钟 心跳 没有收到DataNode报告认为Datanode死掉了。将数据再存储一份。 DataNode(DN) 存储用户的文件对应的数据块(Block) 要定期向NN发送心跳信息，汇报本身及其所有的block信息，健康状况 分布式集群NameNode和DataNode部署在不同机器上 HDFS优缺点 优点 数据冗余 硬件容错 适合存储大文件 处理流式数据 可构建在廉价机器上 缺点 高延迟的数据访问。——在各个机器之间通讯，延迟高。 小文件存储。即使文件大小小于128M，它也会占128M的空间。 6 HDFS环境搭建 下载jdk（java development kit,） 和 hadoop 放到 ~/software目录下 然后解压到 ~/app目录下 因为大数据Hadoop等是用 java 开发的，java 需要在JVM上运行，而JDK就包含了JVM（JVM：java的虚拟机） 1tar -zxvf 压缩包名字 -C ~/app/ 配置环境变量 12345678vi ~/.bash_profileexport JAVA_HOME=/home/hadoop/app/jdk1.8.0_91export PATH=$JAVA_HOME/bin:$PATHexport HADOOP_HOME=/home/hadoop/app/hadoop......export PATH=$HADOOP_HOME/bin:$PATH#保存退出后source ~/.bash_profile 进入到解压后的hadoop目录 修改配置文件 配置文件作用 core-site.xml 指定hdfs的访问方式 hdfs-site.xml 指定namenode 和 datanode 的数据存储位置 mapred-site.xml 配置mapreduce yarn-site.xml 配置yarn 修改hadoop-env.sh 1234cd etc/hadoopvi hadoop-env.sh#找到下面内容添加java homeexport_JAVA_HOME=/home/hadoop/app/jdk1.8.0_91 修改 core-site.xml 在 节点中添加 1234&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://hadoop000:8020&lt;/value&gt;&lt;/property&gt; 修改hdfs-site.xml 在 configuration节点中添加 123456789101112&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/tmp/dfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/tmp/dfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 修改 mapred-site.xml 默认没有这个 从模板文件复制 1cp mapred-site.xml.template mapred-site.xml ​ 在mapred-site.xml 的configuration 节点中添加 1234&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 修改yarn-site.xml configuration 节点中添加 1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; 来到hadoop的bin目录——格式化 1./hadoop namenode -format (这个命令只运行一次) 启动hdfs 进入到 sbin 1./start-dfs.sh 启动启动yarn 在sbin中","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"17-Hadoop概述","slug":"17-Hadoop概述","date":"2021-06-21T10:21:17.000Z","updated":"2021-06-24T16:48:20.361Z","comments":true,"path":"20210621/17-Hadoop概述.html","link":"","permalink":"https://xxren8218.github.io/20210621/17-Hadoop%E6%A6%82%E8%BF%B0.html","excerpt":"","text":"Hadoop概述掌握目标： 知道Hadoop的概念及发展历史 说出hadoop的核心组件 知道hadoop的优势 1 什么是Hadoop Hadoop名字的由来 作者：Doug cutting Hadoop项目作者的孩子给一个棕黄色的大象样子的填充玩具的命名 Hadoop的概念: Apache™ Hadoop® 是一个开源的, 可靠的(reliable), 可扩展的(scalable)分布式计算框架 允许使用简单的编程模型跨计算机集群分布式处理大型数据集 可扩展: 从单个服务器扩展到数千台计算机，每台计算机都提供本地计算和存储 可靠的: 不依靠硬件来提供高可用性(high-availability)，而是在应用层检测和处理故障，从而在计算机集群之上提供高可用服务 Hadoop能做什么? 搭建大型数据仓库 PB级数据的存储 处理 分析 统计等业务 搜索引擎 日志分析 数据挖掘 商业智能(Business Intelligence，简称：BI) 商业智能通常被理解为将企业中现有的数据(订单、库存、交易账目、客户和供应商等数据)转化为知识，帮助企业做出明智的业务经营决策的工具。从技术层面上讲，是数据仓库、数据挖掘等技术的综合运用。 Hadoop发展史 2003-2004年 Google发表了三篇论文 GFS：Google的分布式文件系统Google File System MapReduce: Simplified Data Processing on Large Clusters BigTable：一个大型的分布式数据库 2006年2月Hadoop成为Apache的独立开源项目( Doug Cutting等人实现了DFS和MapReduce机制)。 2006年4月— 标准排序(10 GB每个节点)在188个节点上运行47.9个小时。 2008年4月— 赢得世界最快1TB数据排序在900个节点上用时209秒。 2008年— 淘宝开始投入研究基于Hadoop的系统–云梯。云梯总容量约9.3PB，共有1100台机器，每天处理18000道作业，扫描500TB数据。 2009年3月— Cloudera推出CDH（Cloudera’s Dsitribution Including Apache Hadoop） 2009年5月— Yahoo的团队使用Hadoop对1 TB的数据进行排序只花了62秒时间。 2009年7月— Hadoop Core项目更名为Hadoop Common; 2009年7月— MapReduce和Hadoop Distributed File System (HDFS)成为Hadoop项目的独立子项目。 2012年11月— Apache Hadoop 1.0 Available 2018年4月— Apache Hadoop 3.1 Available 搜索引擎时代 有保存大量网页的需求(单机 集群) 词频统计 【word count】 【PageRank】 数据仓库时代 FaceBook推出Hive（Hive是基于Hadoop的一个数据仓库工具） 曾经进行数分析与统计时, 仅限于数据库,受数据量和计算能力的限制, 我们只能对最重要的数据进行统计和分析(决策数据,财务相关) Hive可以在Hadoop上运行SQL操作, 可以把运行日志, 应用采集数据,数据库数据放到一起分析 数据挖掘时代 啤酒尿不湿 关联分析 用户画像/物品画像 机器学习时代 广义大数据 大数据提高数据存储能力, 为机器学习提供燃料 alpha go siri 小爱 天猫精灵 2 Hadoop核心组件 Hadoop是所有搜索引擎的共性问题的廉价解决方案 如何存储持续增长的海量网页: 单节点 V.S. 分布式存储 如何对持续增长的海量网页进行排序: 超算 V.S. 分布式计算 HDFS 解决分布式存储问题 MapReduce 解决分布式计算问题 Hadoop Common: The common utilities that support the other Hadoop modules.(hadoop的公共组件)—（如将HDFS和MapReduce串起来） Hadoop Distributed File System (HDFS™): A distributed file system that provides high-throughput access to application data.(分布式文件系统) 源自于Google的GFS论文, 论文发表于2003年10月 HDFS是GFS的开源实现 HDFS的特点:扩展性&amp;容错性&amp;海量数量存储 将文件切分成指定大小的数据块, 并在多台机器上保存多个副本（冗余、切割） 数据切分、多副本、容错等操作对用户是透明的——系统自动给用户拆分 下面这张图是数据块多份复制存储的示意 图中对于文件 /users/sameerp/data/part-0，其复制备份数设置为2, 存储的BlockID分别为1、3。 Block1的两个备份存储在DataNode0和DataNode2两个服务器上 Block3的两个备份存储在DataNode4和DataNode6两个服务器上 Hadoop MapReduce: A YARN-based system for parallel processing of large data sets. 分布式计算框架 源于Google的MapReduce论文，论文发表于2004年12月 MapReduce是GoogleMapReduce的开源实现 MapReduce特点:扩展性&amp;容错性&amp;海量数据离线处理（得等） Hadoop YARN: A framework for job scheduling and cluster resource management.(资源调度系统) YARN: Yet Another Resource Negotiator 负责整个集群资源的管理和调度 YARN特点:扩展性&amp;容错性&amp;多框架资源统一调度 3 Hadoop优势 高可靠 数据存储: 数据块多副本 数据计算: 某个节点崩溃, 会自动重新调度作业计算 高扩展性 存储/计算资源不够时，可以横向的线性扩展机器 一个集群中可以包含数以千计的节点 集群可以使用廉价机器，成本低 Hadoop生态系统成熟","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"03-动态规划之 KMP 算法详解","slug":"03-动态规划之-KMP-算法详解","date":"2021-06-21T10:05:59.000Z","updated":"2021-06-21T10:14:07.647Z","comments":true,"path":"20210621/03-动态规划之-KMP-算法详解.html","link":"","permalink":"https://xxren8218.github.io/20210621/03-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B9%8B-KMP-%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3.html","excerpt":"","text":"动态规划之 KMP 算法详解KMP 算法（Knuth-Morris-Pratt 算法）是一个著名的字符串匹配算法，效率很高，但是确实有点复杂。 KMP 算法比较难理解。有一些优秀的同学通过手推 KMP 算法的过程来辅助理解该算法，这是一种办法，不过本文要从逻辑层面帮助读者理解算法的原理。十行代码之间，KMP 灰飞烟灭。 先在开头约定，本文用pat表示模式串，长度为M，txt表示文本串，长度为N。KMP 算法是在txt中查找子串pat，如果存在，返回这个子串的起始索引，否则返回 -1。 为什么我认为 KMP 算法就是个动态规划问题呢，等会有解释。对于动态规划，之前多次强调了要明确dp数组的含义，而且同一个问题可能有不止一种定义dp数组含义的方法，不同的定义会有不同的解法。 常见的 KMP 算法应该是，一波诡异的操作处理pat后形成一个一维的数组next，然后根据这个数组经过又一波复杂操作去匹配txt。时间复杂度 O(N)，空间复杂度 O(M)。其实它这个next数组就相当于dp数组，其中元素的含义跟pat的前缀和后缀有关，判定规则比较复杂，不太好理解。 本文则用一个二维的dp数组（但空间复杂度还是 O(M)），重新定义其中元素的含义，使得代码长度大大减少，可解释性大大提高。 PS：原代码使用的数组名称是dfa（确定有限状态机），本文还是沿用dp数组的名称。 一、KMP 算法概述 首先还是简单介绍一下 KMP 算法和暴力匹配算法的不同在哪里，难点在哪里，和动态规划有啥关系。 暴力的字符串匹配算法很容易写，看一下它的运行逻辑： 1234567891011121314151617# 暴力匹配def search(pat, txt):(pat-&gt;string, txt-&gt;string) M = len(pat) N = len(txt) for i in range(N - M + 1): # 记录每次匹配成功的字串的元素个数 count = 0 for j in range(M): if pat[j] != txt[i + j]: break count += 1 # pat 全部匹配了 if (count == M): return i # txt 中不存在 pat 子串 return -1 对于暴力算法，如果出现不匹配字符，同时回退txt和pat的指针，嵌套 for 循环，时间复杂度 O(MN)，空间复杂度O(1)。最主要的问题是，如果字符串中重复的字符比较多，该算法就显得很蠢。 比如 txt = “aaacaaab” pat = “aaab”： 暴力算法 很明显，pat中根本没有字符 c，根本没必要回退指针i，暴力解法明显多做了很多不必要的操作。 KMP 算法的不同之处在于，它会花费空间来记录一些信息，在上述情况中就会显得很聪明： KMP算法 再比如类似的 txt = “aaaaaaab” pat = “aaab”，暴力解法还会和上面那个例子一样蠢蠢地回退指针i，而 KMP 算法又会耍聪明： KMP算法 因为 KMP 算法知道字符 b 之前的字符 a 都是匹配的，所以每次只需要比较字符 b 是否被匹配就行了。 KMP 算法永不回退txt的指针i，不走回头路（不会重复扫描txt），而是借助dp数组中储存的信息把pat移到正确的位置继续匹配，时间复杂度只需 O(N)，用空间换时间，所以我认为它是一种动态规划算法。 KMP 算法的难点在于，如何计算dp数组中的信息？如何根据这些信息正确地移动pat的指针？这个就需要确定有限状态自动机来辅助了，别怕这种高大上的文学词汇，其实和动态规划的dp数组如出一辙，等你学会了也可以拿这个词去吓唬别人。 还有一点需要明确的是：计算这个dp数组，只和pat串有关。意思是说，只要给我个pat，我就能通过这个模式串计算出dp数组，然后你可以给我不同的txt，我都不怕，利用这个dp数组我都能在 O(N) 时间完成字符串匹配。 具体来说，比如上文举的两个例子： 1234txt1 = &quot;aaacaaab&quot; pat = &quot;aaab&quot;txt2 = &quot;aaaaaaab&quot; pat = &quot;aaab&quot; 我们的txt不同，但是pat是一样的，所以 KMP 算法使用的dp数组是同一个。只不过对于txt1的下面这个即将出现的未匹配情况： dp数组指示pat这样移动： PS：这个j不要理解为索引，它的含义更准确地说应该是状态（state），所以它会出现这个奇怪的位置，后文会详述。 而对于txt2的下面这个即将出现的未匹配情况： dp数组指示pat这样移动： 明白了dp数组只和pat有关，那么我们这样设计 KMP 算法就会比较漂亮： 123456789class KMP(object): def __init__(self, pat): self.pat = pat # 通过 pat 构建 dp 数组 # 需要 O(N) 时间 def search(self, txt): # 借助 dp 数组去匹配 txt # 需要 O(N) 时间 这样，当我们需要用同一pat去匹配不同txt时，就不需要浪费时间构造dp数组了： 123kmp = KMP(&quot;aaab&quot;)pos1 = kmp.search(&quot;aaacaaab&quot;) # 4pos2 = kmp.search(&quot;aaaaaaab&quot;) # 4 二、状态机概述 为什么说 KMP 算法和状态机有关呢？是这样的，我们可以认为pat的匹配就是状态的转移。比如当 pat = “ABABC”： 如上图，圆圈内的数字就是状态，状态 0 是起始状态，状态 5（pat.length）是终止状态。开始匹配时pat处于起始状态，一旦转移到终止状态，就说明在txt中找到了pat。 比如说如果当前处于状态 2，就说明字符 “AB” 被匹配： 另外，处于某个状态时，遇到不同的字符，pat状态转移的行为也不同。比如说假设现在匹配到了状态 4，如果遇到字符 A 就应该转移到状态 3，遇到字符 C 就应该转移到状态 5，如果遇到字符 B 就应该转移到状态 0： 具体什么意思呢，举例解释一下。用变量j表示指向当前状态的指针，当前pat匹配到了状态 4 如果遇到了字符 “A”，根据箭头指示，转移到状态 3 是最聪明的： 如果遇到了字符 “B”，根据箭头指示，只能转移到状态 0（一夜回到解放前）： 如果遇到了字符 “C”，根据箭头指示，应该转移到终止状态 5，这也就意味着匹配完成： 当然了，还可能遇到其他字符，比如 Z，但是显然应该转移到起始状态 0，因为pat中根本都没有字符 Z： 这里为了清晰起见，我们画状态图时就把其他字符转移到状态 0 的箭头省略，只画pat中出现的字符的状态转移： KMP 算法最关键的步骤就是构造这个状态转移图。要确定状态转移的行为，得明确两个变量，一个是当前的匹配状态，另一个是遇到的字符；确定了这两个变量后，就可以知道这个情况下应该转移到哪个状态。 下面看一下 KMP 算法根据这幅状态转移图匹配字符串txt的过程： 请记住这个 GIF 的匹配过程，这就是 KMP 算法的核心逻辑！ 为了描述状态转移图，我们定义一个二维 dp 数组，它的含义如下 123456789101112dp[j][c] = next0 &lt;= j &lt; M，代表当前的状态0 &lt;= c &lt; 256，代表遇到的字符（ASCII 码）0 &lt;= next &lt;= M，代表下一个状态dp[4][&#x27;A&#x27;] = 3 表示：当前是状态 4，如果遇到字符 A，pat 应该转移到状态 3dp[1][&#x27;B&#x27;] = 2 表示：当前是状态 1，如果遇到字符 B，pat 应该转移到状态 2 根据我们这个 dp 数组的定义和刚才状态转移的过程，我们可以先写出 KMP 算法的 search 函数代码： 1234567891011121314def search(String txt): M = len(pat) N = len(txt) # pat 的初始态为 0 j = 0 for i in range(N): # 当前是状态 j，遇到字符 txt[i]， # pat 应该转移到哪个状态？ j = dp[j][ord(txt[i])] # 如果达到终止态，返回匹配开头的索引 if j == M: return i - M + 1 # 没到达终止态，匹配失败 return -1 注意：python的内置函数ord(“a”) 输出字符串的ASCII编码 python的内置函数chr(67) 输出ASCII编码对应的字符串 到这里，应该还是很好理解的吧，dp数组就是我们刚才画的那幅状态转移图，如果不清楚的话回去看下 GIF 的算法演进过程。 下面讲解：如何通过pat构建这个dp数组？ 三、构建状态转移图 回想刚才说的：要确定状态转移的行为，必须明确两个变量，一个是当前的匹配状态，另一个是遇到的字符，而且我们已经根据这个逻辑确定了dp数组的含义，那么构造dp数组的框架就是这样： 123for 0 &lt;= j &lt; M: # 状态 for 0 &lt;= c &lt; 256: # 字符 dp[j][c] = next 这个 next 状态应该怎么求呢？显然，如果遇到的字符c和pat[j]匹配的话，状态就应该向前推进一个，也就是说next = j + 1，我们不妨称这种情况为状态推进： 如果遇到的字符c和pat[j]不匹配的话，状态就要回退（或者原地不动），我们不妨称这种情况为状态重启 那么，如何得知在哪个状态重启呢？解答这个问题之前，我们再定义一个名字：影子状态（我编的名字），用变量X表示。所谓影子状态，就是和当前状态具有相同的前缀。比如下面这种情况： 当前状态j = 4，其影子状态为X = 2，它们都有相同的前缀 “AB”。因为状态X和状态j存在相同的前缀，所以当状态j准备进行状态重启的时候（遇到的字符c和pat[j]不匹配），可以通过X的状态转移图来获得最近的重启位置。 比如说刚才的情况，如果状态j遇到一个字符 “A”，应该转移到哪里呢？首先状态 4 只有遇到 “C” 才能推进状态，遇到 “A” 显然只能进行状态重启。状态j会把这个字符委托给状态X处理，也就是dp[j][&#39;A&#39;] = dp[X][&#39;A&#39;]： 为什么这样可以呢？因为：既然j这边已经确定字符 “A” 无法推进状态，只能回退，而且 KMP 算法就是要尽可能少的回退，以免多余的计算。那么j就可以去问问和自己具有相同前缀的X，如果X遇见 “A” 可以进行「状态推进」，那就转移过去，因为这样回退最少： 当然，如果遇到的字符是 “B”，状态X也不能进行「状态推进」，只能回退，j只要跟着X指引的方向回退就行了： 你也许会问，这个X怎么知道遇到字符 “B” 要回退到状态 0 呢？因为X永远跟在j的身后，状态X如何转移，在之前就已经算出来了。动态规划算法不就是利用过去的结果解决现在的问题吗？ 这样，我们就可以细化一下刚才的框架代码： 123456789for j in range(M): for c in range(256): if c == ord(pat[j]): # 状态推进 dp[j][c] = j + 1 else: # 状态重启 # 委托 X 计算重启位置 dp[j][c] = dp[X][c] 四、代码实现 注意python如何生成二维矩阵 如果之前的内容你都能理解，恭喜你，现在就剩下一个问题：影子状态X是如何得到的呢？下面先直接看完整代码吧。 123456789101112131415161718192021222324252627class KMP: def __init__(self, pat): self.pat = pat M = len(pat) # dp[状态][字符] = 下个状态 self.dp = [[0] * 256 for i in range(M)] ###################### # 二维矩阵的生成 # # M 行 256 列 # ###################### # base case self.dp[0][ord(pat[0])] = 1 # 影子状态 X 初始为 0 X = 0 # 当前状态 j 从 1 开始 for j in range(1, M): for c in range(256): if ord(pat[j]) == c: self.dp[j][c] = j + 1 else: self.dp[j][c] = self.dp[X][c] # 更新影子状态 X = self.dp[X][ord(pat[j])] def search(self): ... 先解释一下这一行代码： 12# base casedp[0][ord(pat[0])] = 1 这行代码是 base case，只有遇到 pat[0] 这个字符才能使状态从 0 转移到 1，遇到其它字符的话还是停留在状态 0。 影子状态X是先初始化为 0，然后随着j的前进而不断更新的。下面看看到底应该如何更新影子状态X： 1234567X = 0;for j in range(1, M) ... # 更新影子状态 # 当前是状态 X，遇到字符 pat[j], # pat 应该转移到哪个状态？ X = dp[X][ord(pat[J])] 更新X其实和search函数中更新状态j的过程是非常相似的： 123456j = 0for i in range(N): # 当前是状态 j，遇到字符 txt[i]， # pat 应该转移到哪个状态？ j = dp[j][ord(txt[i])] ... 其中的原理非常微妙，注意代码中 for 循环的变量初始值，可以这样理解：后者是在txt中匹配pat，前者是在pat中匹配pat[1:]，状态X总是落后状态j一个状态，与j具有最长的相同前缀。所以我把X比喻为影子状态，似乎也有一点贴切。 另外，构建 dp 数组是根据 base casedp[0][..]向后推演。这就是我认为 KMP 算法就是一种动态规划算法的原因。 下面来看一下状态转移图的完整构造过程，你就能理解状态X作用之精妙了： 至此，KMP 算法就已经再无奥妙可言了！看下 KMP 算法的完整代码吧： 123456789101112131415161718192021222324252627282930313233343536373839class KMP: def __init__(self, pat): self.pat = pat M = len(pat) # dp[状态][字符] = 下个状态 self.dp = [[0] * 256 for i in range(M)] ###################### # 二维矩阵的生成 # # M 行 256 列 # ###################### # base case self.dp[0][ord(pat[0])] = 1 # 影子状态 X 初始为 0 X = 0 # 当前状态 j 从 1 开始 for j in range(1, M): for c in range(256): if ord(pat[j]) == c: self.dp[j][c] = j + 1 else: self.dp[j][c] = self.dp[X][c] # 更新影子状态 X = self.dp[X][ord(pat[j])] def search(self, txt): M = len(self.pat) N = len(txt) # pat 的初始态为 0 j = 0 for i in range(N): # 当前是状态 j，遇到字符 txt[i]， # pat 应该转移到哪个状态？ j = self.dp[j][ord(txt[i])] # 如果达到终止态，返回匹配开头的索引 if j == M: return i - M + 1 # 没到达终止态，匹配失败 return -1 经过之前的详细举例讲解，你应该可以理解这段代码的含义了，当然你也可以把 KMP 算法写成一个函数。核心代码也就是两个函数中 for 循环的部分，数一下有超过十行吗？ 五、最后总结 传统的 KMP 算法是使用一个一维数组next记录前缀信息，而本文是使用一个二维数组dp以状态转移的角度解决字符匹配问题，但是空间复杂度仍然是 O(256M) = O(M)。 在pat匹配txt的过程中，只要明确了「当前处在哪个状态」和「遇到的字符是什么」这两个问题，就可以确定应该转移到哪个状态（推进或回退）。 对于一个模式串pat，其总共就有 M 个状态，对于 ASCII 字符，总共不会超过 256 种。所以我们就构造一个数组dp[M][256]来包含所有情况，并且明确dp数组的含义： dp[j][c] = next表示，当前是状态j，遇到了字符c，应该转移到状态next。 明确了其含义，就可以很容易写出 search 函数的代码。 对于如何构建这个dp数组，需要一个辅助状态X，它永远比当前状态j落后一个状态，拥有和j最长的相同前缀，我们给它起了个名字叫「影子状态」。 在构建当前状态j的转移方向时，只有字符pat[j]才能使状态推进（dp[j][pat[j]] = j+1）；而对于其他字符只能进行状态回退，应该去请教影子状态X应该回退到哪里（dp[j][other] = dp[X][other]，其中other是除了pat[j]之外所有字符）。 对于影子状态X，我们把它初始化为 0，并且随着j的前进进行更新，更新的方式和 search 过程更新j的过程非常相似（X = dp[X][pat[j]]）。 KMP 算法也就是动态规划的思路，是按照一套框架来的，无非就是描述问题逻辑，明确dp数组含义，定义 base case 这点破事。 希望这篇文章能让大家对动态规划有更深的理解，并摆脱被 KMP 算法支配的恐惧。","categories":[{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"剑指Offer（五十六）：删除排序链表中重复的结点—2","slug":"剑指Offer（五十六）：删除排序链表中重复的结点—2","date":"2021-06-17T10:04:34.000Z","updated":"2021-06-17T10:05:39.292Z","comments":true,"path":"20210617/剑指Offer（五十六）：删除排序链表中重复的结点—2.html","link":"","permalink":"https://xxren8218.github.io/20210617/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%94%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9A%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E9%93%BE%E8%A1%A8%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E7%BB%93%E7%82%B9%E2%80%942.html","excerpt":"","text":"1.题目在一个排序的[链表)中，存在重复的结点，请删除该[链表]中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;3-&gt;4-&gt;5。 2.思路由于给定的链表是排好序的，因此重复的元素在链表中出现的位置是连续的，因此我们只需要对链表进行一次遍历，就可以删除重复的元素。 具体地，我们从指针cur 指向链表的头节点，随后开始对链表进行遍历。如果当前 cur 与 cur.next 对应的元素相同，那么我们就将 cur.next 从链表中移除；否则说明链表中已经不存在其它与 cur 对应的元素相同的节点，因此可以将 cur 指向 cur.next。 当遍历完整个链表之后，我们返回链表的头节点即可。 细节 当我们遍历到链表的最后一个节点时，cur.next 为空节点，如果不加以判断，访问 cur.next 对应的元素会产生运行错误。因此我们只需要遍历到链表的最后一个节点，而不需要遍历完整个链表。 3.代码12345678910111213141516171819# Definition for singly-linked list.# class ListNode(object):# def __init__(self, val=0, next=None):# self.val = val# self.next = nextclass Solution(object): def deleteDuplicates(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; if not head: return head cur = head while cur.next: if cur.val == cur.next.val: cur.next = cur.next.next else: cur = cur.next return head","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"链表","slug":"链表","permalink":"https://xxren8218.github.io/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"剑指Offer（五十六）：删除排序链表中重复的结点","slug":"剑指Offer（五十六）：删除排序链表中重复的结点","date":"2021-06-17T10:03:30.000Z","updated":"2021-06-17T10:04:03.494Z","comments":true,"path":"20210617/剑指Offer（五十六）：删除排序链表中重复的结点.html","link":"","permalink":"https://xxren8218.github.io/20210617/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%94%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9A%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E9%93%BE%E8%A1%A8%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E7%BB%93%E7%82%B9.html","excerpt":"","text":"1.题目在一个排序的[链表)中，存在重复的结点，请删除该[链表]中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5。 2.思路由于给定的链表是排好序的，因此重复的元素在链表中出现的位置是连续的，因此我们只需要对链表进行一次遍历，就可以删除重复的元素。由于链表的头节点可能会被删除，因此我们需要额外使用一个哑节点（dum node）指向链表的头节点。 具体地，我们从指针 cur 指向链表的哑节点，随后开始对链表进行遍历。如果当前cur.next 与 cur.next.next 对应的元素相同，那么我们就需要将 cur.next 以及所有后面拥有相同元素值的链表节点全部删除。我们记下这个元素值 x，随后不断将 cur.next 从链表中移除，直到 cur.next 为空节点或者其元素值不等于 x 为止。此时，我们将链表中所有元素值为 x 的节点全部删除。 如果当前 cur.next 与 cur.next.next 对应的元素不相同，那么说明链表中只有一个元素值为 cur.next 的节点，那么我们就可以将 cur 指向 cur.next。 当遍历完整个链表之后，我们返回链表的的哑节点的下一个节点 dum.next 即可。 细节 需要注意 cur.next 以及cur.next.next 可能为空节点，如果不加以判断，可能会产生运行错误。 3.代码1234567891011121314151617class Solution: def deleteDuplicates(self, head: ListNode) -&gt; ListNode: if not head: return head dum = ListNode(0, head) cur = dum while cur.next and cur.next.next: if cur.next.val == cur.next.next.val: x = cur.next.val while cur.next and cur.next.val == x: cur.next = cur.next.next else: cur = cur.next return dum.next","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"链表","slug":"链表","permalink":"https://xxren8218.github.io/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"剑指Offer（五十五）：链表中环的入口结点","slug":"剑指Offer（五十五）：链表中环的入口结点","date":"2021-06-17T10:01:28.000Z","updated":"2021-06-17T10:02:40.423Z","comments":true,"path":"20210617/剑指Offer（五十五）：链表中环的入口结点.html","link":"","permalink":"https://xxren8218.github.io/20210617/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%94%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9A%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%8E%AF%E7%9A%84%E5%85%A5%E5%8F%A3%E7%BB%93%E7%82%B9.html","excerpt":"","text":"1.题目一个 [链表] 中包含环，请找出该链表的环的入口结点。 2.思路一：快慢指针可以用两个指针来解决这个问题。先定义两个指针P1和P2指向链表的头结点。如果链表中的环有n个结点，指针P1先在链表上向前移动n步，然后两个指针以相同的速度向前移动。当第二个指针指向的入口结点时，第一个指针已经围绕着揍了一圈又回到了入口结点。 以下图为例，指针P1和P2在初始化时都指向链表的头结点。由于环中有4个结点，指针P1先在链表上向前移动4步。接下来两个指针以相同的速度在链表上向前移动，直到它们相遇。它们相遇的结点正好是环的入口结点。 现在的关键问题是如何知道环中有几点节点呢？ 可以使用快慢指针，一个每次走一步，一个每次走两步。如果两个指针相遇，表明链表中存在环，并且两个指针相遇的结点一定在环中。 随后，我们就从相遇的这个环中结点出发，一边继续向前移动一边计数，当再次回到这个结点时，就可以得到环中结点数目了。 3.思路二：辅助列表可以直接建立一个列表来存放每个node，若不存在node,则添加进去，若存在，直接返回即可。 4.代码一：12345678910111213141516171819202122232425262728293031323334353637383940# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def EntryNodeOfLoop(self, pHead): # write code here if pHead == None: return None meetingnode = self.MeetingNode(pHead) if meetingnode == None: return None nodeslop = 1 node1 = meetingnode while node1.next != meetingnode: node1 = node1.next nodeslop += 1 node1 = pHead for _ in range(nodeslop): node1 = node1.next node2 = pHead while node1 != node2: node1 = node1.next node2 = node2.next return node1 def MeetingNode(self, pHead): slow = pHead.next if slow == None: return None fast = slow.next while fast != None and slow != None: if slow == fast: return fast slow = slow.next fast = fast.next if fast != None: fast = fast.next return None 5.代码二：123456789101112131415161718# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def EntryNodeOfLoop(self, pHead): # write code here if not pHead: return None p1 = pHead l = [] while p1: if p1 in l: return p1 else: l.append(p1) p1 = p1.next","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"链表","slug":"链表","permalink":"https://xxren8218.github.io/tags/%E9%93%BE%E8%A1%A8/"},{"name":"快慢指针","slug":"快慢指针","permalink":"https://xxren8218.github.io/tags/%E5%BF%AB%E6%85%A2%E6%8C%87%E9%92%88/"},{"name":"辅助列表","slug":"辅助列表","permalink":"https://xxren8218.github.io/tags/%E8%BE%85%E5%8A%A9%E5%88%97%E8%A1%A8/"}]},{"title":"剑指Offer（三十六）：两个链表的第一个公共结点","slug":"剑指Offer（三十六）：两个链表的第一个公共结点","date":"2021-06-17T09:59:38.000Z","updated":"2021-06-17T10:00:48.185Z","comments":true,"path":"20210617/剑指Offer（三十六）：两个链表的第一个公共结点.html","link":"","permalink":"https://xxren8218.github.io/20210617/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9A%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E7%BB%93%E7%82%B9.html","excerpt":"","text":"1.题目输入两个[链表]，找出它们的第一个公共结点. 2.思路公共结点的样子： 上图就是一个有公共结点的例子，在公共结点之后，两个链表指针指向的地址是相同的。 123输入：intersectVal = 8, listA = [4,1,8,4,5], listB = [5,0,1,8,4,5], skipA = 2, skipB = 3输出：Reference of the node with value = 8输入解释：相交节点的值为 8 （注意，如果两个列表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [4,1,8,4,5]，链表 B 为 [5,0,1,8,4,5]。在 A 中，相交节点前有 2 个节点；在 B 中，相交节点前有 3 个节点 我们可以把两个链表拼接起来，一个 headA 在前 headB 在后，一个 headB 在前 headA 在后。这样，生成了两个相同长度的链表，那么我们只要同时遍历这两个表，就一定能找到公共结点。时间复杂度O(m+n)，空间复杂度O(m+n)。 3.代码12345678910111213141516171819# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def getIntersectionNode(self, headA, headB): &quot;&quot;&quot; :type head1, head1: ListNode :rtype: ListNode &quot;&quot;&quot; if not headA or not headB: return None cur1, cur2 = headA, headB while cur1 != cur2: cur1 = cur1.next if cur1 != None else headB cur2 = cur2.next if cur2 != None else headA return cur1","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"链表","slug":"链表","permalink":"https://xxren8218.github.io/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"剑指Offer（二十五）：复杂链表的复制","slug":"剑指Offer（二十五）：复杂链表的复制","date":"2021-06-17T09:55:35.000Z","updated":"2021-06-17T09:59:09.941Z","comments":true,"path":"20210617/剑指Offer（二十五）：复杂链表的复制.html","link":"","permalink":"https://xxren8218.github.io/20210617/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9A%E5%A4%8D%E6%9D%82%E9%93%BE%E8%A1%A8%E7%9A%84%E5%A4%8D%E5%88%B6.html","excerpt":"","text":"1.题目请实现 copyRandomList 函数，复制一个复杂链表。在复杂链表中，每个节点除了有一个 next 指针指向下一个节点，还有一个 random 指针指向链表中的任意节点或者 null。 示例 1： 输入：head = [[7,null],[13,0],[11,4],[10,2],[1,0]]输出：[[7,null],[13,0],[11,4],[10,2],[1,0]] 2.思路普通链表的节点定义如下： 12345# Definition for a Node.class Node: def __init__(self, x: int, next: &#x27;Node&#x27; = None): self.val = int(x) self.next = next 本题链表的节点定义如下： 123456# Definition for a Node.class Node: def __init__(self, x: int, next: &#x27;Node&#x27; = None, random: &#x27;Node&#x27; = None): self.val = int(x) self.next = next self.random = random 给定链表的头节点 head ，复制普通链表很简单，只需遍历链表，每轮建立新节点 + 构建前驱节点 pre 和当前节点 node 的引用指向即可。 本题链表的节点新增了 random 指针，指向链表中的 任意节点 或者 null 。这个 random 指针意味着在复制过程中，除了构建前驱节点和当前节点的引用指向 pre.next ，还要构建前驱节点和其随机节点的引用指向 pre.random 。 本题难点： 在复制链表的过程中构建新链表各节点的 random 引用指向。 1234567891011class Solution: def copyRandomList(self, head: &#x27;Node&#x27;) -&gt; &#x27;Node&#x27;: cur = head dum = pre = Node(0) while cur: node = Node(cur.val) # 复制节点 cur pre.next = node # 新链表的 前驱节点 -&gt; 当前节点 # pre.random = &#x27;???&#x27; # 新链表的 「 前驱节点 -&gt; 当前节点 」 无法确定 cur = cur.next # 遍历下一节点 pre = node # 保存当前新节点 return dum.next 本文介绍 「哈希表」 ，「拼接 + 拆分」 两种方法。哈希表方法比较直观；拼接 + 拆分方法的空间复杂度更低。 方法一：哈希表利用哈希表的查询特点，考虑构建 原链表节点 和 新链表对应节点 的键值对映射关系，再遍历构建新链表各节点的 next 和 random 引用指向即可。 方法二：拼接 + 拆分考虑构建 原节点 1 -&gt; 新节点 1 -&gt; 原节点 2 -&gt; 新节点 2 -&gt; …… 的拼接链表，如此便可在访问原节点的 random 指向节点的同时找到新对应新节点的 random 指向节点。 如下图将 B.random.next -&gt; B&#39;.random，即实现了B&#39;到A‘的指向 3.代码一：哈希表12345678910111213141516171819class Solution: def copyRandomList(self, head: &#x27;Node&#x27;) -&gt; &#x27;Node&#x27;: if not head: return dic = &#123;&#125; # 1. 复制各节点，并建立 “原节点 -&gt; 新节点” 的 Map 映射 cur = head while cur: dic[cur] = Node(cur.val) cur = cur.next cur = head # 2. 构建新节点的 next 和 random 指向 while cur: dic[cur].next = dic.get(cur.next) dic[cur].random = dic.get(cur.random) cur = cur.next # 3. 返回新链表的头节点 return dic[head] 4.代码二：拼接 + 拆分1234567891011121314151617181920212223242526class Solution: def copyRandomList(self, head: &#x27;Node&#x27;) -&gt; &#x27;Node&#x27;: if not head: return cur = head # 1. 复制各节点，并构建拼接链表 while cur: tmp = Node(cur.val) tmp.next = cur.next cur.next = tmp cur = tmp.next # 2. 构建各新节点的 random 指向 cur = head while cur: if cur.random: cur.next.random = cur.random.next cur = cur.next.next # 注意位置 # 3. 拆分两链表 cur = res = head.next pre = head while cur.next: # 注意是cur.next，倒数第二个cur节点，不然下面的cur.next.next不成立。 pre.next = pre.next.next cur.next = cur.next.next pre = pre.next cur = cur.next pre.next = None # 单独处理原链表尾节点 return res # 返回新链表头节点","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"链表","slug":"链表","permalink":"https://xxren8218.github.io/tags/%E9%93%BE%E8%A1%A8/"},{"name":"哈希表","slug":"哈希表","permalink":"https://xxren8218.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"}]},{"title":"16-电影推荐(ContentBased)_物品画像","slug":"16-电影推荐-ContentBased-物品画像","date":"2021-06-16T16:53:27.000Z","updated":"2021-06-16T16:55:00.782Z","comments":true,"path":"20210617/16-电影推荐-ContentBased-物品画像.html","link":"","permalink":"https://xxren8218.github.io/20210617/16-%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90-ContentBased-%E7%89%A9%E5%93%81%E7%94%BB%E5%83%8F.html","excerpt":"","text":"基于内容的电影推荐：物品画像物品画像构建步骤： 利用tags.csv中每部电影的标签作为电影的候选关键词 利用TF·IDF计算每部电影的标签的tfidf值，选取TOP-N个关键词作为电影画像标签 将电影的分类词直接作为每部电影的画像标签 基于TF-IDF的特征提取技术前面提到，物品画像的特征标签主要都是指的如电影的导演、演员、图书的作者、出版社等结构话的数据，也就是他们的特征提取，尤其是体征向量的计算是比较简单的，如直接给作品的分类定义0或者1的状态。 但另外一些特征，比如电影的内容简介、电影的影评、图书的摘要等文本数据，这些被称为非结构化数据，首先他们本应该也属于物品的一个特征标签，但是这样的特征标签进行量化时，也就是计算它的特征向量时是很难去定义的。 因此这时就需要借助一些自然语言处理、信息检索等技术，将如用户的文本评论或其他文本内容信息的非结构化数据进行量化处理，从而实现更加完善的物品画像/用户画像。 TF-IDF算法便是其中一种在自然语言处理领域中应用比较广泛的一种算法。可用来提取目标文档中，并得到关键词用于计算对于目标文档的权重，并将这些权重组合到一起得到特征向量。 算法原理TF-IDF自然语言处理领域中计算文档中词或短语的权值的方法，是词频（Term Frequency，TF）和逆转文档频率（Inverse Document Frequency，IDF）的乘积。TF指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被正规化，以防止它偏向长的文件（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。IDF是一个词语普遍重要性的度量，某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。 TF-IDF算法基于一个这样的假设：若一个词语在目标文档中出现的频率高而在其他文档中出现的频率低，那么这个词语就可以用来区分出目标文档。这个假设需要掌握的有两点： 在本文档出现的频率高； 在其他文档出现的频率低。 因此，TF-IDF算法的计算可以分为词频（Term Frequency，TF）和逆转文档频率（Inverse Document Frequency，IDF）两部分，由TF和IDF的乘积来设置文档词语的权重。 TF指的是一个词语在文档中的出现频率。假设文档集包含的文档数为N，文档集中包含关键词k_i的文档数为n_i，f_{ij}表示关键词k_i在文档d_j中出现的次数，f_{dj}表示文档d_j中出现的词语总数，k_i在文档dj中的词频TF_{ij}定义为：TF_{ij}=\\frac {f_{ij}}{f_{dj}}。并且注意，这个数字通常会被正规化，以防止它偏向长的文件（指同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。 IDF是一个词语普遍重要性的度量。表示某一词语在整个文档集中出现的频率，由它计算的结果取对数得到关键词k_i的逆文档频率IDF_i：IDF_i=log\\frac {N}{n_i} 由TF和IDF计算词语的权重为：w_{ij}=TF_{ij}·IDF_{i}=\\frac {f_{ij}}{f_{dj}}·log\\frac {N}{n_i} 结论：TF-IDF与词语在文档中的出现次数成正比，与该词在整个文档集中的出现次数成反比。 用途：在目标文档中，提取关键词(特征标签)的方法就是将该文档所有词语的TF-IDF计算出来并进行对比，取其中TF-IDF值最大的k个数组成目标文档的特征向量用以表示文档。 注意：文档中存在的停用词（Stop Words），如“是”、“的”之类的，对于文档的中心思想表达没有意义的词，在分词时需要先过滤掉再计算其他词语的TF-IDF值。 算法举例对于计算影评的TF-IDF，以电影“加勒比海盗：黑珍珠号的诅咒”为例，假设它总共有1000篇影评，其中一篇影评的总词语数为200，其中出现最频繁的词语为“海盗”、“船长”、“自由”，分别是20、15、10次，并且这3个词在所有影评中被提及的次数分别为1000、500、100，就这3个词语作为关键词的顺序计算如下。 将影评中出现的停用词过滤掉，计算其他词语的词频。以出现最多的三个词为例进行计算如下： “海盗”出现的词频为20/200＝0.1 “船长”出现的词频为15/200=0.075 “自由”出现的词频为10/200=0.05； 计算词语的逆文档频率如下： “海盗”的IDF为：log(1000/1000)=0 “船长”的IDF为：log(1000/500)=0.3“自由”的IDF为：log(1000/100)=1 由1和2计算的结果求出词语的TF-IDF结果，“海盗”为0，“船长”为0.0225，“自由”为0.05。 通过对比可得，该篇影评的关键词排序应为：“自由”、“船长”、“海盗”。把这些词语的TF-IDF值作为它们的权重按照对应的顺序依次排列，就得到这篇影评的特征向量，我们就用这个向量来代表这篇影评，向量中每一个维度的分量大小对应这个属性的重要性。 将总的影评集中所有的影评向量与特定的系数相乘求和，得到这部电影的综合影评向量，与电影的基本属性结合构建视频的物品画像，同理构建用户画像，可采用多种方法计算物品画像和用户画像之间的相似度，为用户做出推荐。 加载数据集1234567891011121314151617181920212223242526272829303132333435363738import pandas as pdimport numpy as np&#x27;&#x27;&#x27;- 利用tags.csv中每部电影的标签作为电影的候选关键词- 利用TF·IDF计算每部电影的标签的tfidf值，选取TOP-N个关键词作为电影画像标签- 并将电影的分类词直接作为每部电影的画像标签&#x27;&#x27;&#x27;def get_movie_dataset(): # 加载基于所有电影的标签 # all-tags.csv来自ml-latest数据集中 # 由于ml-latest-small中标签数据太多，因此借助其来扩充 _tags = pd.read_csv(&quot;datasets/ml-latest-small/all-tags.csv&quot;, usecols=range(1, 3)).dropna() tags = _tags.groupby(&quot;movieId&quot;).agg(list) # 加载电影列表数据集 movies = pd.read_csv(&quot;datasets/ml-latest-small/movies.csv&quot;, index_col=&quot;movieId&quot;) # 将类别词分开 movies[&quot;genres&quot;] = movies[&quot;genres&quot;].apply(lambda x: x.split(&quot;|&quot;)) # 为每部电影匹配对应的标签数据，如果没有将会是NAN movies_index = set(movies.index) &amp; set(tags.index) new_tags = tags.loc[list(movies_index)] ret = movies.join(new_tags) # 构建电影数据集，包含电影Id、电影名称、类别、标签四个字段 # 如果电影没有标签数据，那么就替换为空列表 # map(fun,可迭代对象)--将标签和分类进行合并以便后面的TF-IDF的分析。 movie_dataset = pd.DataFrame( map( lambda x: (x[0], x[1], x[2], x[2]+x[3]) if x[3] is not np.nan else (x[0], x[1], x[2], []), ret.itertuples()) , columns=[&quot;movieId&quot;, &quot;title&quot;, &quot;genres&quot;,&quot;tags&quot;] ) movie_dataset.set_index(&quot;movieId&quot;, inplace=True) return movie_datasetmovie_dataset = get_movie_dataset()print(movie_dataset) 基于TF·IDF提取TOP-N关键词，构建电影画像1234567891011121314151617181920212223242526272829303132333435363738from gensim.models import TfidfModelimport pandas as pdimport numpy as npfrom pprint import pprint# ......def create_movie_profile(movie_dataset): &#x27;&#x27;&#x27; 使用tfidf，分析提取topn关键词 :param movie_dataset: :return: &#x27;&#x27;&#x27; dataset = movie_dataset[&quot;tags&quot;].values from gensim.corpora import Dictionary # 根据数据集建立词袋，并统计词频，将所有词放入一个词典，使用索引进行获取 dct = Dictionary(dataset) # 根据将每条数据，返回对应的词索引和词频 corpus = [dct.doc2bow(line) for line in dataset] # 训练TF-IDF模型，即计算TF-IDF值 model = TfidfModel(corpus) movie_profile = &#123;&#125; for i, mid in enumerate(movie_dataset.index): # 根据每条数据返回，向量 vector = model[corpus[i]] # 按照TF-IDF值得到top-n的关键词 movie_tags = sorted(vector, key=lambda x: x[1], reverse=True)[:30] # 根据关键词提取对应的名称 movie_profile[mid] = dict(map(lambda x:(dct[x[0]], x[1]), movie_tags)) return movie_profilemovie_dataset = get_movie_dataset()pprint(create_movie_profile(movie_dataset)) 完善画像关键词12345678910111213141516171819202122232425262728293031323334353637383940414243444546from gensim.models import TfidfModelimport pandas as pdimport numpy as npfrom pprint import pprint# ......def create_movie_profile(movie_dataset): &#x27;&#x27;&#x27; 使用tfidf，分析提取topn关键词 :param movie_dataset: :return: &#x27;&#x27;&#x27; dataset = movie_dataset[&quot;tags&quot;].values from gensim.corpora import Dictionary # 根据数据集建立词袋，并统计词频，将所有词放入一个词典，使用索引进行获取 dct = Dictionary(dataset) # 根据将每条数据，返回对应的词索引和词频 corpus = [dct.doc2bow(line) for line in dataset] # 训练TF-IDF模型，即计算TF-IDF值 model = TfidfModel(corpus) _movie_profile = [] for i, data in enumerate(movie_dataset.itertuples()): mid = data[0] title = data[1] genres = data[2] vector = model[corpus[i]] movie_tags = sorted(vector, key=lambda x: x[1], reverse=True)[:30] topN_tags_weights = dict(map(lambda x: (dct[x[0]], x[1]), movie_tags)) # 将类别词的添加进去，并设置权重值为1.0 for g in genres: topN_tags_weights[g] = 1.0 topN_tags = [i[0] for i in topN_tags_weights.items()] _movie_profile.append((mid, title, topN_tags, topN_tags_weights)) movie_profile = pd.DataFrame(_movie_profile, columns=[&quot;movieId&quot;, &quot;title&quot;, &quot;profile&quot;, &quot;weights&quot;]) movie_profile.set_index(&quot;movieId&quot;, inplace=True) return movie_profilemovie_dataset = get_movie_dataset()pprint(create_movie_profile(movie_dataset)) 为了根据指定关键词迅速匹配到对应的电影，因此需要对物品画像的标签词，建立倒排索引 倒排索引介绍 通常数据存储数据，都是以物品的ID作为索引，去提取物品的其他信息数据 而倒排索引就是用物品的其他数据作为索引，去提取它们对应的物品的ID列表 123456789101112131415161718# ......&#x27;&#x27;&#x27;建立tag-物品的倒排索引&#x27;&#x27;&#x27;def create_inverted_table(movie_profile): inverted_table = &#123;&#125; for mid, weights in movie_profile[&quot;weights&quot;].iteritems(): for tag, weight in weights.items(): #到inverted_table dict 用tag作为Key去取值 如果取不到就返回[] _ = inverted_table.get(tag, []) _.append((mid, weight)) inverted_table.setdefault(tag, _) return inverted_tableinverted_table = create_inverted_table(movie_profile)pprint(inverted_table)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"15-基于内容的推荐算法","slug":"15-基于内容的推荐算法","date":"2021-06-16T16:51:30.000Z","updated":"2021-06-16T16:54:18.765Z","comments":true,"path":"20210617/15-基于内容的推荐算法.html","link":"","permalink":"https://xxren8218.github.io/20210617/15-%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95.html","excerpt":"","text":"基于内容的推荐算法（Content-Based）简介基于内容的推荐方法是非常直接的，它以物品的内容描述信息为依据来做出的推荐，本质上是基于对物品和用户自身的特征或属性的直接分析和计算。 例如，假设已知电影A是一部喜剧，而恰巧我们得知某个用户喜欢看喜剧电影，那么我们基于这样的已知信息，就可以将电影A推荐给该用户。 基于内容的推荐实现步骤 画像构建。顾名思义，画像就是刻画物品或用户的特征。本质上就是给用户或物品贴标签。 物品画像：例如给电影《战狼2》贴标签，可以有哪些？ “动作”、”吴京”、”吴刚”、”张翰”、”大陆电影”、”国产”、”爱国”、”军事”等等一系列标签是不是都可以贴上 用户画像：例如已知用户的观影历史是：”《战狼1》”、”《战狼2》”、”《建党伟业》”、”《建军大业》”、”《建国大业》”、”《红海行动》”、”《速度与激情1-8》”等，我们是不是就可以分析出该用户的一些兴趣特征如：”爱国”、”战争”、”赛车”、”动作”、”军事”、”吴京”、”韩三平”等标签。 问题：物品的标签来自哪儿？ PGC 物品画像—冷启动 物品自带的属性（物品一产生就具备的）：如电影的标题、导演、演员、类型等等 服务提供方设定的属性（服务提供方为物品附加的属性）：如短视频话题、微博话题（平台拟定） 其他渠道：如爬虫 UGC 冷启动问题 用户在享受服务过程中提供的物品的属性：如用户评论内容，微博话题（用户拟定） 根据PGC内容构建的物品画像的可以解决物品的冷启动问题 基于内容推荐的算法流程： 根据PGC/UGC内容构建物品画像 根据用户行为记录生成用户画像 根据用户画像从物品中寻找最匹配的TOP-N物品进行推荐 物品冷启动处理： 根据PGC内容构建物品画像 利用物品画像计算物品间两两相似情况 为每个物品产生TOP-N最相似的物品进行相关推荐：如与该商品相似的商品有哪些？与该文章相似文章有哪些？","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"14-BiasSVD算法实现","slug":"14-BiasSVD算法实现","date":"2021-06-16T16:50:20.000Z","updated":"2021-06-16T16:50:52.584Z","comments":true,"path":"20210617/14-BiasSVD算法实现.html","link":"","permalink":"https://xxren8218.github.io/20210617/14-BiasSVD%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0.html","excerpt":"","text":"基于矩阵分解的CF算法实现（二）：BiasSVDBiasSvd其实就是前面提到的Funk SVD矩阵分解基础上加上了偏置项。 BiasSvd利用BiasSvd预测用户对物品的评分，$k$表示隐含特征数量： \\begin{split} \\hat {r}_{ui} &=\\mu + b_u + b_i + \\vec {p_{uk}}\\cdot \\vec {q_{ki}} \\\\&=\\mu + b_u + b_i + {\\sum_{k=1}}^k p_{uk}q_{ik} \\end{split}损失函数同样对于评分预测我们利用平方差来构建损失函数： \\begin{split} Cost &= \\sum_{u,i\\in R} (r_{ui}-\\hat{r}_{ui})^2 \\\\&=\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i -{\\sum_{k=1}}^k p_{uk}q_{ik})^2 \\end{split}加入L2正则化： Cost = \\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})^2 + \\lambda(\\sum_U{b_u}^2+\\sum_I{b_i}^2+\\sum_U{p_{uk}}^2+\\sum_I{q_{ik}}^2)对损失函数求偏导： \\begin{split} \\cfrac {\\partial}{\\partial p_{uk}}Cost &= \\cfrac {\\partial}{\\partial p_{uk}}[\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})^2 + \\lambda(\\sum_U{b_u}^2+\\sum_I{b_i}^2+\\sum_U{p_{uk}}^2+\\sum_I{q_{ik}}^2)] \\\\&=2\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\\lambda p_{uk} \\\\\\\\ \\cfrac {\\partial}{\\partial q_{ik}}Cost &= \\cfrac {\\partial}{\\partial q_{ik}}[\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})^2 + \\lambda(\\sum_U{b_u}^2+\\sum_I{b_i}^2+\\sum_U{p_{uk}}^2+\\sum_I{q_{ik}}^2)] \\\\&=2\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})(-p_{uk}) + 2\\lambda q_{ik} \\end{split} \\begin{split} \\cfrac {\\partial}{\\partial b_u}Cost &= \\cfrac {\\partial}{\\partial b_u}[\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})^2 + \\lambda(\\sum_U{b_u}^2+\\sum_I{b_i}^2+\\sum_U{p_{uk}}^2+\\sum_I{q_{ik}}^2)] \\\\&=2\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})(-1) + 2\\lambda b_u \\\\\\\\ \\cfrac {\\partial}{\\partial b_i}Cost &= \\cfrac {\\partial}{\\partial b_i}[\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})^2 + \\lambda(\\sum_U{b_u}^2+\\sum_I{b_i}^2+\\sum_U{p_{uk}}^2+\\sum_I{q_{ik}}^2)] \\\\&=2\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})(-1) + 2\\lambda b_i \\end{split}随机梯度下降法优化梯度下降更新参数$p_{uk}$： \\begin{split} p_{uk}&:=p_{uk} - \\alpha\\cfrac {\\partial}{\\partial p_{uk}}Cost \\\\&:=p_{uk}-\\alpha [2\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\\lambda p_{uk}] \\\\&:=p_{uk}+\\alpha [\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \\lambda p_{uk}] \\end{split} 同理： \\begin{split} q_{ik}&:=q_{ik} + \\alpha[\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \\lambda q_{ik}] \\end{split} b_u:=b_u + \\alpha[\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik}) - \\lambda b_u] b_i:=b_i + \\alpha[\\sum_{u,i\\in R} (r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik}) - \\lambda b_i]随机梯度下降： \\begin{split} &p_{uk}:=p_{uk}+\\alpha [(r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \\lambda_1 p_{uk}] \\\\&q_{ik}:=q_{ik} + \\alpha[(r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \\lambda_2 q_{ik}] \\end{split} b_u:=b_u + \\alpha[(r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik}) - \\lambda_3 b_u] b_i:=b_i + \\alpha[(r_{ui}-\\mu - b_u - b_i-{\\sum_{k=1}}^k p_{uk}q_{ik}) - \\lambda_4 b_i]由于P矩阵和Q矩阵是两个不同的矩阵，通常分别采取不同的正则参数，如$\\lambda_1$和$\\lambda_2$ 算法实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108&#x27;&#x27;&#x27;BiasSvd Model&#x27;&#x27;&#x27;import mathimport randomimport pandas as pdimport numpy as npclass BiasSvd(object): def __init__(self, alpha, reg_p, reg_q, reg_bu, reg_bi, number_LatentFactors=10, number_epochs=10, columns=[&quot;uid&quot;, &quot;iid&quot;, &quot;rating&quot;]): self.alpha = alpha # 学习率 self.reg_p = reg_p self.reg_q = reg_q self.reg_bu = reg_bu self.reg_bi = reg_bi self.number_LatentFactors = number_LatentFactors # 隐式类别数量 self.number_epochs = number_epochs self.columns = columns def fit(self, dataset): &#x27;&#x27;&#x27; fit dataset :param dataset: uid, iid, rating :return: &#x27;&#x27;&#x27; self.dataset = pd.DataFrame(dataset) self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] self.globalMean = self.dataset[self.columns[2]].mean() self.P, self.Q, self.bu, self.bi = self.sgd() def _init_matrix(self): &#x27;&#x27;&#x27; 初始化P和Q矩阵，同时为设置0，1之间的随机值作为初始值 :return: &#x27;&#x27;&#x27; # User-LF P = dict(zip( self.users_ratings.index, np.random.rand(len(self.users_ratings), self.number_LatentFactors).astype(np.float32) )) # Item-LF Q = dict(zip( self.items_ratings.index, np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32) )) return P, Q def sgd(self): &#x27;&#x27;&#x27; 使用随机梯度下降，优化结果 :return: &#x27;&#x27;&#x27; P, Q = self._init_matrix() # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print(&quot;iter%d&quot;%i) error_list = [] for uid, iid, r_ui in self.dataset.itertuples(index=False): v_pu = P[uid] v_qi = Q[iid] err = np.float32(r_ui - self.globalMean - bu[uid] - bi[iid] - np.dot(v_pu, v_qi)) v_pu += self.alpha * (err * v_qi - self.reg_p * v_pu) v_qi += self.alpha * (err * v_pu - self.reg_q * v_qi) P[uid] = v_pu Q[iid] = v_qi bu[uid] += self.alpha * (err - self.reg_bu * bu[uid]) bi[iid] += self.alpha * (err - self.reg_bi * bi[iid]) error_list.append(err ** 2) print(np.sqrt(np.mean(error_list))) return P, Q, bu, bi def predict(self, uid, iid): if uid not in self.users_ratings.index or iid not in self.items_ratings.index: return self.globalMean p_u = self.P[uid] q_i = self.Q[iid] return self.globalMean + self.bu[uid] + self.bi[iid] + np.dot(p_u, q_i)if __name__ == &#x27;__main__&#x27;: dtype = [(&quot;userId&quot;, np.int32), (&quot;movieId&quot;, np.int32), (&quot;rating&quot;, np.float32)] dataset = pd.read_csv(&quot;datasets/ml-latest-small/ratings.csv&quot;, usecols=range(3), dtype=dict(dtype)) bsvd = BiasSvd(0.02, 0.01, 0.01, 0.01, 0.01, 10, 20) bsvd.fit(dataset) while True: uid = input(&quot;uid: &quot;) iid = input(&quot;iid: &quot;) print(bsvd.predict(int(uid), int(iid)))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"13-LFM算法实现","slug":"13-LFM算法实现","date":"2021-06-16T16:46:16.000Z","updated":"2021-06-16T16:48:43.882Z","comments":true,"path":"20210617/13-LFM算法实现.html","link":"","permalink":"https://xxren8218.github.io/20210617/13-LFM%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0.html","excerpt":"","text":"基于矩阵分解的CF算法实现（一）：LFMLFM也就是前面提到的Funk SVD矩阵分解 LFM原理解析LFM(latent factor model)隐语义模型核心思想是通过隐含特征联系用户和物品，如下图： P矩阵是User-LF矩阵，即用户和隐含特征矩阵。LF有三个，表示共总有三个隐含特征。 Q矩阵是LF-Item矩阵，即隐含特征和物品的矩阵 R矩阵是User-Item矩阵，有P*Q得来 能处理稀疏评分矩阵 利用矩阵分解技术，将原始User-Item的评分矩阵（稠密/稀疏）分解为P和Q矩阵，然后利用$PQ$还原出User-Item评分矩阵$R$。整个过程相当于*降维处理，其中： 矩阵值$P_{11}$表示用户1对隐含特征1的权重值 矩阵值$Q_{11}$表示隐含特征1在物品1上的权重值 矩阵值$R_{11}$就表示预测的用户1对物品1的评分，且$R_{11}=\\vec{P_{1,k}}\\cdot \\vec{Q_{k,1}}$ 利用LFM预测用户对物品的评分，$k$表示隐含特征数量： \\begin{split} \\hat {r}_{ui} &=\\vec {p_{uk}}\\cdot \\vec {q_{ik}} \\\\&={\\sum_{k=1}}^k p_{uk}q_{ik} \\end{split}因此最终，我们的目标也就是要求出P矩阵和Q矩阵及其当中的每一个值，然后再对用户-物品的评分进行预测。 损失函数同样对于评分预测我们利用平方差来构建损失函数： \\begin{split} Cost &= \\sum_{u,i\\in R} (r_{ui}-\\hat{r}_{ui})^2 \\\\&=\\sum_{u,i\\in R} (r_{ui}-{\\sum_{k=1}}^k p_{uk}q_{ik})^2 \\end{split}加入L2正则化： Cost = \\sum_{u,i\\in R} (r_{ui}-{\\sum_{k=1}}^k p_{uk}q_{ik})^2 + \\lambda(\\sum_U{p_{uk}}^2+\\sum_I{q_{ik}}^2)对损失函数求偏导： \\begin{split} \\cfrac {\\partial}{\\partial p_{uk}}Cost &= \\cfrac {\\partial}{\\partial p_{uk}}[\\sum_{u,i\\in R} (r_{ui}-{\\sum_{k=1}}^k p_{uk}q_{ik})^2 + \\lambda(\\sum_U{p_{uk}}^2+\\sum_I{q_{ik}}^2)] \\\\&=2\\sum_{u,i\\in R} (r_{ui}-{\\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\\lambda p_{uk} \\\\\\\\ \\cfrac {\\partial}{\\partial q_{ik}}Cost &= \\cfrac {\\partial}{\\partial q_{ik}}[\\sum_{u,i\\in R} (r_{ui}-{\\sum_{k=1}}^k p_{uk}q_{ik})^2 + \\lambda(\\sum_U{p_{uk}}^2+\\sum_I{q_{ik}}^2)] \\\\&=2\\sum_{u,i\\in R} (r_{ui}-{\\sum_{k=1}}^k p_{uk}q_{ik})(-p_{uk}) + 2\\lambda q_{ik} \\end{split}随机梯度下降法优化梯度下降更新参数$p_{uk}$： \\begin{split} p_{uk}&:=p_{uk} - \\alpha\\cfrac {\\partial}{\\partial p_{uk}}Cost \\\\&:=p_{uk}-\\alpha [2\\sum_{u,i\\in R} (r_{ui}-{\\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\\lambda p_{uk}] \\\\&:=p_{uk}+\\alpha [\\sum_{u,i\\in R} (r_{ui}-{\\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \\lambda p_{uk}] \\end{split} 同理： \\begin{split} q_{ik}&:=q_{ik} + \\alpha[\\sum_{u,i\\in R} (r_{ui}-{\\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \\lambda q_{ik}] \\end{split}随机梯度下降： 向量乘法 每一个分量相乘 求和 \\begin{split} &p_{uk}:=p_{uk}+\\alpha [(r_{ui}-{\\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \\lambda_1 p_{uk}] \\\\&q_{ik}:=q_{ik} + \\alpha[(r_{ui}-{\\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \\lambda_2 q_{ik}] \\end{split}由于P矩阵和Q矩阵是两个不同的矩阵，通常分别采取不同的正则参数，如$\\lambda_1$和$\\lambda_2$ 算法实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113&#x27;&#x27;&#x27;LFM Model&#x27;&#x27;&#x27;import pandas as pdimport numpy as np# 评分预测 1-5class LFM(object): def __init__(self, alpha, reg_p, reg_q, number_LatentFactors=10, number_epochs=10, columns=[&quot;uid&quot;, &quot;iid&quot;, &quot;rating&quot;]): self.alpha = alpha # 学习率 self.reg_p = reg_p # P矩阵正则 self.reg_q = reg_q # Q矩阵正则 self.number_LatentFactors = number_LatentFactors # 隐式类别数量 self.number_epochs = number_epochs # 最大迭代次数 self.columns = columns def fit(self, dataset): &#x27;&#x27;&#x27; fit dataset :param dataset: uid, iid, rating :return: &#x27;&#x27;&#x27; self.dataset = pd.DataFrame(dataset) self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] self.globalMean = self.dataset[self.columns[2]].mean() self.P, self.Q = self.sgd() def _init_matrix(self): &#x27;&#x27;&#x27; 初始化P和Q矩阵，同时为设置0，1之间的随机值作为初始值 :return: &#x27;&#x27;&#x27; # User-LF P = dict(zip( self.users_ratings.index, np.random.rand(len(self.users_ratings), self.number_LatentFactors).astype(np.float32) )) # Item-LF Q = dict(zip( self.items_ratings.index, np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32) )) return P, Q def sgd(self): &#x27;&#x27;&#x27; 使用随机梯度下降，优化结果 :return: &#x27;&#x27;&#x27; P, Q = self._init_matrix() for i in range(self.number_epochs): print(&quot;iter%d&quot;%i) error_list = [] for uid, iid, r_ui in self.dataset.itertuples(index=False): # User-LF P ## Item-LF Q v_pu = P[uid] # 用户向量 v_qi = Q[iid] # 物品向量 err = np.float32(r_ui - np.dot(v_pu, v_qi)) v_pu += self.alpha * (err * v_qi - self.reg_p * v_pu) v_qi += self.alpha * (err * v_pu - self.reg_q * v_qi) P[uid] = v_pu Q[iid] = v_qi # for k in range(self.number_of_LatentFactors): # v_pu[k] += self.alpha*(err*v_qi[k] - self.reg_p*v_pu[k]) # v_qi[k] += self.alpha*(err*v_pu[k] - self.reg_q*v_qi[k]) error_list.append(err ** 2) print(np.sqrt(np.mean(error_list))) return P, Q def predict(self, uid, iid): # 如果uid或iid不在，我们使用全剧平均分作为预测结果返回 if uid not in self.users_ratings.index or iid not in self.items_ratings.index: return self.globalMean p_u = self.P[uid] q_i = self.Q[iid] return np.dot(p_u, q_i) def test(self,testset): &#x27;&#x27;&#x27;预测测试集数据&#x27;&#x27;&#x27; for uid, iid, real_rating in testset.itertuples(index=False): try: pred_rating = self.predict(uid, iid) except Exception as e: print(e) else: yield uid, iid, real_rating, pred_ratingif __name__ == &#x27;__main__&#x27;: dtype = [(&quot;userId&quot;, np.int32), (&quot;movieId&quot;, np.int32), (&quot;rating&quot;, np.float32)] dataset = pd.read_csv(&quot;datasets/ml-latest-small/ratings.csv&quot;, usecols=range(3), dtype=dict(dtype)) lfm = LFM(0.02, 0.01, 0.01, 10, 100, [&quot;userId&quot;, &quot;movieId&quot;, &quot;rating&quot;]) lfm.fit(dataset) while True: uid = input(&quot;uid: &quot;) iid = input(&quot;iid: &quot;) print(lfm.predict(int(uid), int(iid)))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"12-基于矩阵分解的协同过滤推荐","slug":"12-基于矩阵分解的协同过滤推荐","date":"2021-06-16T16:42:37.000Z","updated":"2021-06-16T16:45:35.059Z","comments":true,"path":"20210617/12-基于矩阵分解的协同过滤推荐.html","link":"","permalink":"https://xxren8218.github.io/20210617/12-%E5%9F%BA%E4%BA%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90.html","excerpt":"","text":"基于矩阵分解的CF算法矩阵分解发展史Traditional SVD: 通常SVD矩阵分解指的是SVD（奇异值）分解技术，在这我们姑且将其命名为Traditional SVD（传统并经典着）其公式如下： Traditional SVD分解的形式为3个矩阵相乘，中间矩阵为奇异值矩阵。如果想运用SVD分解的话，有一个前提是要求矩阵是稠密的，即矩阵里的元素要非空，否则就不能运用SVD分解。 很显然我们的数据其实绝大多数情况下都是稀疏的，因此如果要使用Traditional SVD，一般的做法是先用均值或者其他统计学方法来填充矩阵，然后再运用Traditional SVD分解降维，但这样做明显对数据的原始性造成一定影响。 FunkSVD（LFM） 刚才提到的Traditional SVD首先需要填充矩阵，然后再进行分解降维，同时存在计算复杂度高的问题，因为要分解成3个矩阵，所以后来提出了Funk SVD的方法，它不在将矩阵分解为3个矩阵，而是分解为2个用户-隐含特征，项目-隐含特征的矩阵，Funk SVD也被称为最原始的LFM模型 借鉴线性回归的思想，通过最小化观察数据的平方来寻求最优的用户和项目的隐含向量表示。同时为了避免过度拟合（Overfitting）观测数据，又提出了带有L2正则项的FunkSVD，上公式： 以上两种最优化函数都可以通过梯度下降或者随机梯度下降法来寻求最优解。 BiasSVD: 在FunkSVD提出来之后，出现了很多变形版本，其中一个相对成功的方法是BiasSVD，顾名思义，即带有偏置项的SVD分解： 它基于的假设和Baseline基准预测是一样的，但这里将Baseline的偏置引入到了矩阵分解中 SVD++: 人们后来又提出了改进的BiasSVD，被称为SVD++，该算法是在BiasSVD的基础上添加了用户的隐式反馈信息： 显示反馈指的用户的评分这样的行为，隐式反馈指用户的浏览记录、购买记录、收听记录等。 SVD++是基于这样的假设：在BiasSVD基础上，认为用户对于项目的历史浏览记录、购买记录、收听记录等可以从侧面反映用户的偏好。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"11-基于回归模型的协同过滤推荐","slug":"11-基于回归模型的协同过滤推荐","date":"2021-06-16T16:37:54.000Z","updated":"2021-06-16T16:41:54.175Z","comments":true,"path":"20210617/11-基于回归模型的协同过滤推荐.html","link":"","permalink":"https://xxren8218.github.io/20210617/11-%E5%9F%BA%E4%BA%8E%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90.html","excerpt":"","text":"基于回归模型的协同过滤推荐如果我们将评分看作是一个连续的值而不是离散的值，那么就可以借助线性回归思想来预测目标用户对某物品的评分。其中一种实现策略被称为Baseline（基准预测）。 Baseline：基准预测Baseline设计思想基于以下的假设： 有些用户的评分普遍高于其他用户，有些用户的评分普遍低于其他用户。比如有些用户天生愿意给别人好评，心慈手软，比较好说话，而有的人就比较苛刻，总是评分不超过3分（5分满分） 一些物品的评分普遍高于其他物品，一些物品的评分普遍低于其他物品。比如一些物品一被生产便决定了它的地位，有的比较受人们欢迎，有的则被人嫌弃。 这个用户或物品普遍高于或低于平均值的差值，我们称为偏置(bias) Baseline目标： 找出每个用户普遍高于或低于他人的偏置值$b_u$ 找出每件物品普遍高于或低于其他物品的偏置值$b_i$ 我们的目标也就转化为寻找最优的$b_u$和$b_i$ 使用Baseline的算法思想预测评分的步骤如下： 计算所有电影的平均评分$\\mu$（即全局平均评分） 计算每个用户评分与平均评分$\\mu$的偏置值$b_u$ 计算每部电影所接受的评分与平均评分$\\mu$的偏置值$b_i$ 预测用户对电影的评分： \\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i 举例： ​ 比如我们想通过Baseline来预测用户A对电影“阿甘正传”的评分，那么首先计算出整个评分数据集的平均评分$\\mu$是3.5分；而用户A是一个比较苛刻的用户，他的评分比较严格，普遍比平均评分低0.5分，即用户A的偏置值$b_i$是-0.5；而电影“阿甘正传”是一部比较热门而且备受好评的电影，它的评分普遍比平均评分要高1.2分，那么电影“阿甘正传”的偏置值$b_i$是+1.2，因此就可以预测出用户A对电影“阿甘正传”的评分为：$3.5+(-0.5)+1.2$，也就是4.2分。 对于所有电影的平均评分$\\mu$是直接能计算出的，因此问题在于要测出每个用户的$b_u$值和每部电影的$b_i$的值。对于线性回归问题，我们可以利用平方差构建损失函数如下： \\begin{split} Cost &= \\sum_{u,i\\in R}(r_{ui}-\\hat{r}_{ui})^2 \\\\&=\\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i)^2 \\end{split} 加入L2正则化： Cost=\\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i)^2 + \\lambda*(\\sum_u {b_u}^2 + \\sum_i {b_i}^2)公式解析： 公式第一部分$ \\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i)^2$是用来寻找与已知评分数据拟合最好的$b_u$和$b_i$ 公式第二部分$\\lambda*(\\sum_u {b_u}^2 + \\sum_i {b_i}^2)$是正则化项，用于避免过拟合现象 对于最小过程的求解，我们一般采用随机梯度下降法或者交替最小二乘法来优化实现。 方法一：随机梯度下降法优化使用随机梯度下降优化算法预测Baseline偏置值 step 1：梯度下降法推导损失函数： \\begin{split} &J(\\theta)=Cost=f(b_u, b_i)\\\\ \\\\ &J(\\theta)=\\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i)^2 + \\lambda*(\\sum_u {b_u}^2 + \\sum_i {b_i}^2) \\end{split}梯度下降参数更新原始公式： \\theta_j:=\\theta_j-\\alpha\\cfrac{\\partial }{\\partial \\theta_j}J(\\theta)梯度下降更新$b_u$: ​ 损失函数偏导推导： \\begin{split} \\cfrac{\\partial}{\\partial b_u} J(\\theta)&=\\cfrac{\\partial}{\\partial b_u} f(b_u, b_i) \\\\&=2\\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i)(-1) + 2\\lambda{b_u} \\\\&=-2\\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i) + 2\\lambda*b_u \\end{split}​ $b_u$更新(因为alpha可以人为控制，所以2可以省略掉)： \\begin{split} b_u&:=b_u - \\alpha*(-\\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i) + \\lambda * b_u)\\\\ &:=b_u + \\alpha*(\\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i) - \\lambda* b_u) \\end{split}同理可得，梯度下降更新$b_i$: b_i:=b_i + \\alpha*(\\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i) -\\lambda*b_i)step 2：随机梯度下降由于随机梯度下降法本质上利用每个样本的损失来更新参数，而不用每次求出全部的损失和，因此使用SGD时： 单样本损失值： \\begin{split} error &=r_{ui}-\\hat{r}_{ui} \\\\&= r_{ui}-(\\mu+b_u+b_i) \\\\&= r_{ui}-\\mu-b_u-b_i \\end{split}参数更新： \\begin{split} b_u&:=b_u + \\alpha*((r_{ui}-\\mu-b_u-b_i) -\\lambda*b_u) \\\\ &:=b_u + \\alpha*(error - \\lambda*b_u) \\\\ \\\\ b_i&:=b_i + \\alpha*((r_{ui}-\\mu-b_u-b_i) -\\lambda*b_i)\\\\ &:=b_i + \\alpha*(error -\\lambda*b_i) \\end{split}step 3：算法实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import pandas as pdimport numpy as npclass BaselineCFBySGD(object): def __init__(self, number_epochs, alpha, reg, columns=[&quot;uid&quot;, &quot;iid&quot;, &quot;rating&quot;]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # 学习率 self.alpha = alpha # 正则参数 self.reg = reg # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): &#x27;&#x27;&#x27; :param dataset: uid, iid, rating :return: &#x27;&#x27;&#x27; self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.sgd() def sgd(self): &#x27;&#x27;&#x27; 利用随机梯度下降，优化bu，bi的值 :return: bu, bi &#x27;&#x27;&#x27; # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print(&quot;iter%d&quot; % i) for uid, iid, real_rating in self.dataset.itertuples(index=False): error = real_rating - (self.global_mean + bu[uid] + bi[iid]) bu[uid] += self.alpha * (error - self.reg * bu[uid]) bi[iid] += self.alpha * (error - self.reg * bi[iid]) return bu, bi def predict(self, uid, iid): predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_ratingif __name__ == &#x27;__main__&#x27;: dtype = [(&quot;userId&quot;, np.int32), (&quot;movieId&quot;, np.int32), (&quot;rating&quot;, np.float32)] dataset = pd.read_csv(&quot;datasets/ml-latest-small/ratings.csv&quot;, usecols=range(3), dtype=dict(dtype)) bcf = BaselineCFBySGD(20, 0.1, 0.1, [&quot;userId&quot;, &quot;movieId&quot;, &quot;rating&quot;]) bcf.fit(dataset) while True: uid = int(input(&quot;uid: &quot;)) iid = int(input(&quot;iid: &quot;)) print(bcf.predict(uid, iid)) Step 4: 准确性指标评估——数据的拆分 训练集 和 测试集 添加test方法，然后使用之前实现accuary方法计算准确性指标 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170import pandas as pdimport numpy as npdef data_split(data_path, x=0.8, random=False): &#x27;&#x27;&#x27; 切分数据集， 这里为了保证用户数量保持不变，将每个用户的评分数据按比例进行拆分 :param data_path: 数据集路径 :param x: 训练集的比例，如x=0.8，则0.2是测试集 :param random: 是否随机切分，默认False :return: 用户-物品评分矩阵 &#x27;&#x27;&#x27; print(&quot;开始切分数据集...&quot;) # 设置要加载的数据字段的类型 dtype = &#123;&quot;userId&quot;: np.int32, &quot;movieId&quot;: np.int32, &quot;rating&quot;: np.float32&#125; # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分 ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3)) testset_index = [] # 为了保证每个用户在测试集和训练集都有数据，因此按userId聚合（如1用户有40条数据，训练集有32条，测试集有8条） for uid in ratings.groupby(&quot;userId&quot;).any().index: user_rating_data = ratings.where(ratings[&quot;userId&quot;]==uid).dropna() if random: # 是否打乱 # 因为不可变类型不能被 shuffle方法作用，所以需要强行转换为列表——pandas的index是不可以修改的。 index = list(user_rating_data.index) np.random.shuffle(index) # 打乱列表 _index = round(len(user_rating_data) * x) # 此处x为测试集所占的百分比 round()取整数 testset_index += list(index[_index:]) else: # 将每个用户的x比例的数据作为训练集，剩余的作为测试集 index = round(len(user_rating_data) * x) testset_index += list(user_rating_data.index.values[index:]) testset = ratings.loc[testset_index] trainset = ratings.drop(testset_index) print(&quot;完成数据集切分...&quot;) return trainset, testsetdef accuray(predict_results, method=&quot;all&quot;): &#x27;&#x27;&#x27; 准确性指标计算方法 :param predict_results: 预测结果，类型为容器，每个元素是一个包含uid,iid,real_rating,pred_rating的序列 :param method: 指标方法，类型为字符串，rmse或mae，否则返回两者rmse和mae :return: &#x27;&#x27;&#x27; def rmse(predict_results): &#x27;&#x27;&#x27; rmse评估指标 :param predict_results: :return: rmse &#x27;&#x27;&#x27; length = 0 _rmse_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 return round(np.sqrt(_rmse_sum / length), 4) def mae(predict_results): &#x27;&#x27;&#x27; mae评估指标 :param predict_results: :return: mae &#x27;&#x27;&#x27; length = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _mae_sum += abs(pred_rating - real_rating) return round(_mae_sum / length, 4) def rmse_mae(predict_results): &#x27;&#x27;&#x27; rmse和mae评估指标 :param predict_results: :return: rmse, mae &#x27;&#x27;&#x27; length = 0 _rmse_sum = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 _mae_sum += abs(pred_rating - real_rating) return round(np.sqrt(_rmse_sum / length), 4), round(_mae_sum / length, 4) if method.lower() == &quot;rmse&quot;: rmse(predict_results) elif method.lower() == &quot;mae&quot;: mae(predict_results) else: return rmse_mae(predict_results)class BaselineCFBySGD(object): def __init__(self, number_epochs, alpha, reg, columns=[&quot;uid&quot;, &quot;iid&quot;, &quot;rating&quot;]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # 学习率 self.alpha = alpha # 正则参数 self.reg = reg # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): &#x27;&#x27;&#x27; :param dataset: uid, iid, rating :return: &#x27;&#x27;&#x27; self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.sgd() def sgd(self): &#x27;&#x27;&#x27; 利用随机梯度下降，优化bu，bi的值 :return: bu, bi &#x27;&#x27;&#x27; # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print(&quot;iter%d&quot; % i) for uid, iid, real_rating in self.dataset.itertuples(index=False): error = real_rating - (self.global_mean + bu[uid] + bi[iid]) bu[uid] += self.alpha * (error - self.reg * bu[uid]) bi[iid] += self.alpha * (error - self.reg * bi[iid]) return bu, bi def predict(self, uid, iid): &#x27;&#x27;&#x27;评分预测&#x27;&#x27;&#x27; if iid not in self.items_ratings.index: raise Exception(&quot;无法预测用户&lt;&#123;uid&#125;&gt;对电影&lt;&#123;iid&#125;&gt;的评分，因为训练集中缺失&lt;&#123;iid&#125;&gt;的数据&quot;.format(uid=uid, iid=iid)) predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_rating def test(self,testset): &#x27;&#x27;&#x27;预测测试集数据&#x27;&#x27;&#x27; for uid, iid, real_rating in testset.itertuples(index=False): try: pred_rating = self.predict(uid, iid) except Exception as e: print(e) else: yield uid, iid, real_rating, pred_ratingif __name__ == &#x27;__main__&#x27;: trainset, testset = data_split(&quot;datasets/ml-latest-small/ratings.csv&quot;, random=True) bcf = BaselineCFBySGD(20, 0.1, 0.1, [&quot;userId&quot;, &quot;movieId&quot;, &quot;rating&quot;]) bcf.fit(trainset) pred_results = bcf.test(testset) rmse, mae = accuray(pred_results) print(&quot;rmse: &quot;, rmse, &quot;mae: &quot;, mae) 方法二：交替最小二乘法优化（ALS）使用交替最小二乘法优化算法预测Baseline偏置值 step 1: 交替最小二乘法推导最小二乘法和梯度下降法一样，可以用于求极值。 最小二乘法思想：对损失函数求偏导，然后再使偏导为0 同样，损失函数： J(\\theta)=\\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i)^2 + \\lambda*(\\sum_u {b_u}^2 + \\sum_i {b_i}^2)对损失函数求偏导： \\cfrac{\\partial}{\\partial b_u} f(b_u, b_i) =-2 \\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i) + 2\\lambda * b_u令偏导为0，则可得： \\sum_{u,i\\in R}(r_{ui}-\\mu-b_u-b_i) = \\lambda* b_u \\\\\\sum_{u,i\\in R}(r_{ui}-\\mu-b_i) = \\sum_{u,i\\in R} b_u+\\lambda * b_u为了简化公式，这里令$\\sum_{u,i\\in R} b_u \\approx |R(u)|*b_u$，即直接假设每一项的偏置都相等，可得： b_u := \\cfrac {\\sum_{u,i\\in R}(r_{ui}-\\mu-b_i)}{\\lambda_1 + |R(u)|}其中$|R(u)|$表示用户$u$的有过评分数量 同理可得： b_i := \\cfrac {\\sum_{u,i\\in R}(r_{ui}-\\mu-b_u)}{\\lambda_2 + |R(i)|}其中$|R(i)|$表示物品$i$收到的评分数量 $b_u$和$b_i$分别属于用户和物品的偏置，因此他们的正则参数可以分别设置两个独立的参数 step 2: 交替最小二乘法应用通过最小二乘推导，我们最终分别得到了$b_u$和$b_i$的表达式，但他们的表达式中却又各自包含对方，因此这里我们将利用一种叫交替最小二乘的方法来计算他们的值： 计算其中一项，先固定其他未知参数，即看作其他未知参数为已知 如求$b_u$时，将$b_i$看作是已知；求$b_i$时，将$b_u$看作是已知；如此反复交替，不断更新二者的值，求得最终的结果。这就是交替最小二乘法（ALS） step 3: 算法实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import pandas as pdimport numpy as npclass BaselineCFByALS(object): def __init__(self, number_epochs, reg_bu, reg_bi, columns=[&quot;uid&quot;, &quot;iid&quot;, &quot;rating&quot;]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # bu的正则参数 self.reg_bu = reg_bu # bi的正则参数 self.reg_bi = reg_bi # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): &#x27;&#x27;&#x27; :param dataset: uid, iid, rating :return: &#x27;&#x27;&#x27; self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.als() def als(self): &#x27;&#x27;&#x27; 利用随机梯度下降，优化bu，bi的值 :return: bu, bi &#x27;&#x27;&#x27; # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print(&quot;iter%d&quot; % i) for iid, uids, ratings in self.items_ratings.itertuples(index=True): _sum = 0 for uid, rating in zip(uids, ratings): _sum += rating - self.global_mean - bu[uid] bi[iid] = _sum / (self.reg_bi + len(uids)) for uid, iids, ratings in self.users_ratings.itertuples(index=True): _sum = 0 for iid, rating in zip(iids, ratings): _sum += rating - self.global_mean - bi[iid] bu[uid] = _sum / (self.reg_bu + len(iids)) return bu, bi def predict(self, uid, iid): predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_ratingif __name__ == &#x27;__main__&#x27;: dtype = [(&quot;userId&quot;, np.int32), (&quot;movieId&quot;, np.int32), (&quot;rating&quot;, np.float32)] dataset = pd.read_csv(&quot;datasets/ml-latest-small/ratings.csv&quot;, usecols=range(3), dtype=dict(dtype)) bcf = BaselineCFByALS(20, 25, 15, [&quot;userId&quot;, &quot;movieId&quot;, &quot;rating&quot;]) bcf.fit(dataset) while True: uid = int(input(&quot;uid: &quot;)) iid = int(input(&quot;iid: &quot;)) print(bcf.predict(uid, iid)) Step 4: 准确性指标评估123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174import pandas as pdimport numpy as npdef data_split(data_path, x=0.8, random=False): &#x27;&#x27;&#x27; 切分数据集， 这里为了保证用户数量保持不变，将每个用户的评分数据按比例进行拆分 :param data_path: 数据集路径 :param x: 训练集的比例，如x=0.8，则0.2是测试集 :param random: 是否随机切分，默认False :return: 用户-物品评分矩阵 &#x27;&#x27;&#x27; print(&quot;开始切分数据集...&quot;) # 设置要加载的数据字段的类型 dtype = &#123;&quot;userId&quot;: np.int32, &quot;movieId&quot;: np.int32, &quot;rating&quot;: np.float32&#125; # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分 ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3)) testset_index = [] # 为了保证每个用户在测试集和训练集都有数据，因此按userId聚合 for uid in ratings.groupby(&quot;userId&quot;).any().index: user_rating_data = ratings.where(ratings[&quot;userId&quot;]==uid).dropna() if random: # 因为不可变类型不能被 shuffle方法作用，所以需要强行转换为列表 index = list(user_rating_data.index) np.random.shuffle(index) # 打乱列表 _index = round(len(user_rating_data) * x) testset_index += list(index[_index:]) else: # 将每个用户的x比例的数据作为训练集，剩余的作为测试集 index = round(len(user_rating_data) * x) testset_index += list(user_rating_data.index.values[index:]) testset = ratings.loc[testset_index] trainset = ratings.drop(testset_index) print(&quot;完成数据集切分...&quot;) return trainset, testsetdef accuray(predict_results, method=&quot;all&quot;): &#x27;&#x27;&#x27; 准确性指标计算方法 :param predict_results: 预测结果，类型为容器，每个元素是一个包含uid,iid,real_rating,pred_rating的序列 :param method: 指标方法，类型为字符串，rmse或mae，否则返回两者rmse和mae :return: &#x27;&#x27;&#x27; def rmse(predict_results): &#x27;&#x27;&#x27; rmse评估指标 :param predict_results: :return: rmse &#x27;&#x27;&#x27; length = 0 _rmse_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 return round(np.sqrt(_rmse_sum / length), 4) def mae(predict_results): &#x27;&#x27;&#x27; mae评估指标 :param predict_results: :return: mae &#x27;&#x27;&#x27; length = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _mae_sum += abs(pred_rating - real_rating) return round(_mae_sum / length, 4) def rmse_mae(predict_results): &#x27;&#x27;&#x27; rmse和mae评估指标 :param predict_results: :return: rmse, mae &#x27;&#x27;&#x27; length = 0 _rmse_sum = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 _mae_sum += abs(pred_rating - real_rating) return round(np.sqrt(_rmse_sum / length), 4), round(_mae_sum / length, 4) if method.lower() == &quot;rmse&quot;: rmse(predict_results) elif method.lower() == &quot;mae&quot;: mae(predict_results) else: return rmse_mae(predict_results)class BaselineCFByALS(object): def __init__(self, number_epochs, reg_bu, reg_bi, columns=[&quot;uid&quot;, &quot;iid&quot;, &quot;rating&quot;]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # bu的正则参数 self.reg_bu = reg_bu # bi的正则参数 self.reg_bi = reg_bi # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): &#x27;&#x27;&#x27; :param dataset: uid, iid, rating :return: &#x27;&#x27;&#x27; self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.als() def als(self): &#x27;&#x27;&#x27; 利用随机梯度下降，优化bu，bi的值 :return: bu, bi &#x27;&#x27;&#x27; # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print(&quot;iter%d&quot; % i) for iid, uids, ratings in self.items_ratings.itertuples(index=True): _sum = 0 for uid, rating in zip(uids, ratings): _sum += rating - self.global_mean - bu[uid] bi[iid] = _sum / (self.reg_bi + len(uids)) for uid, iids, ratings in self.users_ratings.itertuples(index=True): _sum = 0 for iid, rating in zip(iids, ratings): _sum += rating - self.global_mean - bi[iid] bu[uid] = _sum / (self.reg_bu + len(iids)) return bu, bi def predict(self, uid, iid): &#x27;&#x27;&#x27;评分预测&#x27;&#x27;&#x27; if iid not in self.items_ratings.index: raise Exception(&quot;无法预测用户&lt;&#123;uid&#125;&gt;对电影&lt;&#123;iid&#125;&gt;的评分，因为训练集中缺失&lt;&#123;iid&#125;&gt;的数据&quot;.format(uid=uid, iid=iid)) predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_rating def test(self,testset): &#x27;&#x27;&#x27;预测测试集数据&#x27;&#x27;&#x27; for uid, iid, real_rating in testset.itertuples(index=False): try: pred_rating = self.predict(uid, iid) except Exception as e: print(e) else: yield uid, iid, real_rating, pred_ratingif __name__ == &#x27;__main__&#x27;: trainset, testset = data_split(&quot;datasets/ml-latest-small/ratings.csv&quot;, random=True) bcf = BaselineCFByALS(20, 25, 15, [&quot;userId&quot;, &quot;movieId&quot;, &quot;rating&quot;]) bcf.fit(trainset) pred_results = bcf.test(testset) rmse, mae = accuray(pred_results) print(&quot;rmse: &quot;, rmse, &quot;mae: &quot;, mae) 函数求导：","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"10-基于K最近邻的协同过滤推荐","slug":"10-基于K最近邻的协同过滤推荐","date":"2021-06-16T16:36:51.000Z","updated":"2021-06-16T16:37:20.354Z","comments":true,"path":"20210617/10-基于K最近邻的协同过滤推荐.html","link":"","permalink":"https://xxren8218.github.io/20210617/10-%E5%9F%BA%E4%BA%8EK%E6%9C%80%E8%BF%91%E9%82%BB%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90.html","excerpt":"","text":"基于K最近邻的协同过滤推荐基于K最近邻的协同过滤推荐其实本质上就是MemoryBased CF，只不过在选取近邻的时候，加上K最近邻的限制。 这里我们直接根据MemoryBased CF的代码实现 修改以下地方 123456789101112131415class CollaborativeFiltering(object): based = None def __init__(self, k=40, rules=None, use_cache=False, standard=None): &#x27;&#x27;&#x27; :param k: 取K个最近邻来进行预测 :param rules: 过滤规则，四选一，否则将抛异常：&quot;unhot&quot;, &quot;rated&quot;, [&quot;unhot&quot;,&quot;rated&quot;], None :param use_cache: 相似度计算结果是否开启缓存 :param standard: 评分标准化方法，None表示不使用、mean表示均值中心化、zscore表示Z-Score标准化 &#x27;&#x27;&#x27; self.k = 40 self.rules = rules self.use_cache = use_cache self.standard = standard 修改所有的选取近邻的地方的代码，根据相似度来选取K个最近邻 123similar_users = self.similar[uid].drop([uid]).dropna().sort_values(ascending=False)[:self.k]similar_items = self.similar[iid].drop([iid]).dropna().sort_values(ascending=False)[:self.k] 但由于我们的原始数据较少，这里我们的KNN方法的效果会比纯粹的MemoryBasedCF要差","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"09-基于模型的协同过滤推荐","slug":"09-基于模型的协同过滤推荐","date":"2021-06-16T16:35:27.000Z","updated":"2021-06-16T16:36:06.886Z","comments":true,"path":"20210617/09-基于模型的协同过滤推荐.html","link":"","permalink":"https://xxren8218.github.io/20210617/09-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90.html","excerpt":"","text":"Model-Based 协同过滤算法随着机器学习技术的逐渐发展与完善，推荐系统也逐渐运用机器学习的思想来进行推荐。将机器学习应用到推荐系统中的方案真是不胜枚举。以下对Model-Based CF算法做一个大致的分类： 基于分类算法、回归算法、聚类算法 基于矩阵分解的推荐 基于神经网络算法（不需要做特征） 基于图模型算法 接下来我们重点学习以下几种应用较多的方案： 基于K最近邻的协同过滤推荐 基于回归模型的协同过滤推荐 基于矩阵分解的协同过滤推荐","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"二分查找算法详解","slug":"二分查找算法详解","date":"2021-06-15T10:34:40.000Z","updated":"2021-06-17T10:14:48.972Z","comments":true,"path":"20210615/二分查找算法详解.html","link":"","permalink":"https://xxren8218.github.io/20210615/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3.html","excerpt":"","text":"二分查找算法详解先给大家讲个笑话乐呵一下： 有一天阿东到图书馆借了 N 本书，出图书馆的时候，警报响了，于是保安把阿东拦下，要检查一下哪本书没有登记出借。阿东正准备把每一本书在报警器下过一下，以找出引发警报的书，但是保安露出不屑的眼神：你连二分查找都不会吗？于是保安把书分成两堆，让第一堆过一下报警器，报警器响；于是再把这堆书分成两堆…… 最终，检测了 logN 次之后，保安成功的找到了那本引起警报的书，露出了得意和嘲讽的笑容。于是阿东背着剩下的书走了。 从此，图书馆丢了 N - 1 本书。 Although the basic idea of binary search is comparatively straightforward, the details can be surprisingly tricky.. .这句话可以这样理解：思路很简单，细节是魔鬼。 本文就来探究几个最常用的二分查找场景：寻找一个数、寻找左侧边界、寻找右侧边界。 而且，我们就是要深入细节，比如不等号是否应该带等号，mid 是否应该加一等等。分析这些细节的差异以及出现这些差异的原因，保证你能灵活准确地写出正确的二分查找算法。 零、二分查找框架 1234567891011def binarySearch(nums, target): left, right = 0, ... while ...: mid = (left + right) / 2 if nums[mid] == target: ... elif nums[mid] &lt; right: left = ... elif nums[mid] &gt; right: right = ... return ... 分析二分查找的一个技巧是：不要出现 else，而是把所有情况用 elif 写清楚，这样可以清楚地展现所有细节。本文都会使用 elif，旨在讲清楚，读者理解后可自行简化。 其中…标记的部分，就是可能出现细节问题的地方，当你见到一个二分查找的代码时，首先注意这几个地方。后文用实例分析这些地方能有什么样的变化。 另外声明一下，计算 mid 时需要技巧防止溢出，本文暂时忽略这个问题。 一、寻找一个数（基本的二分搜索） 这个场景是最简单的，可能也是大家最熟悉的，即搜索一个数，如果存在，返回其索引，否则返回 -1。 12345678910111213def binarySearch(nums, target): left = 0 right = len(nums) - 1 # 注意 while left &lt;= right: # 注意 mid = (left + right) / 2 if nums[mid] == target: return mid elif nums[mid] &lt; right: left = mid + 1 # 注意 elif nums[mid] &gt; right: right = mid - 1 # 注意 return -1 $\\color{red}{1.}$ 为什么 while 循环的条件中是 &lt;=，而不是 &lt; ？ 答：因为初始化 right 的赋值是 len(nums) - 1，即最后一个元素的索引，而不是 len(nums)。 这二者可能出现在不同功能的二分查找中，区别是：前者相当于两端都闭区间 [left, right]，后者相当于左闭右开区间 [left, right)，因为索引大小为 len(nums) 是越界的。 我们这个算法中使用的是 [left, right] 两端都闭的区间。这个区间就是每次进行搜索的区间，我们不妨称为「搜索区间」 什么时候应该停止搜索呢？当然，找到了目标值的时候可以终止： 12if nums[mid] == target: return mid 但如果没找到，就需要 while 循环终止，然后返回 -1。那 while 循环什么时候应该终止？搜索区间为空的时候应该终止，意味着你没得找了，就等于没找到嘛。 $\\color{red}{while (left &lt;= right)}$ 的终止条件是 left == right + 1，写成区间的形式就是 [right + 1, right]，或者带个具体的数字进去 [3, 2]，可见这时候搜索区间为空，因为没有数字既大于等于 3 又小于等于 2 的吧。所以这时候 while 循环终止是正确的，直接返回 -1 即可。 $\\color{red}{while (left &lt; right)}$ 的终止条件是 left == right，写成区间的形式就是 [right, right]，或者带个具体的数字进去 [2, 2]，这时候搜索区间非空，还有一个数 2，但此时 while 循环终止了。也就是说这区间 [2, 2] 被漏掉了，索引 2 没有被搜索，如果这时候直接返回 -1 就可能出现错误。 当然，如果你非要用 while(left &lt; right) 也可以，我们已经知道了出错的原因，就打个补丁好了。 1234# ...while left &lt; right: # ...return left if nums[mid] == target else -1 $\\color{red}{2.}$为什么 left = mid + 1，right = mid - 1？我看有的代码是 right = mid 或者 left = mid，没有这些加加减减，到底怎么回事，怎么判断？ 答：这也是二分查找的一个难点，不过只要你能理解前面的内容，就能够很容易判断。 刚才明确了「搜索区间」这个概念，而且本算法的搜索区间是两端都闭的，即 [left, right]。那么当我们发现索引 mid 不是要找的 target 时，如何确定下一步的搜索区间呢？ 当然是去搜索 [left, mid - 1] 或者 [mid + 1, right] 对不对？因为 mid 已经搜索过，应该从搜索区间中去除。 $\\color{red}{3.}$ 此算法有什么缺陷？ 答：至此，你应该已经掌握了该算法的所有细节，以及这样处理的原因。但是，这个算法存在局限性。 比如说给你有序数组 nums = [1,2,2,2,3]，target = 2，此算法返回的索引是 2，没错。但是如果我想得到 target 的左侧边界，即索引 1，或者我想得到 target 的右侧边界，即索引 3，这样的话此算法是无法处理的。 这样的需求很常见。你也许会说，找到一个 target 索引，然后向左或向右线性搜索不行吗？可以，但是不好，因为这样难以保证二分查找对数级的复杂度了。 我们后续的算法就来讨论这两种二分查找的算法。 二、寻找左侧边界的二分搜索 直接看代码，其中的标记是需要注意的细节： 12345678910111213141516171819202122def left_bound(nums, target): n = len(nums) if n == 0: return -1 left, right = 0, n # 注意 while left &lt; right: # 注意 mid = (left + right) / 2 if nums[mid] == target: right = mid elif nums[mid] &lt; target: left = mid + 1 elif nums[mid] &gt; target: right = mid # 注意 &quot;&quot;&quot; # target比所有数都大 if left == len(nums) return -1 # 类似之前算法的处理方式 return left if nums[left] == target else -1 &quot;&quot;&quot; return left $\\color{red}{1.}$ 为什么 while 循环的条件中是 &lt;，而不是 &lt;= ？ 答：用相同的方法分析，因为初始化 right = len(nums) 而不是 len(nums) - 1 。因此每次循环的「搜索区间」是 [left, right) 左闭右开。 while(left &lt; right) 终止的条件是 left == right，此时搜索区间 [left, left) 恰巧为空，所以可以正确终止。 $\\color{red}{2.}$为什么没有返回 -1 的操作？如果 nums 中不存在 target 这个值，怎么办？ 答：因为要一步一步来，先理解一下这个「左侧边界」有什么特殊含义： 对于这个数组，算法会返回 1。这个 1 的含义可以这样解读：nums 中小于 2 的元素有 1 个。 比如对于有序数组 nums = [2,3,5,7], target = 1，算法会返回 0，含义是：nums 中小于 1 的元素有 0 个。如果 target = 8，算法会返回 4，含义是：nums 中小于 8 的元素有 4 个。 综上可以看出，函数的返回值（即 left 变量的值）取值区间是闭区间 [0, len(nums)]，所以我们简单添加两行代码就能在正确的时候 return -1： 123456while left &lt; right: # ...# target比所有数都大if left == len(nums) return -1# 类似之前算法的处理方式return left if nums[left] == target else -1 $\\color{red}{3.}$为什么 left = mid + 1，right = mid ？和之前的算法不一样？ 答：这个很好解释，因为我们的「搜索区间」是 [left, right) 左闭右开，所以当 nums[mid] 被检测之后，下一步的搜索区间应该去掉 mid 分割成两个区间，即 [left, mid) 或 [mid + 1, right)。 $\\color{red}{4.}$为什么该算法能够搜索左侧边界？ 答：关键在于对于 nums[mid] == target 这种情况的处理： 12if nums[mid] == target: right = mid 可见，找到 target 时不要立即返回，而是缩小「搜索区间」的上界 right，在区间 [left, mid) 中继续搜索，即不断向左收缩，达到锁定左侧边界的目的。 $\\color{red}{5.}$ 为什么返回 left 而不是 right？ 答：都是一样的，因为 while 终止的条件是 left == right。 三、寻找右侧边界的二分查找 寻找右侧边界和寻找左侧边界的代码差不多，只有两处不同，已标注： 1234567891011121314151617181920def right_bound(nums, target): n = len(nums) if n == 0: return -1 left, right = 0, n while left &lt; right: mid = (left + right) / 2 if nums[mid] == target: left = mid + 1 # 注意 elif nums[mid] &lt; target: left = mid + 1 elif nums[mid] &gt; target: right = mid &quot;&quot;&quot; if left == 0: return -1 return left - 1 if nums[left - 1] == target else -1 &quot;&quot;&quot; return left - 1 # 注意 $\\color{red}{1.}$ 为什么这个算法能够找到右侧边界？ 答：类似地，关键点还是这里： 12if nums[mid] == target: left = mid + 1 当 nums[mid] == target 时，不要立即返回，而是增大「搜索区间」的下界 left，使得区间不断向右收缩，达到锁定右侧边界的目的。 $\\color{red}{2.}$ 为什么最后返回 left - 1 而不像左侧边界的函数，返回 left？而且我觉得这里既然是搜索右侧边界，应该返回 right 才对。 答：首先，while 循环的终止条件是 left == right，所以 left 和 right 是一样的，你非要体现右侧的特点，返回 right - 1 好了。 至于为什么要减一，这是搜索右侧边界的一个特殊点，关键在这个条件判断： 123if nums[mid] == target: left = mid + 1 # 这样想：mid = left -1 因为我们对 left 的更新必须是 left = mid + 1，就是说 while 循环结束时，nums[left] 一定不等于 target 了，而 nums[left - 1] 可能是 target。 至于为什么 left 的更新必须是 left = mid + 1，同左侧边界搜索，就不再赘述。 $\\color{red}{3.}$ 为什么没有返回 -1 的操作？如果 nums 中不存在 target 这个值，怎么办？ 答：类似之前的左侧边界搜索，因为 while 的终止条件是 left == right，就是说 left 的取值范围是 [0, len(nums)]，所以可以添加两行代码，正确地返回 -1： 1234while left &lt; right: # ...if left == 0: return -1return left - 1 if nums[left - 1] == target else -1 四、最后总结 先来梳理一下这些细节差异的因果逻辑: 第一个，最基本的二分查找算法： 1234567因为我们初始化 right = len(nums) - 1所以决定了我们的「搜索区间」是 [left, right]所以决定了 while (left &lt;= right)同时也决定了 left = mid + 1 和 right = mid - 1因为我们只需找到一个 target 的索引即可所以当 nums[mid] == target 时可以立即返回 第二个，寻找左侧边界的二分查找： 12345678因为我们初始化 right = len(nums)所以决定了我们的「搜索区间」是 [left, right)所以决定了 while (left &lt; right)同时也决定了 left = mid + 1 和 right = mid因为我们需找到 target 的最左侧索引所以当 nums[mid] == target 时不要立即返回而要收紧右侧边界以锁定左侧边界 第三个，寻找右侧边界的二分查找： 1234567891011因为我们初始化 right = len(nums)所以决定了我们的「搜索区间」是 [left, right)所以决定了 while (left &lt; right)同时也决定了 left = mid + 1 和 right = mid因为我们需找到 target 的最右侧索引所以当 nums[mid] == target 时不要立即返回而要收紧左侧边界以锁定右侧边界又因为收紧左侧边界时必须 left = mid + 1所以最后无论返回 left 还是 right，必须减一 如果以上内容你都能理解，那么恭喜你，二分查找算法的细节不过如此。 通过本文，你学会了： $\\color{blue}{1.}$分析二分查找代码时，不要出现 else，全部展开成 else if 方便理解。 $\\color{blue}{2.}$注意「搜索区间」和 while 的终止条件，如果存在漏掉的元素，记得在最后检查。 $\\color{blue}{3.}$如需要搜索左右边界，只要在 nums[mid] == target 时做修改即可。搜索右侧时需要减一。 就算遇到其他的二分查找变形，运用这几点技巧，也能保证你写出正确的代码。LeetCode Explore 中有二分查找的专项练习，其中提供了三种不同的代码模板，现在你再去看看，很容易就知道这几个模板的实现原理了。","categories":[{"name":"二分查找","slug":"二分查找","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"}],"tags":[]},{"title":"02-动态规划设计之最长递增子序列","slug":"02-动态规划设计之最长递增子序列","date":"2021-06-15T10:28:49.000Z","updated":"2021-06-17T10:15:50.802Z","comments":true,"path":"20210615/02-动态规划设计之最长递增子序列.html","link":"","permalink":"https://xxren8218.github.io/20210615/02-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E8%AE%BE%E8%AE%A1%E4%B9%8B%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97.html","excerpt":"","text":"02-动态规划设计之最长递增子序列很多人了解了动态规划，但还是不会写状态转移方程，没有思路，怎么办？本文就借助「最长递增子序列」来讲一种设计动态规划的通用技巧：数学归纳思想。 最长递增子序列（Longest Increasing Subsequence，简写 LIS）是比较经典的一个问题，比较容易想到的是动态规划解法，时间复杂度 O(N^2)，我们借这个问题来由浅入深讲解如何写动态规划。 比较难想到的是利用二分查找，时间复杂度是 O(NlogN)，我们通过一种简单的纸牌游戏来辅助理解这种巧妙的解法。 先看一下题目，很容易理解： 注意「子序列」和「子串」这两个名词的区别，子串一定是连续的，而子序列不一定是连续的。下面先来一步一步设计动态规划算法解决这个问题。 一、动态规划解法 动态规划的核心设计思想是数学归纳法。 相信大家对数学归纳法都不陌生，高中就学过，而且思路很简单。比如我们想证明一个数学结论，那么我们先假设这个结论在 k &lt; n 时成立，然后想办法证明 k=n 的时候此结论也成立。如果能够证明出来，那么就说明这个结论对于 k 等于任何数都成立。 类似的，我们设计动态规划算法，不是需要一个 dp 数组吗？我们可以假设 dp[0…i−1] 都已经被算出来了，然后问自己：怎么通过这些结果算出dp[i] ? 直接拿最长递增子序列这个问题举例你就明白了。不过，首先要定义清楚 dp 数组的含义，即 dp[i] 的值到底代表着什么？ 我们的定义是这样的：dp[i] 表示以 nums[i] 这个数结尾的最长递增子序列的长度。 举个例子： 算法演进的过程是这样的: 根据这个定义，我们的最终结果（子序列的最大长度）应该是 dp 数组中的最大值。 1return max(dp) 读者也许会问，刚才这个过程中每个 dp[i] 的结果是我们肉眼看出来的，我们应该怎么设计算法逻辑来正确计算每个 dp[i] 呢？ 这就是动态规划的重头戏了，要思考如何进行状态转移，这里就可以使用数学归纳的思想 我们已经知道了 dp[0…4] 的所有结果，我们如何通过这些已知结果推出 dp[5]呢？ 根据刚才我们对 dp 数组的定义，现在想求 dp[5] 的值，也就是想求以 nums[5] 为结尾的最长递增子序列。 nums[5] = 3，既然是递增子序列，我们只要找到前面那些结尾比 3 小的子序列，然后把 3 接到最后，就可以形成一个新的递增子序列，而且这个新的子序列长度加一。 当然，可能形成很多种新的子序列，但是我们只要最长的，把最长子序列的长度作为 dp[5] 的值即可。 123for j in range(i): if nums[j] &lt; nums[i]: dp[i] = max(dp[i], dp[j]+1) 这段代码的逻辑就可以算出 dp[5]。到这里，这道算法题我们就基本做完了。读者也许会问，我们刚才只是算了 dp[5] 呀，dp[4], dp[3] 这些怎么算呢？ 类似数学归纳法，你已经可以通过 dp[0…4] 算出 dp[5] 了，那么任意 dp[i] 你肯定都可以算出来: 1234for i in range(len(nums)): for j in range(i): if nums[j] &lt; nums[i]: dp[i] = max(dp[i], dp[j]+1) 还有一个细节问题，就是 base case。dp 数组应该全部初始化为 1，因为子序列最少也要包含自己，所以长度最小为 1。下面我们看一下完整代码： 123456789def lengthOfLIS(nums): n = len(nums) dp = [1]*n for i in range(n): for j in range(i): if nums[j] &lt; nums[i]: dp[i] = max(dp[i], dp[j]+1) return max(dp) 至此，这道题就解决了，时间复杂度 O(N^2)。总结一下动态规划的设计流程： 首先明确 dp 数组所存数据的含义。这步很重要，如果不得当或者不够清晰，会阻碍之后的步骤。 然后根据 dp 数组的定义，运用数学归纳法的思想，假设 dp[0…i−1] 都已知，想办法求出 dp[i]，一旦这一步完成，整个题目基本就解决了。 但如果无法完成这一步，很可能就是 dp 数组的定义不够恰当，需要重新定义 dp 数组的含义；或者可能是 dp 数组存储的信息还不够，不足以推出下一步的答案，需要把 dp 数组扩大成二维数组甚至三维数组。 二、二分查找解法 这个解法的时间复杂度会将为 O(NlogN)，但是说实话，正常人基本想不到这种解法（也许玩过某些纸牌游戏的人可以想出来）。所以如果大家了解一下就好，正常情况下能够给出动态规划解法就已经很不错了。 根据题目的意思，我都很难想象这个问题竟然能和二分查找扯上关系。其实最长递增子序列和一种叫做 patience game 的纸牌游戏有关，甚至有一种排序方法就叫做 patience sorting（耐心排序）。 为了简单起见，后文跳过所有数学证明，通过一个简化的例子来理解一下思路。 首先，给你一排扑克牌，我们像遍历数组那样从左到右一张一张处理这些扑克牌，最终要把这些牌分成若干堆 处理这些扑克牌要遵循以下规则： 只能把点数小的牌压到点数比它大的牌上。如果当前牌点数较大没有可以放置的堆，则新建一个堆，把这张牌放进去。如果当前牌有多个堆可供选择，则选择最左边的堆放置。 比如说上述的扑克牌最终会被分成这样 5 堆（我们认为 A 的值是最大的，而不是 1）。 为什么遇到多个可选择堆的时候要放到最左边的堆上呢？因为这样可以保证牌堆顶的牌有序（2, 4, 7, 8, Q），证明略。 按照上述规则执行，可以算出最长递增子序列，牌的堆数就是我们想求的最长递增子序列的长度，证明略。 我们只要把处理扑克牌的过程编程写出来即可。每次处理一张扑克牌不是要找一个合适的牌堆顶来放吗，牌堆顶的牌不是有序吗，这就能用到二分查找了：用二分查找来搜索当前牌应放置的位置。 1234567891011121314151617181920212223242526272829def lengthOfLIS(nums): n = len(nums) top = [0]*n # 初始化牌堆数为 0 piles = 0 for i in range(n): # 要处理的扑克牌 poker = nums[i] # 搜索左侧边界的二分查找 left, right = 0, piles while left &lt; right: mid = (left + right) / 2 if top[mid] &gt; poker: right = mid elif top[mid] &lt; poker: left = mid + 1 else: right = mid # 没找到合适的牌堆，新建一个堆 if left == piles: piles += 1 # 把这张牌放到牌堆顶 top[left] = poker # 牌堆数就是LIS的长度 return piles 至此，二分查找的解法也讲解完毕。 这个解法确实很难想到。首先涉及数学证明，谁能想到按照这些规则执行，就能得到最长递增子序列呢？其次还有二分查找的运用，要是对二分查找的细节不清楚，给了思路也很难写对。 所以，这个方法作为思维拓展好了。但动态规划的设计方法应该完全理解：假设之前的答案已知，利用数学归纳的思想正确进行状态的推演转移，最终得到答案。","categories":[{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"01-初识动态规划","slug":"01-初识动态规划","date":"2021-06-11T10:41:34.000Z","updated":"2021-06-17T10:12:50.073Z","comments":true,"path":"20210611/01-初识动态规划.html","link":"","permalink":"https://xxren8218.github.io/20210611/01-%E5%88%9D%E8%AF%86%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92.html","excerpt":"","text":"01-初识动态规划动态规划问题的一般形式就是求最值。动态规划其实是运筹学的一种最优化方法，只不过在计算机问题上应用比较多，比如说让你求最长递增子序列呀，最小编辑距离呀等等。 既然是要求最值，核心问题是什么呢？求解动态规划的核心问题是穷举。因为要求最值，肯定要把所有可行的答案穷举出来，然后在其中找最值呗。动态规划就这么简单，就是穷举就完事了？我看到的动态规划问题都很难啊！ 首先，动态规划的穷举有点特别，因为这类问题存在「重叠子问题」，如果暴力穷举的话效率会极其低下，所以需要「备忘录」或者「DP table」来优化穷举过程，避免不必要的计算。 而且，动态规划问题一定会具备「最优子结构」，才能通过子问题的最值得到原问题的最值。 另外，虽然动态规划的核心思想就是穷举求最值，但是问题可以千变万化，穷举所有可行解其实并不是一件容易的事，只有列出正确的「状态转移方程」才能正确地穷举。 以上提到的重叠子问题、最优子结构、状态转移方程就是动态规划三要素。具体什么意思等会会举例详解，但是在实际的算法问题中，写出状态转移方程是最困难的，这也就是为什么很多朋友觉得动态规划问题困难的原因，我来提供我研究出来的一个思维框架，辅助你思考状态转移方程： 明确「状态」 -&gt; 定义 dp 数组/函数的含义 -&gt; 明确「选择」-&gt; 明确 base case。 下面通过斐波那契数列问题和凑零钱问题来详解动态规划的基本原理。前者主要是让你明白什么是重叠子问题（斐波那契数列严格来说不是动态规划问题），后者主要集中于如何列出状态转移方程。 一、 斐波那契数列 1.暴力递归斐波那契数列的数学形式就是递归的，写成代码就是这样： 123def fib(N): if (N == 1 and N == 2): return 1 return fib(N - 1) + fib(N - 2) 这个不用多说了，学校老师讲递归的时候似乎都是拿这个举例。我们也知道这样写代码虽然简洁易懂，但是十分低效，低效在哪里？假设 n = 20，请画出递归树。 PS：但凡遇到需要递归的问题，最好都画出递归树，这对你分析算法的复杂度，寻找算法低效的原因都有巨大帮助。 这个递归树怎么理解？就是说想要计算原问题f(20)，我就得先计算出子问题f(19)和f(18)，然后要计算f(19)，我就要先算出子问题f(18)和f(17)，以此类推。最后遇到f(1)或者f(2)的时候，结果已知，就能直接返回结果，递归树不再向下生长了。 递归算法的时间复杂度怎么计算？子问题个数乘以解决一个子问题需要的时间。 子问题个数，即递归树中节点的总数。显然二叉树节点总数为指数级别，所以子问题个数为 O(2^n)。 解决一个子问题的时间，在本算法中，没有循环，只有 f(n - 1) + f(n - 2) 一个加法操作，时间为 O(1)。 所以，这个算法的时间复杂度为 O(2^n)，指数级别，爆炸。 观察递归树，很明显发现了算法低效的原因：存在大量重复计算，比如f(18)被计算了两次，而且你可以看到，以f(18)为根的这个递归树体量巨大，多算一遍，会耗费巨大的时间。更何况，还不止f(18)这一个节点被重复计算，所以这个算法及其低效。 这就是动态规划问题的第一个性质：重叠子问题。下面，我们想办法解决这个问题。 2.带备忘录的递归解法明确了问题，其实就已经把问题解决了一半。即然耗时的原因是重复计算，那么我们可以造一个「备忘录」，每次算出某个子问题的答案后别急着返回，先记到「备忘录」里再返回；每次遇到一个子问题先去「备忘录」里查一查，如果发现之前已经解决过这个问题了，直接把答案拿出来用，不要再耗时去计算了。 一般使用一个数组充当这个「备忘录」，当然你也可以使用哈希表（字典），思想都是一样的。 1234567891011121314def fib(N): if N &lt; 1: return 0 # 备忘录全初始化为 0 memo = [0]*(N+1) # 初始化最简情况 return helper(memo, N)def helper(memo, n): # base case if n == 1 or n == 2: return 1 # 已经计算过 if memo[n] != 0: return memo[n] memo[n] = helper(memo, n - 1) + helper(memo, n - 2) return memo[n] 现在，画出递归树，你就知道「备忘录」到底做了什么： 实际上，带「备忘录」的递归算法，把一棵存在巨量冗余的递归树通过「剪枝」，改造成了一幅不存在冗余的递归图，极大减少了子问题（即递归图中节点）的个数。 递归算法的时间复杂度怎么算？子问题个数乘以解决一个子问题需要的时间。 子问题个数，即图中节点的总数，由于本算法不存在冗余计算，子问题就是f(1),f(2),f(3)…f(20)，数量和输入规模 n = 20 成正比，所以子问题个数为 O(n)。 解决一个子问题的时间，同上，没有什么循环，时间为 O(1)。 所以，本算法的时间复杂度是 O(n)。比起暴力算法，是降维打击。 至此，带备忘录的递归解法的效率已经和迭代的动态规划一样了。实际上，这种解法和迭代的动态规划思想已经差不多，只不过这种方法叫做「自顶向下」，动态规划叫做「自底向上」。 啥叫「自顶向下」？注意我们刚才画的递归树（或者说图），是从上向下延伸，都是从一个规模较大的原问题比如说f(20)，向下逐渐分解规模，直到f(1)和f(2)触底，然后逐层返回答案，这就叫「自顶向下」。 啥叫「自底向上」？反过来，我们直接从最底下，最简单，问题规模最小的f(1)和f(2)开始往上推，直到推到我们想要的答案f(20)，这就是动态规划的思路，这也是为什么动态规划一般都脱离了递归，而是由循环迭代完成计算。 3.dp数组的迭代解法有了上一步「备忘录」的启发，我们可以把这个「备忘录」独立出来成为一张表，就叫做 DP table 吧，在这张表上完成「自底向上」的推算岂不美哉！ 1234567def fib(N): dp = [0]*(N + 1) # base case dp[1] = dp[2] = 1; for i in range (3, N + 1): dp[i] = dp[i - 1] + dp[i - 2] return dp[N] 画个图就很好理解了，而且你发现这个 DP table 特别像之前那个「剪枝」后的结果，只是反过来算而已。实际上，带备忘录的递归解法中的「备忘录」，最终完成后就是这个 DP table，所以说这两种解法其实是差不多的，大部分情况下，效率也基本相同。 这里，引出「状态转移方程」这个名词，实际上就是描述问题结构的数学形式： 为啥叫「状态转移方程」？为了听起来高端。你把 f(n) 想做一个状态 n，这个状态 n 是由状态 n - 1 和状态 n - 2 相加转移而来，这就叫状态转移，仅此而已。 你会发现，上面的几种解法中的所有操作，例如 return f(n - 1) + f(n - 2)，dp[i] = dp[i - 1] + dp[i - 2]，以及对备忘录或 DP table 的初始化操作，都是围绕这个方程式的不同表现形式。可见列出「状态转移方程」的重要性，它是解决问题的核心。很容易发现，其实状态转移方程直接代表着暴力解法。 千万不要看不起暴力解，动态规划问题最困难的就是写出状态转移方程，即这个暴力解。优化方法无非是用备忘录或者 DP table，再无奥妙可言。 这个例子的最后，讲一个细节优化。细心的读者会发现，根据斐波那契数列的状态转移方程，当前状态只和之前的两个状态有关，其实并不需要那么长的一个 DP table 来存储所有的状态，只要想办法存储之前的两个状态就行了。所以，可以进一步优化，把空间复杂度降为 O(1)： 12345678def fib(N): if N == 1 or N == 2:return 1 prev, cur, add = 1, 1, 0 for i in range(3, N + 1): add = prev + cur prev = cur cur = add return cur 有人会问，动态规划的另一个重要特性「最优子结构」，怎么没有涉及？下面会涉及。斐波那契数列的例子严格来说不算动态规划，因为没有涉及求最值，以上旨在演示算法设计螺旋上升的过程。 下面，看第二个例子，凑零钱问题。 二、凑零钱问题 先看下题目：给你k种面值的硬币，面值分别为c1, c2 ... ck，每种硬币的数量无限，再给一个总金额amount，问你最少需要几枚硬币凑出这个金额，如果不可能凑出，算法返回 -1 。算法的函数签名如下： 12345678# coins 中是可选硬币面值，amount 是目标金额class Solution(object): def coinChange(self, coins, amount): &quot;&quot;&quot; :type coins: List[int] :type amount: int :rtype: int &quot;&quot;&quot; 比如说k = 3，面值分别为 1，2，5，总金额amount = 11。那么最少需要 3 枚硬币凑出，即 11 = 5 + 5 + 1。 你认为计算机应该如何解决这个问题？显然，就是把所有肯能的凑硬币方法都穷举出来，然后找找看最少需要多少枚硬币。 1.暴力递归首先，这个问题是动态规划问题，因为它具有「最优子结构」。要符合「最优子结构」，子问题间必须互相独立。啥叫相互独立？你肯定不想看数学证明，我用一个直观的例子来讲解。 比如说，你的原问题是考出最高的总成绩，那么你的子问题就是要把语文考到最高，数学考到最高…… 为了每门课考到最高，你要把每门课相应的选择题分数拿到最高，填空题分数拿到最高…… 当然，最终就是你每门课都是满分，这就是最高的总成绩。 得到了正确的结果：最高的总成绩就是总分。因为这个过程符合最优子结构，“每门科目考到最高”这些子问题是互相独立，互不干扰的。 但是，如果加一个条件：你的语文成绩和数学成绩会互相制约，此消彼长。这样的话，显然你能考到的最高总成绩就达不到总分了，按刚才那个思路就会得到错误的结果。因为子问题并不独立，语文数学成绩无法同时最优，所以最优子结构被破坏。 回到凑零钱问题，为什么说它符合最优子结构呢？比如你想求amount = 11时的最少硬币数（原问题），如果你知道凑出amount = 10的最少硬币数（子问题），你只需要把子问题的答案加一（再选一枚面值为 1 的硬币）就是原问题的答案，因为硬币的数量是没有限制的，子问题之间没有相互制，是互相独立的。 那么，既然知道了这是个动态规划问题，就要思考如何列出正确的状态转移方程。 先确定「状态」，也就是原问题和子问题中变化的变量。由于硬币数量无限，所以唯一的状态就是目标金额amount。 然后确定dp函数的定义：函数 dp(n)表示，当前的目标金额是n，至少需要dp(n)个硬币凑出该金额。 然后确定「选择」并择优，也就是对于每个状态，可以做出什么选择改变当前状态。具体到这个问题，无论当的目标金额是多少，选择就是从面额列表coins中选择一个硬币，然后目标金额就会减少： 123456789def coinChange(coins, amount): # 定义：要凑出金额 n，至少要 dp(n) 个硬币 def dp(n): # 做选择，需要硬币最少的那个结果就是答案 for coin in coins: res = min(res, 1 + dp(n - coin)) return res # 我们要求目标金额是 amount return dp(amount) 最后明确 base case，显然目标金额为 0 时，所需硬币数量为 0；当目标金额小于 0 时，无解，返回 -1： 12345678910111213141516def coinChange(coins, amount): def dp(n): # base case if n == 0: return 0 if n &lt; 0: return -1 # 求最小值，所以初始化为正无穷 res = float(&#x27;INF&#x27;) for coin in coins: subproblem = dp(n - coin) # 子问题无解，跳过 if subproblem == -1: continue res = min(res, 1 + subproblem) return res if res != float(&#x27;INF&#x27;) else -1 return dp(amount) 至此，状态转移方程其实已经完成了，以上算法已经是暴力解法了，以上代码的数学形式就是状态转移方程： 至此，这个问题其实就解决了，只不过需要消除一下重叠子问题，比如amount = 11, coins = &#123;1,2,5&#125;时画出递归树看看： 时间复杂度分析：子问题总数 x 解决每个子问题的时间。 子问题总数为递归树节点个数，这个比较难看出来，是 O(n^k)，总之是指数级别的。每个子问题中含有一个 for 循环，复杂度为 O(k)。所以总时间复杂度为 O(k * n^k)，指数级别。 2.带备忘录的递归只需要稍加修改，就可以通过备忘录消除子问题： 1234567891011121314151617181920def coinChange(coins, amount): # 备忘录 memo = dict() def dp(n): # 查备忘录，避免重复计算 if n in memo: return memo[n] if n == 0: return 0 if n &lt; 0: return -1 res = float(&#x27;INF&#x27;) for coin in coins: subproblem = dp(n - coin) if subproblem == -1: continue res = min(res, 1 + subproblem) # 记入备忘录 memo[n] = res if res != float(&#x27;INF&#x27;) else -1 return memo[n] return dp(amount) 很显然「备忘录」大大减小了子问题数目，完全消除了子问题的冗余，所以子问题总数不会超过金额数 n，即子问题数目为 O(n)。处理一个子问题的时间不变，仍是 O(k)，所以总的时间复杂度是 O(kn)。 3.dp数组的迭代解法当然，我们也可以自底向上使用 dp table 来消除重叠子问题，dp数组的定义和刚才dp函数类似，定义也是一样的： dp[i] = x表示，当目标金额为i时，至少需要x枚硬币。 123456789101112def coinChange(coins, amount): # 数组大小为 amount + 1，初始值也为 amount + 1 dp = [amount + 1]*(amount + 1) # base case dp[0] = 0 for i in range(len(dp)): # 内层 for 在求所有子问题 + 1 的最小值 for coin in coins: # 子问题无解，跳过 if (i - coin &lt; 0): continue dp[i] = min(dp[i], 1 + dp[i - coin]) return dp[amount] if dp[amount] != amount + 1 else -1 PS：为啥dp数组初始化为amount + 1呢，因为凑成amount金额的硬币数最多只可能等于amount（全用 1 元面值的硬币），所以初始化为amount + 1就相当于初始化为正无穷，便于后续取最小值。 三、最后总结 第一个斐波那契数列的问题，解释了如何通过「备忘录」或者「dp table」的方法来优化递归树，并且明确了这两种方法本质上是一样的，只是自顶向下和自底向上的不同而已。 第二个凑零钱的问题，展示了如何流程化确定「状态转移方程」，只要通过状态转移方程写出暴力递归解，剩下的也就是优化递归树，消除重叠子问题而已。 如果你不太了解动态规划，还能看到这里，真得给你鼓掌，相信你已经掌握了这个算法的设计技巧。 计算机解决问题其实没有任何奇技淫巧，它唯一的解决办法就是穷举，穷举所有可能性。算法设计无非就是先思考“如何穷举”，然后再追求“如何聪明地穷举”。 列出动态转移方程，就是在解决“如何穷举”的问题。之所以说它难，一是因为很多穷举需要递归实现，二是因为有的问题本身的解空间复杂，不那么容易穷举完整。 备忘录、DP table 就是在追求“如何聪明地穷举”。用空间换时间的思路，是降低时间复杂度的不二法门。","categories":[{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"剑指Offer（十六）：合并两个排序的链表","slug":"剑指Offer（十六）：合并两个排序的链表","date":"2021-06-11T10:37:37.000Z","updated":"2021-06-17T10:00:59.123Z","comments":true,"path":"20210611/剑指Offer（十六）：合并两个排序的链表.html","link":"","permalink":"https://xxren8218.github.io/20210611/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9A%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8.html","excerpt":"","text":"1.题目输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 2.思路一先判断输入的链表是否为空的指针。如果第一个链表为空，则直接返回第二个链表；如果第二个链表为空，则直接返回第一个链表。如果两个链表都是空链表，合并的结果是得到一个空链表。 两个链表都是排序好的，我们只需要从头遍历链表，判断当前指针，哪个链表中的值小，即赋给合并链表指针即可。使用递归就可以轻松实现。 3.代码一123456789101112131415161718192021# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if not pHead1: return pHead2 if not pHead2: return pHead1 pMergehead = None if pHead1.val &lt; pHead2.val: pMergehead = pHead1 pMergehead.next = self.Merge(pHead1.next, pHead2) else: pMergehead = pHead2 pMergehead.next = self.Merge(pHead1, pHead2.next) return pMergehead 思路二 代码二12345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def mergeTwoLists(self, l1, l2): &quot;&quot;&quot; :type l1: ListNode :type l2: ListNode :rtype: ListNode &quot;&quot;&quot; l = ListNode(0) cur = dum = l while l1 and l2: if l1.val &lt; l2.val: cur.next = l1 l1 = l1.next else: cur.next = l2 l2 = l2.next cur = cur.next # 其中一个为空时，将另外一个接入即可 cur.next = l1 if l1 else l2 return dum.next","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"多指针","slug":"多指针","permalink":"https://xxren8218.github.io/tags/%E5%A4%9A%E6%8C%87%E9%92%88/"},{"name":"递归","slug":"递归","permalink":"https://xxren8218.github.io/tags/%E9%80%92%E5%BD%92/"},{"name":"链表","slug":"链表","permalink":"https://xxren8218.github.io/tags/%E9%93%BE%E8%A1%A8/"},{"name":"伪头节点","slug":"伪头节点","permalink":"https://xxren8218.github.io/tags/%E4%BC%AA%E5%A4%B4%E8%8A%82%E7%82%B9/"}]},{"title":"剑指Offer（十五）：反转链表","slug":"剑指Offer（十五）：反转链表","date":"2021-06-11T10:35:07.000Z","updated":"2021-06-11T10:48:50.205Z","comments":true,"path":"20210611/剑指Offer（十五）：反转链表.html","link":"","permalink":"https://xxren8218.github.io/20210611/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9A%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8.html","excerpt":"","text":"1.题目输入一个链表，反转链表后，输出链表的所有元素。 2.思路一采用递归的思想：last = reverseList(head.next)，即：将1-&gt;2-&gt;3-&gt;4-&gt;5，变为(last)5-&gt;4-&gt;3-&gt;2&lt;-1(head),然后进行转换即可 3.代码一123456789101112131415161718# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def reverseList(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; if not head:return None if head.next == None:return head last = self.reverseList(head.next) head.next.next = head head.next = None return last 思路二我们使用三个指针，分别指向当前遍历到的结点、它的前一个结点以及后一个结点。在遍历的时候，做当前结点的尾结点和前一个结点的替换。 代码二12345678910111213141516# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回ListNode def ReverseList(self, pHead): # write code here last = None while pHead: tmp = pHead.next pHead.next = last last = pHead pHead = tmp return last","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"多指针","slug":"多指针","permalink":"https://xxren8218.github.io/tags/%E5%A4%9A%E6%8C%87%E9%92%88/"},{"name":"递归","slug":"递归","permalink":"https://xxren8218.github.io/tags/%E9%80%92%E5%BD%92/"},{"name":"链表","slug":"链表","permalink":"https://xxren8218.github.io/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"剑指Offer（十四）：链表中倒数第k个结点","slug":"剑指Offer（十四）：链表中倒数第k个结点","date":"2021-06-11T10:00:55.000Z","updated":"2021-06-11T10:47:48.196Z","comments":true,"path":"20210611/剑指Offer（十四）：链表中倒数第k个结点.html","link":"","permalink":"https://xxren8218.github.io/20210611/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9A%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9.html","excerpt":"","text":"1.题目输入一个链表，输出该链表中倒数第k个结点。 2.思路一建立一个辅助的列表，将链表的值进行存储，然后取出倒数第k个值。 3.代码一12345678910111213141516171819202122# class ListNode:# def __init__(self, x):# self.val = x# self.next = None## 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可## # @param pHead ListNode类 # @param k int整型 # @return ListNode类#class Solution: def FindKthToTail(self , pHead , k ): # write code here l=[] while pHead: l.append(pHead) pHead = pHead.next if len(l) &lt; k or k &lt; 1: return None return l[-k] 思路二我们可以定义两个指针。第一个指针从链表的头指针开始遍历向前走k-1，第二个指针保持不动；从第k步开始，第二个指针也开始从链表的头指针开始遍历。由于两个指针的距离保持在k-1，当第一个（走在前面的）指针到达链表的尾结点时，第二个指针（走在后面的）指针正好是倒数第k个结点。 效果示意图，以链表总共6个结点，求倒数第3个结点为例： 除此之外，要注意代码的鲁棒性。需要判断传入参数合法性问题。 代码二12345678910111213141516171819202122# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None class Solution: def FindKthToTail(self, head, k): # write code here if head == None or k == 0: return None phead = head pbehind = head for i in range(k-1): if phead.next == None: return None else: phead = phead.next while phead.next != None: phead = phead.next pbehind = pbehind.next return pbehind","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"多指针","slug":"多指针","permalink":"https://xxren8218.github.io/tags/%E5%A4%9A%E6%8C%87%E9%92%88/"},{"name":"链表","slug":"链表","permalink":"https://xxren8218.github.io/tags/%E9%93%BE%E8%A1%A8/"},{"name":"辅助列表","slug":"辅助列表","permalink":"https://xxren8218.github.io/tags/%E8%BE%85%E5%8A%A9%E5%88%97%E8%A1%A8/"}]},{"title":"剑指Offer（三）：从尾到头打印链表","slug":"剑指Offer（三）：从尾到头打印链表","date":"2021-06-11T09:58:16.000Z","updated":"2021-06-11T09:59:43.962Z","comments":true,"path":"20210611/剑指Offer（三）：从尾到头打印链表.html","link":"","permalink":"https://xxren8218.github.io/20210611/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8.html","excerpt":"","text":"1.题目通常，这种情况下，我们不希望修改原链表的结构。返回一个反序的链表，这就是经典的“后进先出”，我们可以使用栈实现这种顺序。每经过一个结点的时候，把该结点放到一个栈中。当遍历完整个链表后，再从栈顶开始逐个输出结点的值，给一个新的链表结构，这样链表就实现了反转。 2.思路采用辅助栈的策略，重新创建一个列表，每次来向首位插入元素即可。 3.代码123456789101112131415# encoding:utf-8class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] def printListFromTailToHead(self, listNode): # write code here stack = [] while listNode: stack.insert(0,listNode.val) listNode = listNode.next return stack","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"辅助栈","slug":"辅助栈","permalink":"https://xxren8218.github.io/tags/%E8%BE%85%E5%8A%A9%E6%A0%88/"}]},{"title":"08-算法实现：Item-Based CF 预测评分","slug":"08-算法实现：Item-Based-CF-预测评分","date":"2021-06-10T09:40:42.000Z","updated":"2021-06-10T09:41:33.976Z","comments":true,"path":"20210610/08-算法实现：Item-Based-CF-预测评分.html","link":"","permalink":"https://xxren8218.github.io/20210610/08-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%EF%BC%9AItem-Based-CF-%E9%A2%84%E6%B5%8B%E8%AF%84%E5%88%86.html","excerpt":"","text":"案例—算法实现：Item-Based CF 预测评分评分预测公式： pred(u,i)=\\hat{r}_{ui}=\\cfrac{\\sum_{j\\in I_{rated}}sim(i,j)*r_{uj}}{\\sum_{j\\in I_{rated}}sim(i,j)}算法实现 实现评分预测方法：predict 方法说明： 利用原始评分矩阵、以及物品间两两相似度，预测指定用户对指定物品的评分。 如果无法预测，则抛出异常 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# ......def predict(uid, iid, ratings_matrix, item_similar): &#x27;&#x27;&#x27; 预测给定用户对给定物品的评分值 :param uid: 用户ID :param iid: 物品ID :param ratings_matrix: 用户-物品评分矩阵 :param item_similar: 物品两两相似度矩阵 :return: 预测的评分值 &#x27;&#x27;&#x27; print(&quot;开始预测用户&lt;%d&gt;对电影&lt;%d&gt;的评分...&quot;%(uid, iid)) # 1. 找出iid物品的相似物品 similar_items = item_similar[iid].drop([iid]).dropna() # 相似物品筛选规则：正相关的物品 similar_items = similar_items.where(similar_items&gt;0).dropna() if similar_items.empty is True: raise Exception(&quot;物品&lt;%d&gt;没有相似的物品&quot; %id) # 2. 从iid物品的近邻相似物品中筛选出uid用户评分过的物品 ids = set(ratings_matrix.ix[uid].dropna().index)&amp;set(similar_items.index) finally_similar_items = similar_items.ix[list(ids)] # 3. 结合iid物品与其相似物品的相似度和uid用户对其相似物品的评分，预测uid对iid的评分 sum_up = 0 # 评分预测公式的分子部分的值 sum_down = 0 # 评分预测公式的分母部分的值 for sim_iid, similarity in finally_similar_items.iteritems(): # 近邻物品的评分数据 sim_item_rated_movies = ratings_matrix[sim_iid].dropna() # uid用户对相似物品物品的评分 sim_item_rating_from_user = sim_item_rated_movies[uid] # 计算分子的值 sum_up += similarity * sim_item_rating_from_user # 计算分母的值 sum_down += similarity # 计算预测的评分值并返回 predict_rating = sum_up/sum_down print(&quot;预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f&quot; % (uid, iid, predict_rating)) return round(predict_rating, 2)if __name__ == &#x27;__main__&#x27;: ratings_matrix = load_data(DATA_PATH) item_similar = compute_pearson_similarity(ratings_matrix, based=&quot;item&quot;) # 预测用户1对物品1的评分 predict(1, 1, ratings_matrix, item_similar) # 预测用户1对物品2的评分 predict(1, 2, ratings_matrix, item_similar) 实现预测全部评分方法：predict_all 123456789101112131415161718192021222324252627# ......def predict_all(uid, ratings_matrix, item_similar): &#x27;&#x27;&#x27; 预测全部评分 :param uid: 用户id :param ratings_matrix: 用户-物品打分矩阵 :param item_similar: 物品两两间的相似度 :return: 生成器，逐个返回预测评分 &#x27;&#x27;&#x27; # 准备要预测的物品的id列表 item_ids = ratings_matrix.columns # 逐个预测 for iid in item_ids: try: rating = predict(uid, iid, ratings_matrix, item_similar) except Exception as e: print(e) else: yield uid, iid, ratingif __name__ == &#x27;__main__&#x27;: ratings_matrix = load_data(DATA_PATH) item_similar = compute_pearson_similarity(ratings_matrix, based=&quot;item&quot;) for i in predict_all(1, ratings_matrix, item_similar): pass 添加过滤规则 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def _predict_all(uid, item_ids,ratings_matrix, item_similar): &#x27;&#x27;&#x27; 预测全部评分 :param uid: 用户id :param item_ids: 要预测物品id列表 :param ratings_matrix: 用户-物品打分矩阵 :param item_similar: 物品两两间的相似度 :return: 生成器，逐个返回预测评分 &#x27;&#x27;&#x27; # 逐个预测 for iid in item_ids: try: rating = predict(uid, iid, ratings_matrix, item_similar) except Exception as e: print(e) else: yield uid, iid, ratingdef predict_all(uid, ratings_matrix, item_similar, filter_rule=None): &#x27;&#x27;&#x27; 预测全部评分，并可根据条件进行前置过滤 :param uid: 用户ID :param ratings_matrix: 用户-物品打分矩阵 :param item_similar: 物品两两间的相似度 :param filter_rule: 过滤规则，只能是四选一，否则将抛异常：&quot;unhot&quot;,&quot;rated&quot;,[&quot;unhot&quot;,&quot;rated&quot;],None :return: 生成器，逐个返回预测评分 &#x27;&#x27;&#x27; if not filter_rule: item_ids = ratings_matrix.columns elif isinstance(filter_rule, str) and filter_rule == &quot;unhot&quot;: &#x27;&#x27;&#x27;过滤非热门电影&#x27;&#x27;&#x27; # 统计每部电影的评分数 count = ratings_matrix.count() # 过滤出评分数高于10的电影，作为热门电影 item_ids = count.where(count&gt;10).dropna().index elif isinstance(filter_rule, str) and filter_rule == &quot;rated&quot;: &#x27;&#x27;&#x27;过滤用户评分过的电影&#x27;&#x27;&#x27; # 获取用户对所有电影的评分记录 user_ratings = ratings_matrix.ix[uid] # 评分范围是1-5，小于6的都是评分过的，除此以外的都是没有评分的 _ = user_ratings&lt;6 item_ids = _.where(_==False).dropna().index elif isinstance(filter_rule, list) and set(filter_rule) == set([&quot;unhot&quot;, &quot;rated&quot;]): &#x27;&#x27;&#x27;过滤非热门和用户已经评分过的电影&#x27;&#x27;&#x27; count = ratings_matrix.count() ids1 = count.where(count &gt; 10).dropna().index user_ratings = ratings_matrix.ix[uid] _ = user_ratings &lt; 6 ids2 = _.where(_ == False).dropna().index # 取二者交集 item_ids = set(ids1)&amp;set(ids2) else: raise Exception(&quot;无效的过滤参数&quot;) yield from _predict_all(uid, item_ids, ratings_matrix, item_similar)if __name__ == &#x27;__main__&#x27;: ratings_matrix = load_data(DATA_PATH) item_similar = compute_pearson_similarity(ratings_matrix, based=&quot;item&quot;) for result in predict_all(1, ratings_matrix, item_similar, filter_rule=[&quot;unhot&quot;, &quot;rated&quot;]): print(result) 为指定用户推荐TOP-N结果 12345678910111213# ......def top_k_rs_result(k): ratings_matrix = load_data(DATA_PATH) item_similar = compute_pearson_similarity(ratings_matrix, based=&quot;item&quot;) results = predict_all(1, ratings_matrix, item_similar, filter_rule=[&quot;unhot&quot;, &quot;rated&quot;]) return sorted(results, key=lambda x: x[2], reverse=True)[:k]if __name__ == &#x27;__main__&#x27;: from pprint import pprint result = top_k_rs_result(20) pprint(result)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"07-算法实现：User-Based CF 预测评分","slug":"07-算法实现：User-Based-CF-预测评分","date":"2021-06-10T09:39:12.000Z","updated":"2021-06-10T09:40:19.485Z","comments":true,"path":"20210610/07-算法实现：User-Based-CF-预测评分.html","link":"","permalink":"https://xxren8218.github.io/20210610/07-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%EF%BC%9AUser-Based-CF-%E9%A2%84%E6%B5%8B%E8%AF%84%E5%88%86.html","excerpt":"","text":"案例—算法实现：User-Based CF 预测评分评分预测公式： pred(u,i)=\\hat{r}_{ui}=\\cfrac{\\sum_{v\\in U}sim(u,v)*r_{vi}}{\\sum_{v\\in U}|sim(u,v)|}算法实现 实现评分预测方法：predict 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# ......def predict(uid, iid, ratings_matrix, user_similar): &#x27;&#x27;&#x27; 预测给定用户对给定物品的评分值 :param uid: 用户ID :param iid: 物品ID :param ratings_matrix: 用户-物品评分矩阵 :param user_similar: 用户两两相似度矩阵 :return: 预测的评分值 &#x27;&#x27;&#x27; print(&quot;开始预测用户&lt;%d&gt;对电影&lt;%d&gt;的评分...&quot;%(uid, iid)) # 1. 找出uid用户的相似用户 similar_users = user_similar[uid].drop([uid]).dropna() # 相似用户筛选规则：正相关的用户 similar_users = similar_users.where(similar_users&gt;0).dropna() if similar_users.empty is True: raise Exception(&quot;用户&lt;%d&gt;没有相似的用户&quot; % uid) # 2. 从uid用户的近邻相似用户中筛选出对iid物品有评分记录的近邻用户 ids = set(ratings_matrix[iid].dropna().index)&amp;set(similar_users.index) finally_similar_users = similar_users.ix[list(ids)] # 3. 结合uid用户与其近邻用户的相似度预测uid用户对iid物品的评分 sum_up = 0 # 评分预测公式的分子部分的值 sum_down = 0 # 评分预测公式的分母部分的值 for sim_uid, similarity in finally_similar_users.iteritems(): # 近邻用户的评分数据 sim_user_rated_movies = ratings_matrix.ix[sim_uid].dropna() # 近邻用户对iid物品的评分 sim_user_rating_for_item = sim_user_rated_movies[iid] # 计算分子的值 sum_up += similarity * sim_user_rating_for_item # 计算分母的值 sum_down += similarity # 计算预测的评分值并返回 predict_rating = sum_up/sum_down print(&quot;预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f&quot; % (uid, iid, predict_rating)) return round(predict_rating, 2)if __name__ == &#x27;__main__&#x27;: ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based=&quot;user&quot;) # 预测用户1对物品1的评分 predict(1, 1, ratings_matrix, user_similar) # 预测用户1对物品2的评分 predict(1, 2, ratings_matrix, user_similar) 实现预测全部评分方法：predict_all 123456789101112131415161718192021222324252627# ......def predict_all(uid, ratings_matrix, user_similar): &#x27;&#x27;&#x27; 预测全部评分 :param uid: 用户id :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :return: 生成器，逐个返回预测评分 &#x27;&#x27;&#x27; # 准备要预测的物品的id列表 item_ids = ratings_matrix.columns # 逐个预测 for iid in item_ids: try: rating = predict(uid, iid, ratings_matrix, user_similar) except Exception as e: print(e) else: yield uid, iid, ratingif __name__ == &#x27;__main__&#x27;: ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based=&quot;user&quot;) for i in predict_all(1, ratings_matrix, user_similar): pass 添加过滤规则 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def _predict_all(uid, item_ids, ratings_matrix, user_similar): &#x27;&#x27;&#x27; 预测全部评分 :param uid: 用户id :param item_ids: 要预测的物品id列表 :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :return: 生成器，逐个返回预测评分 &#x27;&#x27;&#x27; # 逐个预测 for iid in item_ids: try: rating = predict(uid, iid, ratings_matrix, user_similar) except Exception as e: print(e) else: yield uid, iid, ratingdef predict_all(uid, ratings_matrix, user_similar, filter_rule=None): &#x27;&#x27;&#x27; 预测全部评分，并可根据条件进行前置过滤 :param uid: 用户ID :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :param filter_rule: 过滤规则，只能是四选一，否则将抛异常：&quot;unhot&quot;,&quot;rated&quot;,[&quot;unhot&quot;,&quot;rated&quot;],None :return: 生成器，逐个返回预测评分 &#x27;&#x27;&#x27; if not filter_rule: item_ids = ratings_matrix.columns elif isinstance(filter_rule, str) and filter_rule == &quot;unhot&quot;: &#x27;&#x27;&#x27;过滤非热门电影&#x27;&#x27;&#x27; # 统计每部电影的评分数 count = ratings_matrix.count() # 过滤出评分数高于10的电影，作为热门电影 item_ids = count.where(count&gt;10).dropna().index elif isinstance(filter_rule, str) and filter_rule == &quot;rated&quot;: &#x27;&#x27;&#x27;过滤用户评分过的电影&#x27;&#x27;&#x27; # 获取用户对所有电影的评分记录 user_ratings = ratings_matrix.ix[uid] # 评分范围是1-5，小于6的都是评分过的，除此以外的都是没有评分的 _ = user_ratings&lt;6 item_ids = _.where(_==False).dropna().index elif isinstance(filter_rule, list) and set(filter_rule) == set([&quot;unhot&quot;, &quot;rated&quot;]): &#x27;&#x27;&#x27;过滤非热门和用户已经评分过的电影&#x27;&#x27;&#x27; count = ratings_matrix.count() ids1 = count.where(count &gt; 10).dropna().index user_ratings = ratings_matrix.ix[uid] _ = user_ratings &lt; 6 ids2 = _.where(_ == False).dropna().index # 取二者交集 item_ids = set(ids1)&amp;set(ids2) else: raise Exception(&quot;无效的过滤参数&quot;) yield from _predict_all(uid, item_ids, ratings_matrix, user_similar)if __name__ == &#x27;__main__&#x27;: ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based=&quot;user&quot;) for result in predict_all(1, ratings_matrix, user_similar, filter_rule=[&quot;unhot&quot;, &quot;rated&quot;]): print(result) 根据预测评分为指定用户进行TOP-N推荐： 12345678910111213# ......def top_k_rs_result(k): ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based=&quot;user&quot;) results = predict_all(1, ratings_matrix, user_similar, filter_rule=[&quot;unhot&quot;, &quot;rated&quot;]) return sorted(results, key=lambda x: x[2], reverse=True)[:k]if __name__ == &#x27;__main__&#x27;: from pprint import pprint result = top_k_rs_result(20) pprint(result)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"06-案例--基于协同过滤的电影推荐","slug":"06-案例-基于协同过滤的电影推荐","date":"2021-06-10T09:37:29.000Z","updated":"2021-06-10T09:38:38.977Z","comments":true,"path":"20210610/06-案例-基于协同过滤的电影推荐.html","link":"","permalink":"https://xxren8218.github.io/20210610/06-%E6%A1%88%E4%BE%8B-%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90.html","excerpt":"","text":"案例—基于协同过滤的电影推荐前面我们已经基本掌握了协同过滤推荐算法，以及其中两种最基本的实现方案：User-Based CF和Item-Based CF，下面我们将利用真是的数据来进行实战演练。 案例需求 演示效果 分析案例 数据集下载MovieLens Latest Datasets Small 建议下载ml-latest-small.zip，数据量小，便于我们单机使用和运行 目标：根据ml-latest-small/ratings.csv（用户-电影评分数据），分别实现User-Based CF和Item-Based CF，并进行电影评分的预测，然后为用户实现电影推荐 数据集加载 加载ratings.csv，并转换为用户-电影评分矩阵 123456789101112131415161718192021222324252627282930313233343536373839import osimport pandas as pdimport numpy as npDATA_PATH = &quot;./datasets/ml-latest-small/ratings.csv&quot;CACHE_DIR = &quot;./datasets/cache/&quot;def load_data(data_path): &#x27;&#x27;&#x27; 加载数据 :param data_path: 数据集路径 :param cache_path: 数据集缓存路径 :return: 用户-物品评分矩阵 &#x27;&#x27;&#x27; # 数据集缓存地址 cache_path = os.path.join(CACHE_DIR, &quot;ratings_matrix.cache&quot;) print(&quot;开始加载数据集...&quot;) if os.path.exists(cache_path): # 判断是否存在缓存文件 print(&quot;加载缓存中...&quot;) ratings_matrix = pd.read_pickle(cache_path) print(&quot;从缓存加载数据集完毕&quot;) else: print(&quot;加载新数据中...&quot;) # 设置要加载的数据字段的类型 dtype = &#123;&quot;userId&quot;: np.int32, &quot;movieId&quot;: np.int32, &quot;rating&quot;: np.float32&#125; # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分 ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3)) # 透视表，将电影ID转换为列名称，转换成为一个User-Movie的评分矩阵 ratings_matrix = ratings.pivot_table(index=[&quot;userId&quot;], columns=[&quot;movieId&quot;], values=&quot;rating&quot;) # 存入缓存文件 ratings_matrix.to_pickle(cache_path) print(&quot;数据集加载完毕&quot;) return ratings_matrixif __name__ == &#x27;__main__&#x27;: ratings_matrix = load_data(DATA_PATH) print(ratings_matrix) 相似度计算 计算用户或物品两两相似度： 12345678910111213141516171819202122232425262728293031323334353637383940414243# ......def compute_pearson_similarity(ratings_matrix, based=&quot;user&quot;): &#x27;&#x27;&#x27; 计算皮尔逊相关系数 :param ratings_matrix: 用户-物品评分矩阵 :param based: &quot;user&quot; or &quot;item&quot; :return: 相似度矩阵 &#x27;&#x27;&#x27; user_similarity_cache_path = os.path.join(CACHE_DIR, &quot;user_similarity.cache&quot;) item_similarity_cache_path = os.path.join(CACHE_DIR, &quot;item_similarity.cache&quot;) # 基于皮尔逊相关系数计算相似度 # 用户相似度 if based == &quot;user&quot;: if os.path.exists(user_similarity_cache_path): print(&quot;正从缓存加载用户相似度矩阵&quot;) similarity = pd.read_pickle(user_similarity_cache_path) else: print(&quot;开始计算用户相似度矩阵&quot;) similarity = ratings_matrix.T.corr() similarity.to_pickle(user_similarity_cache_path) elif based == &quot;item&quot;: if os.path.exists(item_similarity_cache_path): print(&quot;正从缓存加载物品相似度矩阵&quot;) similarity = pd.read_pickle(item_similarity_cache_path) else: print(&quot;开始计算物品相似度矩阵&quot;) similarity = ratings_matrix.corr() similarity.to_pickle(item_similarity_cache_path) else: raise Exception(&quot;Unhandled &#x27;based&#x27; Value: %s&quot;%based) print(&quot;相似度矩阵计算/加载完毕&quot;) return similarityif __name__ == &#x27;__main__&#x27;: ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based=&quot;user&quot;) print(user_similar) item_similar = compute_pearson_similarity(ratings_matrix, based=&quot;item&quot;) print(item_similar) 注意以上实现，仅用于实验阶段，因为工业上、或生产环境中，数据量是远超过我们本例中使用的数据量的，而pandas是无法支撑起大批量数据的运算的，因此工业上通常会使用spark、mapReduce等分布式计算框架来实现，我们后面的课程中也是建立在此基础上进行实践的。 但是正如前面所说，推荐算法的思想和理念都是统一的，不论使用什么平台工具、有多大的数据体量，其背后的实现原理都是不变的。 所以在本节，大家要深刻去学习的是推荐算法的业务流程，以及在具体的业务场景中，如本例的电影推荐，如何实现出推荐算法，并产生推荐结果。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"05-推荐系统冷启动问题","slug":"05-推荐系统冷启动问题","date":"2021-06-10T09:33:03.000Z","updated":"2021-06-10T09:46:03.192Z","comments":true,"path":"20210610/05-推荐系统冷启动问题.html","link":"","permalink":"https://xxren8218.github.io/20210610/05-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%86%B7%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98.html","excerpt":"","text":"推荐系统的冷启动问题 推荐系统冷启动概念 ⽤户冷启动：如何为新⽤户做个性化推荐 物品冷启动：如何将新物品推荐给⽤户（协同过滤） 系统冷启动：⽤户冷启动+物品冷启动 本质是推荐系统依赖历史数据，没有历史数据⽆法预测⽤户偏好 用户冷启动 1.收集⽤户特征 ⽤户注册信息：性别、年龄、地域 设备信息：定位、⼿机型号、app列表 社交信息、推⼴素材、安装来源 2 引导用户填写兴趣 3 使用其它站点的行为数据, 例如腾讯视频&amp;QQ音乐 今日头条&amp;抖音 4 新老用户推荐策略的差异 新⽤户在冷启动阶段更倾向于热门排⾏榜，⽼⽤户会更加需要长尾推荐 Explore Exploit⼒度 使⽤单独的特征和模型预估 举例 性别与电视剧的关系 物品冷启动 给物品打标签 利用物品的内容信息，将新物品先投放给曾经喜欢过和它内容相似的其他物品的用户。 系统冷启动 基于内容的推荐 系统早期 基于内容的推荐逐渐过渡到协同过滤 基于内容的推荐和协同过滤的推荐结果都计算出来 加权求和得到最终推荐结果","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"04-推荐系统评估","slug":"04-推荐系统评估","date":"2021-06-10T09:30:03.000Z","updated":"2021-06-10T09:32:24.281Z","comments":true,"path":"20210610/04-推荐系统评估.html","link":"","permalink":"https://xxren8218.github.io/20210610/04-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0.html","excerpt":"","text":"二 推荐系统评估 好的推荐系统可以实现用户, 服务提供方, 内容提供方的共赢 显示反馈和隐式反馈 显式反馈 隐式反馈 例子 电影/书籍评分 是否喜欢这个推荐 播放/点击 评论 下载 购买 准确性 高 低 数量 少 多 获取成本 高 低 常用评估指标 • 准确性 • 信任度• 满意度 • 实时性• 覆盖率 • 鲁棒性• 多样性 • 可扩展性• 新颖性 • 商业⽬标• 惊喜度 • ⽤户留存 准确性 (理论角度) Netflix 美国录像带租赁 评分预测 RMSE MAE topN推荐 召回率 精准率 准确性 (业务角度) 覆盖度 信息熵 对于推荐越大越好 覆盖率 多样性&amp;新颖性&amp;惊喜性 多样性：推荐列表中两两物品的不相似性。（相似性如何度量？ 新颖性：未曾关注的类别、作者；推荐结果的平均流⾏度 惊喜性：历史不相似（惊）但很满意（喜） 往往需要牺牲准确性 使⽤历史⾏为预测⽤户对某个物品的喜爱程度 系统过度强调实时性 Exploitation &amp; Exploration 探索与利用问题 Exploitation(开发 利用)：选择现在可能最佳的⽅案 Exploration(探测 搜索)：选择现在不确定的⼀些⽅案，但未来可能会有⾼收益的⽅案 在做两类决策的过程中，不断更新对所有决策的不确定性的认知，优化长期的⽬标 EE问题实践 兴趣扩展: 相似话题, 搭配推荐 人群算法: userCF 用户聚类 平衡个性化推荐和热门推荐比例 随机丢弃用户行为历史 随机扰动模型参数 EE可能带来的问题 探索伤害用户体验, 可能导致用户流失 探索带来的长期收益(留存率)评估周期长, KPI压力大 如何平衡实时兴趣和长期兴趣 如何平衡短期产品体验和长期系统生态 如何平衡大众口味和小众需求 评估方法 问卷调查: 成本高 离线评估: 只能在用户看到过的候选集上做评估, 且跟线上真实效果存在偏差 只能评估少数指标 速度快, 不损害用户体验 在线评估: 灰度发布 &amp; A/B测试 50% 全量上线 实践: 离线评估和在线评估结合, 定期做问卷调查","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"03-推荐算法","slug":"03-推荐算法","date":"2021-06-01T16:22:39.000Z","updated":"2021-06-07T15:37:54.040Z","comments":true,"path":"20210602/03-推荐算法.html","link":"","permalink":"https://xxren8218.github.io/20210602/03-%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95.html","excerpt":"","text":"推荐算法 推荐模型构建流程 推荐算法概述 基于协同过滤的推荐算法 协同过滤实现 一 推荐模型构建流程Data(数据)-&gt;Features(特征)-&gt;ML Algorithm(机器学习算法)-&gt;Prediction Output(预测输出) 数据清洗/数据处理 数据来源 显性数据 Rating 打分 Comments 评论/评价 隐形数据 Order history 历史订单 Cart events 加购物车 Page views 页面浏览 Click-thru 点击 Search log 搜索记录 数据量/数据能否满足要求 特征工程 从数据中筛选特征 一个给定的商品，可能被拥有类似品味或需求的用户购买 使用用户行为数据描述商品 用数据表示特征 将所有用户行为合并在一起 ，形成一个user-item 矩阵 选择合适的算法 产生推荐结果 二 最经典的推荐算法：协同过滤推荐算法（Collaborative Filtering）算法思想：物以类聚，人以群分 基本的协同过滤推荐算法基于以下假设： “跟你喜好相似的人喜欢的东西你也很有可能喜欢” ：基于用户的协同过滤推荐（User-based CF）——好哥们 “跟你喜欢的东西相似的东西你也很有可能喜欢 ”：基于物品的协同过滤推荐（Item-based CF）——iphone8、iphoneXs 实现协同过滤推荐有以下几个步骤： 找出最相似的人或物品：TOP-N相似的人或物品 通过计算两两的相似度来进行排序，即可找出TOP-N相似的人或物品 根据相似的人或物品产生推荐结果 利用TOP-N结果生成初始推荐结果，然后过滤掉用户已经有过记录的物品或明确表示不感兴趣的物品 以下是一个简单的示例，数据集相当于一个用户对物品的购买记录表：打勾表示用户对物品的有购买记录 关于相似度计算这里先用一个简单的思想：如有两个同学X和Y，X同学爱好[足球、篮球、乒乓球]，Y同学爱好[网球、足球、篮球、羽毛球]，可见他们的共同爱好有2个，那么他们的相似度可以用：2/3 * 2/4 = 1/3 ≈ 0.33 来表示。 User-Based CF Item-Based CF 通过前面两个demo，相信大家应该已经对协同过滤推荐算法的设计与实现有了比较清晰的认识。 三 相似度计算(Similarity Calculation) 相似度的计算方法 数据分类 实数值(物品评分情况) 布尔值(用户的行为 是否点击 是否收藏) 欧氏距离, 是一个欧式空间下度量距离的方法. 两个物体, 都在同一个空间下表示为两个点, 假如叫做p,q, 分别都是n个坐标, 那么欧式距离就是衡量这两个点之间的距离. 欧氏距离不适用于布尔向量之间 E(p,q) = \\sqrt{\\sum_{i=1}^n (p_i - q_i)^2} ​ 欧氏距离的值是一个非负数, 最大值正无穷, 通常计算相似度的结果希望是[-1,1]或[0,1]之间,一般可以使用 ​ 如下转化公式: ​ 杰卡德相似度&amp;余弦相似度&amp;皮尔逊相关系数 余弦相似度 度量的是两个向量之间的夹角, 用夹角的余弦值来度量相似的情况 两个向量的夹角为0是,余弦值为1, 当夹角为90度是余弦值为0,为180度是余弦值为-1 余弦相似度在度量文本相似度, 用户相似度 物品相似度的时候较为常用 余弦相似度的特点, 与向量长度无关,余弦相似度计算要对向量长度归一化, 两个向量只要方向一致,无论程度强弱, 都可以视为’相似’ 皮尔逊相关系数Pearson 实际上也是一种余弦相似度, 不过先对向量做了中心化, 向量a b 各自减去向量的均值后, 再计算余弦相似度 皮尔逊相似度计算结果在-1,1之间 -1表示负相关, 1表示正相关 度量两个变量是不是同增同减 皮尔逊相关系数度量的是两个变量的变化趋势是否一致, 不适合计算布尔值向量之间的相关度 杰卡德相似度 Jaccard 两个集合的交集元素个数在并集中所占的比例, 非常适用于布尔向量表示 分子是两个布尔向量做点积计算, 得到的就是交集元素的个数 分母是两个布尔向量做或运算, 再求元素和 余弦相似度适合用户评分数据(实数值), 杰卡德相似度适用于隐式反馈数据(0,1布尔值)(是否收藏,是否点击,是否加购物车) 余弦相似度 皮尔逊相关系数 计算出用户1和其它用户之间的相似度 按照相似度大小排序, K近邻 如K取4: 取出近邻用户的购物清单 去除用户1已经购买过的商品 在剩余的物品中根据评分排序 物品相似度计算 余弦相似度对绝对值大小不敏感带来的问题 用户A对两部电影评分分别是1分和2分, 用户B对同样这两部电影进行评分是4分,5分 用余弦相似度计算,两个用户的相似度达到0.98 可以采用改进的余弦相似度, 先计算向量每个维度上的均值, 然后每个向量在各个维度上都减去均值后,在计算余弦相似度, 用调整的余弦相似度计算得到的相似度是-0.1 物品相似度计算案例 找出物品1的相似商品 选择最近似的物品 基于用户与物品的协同过滤比较 协同过滤推荐算法代码实现： 构建数据集： 12345678910users = [&quot;User1&quot;, &quot;User2&quot;, &quot;User3&quot;, &quot;User4&quot;, &quot;User5&quot;]items = [&quot;Item A&quot;, &quot;Item B&quot;, &quot;Item C&quot;, &quot;Item D&quot;, &quot;Item E&quot;]# 构建数据集datasets = [ [&quot;buy&quot;,None,&quot;buy&quot;,&quot;buy&quot;,None], [&quot;buy&quot;,None,None,&quot;buy&quot;,&quot;buy&quot;], [&quot;buy&quot;,None,&quot;buy&quot;,None,None], [None,&quot;buy&quot;,None,&quot;buy&quot;,&quot;buy&quot;], [&quot;buy&quot;,&quot;buy&quot;,&quot;buy&quot;,None,&quot;buy&quot;],] 计算时我们数据通常都需要对数据进行处理，或者编码，目的是为了便于我们对数据进行运算处理，比如这里是比较简单的情形，我们用1、0分别来表示用户的是否购买过该物品，则我们的数据集其实应该是这样的： 12345678910111213141516users = [&quot;User1&quot;, &quot;User2&quot;, &quot;User3&quot;, &quot;User4&quot;, &quot;User5&quot;]items = [&quot;Item A&quot;, &quot;Item B&quot;, &quot;Item C&quot;, &quot;Item D&quot;, &quot;Item E&quot;]# 用户购买记录数据集datasets = [ [1,0,1,1,0], [1,0,0,1,1], [1,0,1,0,0], [0,1,0,1,1], [1,1,1,0,1],]import pandas as pddf = pd.DataFrame(datasets, columns=items, index=users)print(df) 有了数据集，接下来我们就可以进行相似度的计算，不过对于相似度的计算其实是有很多专门的相似度计算方法的，比如余弦相似度、皮尔逊相关系数、杰卡德相似度等等。这里我们选择使用杰卡德相似系数[0,1] 12345678910111213141516171819# 直接计算某两项的杰卡德相似系数from sklearn.metrics import jaccard_similarity_score# 计算Item A 和Item B的相似度print(jaccard_similarity_score(df[&quot;Item A&quot;], df[&quot;Item B&quot;]))# 计算所有的数据两两的杰卡德相似系数from sklearn.metrics.pairwise import pairwise_distances# 计算用户间相似度user_similar = 1 - pairwise_distances(df, metric=&quot;jaccard&quot;)user_similar = pd.DataFrame(user_similar, columns=users, index=users)print(&quot;用户之间的两两相似度：&quot;)print(user_similar)# 计算物品间相似度item_similar = 1 - pairwise_distances(df.T, metric=&quot;jaccard&quot;)item_similar = pd.DataFrame(item_similar, columns=items, index=items)print(&quot;物品之间的两两相似度：&quot;)print(item_similar) 有了两两的相似度，接下来就可以筛选TOP-N相似结果，并进行推荐了 User-Based CF 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import pandas as pdimport numpy as npfrom pprint import pprintusers = [&quot;User1&quot;, &quot;User2&quot;, &quot;User3&quot;, &quot;User4&quot;, &quot;User5&quot;]items = [&quot;Item A&quot;, &quot;Item B&quot;, &quot;Item C&quot;, &quot;Item D&quot;, &quot;Item E&quot;]# 用户购买记录数据集datasets = [ [1,0,1,1,0], [1,0,0,1,1], [1,0,1,0,0], [0,1,0,1,1], [1,1,1,0,1],]df = pd.DataFrame(datasets, columns=items, index=users)# 计算所有的数据两两的杰卡德相似系数from sklearn.metrics.pairwise import pairwise_distances# 计算用户间相似度user_similar = 1 - pairwise_distances(df, metric=&quot;jaccard&quot;)user_similar = pd.DataFrame(user_similar, columns=users, index=users)print(&quot;用户之间的两两相似度：&quot;)print(user_similar)topN_users = &#123;&#125;# 遍历每一行数据for i in user_similar.index: # 取出每一列数据，并删除自身，然后排序数据 _df = user_similar.loc[i].drop([i]) _df_sorted = _df.sort_values(ascending=False) top2 = list(_df_sorted.index[:2]) topN_users[i] = top2print(&quot;Top2相似用户：&quot;)print(topN_users)rs_results = &#123;&#125;# 构建推荐结果for user, sim_users in topN_users.items(): rs_result = set() # 存储推荐结果 for sim_user in sim_users: # 构建初始的推荐结果 rs_result = rs_result.union(set(df.ix[sim_user].replace(0,np.nan).dropna().index)) # 过滤掉已经购买过的物品 rs_result -= set(df.ix[user].replace(0,np.nan).dropna().index) rs_results[user] = rs_resultprint(&quot;最终推荐结果：&quot;)pprint(rs_results) Item-Based CF 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import pandas as pdimport numpy as npfrom pprint import pprintusers = [&quot;User1&quot;, &quot;User2&quot;, &quot;User3&quot;, &quot;User4&quot;, &quot;User5&quot;]items = [&quot;Item A&quot;, &quot;Item B&quot;, &quot;Item C&quot;, &quot;Item D&quot;, &quot;Item E&quot;]# 用户购买记录数据集datasets = [ [1,0,1,1,0], [1,0,0,1,1], [1,0,1,0,0], [0,1,0,1,1], [1,1,1,0,1],]df = pd.DataFrame(datasets, columns=items, index=users)# 计算所有的数据两两的杰卡德相似系数from sklearn.metrics.pairwise import pairwise_distances# 计算物品间相似度item_similar = 1 - pairwise_distances(df.T, metric=&quot;jaccard&quot;)item_similar = pd.DataFrame(item_similar, columns=items, index=items)print(&quot;物品之间的两两相似度：&quot;)print(item_similar)topN_items = &#123;&#125;# 遍历每一行数据for i in item_similar.index: # 取出每一列数据，并删除自身，然后排序数据 _df = item_similar.loc[i].drop([i]) _df_sorted = _df.sort_values(ascending=False) top2 = list(_df_sorted.index[:2]) topN_items[i] = top2print(&quot;Top2相似物品：&quot;)pprint(topN_items)rs_results = &#123;&#125;# 构建推荐结果for user in df.index: # 遍历所有用户 rs_result = set() for item in df.ix[user].replace(0,np.nan).dropna().index: # 取出每个用户当前已购物品列表 # 根据每个物品找出最相似的TOP-N物品，构建初始推荐结果 rs_result = rs_result.union(topN_items[item]) # 过滤掉用户已购的物品 rs_result -= set(df.ix[user].replace(0,np.nan).dropna().index) # 添加到结果中 rs_results[user] = rs_resultprint(&quot;最终推荐结果：&quot;)pprint(rs_results) 关于协同过滤推荐算法使用的数据集 在前面的demo中，我们只是使用用户对物品的一个购买记录，类似也可以是比如浏览点击记录、收听记录等等。这样数据我们预测的结果其实相当于是在预测用户是否对某物品感兴趣，对于喜好程度不能很好的预测。 因此在协同过滤推荐算法中其实会更多的利用用户对物品的“评分”数据来进行预测，通过评分数据集，我们可以预测用户对于他没有评分过的物品的评分。其实现原理和思想和都是一样的，只是使用的数据集是用户-物品的评分数据。 关于用户-物品评分矩阵 用户-物品的评分矩阵，根据评分矩阵的稀疏程度会有不同的解决方案 稠密评分矩阵 稀疏评分矩阵 这里先介绍稠密评分矩阵的处理，稀疏矩阵的处理相对会复杂一些，我们到后面再来介绍。 使用协同过滤推荐算法对用户进行评分预测 数据集： 目的：预测用户1对物品E的评分 构建数据集：注意这里构建评分数据时，对于缺失的部分我们需要保留为None，如果设置为0那么会被当作评分值为0去对待 12345678910users = [&quot;User1&quot;, &quot;User2&quot;, &quot;User3&quot;, &quot;User4&quot;, &quot;User5&quot;]items = [&quot;Item A&quot;, &quot;Item B&quot;, &quot;Item C&quot;, &quot;Item D&quot;, &quot;Item E&quot;]# 用户购买记录数据集datasets = [ [5,3,4,4,None], [3,1,2,3,3], [4,3,4,3,5], [3,3,1,5,4], [1,5,5,2,1],] 计算相似度：对于评分数据这里我们采用皮尔逊相关系数[-1,1]来计算，-1表示强负相关，+1表示强正相关 pandas中corr方法可直接用于计算皮尔逊相关系数 12345678910111213df = pd.DataFrame(datasets, columns=items, index=users)print(&quot;用户之间的两两相似度：&quot;)# 直接计算皮尔逊相关系数# 默认是按列进行计算，因此如果计算用户间的相似度，当前需要进行转置user_similar = df.T.corr()print(user_similar.round(4))print(&quot;物品之间的两两相似度：&quot;)item_similar = df.corr()print(item_similar.round(4)) 123456789101112131415# 运行结果：用户之间的两两相似度： User1 User2 User3 User4 User5User1 1.0000 0.8528 0.7071 0.0000 -0.7921User2 0.8528 1.0000 0.4677 0.4900 -0.9001User3 0.7071 0.4677 1.0000 -0.1612 -0.4666User4 0.0000 0.4900 -0.1612 1.0000 -0.6415User5 -0.7921 -0.9001 -0.4666 -0.6415 1.0000物品之间的两两相似度： Item A Item B Item C Item D Item EItem A 1.0000 -0.4767 -0.1231 0.5322 0.9695Item B -0.4767 1.0000 0.6455 -0.3101 -0.4781Item C -0.1231 0.6455 1.0000 -0.7206 -0.4276Item D 0.5322 -0.3101 -0.7206 1.0000 0.5817Item E 0.9695 -0.4781 -0.4276 0.5817 1.0000 可以看到与用户1最相似的是用户2和用户3；与物品A最相似的物品分别是物品E和物品D。 注意：我们在预测评分时，往往是通过与其有正相关的用户或物品进行预测，如果不存在正相关的情况，那么将无法做出预测。这一点尤其是在稀疏评分矩阵中尤为常见，因为稀疏评分矩阵中很难得出正相关系数。 评分预测： User-Based CF 评分预测：使用用户间的相似度进行预测 关于评分预测的方法也有比较多的方案，下面介绍一种效果比较好的方案，该方案考虑了用户本身的评分评分以及近邻用户的加权平均相似度打分来进行预测： pred(u,i)=\\hat{r}_{ui}=\\cfrac{\\sum_{v\\in U}sim(u,v)*r_{vi}}{\\sum_{v\\in U}|sim(u,v)|} 我们要预测用户1对物品E的评分，那么可以根据与用户1最近邻的用户2和用户3进行预测，计算如下： ​ pred(u_1, i_5) =\\cfrac{0.85*3+0.71*5}{0.85+0.71} = 3.91 最终预测出用户1对物品5的评分为3.91 Item-Based CF 评分预测：使用物品间的相似度进行预测 这里利用物品相似度预测的计算同上，同样考虑了用户自身的平均打分因素，结合预测物品与相似物品的加权平均相似度打分进行来进行预测 pred(u,i)=\\hat{r}_{ui}=\\cfrac{\\sum_{j\\in I_{rated}}sim(i,j)*r_{uj}}{\\sum_{j\\in I_{rated}}sim(i,j)} 我们要预测用户1对物品E的评分，那么可以根据与物品E最近邻的物品A和物品D进行预测，计算如下： pred(u_1, i_5) = \\cfrac {0.97*5+0.58*4}{0.97+0.58} = 4.63 对比可见，User-Based CF预测评分和Item-Based CF的评分结果也是存在差异的，因为严格意义上他们其实应当属于两种不同的推荐算法，各自在不同的领域不同场景下，都会比另一种的效果更佳，但具体哪一种更佳，必须经过合理的效果评估，因此在实现推荐系统时这两种算法往往都是需要去实现的，然后对产生的推荐效果进行评估分析选出更优方案。 基于模型的方法 思想 通过机器学习算法，在数据中找出模式，并将用户与物品间的互动方式模式化 基于模型的协同过滤方式是构建协同过滤更高级的算法 近邻模型的问题 物品之间存在相关性, 信息量并不随着向量维度增加而线性增加 矩阵元素稀疏, 计算结果不稳定,增减一个向量维度, 导致近邻结果差异很大的情况存在 算法分类 基于图的模型 基于矩阵分解的方法 基于图的模型 基于邻域的模型看做基于图的模型的简单形式 原理 将用户的行为数据表示为二分图 基于二分图为用户进行推荐 根据两个顶点之间的路径数、路径长度和经过的顶点数来评价两个顶点的相关性 基于矩阵分解的模型 原理 根据用户与物品的潜在表现，我们就可以预测用户对未评分的物品的喜爱程度 把原来的大矩阵, 近似分解成两个小矩阵的乘积, 在实际推荐计算时不再使用大矩阵, 而是使用分解得到的两个小矩阵 用户-物品评分矩阵A是M X N维, 即一共有M个用户, n个物品 我们选一个很小的数 K (K&lt;&lt; M, K&lt;&lt;N) 通过计算得到两个矩阵U V U是M K矩阵 , 矩阵V是 N K $U_{mk} V^{T}_{nk} 约等于 A_{m*n}$ 类似这样的计算过程就是矩阵分解 基于矩阵分解的方法 ALS交替最小二乘 ALS-WR(加权正则化交替最小二乘法): alternating-least-squares with weighted-λ –regularization 将用户(user)对商品(item)的评分矩阵分解为两个矩阵：一个是用户对商品隐含特征的偏好矩阵，另一个是商品所包含的隐含特征的矩阵。在这个矩阵分解的过程中，评分缺失项得到了填充，也就是说我们可以基于这个填充的评分来给用户做商品推荐了。 SVD奇异值分解矩阵 ALS方法 ALS的矩阵分解算法常应用于推荐系统中，将用户(user)对商品(item)的评分矩阵，分解为用户对商品隐含特征的偏好矩阵，和商品在隐含特征上的映射矩阵。 与传统的矩阵分解SVD方法来分解矩阵R(R∈ℝm×n)不同的是，ALS(alternating least squares)希望找到两个低维矩阵，以 R̃ =XY 来逼近矩阵R，其中 ，X∈ℝm×d，Y∈ℝd×n，这样，将问题的复杂度由O(mn)转换为O((m+n)d)。 计算X和Y过程：首先用一个小于1的随机数初始化Y，并根据公式求X，此时就可以得到初始的XY矩阵了，根据平方差和得到的X，重新计算并覆盖Y，计算差平方和，反复进行以上两步的计算，直到差平方和小于一个预设的数，或者迭代次数满足要求则停止","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"02-推荐系统设计","slug":"02-推荐系统设计","date":"2021-06-01T16:16:15.000Z","updated":"2021-06-01T16:22:07.871Z","comments":true,"path":"20210602/02-推荐系统设计.html","link":"","permalink":"https://xxren8218.github.io/20210602/02-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1.html","excerpt":"","text":"二 推荐系统设计2.1 推荐系统要素 UI 和 UE(前端界面) 数据 (Lambda架构) 业务知识 算法 2.2 推荐系统架构 推荐系统整体架构 大数据Lambda架构 由Twitter工程师Nathan Marz(storm项目发起人)提出 Lambda系统架构提供了一个结合实时数据和Hadoop预先计算的数据环境和混合平台, 提供一个实时的数据视图 分层架构 批处理层 数据不可变, 可进行任何计算, 可水平扩展 高延迟 几分钟~几小时(计算量和数据量不同) 日志收集 Flume 分布式存储 Hadoop hdfs 分布式计算 Hadoop MapReduce &amp; spark 视图存储数据库 nosql(HBase/Cassandra) Redis/memcache MySQL 实时处理层 流式处理, 持续计算 存储和分析某个窗口期内的数据 最终正确性(Eventual accuracy) 实时数据收集 flume &amp; kafka 实时数据分析 spark streaming/storm/flink 服务层 支持随机读 需要在非常短的时间内返回结果 读取批处理层和实时处理层结果并对其归并 Lambda架构图 推荐算法架构 召回阶段(海选) 召回决定了最终推荐结果的天花板 常用算法: 协同过滤(基于用户 基于物品的) 基于内容 (根据用户行为总结出自己的偏好 根据偏好 通过文本挖掘技术找到内容上相似的商品) 基于隐语义（矩阵分解） 排序阶段 召回决定了最终推荐结果的天花板, 排序逼近这个极限, 决定了最终的推荐效果 CTR预估 (点击率预估 使用LR算法) 估计用户是否会点击某个商品 需要用户的点击数据 策略调整 推荐系统的整体架构","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"01-推荐系统简介","slug":"01-推荐系统简介","date":"2021-06-01T16:12:16.000Z","updated":"2021-06-01T16:16:27.124Z","comments":true,"path":"20210602/01-推荐系统简介.html","link":"","permalink":"https://xxren8218.github.io/20210602/01-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%80%E4%BB%8B.html","excerpt":"","text":"一 推荐系统简介​ 个性化推荐(推荐系统)经历了多年的发展，已经成为互联网产品的标配，也是AI成功落地的分支之一，在电商(淘宝/京东)、资讯(今日头条/微博)、音乐(网易云音乐/QQ音乐)、短视频(抖音/快手)等热门应用中,推荐系统都是核心组件之一。 推荐系统产生背景 信息过载 &amp; 用户需求不明确 分类⽬录（1990s）：覆盖少量热门⽹站。Hao123 Yahoo 搜索引擎（2000s）：通过搜索词明确需求。Google Baidu 推荐系统（2010s）：不需要⽤户提供明确的需求，通过分析⽤户的历史⾏为给⽤户的兴趣进⾏建模，从⽽主动给⽤户推荐能够满⾜他们兴趣和需求的信息。 什么是推荐系统 没有明确需求的用户访问了我们的服务, 且服务的物品对用户构成了信息过载, 系统通过一定的规则对物品进行排序,并将排在前面的物品展示给用户,这样的系统就是推荐系统 推荐系统 V.S. 搜索引擎 搜索 推荐 行为方式 主动 被动 意图 明确 模糊 个性化 弱 强 流量分布 马太效应（二八效应），基本上返回的都是一样的优质的网站。如 CSDN 长尾效应 （与马太效应相反），兼顾更多的小厂 目标 快速满足 持续服务 （收集注意力，卖广告） 评估指标 简明 复杂 推荐系统的作用 高效连接用户和物品, 发现长尾商品 留住用户和内容生产者, 实现商业目标 推荐系统的工作原理 社会化推荐 向朋友咨询, 社会化推荐, 让好友给自己推荐物品 基于内容的推荐 打开搜索引擎, 输入自己喜欢的演员的名字, 然后看看返回结果中还有什么电影是自己没看过的 基于流行度的推荐 查看票房排行榜, 基于协同过滤的推荐 找到和自己历史兴趣相似的用户, 看看他们最近在看什么电影 推荐系统的应用场景 feed 流 信息流 推荐系统和Web项目的区别 稳定的信息流通系统 V.S. 通过信息过滤实现目标提升 web项目: 处理复杂逻辑 处理高并发 实现高可用 为用户提供稳定服务, 构建一个稳定的信息流通的服务 推荐系统: 追求指标增长, 留存率/阅读时间/GMV (Gross Merchandise Volume电商网站成交金额)/视频网站VV (Video View) 确定 V.S. 不确定思维 web项目: 对结果有确定预期 推荐系统: 结果是概率问题","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"}]},{"title":"剑指Offer（五十一）：构建乘积数组","slug":"剑指Offer（五十一）：构建乘积数组","date":"2021-06-01T16:09:27.000Z","updated":"2021-06-01T16:11:16.261Z","comments":true,"path":"20210602/剑指Offer（五十一）：构建乘积数组.html","link":"","permalink":"https://xxren8218.github.io/20210602/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%94%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E6%9E%84%E5%BB%BA%E4%B9%98%E7%A7%AF%E6%95%B0%E7%BB%84.html","excerpt":"","text":"1.题目给定一个数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1],其中B中的元素B[i]=A[0]A[1]…A[i-1]A[i+1]…A[n-1]。不能使用除法。 2.思路暴力解法：直接构造一个B列表，使得B[i]都初始化为i，再分别乘a中除了i的所有元素。 3.代码——力扣超时43/4412345678910# -*- coding:utf-8 -*-class Solution: def multiply(self, A): # write code here B = [i for i in range(len(A))] for i in B: B[i] = 1 for j in (A[:i]+A[i+1:]): B[i] *= j return B 新思路： 代码12345678910class Solution: def constructArr(self, a: List[int]) -&gt; List[int]: b, tmp = [1] * len(a), 1 for i in range(1, len(a)): b[i] = b[i - 1] * a[i - 1] # 下三角 for i in range(len(a) - 2, -1, -1): tmp *= a[i + 1] # 上三角 b[i] *= tmp # 下三角 * 上三角 return b","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"}]},{"title":"剑指Offer（五十）：数组中重复的数字","slug":"剑指Offer（五十）：数组中重复的数字","date":"2021-06-01T16:08:08.000Z","updated":"2021-06-01T16:08:53.484Z","comments":true,"path":"20210602/剑指Offer（五十）：数组中重复的数字.html","link":"","permalink":"https://xxren8218.github.io/20210602/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%94%E5%8D%81%EF%BC%89%EF%BC%9A%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E5%AD%97.html","excerpt":"","text":"1.题目在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 2.思路直接哈希表解决。 3.代码12345678910111213class Solution(object): def findRepeatNumber(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; dic = &#123;&#125; for i in nums: if i not in dic: dic[i] = 1 else: return i return &#x27; &#x27;","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"哈希表","slug":"哈希表","permalink":"https://xxren8218.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"}]},{"title":"剑指Offer（三十七）：数字在排序数组中出现的次数","slug":"剑指Offer（三十七）：数字在排序数组中出现的次数","date":"2021-06-01T16:05:07.000Z","updated":"2021-06-01T16:07:33.325Z","comments":true,"path":"20210602/剑指Offer（三十七）：数字在排序数组中出现的次数.html","link":"","permalink":"https://xxren8218.github.io/20210602/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9A%E6%95%B0%E5%AD%97%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0.html","excerpt":"","text":"1.题目统计一个数字在排序数组中出现的次数。 2.思路我最先想到的是count方法，直接可以计数。那么就会失去本题原来的意义。 3.代码12345# -*- coding:utf-8 -*-class Solution: def GetNumberOfK(self, data, k): # write code here return data.count(k) 思路二——二分查找既然是排序的数组，那第一感觉肯定是二分查找； 代码112345678910111213141516171819class Solution: def search(self, nums: [int], target: int) -&gt; int: # 搜索右边界 right i, j = 0, len(nums) - 1 while i &lt;= j: m = (i + j) // 2 if nums[m] &lt;= target: i = m + 1 else: j = m - 1 right = i # 若数组中无 target ，则提前返回 if j &gt;= 0 and nums[j] != target: return 0 # 搜索左边界 left i = 0 while i &lt;= j: m = (i + j) // 2 if nums[m] &lt; target: i = m + 1 else: j = m - 1 left = j return right - left - 1 代码212345678910111213class Solution(object): def search(self, nums, target): def helper(nums,target): i, j = 0, len(nums)-1 while i &lt;= j: m = (i + j)//2 if nums[m] &lt;= target: i = m + 1 else: j = m - 1 return i return helper(nums,target) - helper(nums,target-1)","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"二分法","slug":"二分法","permalink":"https://xxren8218.github.io/tags/%E4%BA%8C%E5%88%86%E6%B3%95/"}]},{"title":"剑指Offer（四十）：数组中只出现一次的数字","slug":"剑指Offer（四十）：数组中只出现一次的数字","date":"2021-06-01T16:03:24.000Z","updated":"2021-06-01T16:04:20.663Z","comments":true,"path":"20210602/剑指Offer（四十）：数组中只出现一次的数字.html","link":"","permalink":"https://xxren8218.github.io/20210602/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%EF%BC%89%EF%BC%9A%E6%95%B0%E7%BB%84%E4%B8%AD%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97.html","excerpt":"","text":"1.题目一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。要求时间复杂度是O(n)，空间复杂度是O(1)。 2.思路直接使用一个哈希表进行计数即可。 3.代码12345678910111213141516class Solution(object): def firstUniqChar(self, s): &quot;&quot;&quot; :type s: str :rtype: str &quot;&quot;&quot; dic = &#123;&#125; for i in s: if i not in dic: dic[i] = 1 else:dic[i] += 1 for j in s: if dic[j] == 1: return j return &#x27; &#x27;","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"哈希表","slug":"哈希表","permalink":"https://xxren8218.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"}]},{"title":"剑指Offer（三十五）：数组中的逆序对","slug":"剑指Offer（三十五）：数组中的逆序对","date":"2021-06-01T15:58:50.000Z","updated":"2021-06-01T16:02:22.658Z","comments":true,"path":"20210601/剑指Offer（三十五）：数组中的逆序对.html","link":"","permalink":"https://xxren8218.github.io/20210601/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9A%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9.html","excerpt":"","text":"1.题目在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007。 题目保证输入的数组中没有的相同的数字 数据范围： 对于%50的数据,size&lt;=10^4 对于%75的数据,size&lt;=10^5 对于%100的数据,size&lt;=2*10^5 2.思路——暴力解法直接暴力解法——O（n^2），代码超时 3.代码12345678910111213141516# -*- coding:utf-8 -*-class Solution(object): def reversePairs(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; size = len(nums) if size &lt; 2: return 0 res = 0 for i in range(0, size - 1): for j in range(i + 1, size): if nums[i] &gt; nums[j]: res += 1 return res 解法二——归并排序思路如数组{7,5,6,4}，逆序对总共有5对，{7,5}，{7,6}，{7,4}，{5,4}，{6,4}；。分治思想，采用归并排序的思路来处理，如下图，先分后治：先把数组分解成两个长度为2的子数组，再把这两个子数组分解成两个长度为1的子数组。接下来一边合并相邻的子数组，一边统计逆序对的数目。在第一对长度为1的子数组{7}、{5}中7&gt;5，因此（7,5）组成一个逆序对。同样在第二对长度为1的子数组{6}，{4}中也有逆序对（6,4），由于已经统计了这两对子数组内部的逆序对，因此需要把这两对子数组进行排序，避免在之后的统计过程中重复统计。逆序对的总数 = 左边数组中的逆序对的数量 + 右边数组中逆序对的数量 + 左右结合成新的顺序数组时中出现的逆序对的数量总结一下： 这是一个归并排序的合并过程，主要是考虑合并两个有序序列时，计算逆序对数。 对于两个升序序列，设置两个下标：两个有序序列的末尾。每次比较两个末尾值，如果前末尾大于后末尾值，则有”后序列当前长度“个逆序对；否则不构成逆序对。然后把较大值拷贝到辅助数组的末尾，即最终要将两个有序序列合并到辅助数组并有序。 这样，每次在合并前，先递归地处理左半段、右半段，则左、右半段有序，且左右半段的逆序对数可得到，再计算左右半段合并时逆序对的个数。 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344# -*- coding:utf-8 -*-class Solution: ##### def __init__(self): self.count = 0 def merge_sort(self, alist): # 先进性拆分 n = len(alist) if n &lt;= 1: return alist mid = n // 2 # left 采用归并排序后形成的有序的新的列表 left_li = self.merge_sort(alist[:mid]) # right 采用归并排序后形成的有序的新的列表 right_li = self.merge_sort(alist[mid:]) # 将两个有序的子序列合并为一个新的整体 # merge(left, right) left_pointer, right_pointer = 0, 0 result = [] while left_pointer &lt; len(left_li) and right_pointer &lt; len(right_li): if left_li[left_pointer] &lt;= right_li[right_pointer]: result.append(left_li[left_pointer]) left_pointer += 1 else: result.append(right_li[right_pointer]) right_pointer += 1 ####### [3] 4 5 [1] 2 6 左边游标第一个比后面的大，则，后面的4,5肯定比 1 也大 self.count += (len(left_li) - (left_pointer)) # 其中一个列表取完了，另一个没有。进行判断。list[n:]返回空列表。可以同时判断两个列表情况。 result += left_li[left_pointer:] result += right_li[right_pointer:] return result def InversePairs(self, data): # write code here self.merge_sort(data) return self.count","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"归并排序","slug":"归并排序","permalink":"https://xxren8218.github.io/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/"},{"name":"递归","slug":"递归","permalink":"https://xxren8218.github.io/tags/%E9%80%92%E5%BD%92/"}]},{"title":"时间序列——ARIMA模型参数的选择","slug":"时间序列——ARIMA模型参数的选择","date":"2021-05-25T16:57:08.000Z","updated":"2021-05-25T16:59:30.782Z","comments":true,"path":"20210526/时间序列——ARIMA模型参数的选择.html","link":"","permalink":"https://xxren8218.github.io/20210526/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E2%80%94%E2%80%94ARIMA%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9.html","excerpt":"","text":"ARIMA的模型参数选择 AIC（赤池信息准则） BIC（贝叶斯信息准则） 123456789101112131415161718192021222324252627282930%load_ext autoreload%autoreload 2%matplotlib inline%config InlineBackend.figure_format=&#x27;retina&#x27;from __future__ import absolute_import, division, print_functionimport sysimport osimport pandas as pdimport numpy as np# TSA from Statsmodelsimport statsmodels.api as smimport statsmodels.formula.api as smfimport statsmodels.tsa.api as smt# Display and Plottingimport matplotlib.pylab as pltimport seaborn as snspd.set_option(&#x27;display.float_format&#x27;, lambda x: &#x27;%.5f&#x27; % x) # pandasnp.set_printoptions(precision=5, suppress=True) # numpypd.set_option(&#x27;display.max_columns&#x27;, 100)pd.set_option(&#x27;display.max_rows&#x27;, 100)# seaborn plotting stylesns.set(style=&#x27;ticks&#x27;, context=&#x27;poster&#x27;) 1234filename_ts = &#x27;data/series1.csv&#x27;ts_df = pd.read_csv(filename_ts, index_col=0, parse_dates=[0])n_sample = ts_df.shape[0] 12print(ts_df.shape)print(ts_df.head()) (120, 1) value 2006-06-01 0.21507 2006-07-01 1.14225 2006-08-01 0.08077 2006-09-01 -0.73952 2006-10-01 0.53552 1234567891011# Create a training sample and testing sample before analyzing the seriesn_train=int(0.95*n_sample)+1n_forecast=n_sample-n_train#ts_dfts_train = ts_df.iloc[:n_train][&#x27;value&#x27;]ts_test = ts_df.iloc[n_train:][&#x27;value&#x27;]print(ts_train.shape)print(ts_test.shape)print(&quot;Training Series:&quot;, &quot;\\n&quot;, ts_train.tail(), &quot;\\n&quot;)print(&quot;Testing Series:&quot;, &quot;\\n&quot;, ts_test.head()) (115,) (5,) Training Series: 2015-08-01 0.60371 2015-09-01 -1.27372 2015-10-01 -0.93284 2015-11-01 0.08552 2015-12-01 1.20534 Name: value, dtype: float64 Testing Series: 2016-01-01 2.16411 2016-02-01 0.95226 2016-03-01 0.36485 2016-04-01 -2.26487 2016-05-01 -2.38168 Name: value, dtype: float64 12345678910111213141516171819def tsplot(y, lags=None, title=&#x27;&#x27;, figsize=(14, 8)): fig = plt.figure(figsize=figsize) layout = (2, 2) ts_ax = plt.subplot2grid(layout, (0, 0)) hist_ax = plt.subplot2grid(layout, (0, 1)) acf_ax = plt.subplot2grid(layout, (1, 0)) pacf_ax = plt.subplot2grid(layout, (1, 1)) y.plot(ax=ts_ax) ts_ax.set_title(title) y.plot(ax=hist_ax, kind=&#x27;hist&#x27;, bins=25) hist_ax.set_title(&#x27;Histogram&#x27;) smt.graphics.plot_acf(y, lags=lags, ax=acf_ax) smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax) [ax.set_xlim(0) for ax in [acf_ax, pacf_ax]] sns.despine() fig.tight_layout() return ts_ax, acf_ax, pacf_ax 1tsplot(ts_train, title=&#x27;A Given Training Series&#x27;, lags=20); 12345# Model Estimation# Fit the modelarima200 = sm.tsa.SARIMAX(ts_train, order=(2,0,0)) # order 表示ARIMA模型的指定的参数 p, d，qmodel_results = arima200.fit() 有了不同的参数都满足如何选取呢？ 选取最简单的模型 AIC（赤池信息准则） BIC（贝叶斯信息准则） 越小越好 123456789101112131415161718192021222324252627282930import itertoolsp_min = 0d_min = 0q_min = 0p_max = 4d_max = 0q_max = 4# Initialize a DataFrame to store the resultsresults_bic = pd.DataFrame(index=[&#x27;AR&#123;&#125;&#x27;.format(i) for i in range(p_min,p_max+1)], columns=[&#x27;MA&#123;&#125;&#x27;.format(i) for i in range(q_min,q_max+1)])for p,d,q in itertools.product(range(p_min,p_max+1), range(d_min,d_max+1), range(q_min,q_max+1)): if p==0 and d==0 and q==0: results_bic.loc[&#x27;AR&#123;&#125;&#x27;.format(p), &#x27;MA&#123;&#125;&#x27;.format(q)] = np.nan continue try: model = sm.tsa.SARIMAX(ts_train, order=(p, d, q), #enforce_stationarity=False, #enforce_invertibility=False, ) results = model.fit() results_bic.loc[&#x27;AR&#123;&#125;&#x27;.format(p), &#x27;MA&#123;&#125;&#x27;.format(q)] = results.bic except: continueresults_bic = results_bic[results_bic.columns].astype(float) 12345678fig, ax = plt.subplots(figsize=(10, 8))ax = sns.heatmap(results_bic, mask=results_bic.isnull(), ax=ax, annot=True, fmt=&#x27;.2f&#x27;, );ax.set_title(&#x27;BIC&#x27;); 123456# Alternative model selection method, limited to only searching AR and MA parameterstrain_results = sm.tsa.arma_order_select_ic(ts_train, ic=[&#x27;aic&#x27;, &#x27;bic&#x27;], trend=&#x27;nc&#x27;, max_ar=4, max_ma=4)print(&#x27;AIC&#x27;, train_results.aic_min_order)print(&#x27;BIC&#x27;, train_results.bic_min_order) AIC (4, 2) BIC (1, 1) 12# 残差分析 正态分布 QQ图线性model_results.plot_diagnostics(figsize=(16, 12)); QQ 图若是直线，则是正态分布。符合要求。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"时间序列——ARIMA的一些概念","slug":"时间序列——ARIMA的一些概念","date":"2021-05-25T16:51:04.000Z","updated":"2021-05-25T16:54:41.257Z","comments":true,"path":"20210526/时间序列——ARIMA的一些概念.html","link":"","permalink":"https://xxren8218.github.io/20210526/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E2%80%94%E2%80%94ARIMA%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5.html","excerpt":"","text":"ARIMA模型的一些概念 差分 ACF 自相关函数 PACF 偏自相关函数 需要用到statsmodels模块 1234567891011121314151617181920212223242526272829303132333435%load_ext autoreload%autoreload 2%matplotlib inline%config InlineBackend.figure_format=&#x27;retina&#x27;from __future__ import absolute_import, division, print_function# http://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost # 很多可以装的python库import sysimport osimport pandas as pdimport numpy as np# # Remote Data Access# import pandas_datareader.data as web# import datetime# # reference: https://pandas-datareader.readthedocs.io/en/latest/remote_data.html# TSA from Statsmodelsimport statsmodels.api as smimport statsmodels.formula.api as smfimport statsmodels.tsa.api as smt# Display and Plottingimport matplotlib.pylab as pltimport seaborn as snspd.set_option(&#x27;display.float_format&#x27;, lambda x: &#x27;%.5f&#x27; % x) # pandasnp.set_printoptions(precision=5, suppress=True) # numpypd.set_option(&#x27;display.max_columns&#x27;, 100)pd.set_option(&#x27;display.max_rows&#x27;, 100)# seaborn plotting stylesns.set(style=&#x27;ticks&#x27;, context=&#x27;poster&#x27;) The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload 1234#Read the data#美国消费者信心指数Sentiment = &#x27;data/sentiment.csv&#x27;Sentiment = pd.read_csv(Sentiment, index_col=0, parse_dates=[0]) 1Sentiment.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } UMCSENT DATE 2000-01-01 112.00000 2000-02-01 111.30000 2000-03-01 107.10000 2000-04-01 109.20000 2000-05-01 110.70000 12# Select the series from 2005 - 2016sentiment_short = Sentiment.loc[&#x27;2005&#x27;:&#x27;2016&#x27;] 1234sentiment_short.plot(figsize=(12,8))plt.legend(bbox_to_anchor=(1.25, 0.5))plt.title(&quot;Consumer Sentiment&quot;)sns.despine() 12345sentiment_short[&#x27;diff_1&#x27;] = sentiment_short[&#x27;UMCSENT&#x27;].diff(1) # 做一个是时间间隔算差分值sentiment_short[&#x27;diff_2&#x27;] = sentiment_short[&#x27;diff_1&#x27;].diff(1) # 对一阶差分的基础上进行算差分，得到二阶差分。sentiment_short.plot(subplots=True, figsize=(18, 12)) array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001D9383BACF8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001D939FAB6A0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001D93A139B70&gt;], dtype=object) 1234del sentiment_short[&#x27;diff_2&#x27;]del sentiment_short[&#x27;diff_1&#x27;]sentiment_short.head()print (type(sentiment_short)) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; 1234567891011fig = plt.figure(figsize=(12,8))ax1 = fig.add_subplot(211)fig = sm.graphics.tsa.plot_acf(sentiment_short, lags=20,ax=ax1)ax1.xaxis.set_ticks_position(&#x27;bottom&#x27;)fig.tight_layout();ax2 = fig.add_subplot(212)fig = sm.graphics.tsa.plot_pacf(sentiment_short, lags=20, ax=ax2)ax2.xaxis.set_ticks_position(&#x27;bottom&#x27;)fig.tight_layout(); 12345678910111213141516171819202122# 散点图也可以表示lags=9ncols=3nrows=int(np.ceil(lags/ncols))fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(4*ncols, 4*nrows))for ax, lag in zip(axes.flat, np.arange(1,lags+1, 1)): lag_str = &#x27;t-&#123;&#125;&#x27;.format(lag) X = (pd.concat([sentiment_short, sentiment_short.shift(-lag)], axis=1, keys=[&#x27;y&#x27;] + [lag_str]).dropna()) X.plot(ax=ax, kind=&#x27;scatter&#x27;, y=&#x27;y&#x27;, x=lag_str); corr = X.corr().as_matrix()[0][1] ax.set_ylabel(&#x27;Original&#x27;) ax.set_title(&#x27;Lag: &#123;&#125; (corr=&#123;:.2f&#125;)&#x27;.format(lag_str, corr)); ax.set_aspect(&#x27;equal&#x27;); sns.despine();fig.tight_layout(); 12345678910111213141516171819202122# 更直观一些# 模板——只需要把自己的数据放进来，就可进行分析。def tsplot(y, lags=None, title=&#x27;&#x27;, figsize=(14, 8)): fig = plt.figure(figsize=figsize) layout = (2, 2) ts_ax = plt.subplot2grid(layout, (0, 0)) hist_ax = plt.subplot2grid(layout, (0, 1)) acf_ax = plt.subplot2grid(layout, (1, 0)) pacf_ax = plt.subplot2grid(layout, (1, 1)) y.plot(ax=ts_ax) ts_ax.set_title(title) y.plot(ax=hist_ax, kind=&#x27;hist&#x27;, bins=25) hist_ax.set_title(&#x27;Histogram&#x27;) smt.graphics.plot_acf(y, lags=lags, ax=acf_ax) smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax) [ax.set_xlim(0) for ax in [acf_ax, pacf_ax]] sns.despine() plt.tight_layout() return ts_ax, acf_ax, pacf_ax 1tsplot(sentiment_short, title=&#x27;Consumer Sentiment&#x27;, lags=36);","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"时间序列——pandas的滑动窗口","slug":"时间序列——pandas的滑动窗口","date":"2021-05-25T16:49:14.000Z","updated":"2021-05-25T16:55:27.470Z","comments":true,"path":"20210526/时间序列——pandas的滑动窗口.html","link":"","permalink":"https://xxren8218.github.io/20210526/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E2%80%94%E2%80%94pandas%E7%9A%84%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3.html","excerpt":"","text":"pandas滑动窗口 进行预测单独拿某一天的值不合理，可以用一个滑动窗口，取一个平均值，比较平稳。 1234%matplotlib inline import matplotlib.pylabimport numpy as npimport pandas as pd 1df = pd.Series(np.random.randn(600), index = pd.date_range(&#x27;7/1/2021&#x27;, freq = &#x27;D&#x27;, periods = 600)) 1df.head() 2021-07-01 -1.507234 2021-07-02 1.460754 2021-07-03 -0.092146 2021-07-04 2.486930 2021-07-05 1.214179 Freq: D, dtype: float64 12r = df.rolling(window = 10) # 滑动窗口的大小。默认从左边开始滑动。r Rolling [window=10,center=False,axis=0] 123# r.max, r.median, r.std, r.skew, r.sum, r.varprint(r.mean()) # 小于窗口长度不会计算！为NaN 2021-07-01 NaN 2021-07-02 NaN 2021-07-03 NaN 2021-07-04 NaN 2021-07-05 NaN 2021-07-06 NaN 2021-07-07 NaN 2021-07-08 NaN 2021-07-09 NaN 2021-07-10 0.413176 2021-07-11 0.489658 2021-07-12 0.342502 2021-07-28 -0.162861 2021-07-29 -0.214442 2021-07-30 -0.257007 ... 2023-02-17 0.289090 2023-02-18 -0.049574 2023-02-19 -0.095974 2023-02-20 -0.226629 Freq: D, Length: 600, dtype: float64 1234567import matplotlib.pyplot as plt%matplotlib inlineplt.figure(figsize=(15, 5))df.plot(style=&#x27;r--&#x27;)df.rolling(window=10).mean().plot(style=&#x27;b&#x27;) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1bafae87630&gt; 可以看到滑动窗口的平均值更平稳一些","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"时间序列——pandas的数据重采样","slug":"时间序列——pandas的数据重采样","date":"2021-05-25T16:48:15.000Z","updated":"2021-05-25T16:48:52.467Z","comments":true,"path":"20210526/时间序列——pandas的数据重采样.html","link":"","permalink":"https://xxren8218.github.io/20210526/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E2%80%94%E2%80%94pandas%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8D%E9%87%87%E6%A0%B7.html","excerpt":"","text":"pandas数据重采样 时间数据由一个频率转换到另一个频率 降采样 升采样 12import pandas as pdimport numpy as np 123rng = pd.date_range(&#x27;1/1/2011&#x27;, periods=90, freq=&#x27;D&#x27;)ts = pd.Series(np.random.randn(len(rng)), index=rng)ts.head() 2011-01-01 -1.025562 2011-01-02 0.410895 2011-01-03 0.660311 2011-01-04 0.710293 2011-01-05 0.444985 Freq: D, dtype: float64 1ts.resample(&#x27;M&#x27;).sum() 2011-01-31 2.510102 2011-02-28 0.583209 2011-03-31 2.749411 Freq: M, dtype: float64 1ts.resample(&#x27;3D&#x27;).sum() 2011-01-01 0.045643 2011-01-04 -2.255206 2011-01-07 0.571142 2011-01-10 0.835032 2011-01-13 -0.396766 2011-01-16 -1.156253 2011-01-19 -1.286884 2011-01-22 2.883952 2011-01-25 1.566908 2011-01-28 1.435563 2011-01-31 0.311565 2011-02-03 -2.541235 2011-02-06 0.317075 2011-02-09 1.598877 2011-02-12 -1.950509 2011-02-15 2.928312 2011-02-18 -0.733715 2011-02-21 1.674817 2011-02-24 -2.078872 2011-02-27 2.172320 2011-03-02 -2.022104 2011-03-05 -0.070356 2011-03-08 1.276671 2011-03-11 -2.835132 2011-03-14 -1.384113 2011-03-17 1.517565 2011-03-20 -0.550406 2011-03-23 0.773430 2011-03-26 2.244319 2011-03-29 2.951082 Freq: 3D, dtype: float64 12day3Ts = ts.resample(&#x27;3D&#x27;).mean()day3Ts 2011-01-01 0.015214 2011-01-04 -0.751735 2011-01-07 0.190381 2011-01-10 0.278344 2011-01-13 -0.132255 2011-01-16 -0.385418 2011-01-19 -0.428961 2011-01-22 0.961317 2011-01-25 0.522303 2011-01-28 0.478521 2011-01-31 0.103855 2011-02-03 -0.847078 2011-02-06 0.105692 2011-02-09 0.532959 2011-02-12 -0.650170 2011-02-15 0.976104 2011-02-18 -0.244572 2011-02-21 0.558272 2011-02-24 -0.692957 2011-02-27 0.724107 2011-03-02 -0.674035 2011-03-05 -0.023452 2011-03-08 0.425557 2011-03-11 -0.945044 2011-03-14 -0.461371 2011-03-17 0.505855 2011-03-20 -0.183469 2011-03-23 0.257810 2011-03-26 0.748106 2011-03-29 0.983694 Freq: 3D, dtype: float64 降采样容易，但是升采样就有些难度了。 将上面的3天的数据变成1天统计的数据 1print(day3Ts.resample(&#x27;D&#x27;).asfreq()) 2011-01-01 0.015214 2011-01-02 NaN 2011-01-03 NaN 2011-01-04 -0.751735 2011-01-05 NaN 2011-01-06 NaN 2011-01-07 0.190381 2011-01-08 NaN 2011-01-09 NaN 2011-01-10 0.278344 2011-01-11 NaN 2011-01-12 NaN 2011-01-13 -0.132255 2011-01-14 NaN 2011-01-15 NaN 2011-01-16 -0.385418 2011-01-17 NaN 2011-01-18 NaN 2011-01-19 -0.428961 2011-01-20 NaN 2011-01-21 NaN 2011-01-22 0.961317 2011-01-23 NaN 2011-01-24 NaN 2011-01-25 0.522303 2011-01-26 NaN 2011-01-27 NaN 2011-01-28 0.478521 2011-01-29 NaN 2011-01-30 NaN ... 2011-02-28 NaN 2011-03-01 NaN 2011-03-02 -0.674035 2011-03-03 NaN 2011-03-04 NaN 2011-03-05 -0.023452 2011-03-06 NaN 2011-03-07 NaN 2011-03-08 0.425557 2011-03-09 NaN 2011-03-10 NaN 2011-03-11 -0.945044 2011-03-12 NaN 2011-03-13 NaN 2011-03-14 -0.461371 2011-03-15 NaN 2011-03-16 NaN 2011-03-17 0.505855 2011-03-18 NaN 2011-03-19 NaN 2011-03-20 -0.183469 2011-03-21 NaN 2011-03-22 NaN 2011-03-23 0.257810 2011-03-24 NaN 2011-03-25 NaN 2011-03-26 0.748106 2011-03-27 NaN 2011-03-28 NaN 2011-03-29 0.983694 Freq: D, Length: 88, dtype: float64 插值方法： ffill 空值取前面的值 bfill 空值取后面的值 interpolate 线性取值 1day3Ts.resample(&#x27;D&#x27;).ffill(1) # 按照前面的数据进行插值 2011-01-01 0.015214 2011-01-02 0.015214 2011-01-03 NaN 2011-01-04 -0.751735 2011-01-05 -0.751735 2011-01-06 NaN 2011-01-07 0.190381 2011-01-08 0.190381 2011-01-09 NaN 2011-01-10 0.278344 2011-01-11 0.278344 2011-01-12 NaN 2011-01-13 -0.132255 2011-01-14 -0.132255 2011-01-15 NaN 2011-01-16 -0.385418 2011-01-17 -0.385418 2011-01-18 NaN 2011-01-19 -0.428961 2011-01-20 -0.428961 2011-01-21 NaN 2011-01-22 0.961317 2011-01-23 0.961317 2011-01-24 NaN 2011-01-25 0.522303 2011-01-26 0.522303 2011-01-27 NaN 2011-01-28 0.478521 2011-01-29 0.478521 2011-01-30 NaN ... 2011-02-28 0.724107 2011-03-01 NaN 2011-03-02 -0.674035 2011-03-03 -0.674035 2011-03-04 NaN 2011-03-05 -0.023452 2011-03-06 -0.023452 2011-03-07 NaN 2011-03-08 0.425557 2011-03-09 0.425557 2011-03-10 NaN 2011-03-11 -0.945044 2011-03-12 -0.945044 2011-03-13 NaN 2011-03-14 -0.461371 2011-03-15 -0.461371 2011-03-16 NaN 2011-03-17 0.505855 2011-03-18 0.505855 2011-03-19 NaN 2011-03-20 -0.183469 2011-03-21 -0.183469 2011-03-22 NaN 2011-03-23 0.257810 2011-03-24 0.257810 2011-03-25 NaN 2011-03-26 0.748106 2011-03-27 0.748106 2011-03-28 NaN 2011-03-29 0.983694 Freq: D, Length: 88, dtype: float64 1day3Ts.resample(&#x27;D&#x27;).bfill(1) 2011-01-01 0.015214 2011-01-02 NaN 2011-01-03 -0.751735 2011-01-04 -0.751735 2011-01-05 NaN 2011-01-06 0.190381 2011-01-07 0.190381 2011-01-08 NaN 2011-01-09 0.278344 2011-01-10 0.278344 2011-01-11 NaN 2011-01-12 -0.132255 2011-01-13 -0.132255 2011-01-14 NaN 2011-01-15 -0.385418 2011-01-16 -0.385418 2011-01-17 NaN 2011-01-18 -0.428961 2011-01-19 -0.428961 2011-01-20 NaN 2011-01-21 0.961317 2011-01-22 0.961317 2011-01-23 NaN 2011-01-24 0.522303 2011-01-25 0.522303 2011-01-26 NaN 2011-01-27 0.478521 2011-01-28 0.478521 2011-01-29 NaN 2011-01-30 0.103855 ... 2011-02-28 NaN 2011-03-01 -0.674035 2011-03-02 -0.674035 2011-03-03 NaN 2011-03-04 -0.023452 2011-03-05 -0.023452 2011-03-06 NaN 2011-03-07 0.425557 2011-03-08 0.425557 2011-03-09 NaN 2011-03-10 -0.945044 2011-03-11 -0.945044 2011-03-12 NaN 2011-03-13 -0.461371 2011-03-14 -0.461371 2011-03-15 NaN 2011-03-16 0.505855 2011-03-17 0.505855 2011-03-18 NaN 2011-03-19 -0.183469 2011-03-20 -0.183469 2011-03-21 NaN 2011-03-22 0.257810 2011-03-23 0.257810 2011-03-24 NaN 2011-03-25 0.748106 2011-03-26 0.748106 2011-03-27 NaN 2011-03-28 0.983694 2011-03-29 0.983694 Freq: D, Length: 88, dtype: float64 1day3Ts.resample(&#x27;D&#x27;).interpolate(&#x27;linear&#x27;) # 1和4有数据，连城线，在对应点取值。 2011-01-01 0.015214 2011-01-02 -0.240435 2011-01-03 -0.496085 2011-01-04 -0.751735 2011-01-05 -0.437697 2011-01-06 -0.123658 2011-01-07 0.190381 2011-01-08 0.219702 2011-01-09 0.249023 2011-01-10 0.278344 2011-01-11 0.141478 2011-01-12 0.004611 2011-01-13 -0.132255 2011-01-14 -0.216643 2011-01-15 -0.301030 2011-01-16 -0.385418 2011-01-17 -0.399932 2011-01-18 -0.414447 2011-01-19 -0.428961 2011-01-20 0.034465 2011-01-21 0.497891 2011-01-22 0.961317 2011-01-23 0.814979 2011-01-24 0.668641 2011-01-25 0.522303 2011-01-26 0.507709 2011-01-27 0.493115 2011-01-28 0.478521 2011-01-29 0.353632 2011-01-30 0.228744 ... 2011-02-28 0.258060 2011-03-01 -0.207988 2011-03-02 -0.674035 2011-03-03 -0.457174 2011-03-04 -0.240313 2011-03-05 -0.023452 2011-03-06 0.126218 2011-03-07 0.275887 2011-03-08 0.425557 2011-03-09 -0.031310 2011-03-10 -0.488177 2011-03-11 -0.945044 2011-03-12 -0.783820 2011-03-13 -0.622595 2011-03-14 -0.461371 2011-03-15 -0.138962 2011-03-16 0.183446 2011-03-17 0.505855 2011-03-18 0.276080 2011-03-19 0.046306 2011-03-20 -0.183469 2011-03-21 -0.036376 2011-03-22 0.110717 2011-03-23 0.257810 2011-03-24 0.421242 2011-03-25 0.584674 2011-03-26 0.748106 2011-03-27 0.826636 2011-03-28 0.905165 2011-03-29 0.983694 Freq: D, Length: 88, dtype: float64","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"时间序列——pandas生成时间序列","slug":"时间序列——pandas生成时间序列","date":"2021-05-25T16:46:08.000Z","updated":"2021-05-25T16:56:35.852Z","comments":true,"path":"20210526/时间序列——pandas生成时间序列.html","link":"","permalink":"https://xxren8218.github.io/20210526/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E2%80%94%E2%80%94pandas%E7%94%9F%E6%88%90%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97.html","excerpt":"","text":"pandas生成时间序列123import pandas as pdimport numpy as npfrom datetime import datetime as dt 时间序列 时间戳（timestamp）具体到时间点 固定周期（period） 时间间隔（interval） date_range 可以指定开始时间与周期 H：小时 D：天 M：月 123# TIMES # 2021 Jul 1 7/1/2021 1/7/2021 2021-07-01 2021/07/01都可以rng = pd.date_range(&#x27;2021-07-01&#x27;, periods = 10, freq = &#x27;3D&#x27;)rng DatetimeIndex([&#39;2021-07-01&#39;, &#39;2021-07-04&#39;, &#39;2021-07-07&#39;, &#39;2021-07-10&#39;, &#39;2021-07-13&#39;, &#39;2021-07-16&#39;, &#39;2021-07-19&#39;, &#39;2021-07-22&#39;, &#39;2021-07-25&#39;, &#39;2021-07-28&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;3D&#39;) 123time=pd.Series(np.random.randn(20), index=pd.date_range(dt(2021,1,1),periods=20))print(time) 2021-01-01 0.608358 2021-01-02 -0.402434 2021-01-03 -1.917616 2021-01-04 1.568936 2021-01-05 -0.668169 2021-01-06 -0.148038 2021-01-07 -0.393429 2021-01-08 0.052060 2021-01-09 -0.074732 2021-01-10 1.457457 2021-01-11 -0.106878 2021-01-12 0.340505 2021-01-13 0.694755 2021-01-14 0.261571 2021-01-15 0.231021 2021-01-16 -0.454639 2021-01-17 -0.313779 2021-01-18 0.311580 2021-01-19 -1.375962 2021-01-20 1.450409 Freq: D, dtype: float64 truncate过滤1time.truncate(before=&#x27;2021-1-10&#x27;) # 之前的都没有了 2021-01-10 1.457457 2021-01-11 -0.106878 2021-01-12 0.340505 2021-01-13 0.694755 2021-01-14 0.261571 2021-01-15 0.231021 2021-01-16 -0.454639 2021-01-17 -0.313779 2021-01-18 0.311580 2021-01-19 -1.375962 2021-01-20 1.450409 Freq: D, dtype: float64 1time.truncate(after=&#x27;2021-1-10&#x27;) # 之后的都没了 2021-01-01 0.608358 2021-01-02 -0.402434 2021-01-03 -1.917616 2021-01-04 1.568936 2021-01-05 -0.668169 2021-01-06 -0.148038 2021-01-07 -0.393429 2021-01-08 0.052060 2021-01-09 -0.074732 2021-01-10 1.457457 Freq: D, dtype: float64 1print(time[&#x27;2021-01-15&#x27;]) 0.2310208242057297 1print(time[&#x27;2021-01-15&#x27;:&#x27;2021-01-20&#x27;]) 2021-01-15 0.231021 2021-01-16 -0.454639 2021-01-17 -0.313779 2021-01-18 0.311580 2021-01-19 -1.375962 2021-01-20 1.450409 Freq: D, dtype: float64 12data=pd.date_range(&#x27;2020-01-01&#x27;,&#x27;2021-01-01&#x27;,freq=&#x27;M&#x27;)print(data) DatetimeIndex([&#39;2020-01-31&#39;, &#39;2020-02-29&#39;, &#39;2020-03-31&#39;, &#39;2020-04-30&#39;, &#39;2020-05-31&#39;, &#39;2020-06-30&#39;, &#39;2020-07-31&#39;, &#39;2020-08-31&#39;, &#39;2020-09-30&#39;, &#39;2020-10-31&#39;, &#39;2020-11-30&#39;, &#39;2020-12-31&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;M&#39;) 12# 时间戳pd.Timestamp(&#x27;2021-07-10&#x27;) Timestamp(&#39;2021-07-10 00:00:00&#39;) 12# 可以指定更多细节pd.Timestamp(&#x27;2021-07-10 10&#x27;) Timestamp(&#39;2021-07-10 10:00:00&#39;) 1pd.Timestamp(&#x27;2021-07-10 10:15&#x27;) Timestamp(&#39;2021-07-10 10:15:00&#39;) 1# How much detail can you add? 1t = pd.Timestamp(&#x27;2021-07-10 10:15&#x27;) 12# 时间区间pd.Period(&#x27;2021-01&#x27;) Period(&#39;2021-01&#39;, &#39;M&#39;) 1pd.Period(&#x27;2021-01-01&#x27;) Period(&#39;2021-01-01&#39;, &#39;D&#39;) 12# TIME OFFSETS 对时间的加减！pd.Timedelta(&#x27;1 day&#x27;) Timedelta(&#39;1 days 00:00:00&#39;) 1pd.Period(&#x27;2021-01-01 10:10&#x27;) + pd.Timedelta(&#x27;1 day&#x27;) Period(&#39;2021-01-02 10:10&#39;, &#39;T&#39;) 1pd.Timestamp(&#x27;2021-01-01 10:10&#x27;) + pd.Timedelta(&#x27;1 day&#x27;) Timestamp(&#39;2021-01-02 10:10:00&#39;) 1pd.Timestamp(&#x27;2021-01-01 10:10&#x27;) + pd.Timedelta(&#x27;15 ns&#x27;) Timestamp(&#39;2021-01-01 10:10:00.000000015&#39;) 1p1 = pd.period_range(&#x27;2021-01-01 10:10&#x27;, freq = &#x27;25H&#x27;, periods = 10) 1p2 = pd.period_range(&#x27;2021-01-01 10:10&#x27;, freq = &#x27;1D1H&#x27;, periods = 10) 1p1 PeriodIndex([&#39;2021-01-01 10:00&#39;, &#39;2021-01-02 11:00&#39;, &#39;2021-01-03 12:00&#39;, &#39;2021-01-04 13:00&#39;, &#39;2021-01-05 14:00&#39;, &#39;2021-01-06 15:00&#39;, &#39;2021-01-07 16:00&#39;, &#39;2021-01-08 17:00&#39;, &#39;2021-01-09 18:00&#39;, &#39;2021-01-10 19:00&#39;], dtype=&#39;period[25H]&#39;, freq=&#39;25H&#39;) 1p2 PeriodIndex([&#39;2021-01-01 10:00&#39;, &#39;2021-01-02 11:00&#39;, &#39;2021-01-03 12:00&#39;, &#39;2021-01-04 13:00&#39;, &#39;2021-01-05 14:00&#39;, &#39;2021-01-06 15:00&#39;, &#39;2021-01-07 16:00&#39;, &#39;2021-01-08 17:00&#39;, &#39;2021-01-09 18:00&#39;, &#39;2021-01-10 19:00&#39;], dtype=&#39;period[25H]&#39;, freq=&#39;25H&#39;) 1234# 指定索引rng = pd.date_range(&#x27;2016 Jul 1&#x27;, periods = 10, freq = &#x27;D&#x27;)rngpd.Series(range(len(rng)), index = rng) 2016-07-01 0 2016-07-02 1 2016-07-03 2 2016-07-04 3 2016-07-05 4 2016-07-06 5 2016-07-07 6 2016-07-08 7 2016-07-09 8 2016-07-10 9 Freq: D, dtype: int32 123periods = [pd.Period(&#x27;2021-01&#x27;), pd.Period(&#x27;2021-02&#x27;), pd.Period(&#x27;2021-03&#x27;)]ts = pd.Series(np.random.randn(len(periods)), index = periods)ts 2021-01 0.997422 2021-02 2.006248 2021-03 0.606481 Freq: M, dtype: float64 1type(ts.index) pandas.core.indexes.period.PeriodIndex 123# 时间戳和时间周期可以转换ts = pd.Series(range(10), pd.date_range(&#x27;07-10-21 8:00&#x27;, periods = 10, freq = &#x27;H&#x27;))ts 2021-07-10 08:00:00 0 2021-07-10 09:00:00 1 2021-07-10 10:00:00 2 2021-07-10 11:00:00 3 2021-07-10 12:00:00 4 2021-07-10 13:00:00 5 2021-07-10 14:00:00 6 2021-07-10 15:00:00 7 2021-07-10 16:00:00 8 2021-07-10 17:00:00 9 Freq: H, dtype: int32 12ts_period = ts.to_period()ts_period 2021-07-10 08:00 0 2021-07-10 09:00 1 2021-07-10 10:00 2 2021-07-10 11:00 3 2021-07-10 12:00 4 2021-07-10 13:00 5 2021-07-10 14:00 6 2021-07-10 15:00 7 2021-07-10 16:00 8 2021-07-10 17:00 9 Freq: H, dtype: int32 1ts_period[&#x27;2021-07-10 08:30&#x27;:&#x27;2021-07-10 11:45&#x27;] # 时间周期包括8:00 2021-07-10 08:00 0 2021-07-10 09:00 1 2021-07-10 10:00 2 2021-07-10 11:00 3 Freq: H, dtype: int32 1ts[&#x27;2021-07-10 08:30&#x27;:&#x27;2021-07-10 11:45&#x27;] # 时间戳不包活8:00 2021-07-10 09:00:00 1 2021-07-10 10:00:00 2 2021-07-10 11:00:00 3 Freq: H, dtype: int32","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"支持向量机","slug":"支持向量机","date":"2021-05-25T10:42:17.000Z","updated":"2021-05-25T10:49:25.077Z","comments":true,"path":"20210525/支持向量机.html","link":"","permalink":"https://xxren8218.github.io/20210525/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.html","excerpt":"","text":"支持向量机（SVM） 低纬不可分的东西转化为高纬可分割的东西 1234567%matplotlib inlineimport numpy as npimport matplotlib.pyplot as pltfrom scipy import stats# use seaborn plotting defaultsimport seaborn as sns; sns.set() 支持向量基本原理 如何解决这个线性不可分问题呢？咱们给它映射到高维来试试 $z=x^2+y^2$. 例子12345# 随机来点数据from sklearn.datasets.samples_generator import make_blobsX, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60) # cluster_std=0.60 簇的离散程度。越小，越集中，越好分类plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#x27;autumn&#x27;) &lt;matplotlib.collections.PathCollection at 0x1ba274e1ac8&gt; 随便的画几条分割线，哪个好来这？ 12345678xfit = np.linspace(-1, 3.5)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#x27;autumn&#x27;)plt.plot([0.6], [2.1], &#x27;x&#x27;, color=&#x27;red&#x27;, markeredgewidth=2, markersize=10)for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]: plt.plot(xfit, m * xfit + b, &#x27;-k&#x27;)plt.xlim(-1, 3.5); Support Vector Machines: 最小化 雷区12345678910xfit = np.linspace(-1, 3.5)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#x27;autumn&#x27;)for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]: yfit = m * xfit + b plt.plot(xfit, yfit, &#x27;-k&#x27;) plt.fill_between(xfit, yfit - d, yfit + d, edgecolor=&#x27;none&#x27;, color=&#x27;#AAAAAA&#x27;, alpha=0.4)plt.xlim(-1, 3.5); 训练一个基本的SVM123from sklearn.svm import SVC # &quot;Support vector classifier&quot;model = SVC(kernel=&#x27;linear&#x27;)model.fit(X, y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) 123456789101112131415161718192021222324252627# 绘图函数def plot_svc_decision_function(model, ax=None, plot_support=True): &quot;&quot;&quot;Plot the decision function for a 2D SVC&quot;&quot;&quot; if ax is None: ax = plt.gca() xlim = ax.get_xlim() ylim = ax.get_ylim() # create grid to evaluate model x = np.linspace(xlim[0], xlim[1], 30) y = np.linspace(ylim[0], ylim[1], 30) Y, X = np.meshgrid(y, x) xy = np.vstack([X.ravel(), Y.ravel()]).T P = model.decision_function(xy).reshape(X.shape) # plot decision boundary and margins ax.contour(X, Y, P, colors=&#x27;k&#x27;, levels=[-1, 0, 1], alpha=0.5, linestyles=[&#x27;--&#x27;, &#x27;-&#x27;, &#x27;--&#x27;]) # plot support vectors if plot_support: ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, linewidth=1, facecolors=&#x27;none&#x27;); ax.set_xlim(xlim) ax.set_ylim(ylim) 12plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#x27;autumn&#x27;)plot_svc_decision_function(model); 这条线就是我们希望得到的决策边界啦 观察发现有3个点做了特殊的标记，它们恰好都是边界上的点 它们就是我们的support vectors（支持向量） 在Scikit-Learn中, 它们存储在这个位置 support_vectors_（一个属性） 1model.support_vectors_ array([[ 0.44359863, 3.11530945], [ 2.33812285, 3.43116792], [ 2.06156753, 1.96918596]]) 观察可以发现，只需要支持向量我们就可以把模型构建出来 接下来我们尝试一下，用不同多的数据点，看看效果会不会发生变化 分别使用60个和120个数据点 12345678910111213141516171819def plot_svm(N=10, ax=None): X, y = make_blobs(n_samples=200, centers=2, random_state=0, cluster_std=0.60) X = X[:N] y = y[:N] model = SVC(kernel=&#x27;linear&#x27;, C=1E10) model.fit(X, y) ax = ax or plt.gca() ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#x27;autumn&#x27;) ax.set_xlim(-1, 4) ax.set_ylim(-1, 6) plot_svc_decision_function(model, ax)fig, ax = plt.subplots(1, 2, figsize=(16, 6))fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)for axi, N in zip(ax, [60, 120]): plot_svm(N, axi) axi.set_title(&#x27;N = &#123;0&#125;&#x27;.format(N)) 左边是60个点的结果，右边的是120个点的结果 观察发现，只要支持向量没变，其他的数据怎么加无所谓！ 引入核函数的SVM 首先我们先用线性的核来看一下在下面这样比较难的数据集上还能分了吗？ 1234567from sklearn.datasets.samples_generator import make_circlesX, y = make_circles(100, factor=.1, noise=.1)clf = SVC(kernel=&#x27;linear&#x27;).fit(X, y)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#x27;autumn&#x27;)plot_svc_decision_function(clf, plot_support=False); 坏菜喽，分不了了，那咋办呢？试试高维核变换吧！ We can visualize this extra data dimension using a three-dimensional plot: 123456789101112# 加入了新的维度rfrom mpl_toolkits import mplot3dr = np.exp(-(X ** 2).sum(1))def plot_3D(elev=30, azim=30, X=X, y=y): ax = plt.subplot(projection=&#x27;3d&#x27;) ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap=&#x27;autumn&#x27;) ax.view_init(elev=elev, azim=azim) ax.set_xlabel(&#x27;x&#x27;) ax.set_ylabel(&#x27;y&#x27;) ax.set_zlabel(&#x27;r&#x27;)plot_3D(elev=45, azim=45, X=X, y=y) 123# 加入径向基函数（高斯核函数或者RBF核函数都一样）clf = SVC(kernel=&#x27;rbf&#x27;, C=1E6)clf.fit(X, y) SVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) 12345# 这回牛逼了！plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#x27;autumn&#x27;)plot_svc_decision_function(clf)plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=300, lw=1, facecolors=&#x27;none&#x27;); 使用这种核支持向量机，我们学习一个合适的非线性决策边界。这种核变换策略在机器学习中经常被使用！ 调节SVM参数: Soft Margin问题调节C参数 当C趋近于无穷大时：意味着分类严格不能有错误 当C趋近于很小的时：意味着可以有更大的错误容忍 123X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.8)plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#x27;autumn&#x27;); 1234567891011121314X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.8)fig, ax = plt.subplots(1, 2, figsize=(16, 6))fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)for axi, C in zip(ax, [10.0, 0.1]): # 两个C参数 10.0和0.1 model = SVC(kernel=&#x27;linear&#x27;, C=C).fit(X, y) axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#x27;autumn&#x27;) plot_svc_decision_function(model, axi) axi.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, lw=1, facecolors=&#x27;none&#x27;); axi.set_title(&#x27;C = &#123;0:.1f&#125;&#x27;.format(C), size=14) 左边的泛化能力小。 右边的泛化能力强一点。 1234567891011121314X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=1.1)fig, ax = plt.subplots(1, 2, figsize=(16, 6))fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)for axi, gamma in zip(ax, [10.0, 0.1]): model = SVC(kernel=&#x27;rbf&#x27;, gamma=gamma).fit(X, y) # gamma越高模型越复杂。返还能力越弱。 axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#x27;autumn&#x27;) plot_svc_decision_function(model, axi) axi.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, lw=1, facecolors=&#x27;none&#x27;); axi.set_title(&#x27;gamma = &#123;0:.1f&#125;&#x27;.format(gamma), size=14) 越复杂的边界泛化能力越低。 Example: Face RecognitionAs an example of support vector machines in action, let’s take a look at the facial recognition problem.We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures.A fetcher for the dataset is built into Scikit-Learn: 1234from sklearn.datasets import fetch_lfw_peoplefaces = fetch_lfw_people(min_faces_per_person=60)print(faces.target_names)print(faces.images.shape) [&#39;Ariel Sharon&#39; &#39;Colin Powell&#39; &#39;Donald Rumsfeld&#39; &#39;George W Bush&#39; &#39;Gerhard Schroeder&#39; &#39;Hugo Chavez&#39; &#39;Junichiro Koizumi&#39; &#39;Tony Blair&#39;] (1348, 62, 47) Let’s plot a few of these faces to see what we’re working with: 12345fig, ax = plt.subplots(3, 5)for i, axi in enumerate(ax.flat): axi.imshow(faces.images[i], cmap=&#x27;bone&#x27;) axi.set(xticks=[], yticks=[], xlabel=faces.target_names[faces.target[i]]) 每个图的大小是 [62×47] 在这里我们就把每一个像素点当成了一个特征，但是这样特征太多了，用PCA降维一下吧！ 12345678from sklearn.svm import SVC#from sklearn.decomposition import RandomizedPCAfrom sklearn.decomposition import PCAfrom sklearn.pipeline import make_pipelinepca = PCA(n_components=150, whiten=True, random_state=42)svc = SVC(kernel=&#x27;rbf&#x27;, class_weight=&#x27;balanced&#x27;)model = make_pipeline(pca, svc) 123from sklearn.model_selection import train_test_splitXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=40) 使用grid search cross-validation来选择我们的参数 1234567from sklearn.model_selection import GridSearchCVparam_grid = &#123;&#x27;svc__C&#x27;: [1, 5, 10], &#x27;svc__gamma&#x27;: [0.0001, 0.0005, 0.001]&#125;grid = GridSearchCV(model, param_grid)%time grid.fit(Xtrain, ytrain)print(grid.best_params_) Wall time: 51.5 s &#123;&#39;svc__C&#39;: 5, &#39;svc__gamma&#39;: 0.001&#125; 123model = grid.best_estimator_yfit = model.predict(Xtest)yfit.shape (337,) 看看咋样吧！ 1234567fig, ax = plt.subplots(4, 6)for i, axi in enumerate(ax.flat): axi.imshow(Xtest[i].reshape(62, 47), cmap=&#x27;bone&#x27;) axi.set(xticks=[], yticks=[]) axi.set_ylabel(faces.target_names[yfit[i]].split()[-1], color=&#x27;black&#x27; if yfit[i] == ytest[i] else &#x27;red&#x27;)fig.suptitle(&#x27;Predicted Names; Incorrect Labels in Red&#x27;, size=14); 123from sklearn.metrics import classification_reportprint(classification_report(ytest, yfit, target_names=faces.target_names)) precision recall f1-score support Ariel Sharon 0.50 0.50 0.50 16 Colin Powell 0.69 0.81 0.75 54 Donald Rumsfeld 0.83 0.85 0.84 34 George W Bush 0.94 0.88 0.91 136 Gerhard Schroeder 0.72 0.85 0.78 27 Hugo Chavez 0.81 0.72 0.76 18 Junichiro Koizumi 0.87 0.87 0.87 15 Tony Blair 0.85 0.76 0.80 37 avg / total 0.83 0.82 0.82 337 精度(precision) = 正确预测的个数(TP)/被预测正确的个数(TP+FP) 召回率(recall)=正确预测的个数(TP)/预测个数(TP+FN) F1 = 2精度召回率/(精度+召回率) 1234567from sklearn.metrics import confusion_matrixmat = confusion_matrix(ytest, yfit)sns.heatmap(mat.T, square=True, annot=True, fmt=&#x27;d&#x27;, cbar=False, xticklabels=faces.target_names, yticklabels=faces.target_names)plt.xlabel(&#x27;true label&#x27;)plt.ylabel(&#x27;predicted label&#x27;); 这样显示出来能帮助我们查看哪些人更容易弄混","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"剑指Offer（三十二）：把数组排成最小的数","slug":"剑指Offer（三十二）：把数组排成最小的数","date":"2021-05-25T10:40:41.000Z","updated":"2021-05-25T10:41:45.030Z","comments":true,"path":"20210525/剑指Offer（三十二）：把数组排成最小的数.html","link":"","permalink":"https://xxren8218.github.io/20210525/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9A%E6%8A%8A%E6%95%B0%E7%BB%84%E6%8E%92%E6%88%90%E6%9C%80%E5%B0%8F%E7%9A%84%E6%95%B0.html","excerpt":"","text":"1.题目输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 2.思路遇到这个题，全排列当然可以做，但是时间复杂度为O(n!)。在这里我们自己定义一个规则，对拼接后的字符串进行比较。 排序规则如下 若ab &gt; ba 则 a 大于 b， 若ab &lt; ba 则 a 小于 b， 若ab = ba 则 a 等于 b； 根据上述规则，我们需要先将数字转换成字符串再进行比较，因为需要串起来进行比较。比较完之后，按顺序输出即可。 3.代码123456789# -*- coding:utf-8 -*-class Solution: def PrintMinNumber(self, numbers): # write code here if not numbers: return &#x27;&#x27; compare = lambda a,b:cmp(str(a)+str(b),str(b)+str(a)) # 注意 lambda 匿名函数的使用。python2中sorted（可迭代对象,key,cmp）的使用,以及为什么要比较字符串。 li = sorted(numbers, cmp=compare) return &#x27;&#x27;.join(str(i) for i in li)","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"字符串","slug":"字符串","permalink":"https://xxren8218.github.io/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"name":"排序","slug":"排序","permalink":"https://xxren8218.github.io/tags/%E6%8E%92%E5%BA%8F/"},{"name":"内置函数","slug":"内置函数","permalink":"https://xxren8218.github.io/tags/%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/"}]},{"title":"剑指Offer（三十）：连续子数组的最大和","slug":"剑指Offer（三十）：连续子数组的最大和","date":"2021-05-25T10:38:01.000Z","updated":"2021-05-25T10:40:21.132Z","comments":true,"path":"20210525/剑指Offer（三十）：连续子数组的最大和.html","link":"","permalink":"https://xxren8218.github.io/20210525/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%E5%8D%81%EF%BC%89%EF%BC%9A%E8%BF%9E%E7%BB%AD%E5%AD%90%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%A4%A7%E5%92%8C.html","excerpt":"","text":"1.题目输入一个整型数组，数组中的一个或连续多个整数组成一个子数组。求所有子数组的和的最大值。 要求时间复杂度为O(n)。 例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。(子向量的长度至少是1) 2.思路数组分析：下图是我们计算数组（1，-2，3，10，-4，7，2，-5）中子数组的最大和的过程。通过分析我们发现，累加的子数组和，如果大于零，那么我们继续累加就行；否则，则需要剔除原来的累加和重新开始。 3.代码12345678910111213141516171819# -*- coding:utf-8 -*-class Solution(object): def maxSubArray(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; if not nums: return [] max = nums[0] cur = nums[0] for i in nums[1:]: if cur &lt; 0: cur = i else: cur += i if cur &gt; max: max = cur return max 思路二——动态规划 代码12345678910111213141516171819class Solution(object): def maxSubArray(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; if not nums: return [] # dp[i]表示以num[i]结尾的最大的连续子数和 dp = [0]*len(nums) # 初始值 dp[0] = nums[0] for i in range(1,len(nums)): # 找出元素关系式 if dp[i-1] &gt;= 0: dp[i] = dp[i-1] + nums[i] else: dp[i] = nums[i] return max(dp)","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}]},{"title":"剑指Offer（二十八）：数组中出现次数超过一半的数字","slug":"剑指Offer（二十八）：数组中出现次数超过一半的数字","date":"2021-05-24T10:51:43.000Z","updated":"2021-05-24T10:53:48.799Z","comments":true,"path":"20210524/剑指Offer（二十八）：数组中出现次数超过一半的数字.html","link":"","permalink":"https://xxren8218.github.io/20210524/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%8C%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9A%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E8%B6%85%E8%BF%87%E4%B8%80%E5%8D%8A%E7%9A%84%E6%95%B0%E5%AD%97.html","excerpt":"","text":"1.题目数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0 2.思路一——暴力解法循环遍历所有的数字，调用方法count()，与数组的个数的1/2进行比较即可。 3.代码123456789# -*- coding:utf-8 -*-class Solution: def MoreThanHalfNum_Solution(self, numbers): # write code here for i in numbers: if numbers.count(i)&gt;len(numbers)//2: return i else: return 0 其他解法上述解法在力扣超时。有别的解法， 摩尔投票1234567891011121314class Solution(object): def majorityElement(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; vote = 0 for num in nums: if vote == 0: x = num if num == x: vote += 1 else: vote -= 1 return x 排序12345678class Solution(object): def majorityElement(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; nums.sort() return nums[len(nums)/2] 哈希表1234567891011121314151617181920class Solution(object): def majorityElement(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; if not nums: return [] if len(nums) == 1: return nums[0] hashable = dict() for num in nums: if num not in hashable: hashable[num] = 1 else: hashable[num] += 1 if hashable[num] &gt; len(nums) / 2: return num return []","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"哈希表","slug":"哈希表","permalink":"https://xxren8218.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"辅助素组","slug":"辅助素组","permalink":"https://xxren8218.github.io/tags/%E8%BE%85%E5%8A%A9%E7%B4%A0%E7%BB%84/"}]},{"title":"剑指Offer（十三）：调整数组顺序使奇数位于偶数前面","slug":"剑指Offer（十三）：调整数组顺序使奇数位于偶数前面","date":"2021-05-24T10:49:25.000Z","updated":"2021-05-24T10:50:50.096Z","comments":true,"path":"20210524/剑指Offer（十三）：调整数组顺序使奇数位于偶数前面.html","link":"","permalink":"https://xxren8218.github.io/20210524/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9A%E8%B0%83%E6%95%B4%E6%95%B0%E7%BB%84%E9%A1%BA%E5%BA%8F%E4%BD%BF%E5%A5%87%E6%95%B0%E4%BD%8D%E4%BA%8E%E5%81%B6%E6%95%B0%E5%89%8D%E9%9D%A2.html","excerpt":"","text":"1.题目输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 2.思路一——辅助数组对数组中的数字进行遍历，判断奇偶，分别放在两个辅助列表中，最后将两个辅助列表合并即可。 3.代码1234567891011class Solution: def reOrderArray(self , array): # write code here odd = [] even = [] for i in range(len(array)): if array[i] % 2: # 1 为True, 0 为 False odd.append(array[i]) else: even.append(array[i]) return odd + even 别的思路：——首尾双指针 首尾双指针定义头指针 left ，尾指针 right .left 一直往右移，直到它指向的值为偶数right 一直往左移， 直到它指向的值为奇数交换 nums[left] 和 nums[right] .重复上述操作，直到 left==right . 代码12345678910111213class Solution: def exchange(self, nums: List[int]) -&gt; List[int]: left, right = 0, len(nums) - 1 while left &lt;= right: while left &lt;= right and nums[left] % 2 == 1: left += 1 while left &lt;= right and nums[right] % 2 == 0: right -= 1 if left &gt; right: break nums[left], nums[right] = nums[right], nums[left] return nums 思路三——快慢双指针定义快慢双指针 fast 和 low ，fast 在前， low 在后 .fast的作用是向前搜索奇数位置，low的作用是指向下一个奇数应当存放的位置fast向前移动，当它搜索到奇数时，将它和 nums[low] 交换，此时 low 向前移动一个位置 .重复上述操作，直到 fast 指向数组末尾 . 代码1234567891011121314class Solution(object): def exchange(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[int] &quot;&quot;&quot; low, fast = 0, 0 while fast &lt; len(nums): if nums[fast] % 2 == 1: nums[low], nums[fast] = nums[fast], nums[low] low += 1 fast += 1 return nums","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"双指针","slug":"双指针","permalink":"https://xxren8218.github.io/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"快慢指针","slug":"快慢指针","permalink":"https://xxren8218.github.io/tags/%E5%BF%AB%E6%85%A2%E6%8C%87%E9%92%88/"}]},{"title":"剑指Offer（六）：旋转数组的最小数字","slug":"剑指Offer（六）：旋转数组的最小数字","date":"2021-05-24T10:46:50.000Z","updated":"2021-05-24T10:51:12.546Z","comments":true,"path":"20210524/剑指Offer（六）：旋转数组的最小数字.html","link":"","permalink":"https://xxren8218.github.io/20210524/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97.html","excerpt":"","text":"1.题目把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 2.思路一：线性查找O(n)直接调用min()方法即可，时间复杂度O(n) 3.代码12345678class Solution(object): def minArray(self, numbers): &quot;&quot;&quot; :type numbers: List[int] :rtype: int &quot;&quot;&quot; if not numbers:return 0 return min(numbers) 思路二：二分查找O（logn） 排序数组的查找问题首先考虑使用 二分法 解决，其可将 遍历法 的 线性级别 时间复杂度降低至 对数级别 。 代码1234567891011121314class Solution(object): def minArray(self, numbers): &quot;&quot;&quot; :type numbers: List[int] :rtype: int &quot;&quot;&quot; i, j = 0, len(numbers)-1 while i &lt; j: m = (i + j) / 2 if numbers[m] &gt; numbers[j]:i = m + 1 elif numbers[m] &lt; numbers[j]: j = m else: j -= 1 return numbers[i]","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"双指针","slug":"双指针","permalink":"https://xxren8218.github.io/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"二分法","slug":"二分法","permalink":"https://xxren8218.github.io/tags/%E4%BA%8C%E5%88%86%E6%B3%95/"}]},{"title":"剑指Offer（一）：二维数组（排序数组）中的查找","slug":"剑指Offer（一）：二维数组（排序数组）中的查找","date":"2021-05-24T10:44:43.000Z","updated":"2021-05-24T10:46:10.195Z","comments":true,"path":"20210524/剑指Offer（一）：二维数组（排序数组）中的查找.html","link":"","permalink":"https://xxren8218.github.io/20210524/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%EF%BC%88%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%EF%BC%89%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE.html","excerpt":"","text":"1.题目在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。 请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 2.思路一：暴力解法依次遍历整个数组的每一个元素，如果存在返回True,否则返回False 复杂度分析 时间复杂度：O(nm)。二维数组中的每个元素都被遍历，因此时间复杂度为二维数组的大小。 3.代码123456789101112class Solution(object): def findNumberIn2DArray(self, matrix, target): &quot;&quot;&quot; :type matrix: List[List[int]] :type target: int :rtype: bool &quot;&quot;&quot; for i in range(0,len(matrix)): for j in range(0,len(matrix[0])): if target == matrix[i][j]: return True return False 思路二：线性查找首先选取数组中右上角的数字。如果该数字等于要查找的数字，查找过程结束； 如果该数字大于要查找的数组，剔除这个数字所在的列； 如果该数字小于要查找的数字，剔除这个数字所在的行。 也就是说如果要查找的数字不在数组的右上角，则每一次都在数组的查找范围中剔除一行或者一列，这样每一步都可以缩小查找的范围，直到找到要查找的数字，或者查找范围为空。 复杂度分析时间复杂度：O(n+m)。访问到的下标的行最多增加 n 次，列最多减少 m 次，因此循环体最多执行 n + m 次。代码1234567891011121314151617class Solution(object): def findNumberIn2DArray(self, matrix, target): &quot;&quot;&quot; :type matrix: List[List[int]] :type target: int :rtype: bool &quot;&quot;&quot; if not matrix:return False rows = len(matrix) cols = len(matrix[0]) row = 0 col = cols - 1 while row &lt; rows and col &gt;= 0: if matrix[row][col] == target:return True elif matrix[row][col] &gt; target:col -= 1 else: row += 1 return False","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"双指针","slug":"双指针","permalink":"https://xxren8218.github.io/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"查找","slug":"查找","permalink":"https://xxren8218.github.io/tags/%E6%9F%A5%E6%89%BE/"}]},{"title":"剑指Offer（十一）：二进制中1的个数","slug":"剑指Offer（十一）：二进制中1的个数","date":"2021-05-20T11:19:52.000Z","updated":"2021-05-20T11:23:18.013Z","comments":true,"path":"20210520/剑指Offer（十一）：二进制中1的个数.html","link":"","permalink":"https://xxren8218.github.io/20210520/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0.html","excerpt":"","text":"1.题目输入一个二进制数，输出该数二进制表示中1的个数。其中负数用补码表示。 2.思路先与1进行与运算，若为1，则该位为1，再判断前一位。 前一位的判断直接可以使用位运算 &gt;&gt; 向右移动一位。 3.代码1234567891011class Solution(object): def hammingWeight(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; res = 0 while n: res += n &amp; 1 # 等价于 if n &amp; 1 == 1: res +=1，但是前一种判断次数更少。 n &gt;&gt;= 1 return res 思路二将二进制转化为字符串，直接进行进行调用方法count - 由于输入的二进制比如1011，计算机不会将其对待为 0b1011,而是认为是一千零一十一，所以先转为二进制 代码12345678class Solution(object): def hammingWeight(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; return str(bin(n)).count(&quot;1&quot;) 思路三 巧用 n&amp;(n-1)此处若输入一个整数呢？注意消除负数位的影响。 如果一个整数不为0，那么这个整数至少有一位是1。如果我们把这个整数减1，那么原来处在整数最右边的1就会变为0，原来在1后面的所有的0都会变成1(如果最右边的1后面还有0的话)。其余所有位将不会受到影响。 举个例子：一个二进制数1100，从右边数起第三位是处于最右边的一个1。减去1后，第三位变成0，它后面的两位0变成了1，而前面的1保持不变，因此得到的结果是1011.我们发现减1的结果是把最右边的一个1开始的所有位都取反了。这个时候如果我们再把原来的整数和减去1之后的结果做与运算，从原来整数最右边一个1那一位开始所有位都会变成0。如1100&1011=1000.也就是说，把一个整数减去1，再和原整数做与运算，会把该整数最右边一个1变成0.那么一个整数的二进制有多少个1，就可以进行多少次这样的操作。 - 在Python中，由于负数使用补码表示的，对于负数，最高位为1，而负数在计算机是以补码存在的，往右移，符号位不变，符号位1往右移，最终可能会出现全1的情况，导致死循环。与0xffffffff相与，就可以消除负数的影响。 代码12345678910# -*- coding:utf-8 -*-class Solution: def NumberOf1(self, n): # write code here res = 0 if n &lt; 0: n = n &amp; 0xffffffff while n: n &amp;= n-1 res += 1 return res","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"字符串","slug":"字符串","permalink":"https://xxren8218.github.io/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"name":"位运算","slug":"位运算","permalink":"https://xxren8218.github.io/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"}]},{"title":"剑指Offer（十二）：数值的整数次方","slug":"剑指Offer（十二）：数值的整数次方","date":"2021-05-20T11:09:11.000Z","updated":"2021-05-20T11:19:14.145Z","comments":true,"path":"20210520/剑指Offer（十二）：数值的整数次方.html","link":"","permalink":"https://xxren8218.github.io/20210520/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9A%E6%95%B0%E5%80%BC%E7%9A%84%E6%95%B4%E6%95%B0%E6%AC%A1%E6%96%B9.html","excerpt":"","text":"1.题目给定一个double类型的浮点数x和int类型的整数n。求x的n次方。 2.思路正常思路是： 若n&gt;0: xxx…x,一共乘n次 若n&lt;0: 1/xxx…x,一共乘n次。 若n=0：返回零，但是代码在力扣超时。时间复杂度O（n）3.代码12345678910111213141516171819202122class Solution(object): def myPow(self, x, n): &quot;&quot;&quot; :type x: float :type n: int :rtype: float &quot;&quot;&quot; if x == 0:return 0 res = x i = 0 if n &gt; 0: while i &lt; n-1: res *= x i += 1 return res if n &lt; 0: while i &lt; -n-1: res *= x i += 1 return 1/res if n == 0: return 1.0 下面介绍快速幂的方法——时间复杂度O（logn） 指数减半，底数平方的方法。 思路如13，我们可以转换成1101，则X13就等于x1101b,然后展开，如图一同样我们也将所给的数字进行转化。如图二当bi=0，其数值为0当bi=1，值为x的2i次方，如图三我们可以发现xi的规律就是从右向左看，后一个数字都是前一个数字的平方。如图、五所以：采用循环我们可以将n与1进行与运算，看当前位数为0与否： 不为零时候（为1），则结果 res * x 为零时候，则给结果不变而后进行位运算右移动，移动以后的 x*= x当n&lt;0, 我们可以令 x = 1/x, n = -n，则可以用同样的方法进行计算。当x=0,分母不能为0，直接返回0代码1234567891011121314151617class Solution(object): def myPow(self, x, n): &quot;&quot;&quot; :type x: float :type n: int :rtype: float &quot;&quot;&quot; res = 1 if x == 0:return 0 if n &lt; 0: x,n = 1/x, -n while n: if n &amp; 1: res *= x n &gt;&gt;= 1 x *= x return res","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"位运算","slug":"位运算","permalink":"https://xxren8218.github.io/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"},{"name":"快速幂","slug":"快速幂","permalink":"https://xxren8218.github.io/tags/%E5%BF%AB%E9%80%9F%E5%B9%82/"}]},{"title":"剑指Offer（十九）：顺时针打印矩阵","slug":"剑指Offer（十九）：顺时针打印矩阵","date":"2021-05-20T11:05:09.000Z","updated":"2021-05-24T10:58:33.276Z","comments":true,"path":"20210520/剑指Offer（十九）：顺时针打印矩阵.html","link":"","permalink":"https://xxren8218.github.io/20210520/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89%EF%BC%9A%E9%A1%BA%E6%97%B6%E9%92%88%E6%89%93%E5%8D%B0%E7%9F%A9%E9%98%B5.html","excerpt":"","text":"1.题目输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下矩阵：则依次打印出数组：1，2，3，4，8，12，16，15，14，13，9，5，6，7，11，10。 2.思路可以发现，顺时针打印矩阵的顺序是 “从左向右、从上向下、从右向左、从下向上” 循环。定义四个变量来确定边界。top、bottom、right、left。——注意他们范围的确定。然后进行循环判断即可。具体如图。——注意循环终止条件。 注意： 对于range(a,b)，反向的话用range(b,a,-1)即可!(逗号) 3.代码123456789101112131415161718192021222324252627282930313233# -*- coding:utf-8 -*-class Solution: # matrix类型为二维列表，需要返回列表 def printMatrix(self, matrix): # write code here if matrix == []: return [] top = 0 bottom = len(matrix)-1 left = 0 right = len(matrix[0])-1 res = [] while True: # → for i in range(left, right+1): res.append(matrix[top][i]) top += 1 if top &gt; bottom:break # ↓ for i in range(top, bottom+1): res.append(matrix[i][right]) right -= 1 if right &lt; left:break # ← for i in range(right, left-1, -1): res.append(matrix[bottom][i]) bottom -= 1 if bottom &lt; top:break # ↑ for i in range(bottom, top-1, -1): res.append(matrix[i][left]) left += 1 if left &gt; right:break return res","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"多指针","slug":"多指针","permalink":"https://xxren8218.github.io/tags/%E5%A4%9A%E6%8C%87%E9%92%88/"}]},{"title":"推荐系统——协同过滤&隐语义模型","slug":"推荐系统——系统过滤-隐语义模型","date":"2021-05-19T16:47:00.000Z","updated":"2021-05-20T11:03:56.849Z","comments":true,"path":"20210520/推荐系统——系统过滤-隐语义模型.html","link":"","permalink":"https://xxren8218.github.io/20210520/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E7%B3%BB%E7%BB%9F%E8%BF%87%E6%BB%A4-%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B.html","excerpt":"","text":"推荐系统——协同过滤—&gt;基于统计方法本节需要用到 scikit-surprise 库 pip install surprise 进行安装（需要C++编译环境） 123456789101112131415161718192021from surprise import KNNBasic,SVD # KNNBasic最基础的协同过滤算法（可以基于用户或者物品） SVD是基于矩阵分解的！from surprise import Dataset # Dataset默认的数据库进行练习。基础的电影的数据。（下方网址）from surprise import evaluate, print_perf# http://surprise.readthedocs.io/en/stable/index.html# http://files.grouplens.org/datasets/movielens/ml-100k-README.txt# Load the movielens-100k dataset (download it if needed),# and split it into 3 folds for cross-validation.data = Dataset.load_builtin(&#x27;ml-100k&#x27;) # 拿到内置的数据集data.split(n_folds=3) # 进行交叉验证的折数。# We&#x27;ll use the famous KNNBasic algorithm.algo = KNNBasic()# Evaluate performances of our algorithm on the dataset.perf = evaluate(algo, data, measures=[&#x27;RMSE&#x27;, &#x27;MAE&#x27;]) ###################################################### 之前的基本是 fit 这里不一样，可以自己指定三个参数（算法名字，数据，评估方法）# 这里制定了均方误差和绝对误差#####################################################print_perf(perf) Evaluating RMSE, MAE of algorithm KNNBasic. ------------ Fold 1 Computing the msd similarity matrix... Done computing similarity matrix. RMSE: 0.9876 MAE: 0.7807 ------------ Fold 2 Computing the msd similarity matrix... Done computing similarity matrix. RMSE: 0.9871 MAE: 0.7796 ------------ Fold 3 Computing the msd similarity matrix... Done computing similarity matrix. RMSE: 0.9902 MAE: 0.7818 ------------ ------------ Mean RMSE: 0.9883 Mean MAE : 0.7807 ------------ ------------ Fold 1 Fold 2 Fold 3 Mean MAE 0.7807 0.7796 0.7818 0.7807 RMSE 0.9876 0.9871 0.9902 0.9883 推荐系统——进行矩阵分解求解（隐语义模型）—&gt;基于模型 需要进行迭代求解，需要传一些参数 12345678910111213from surprise import GridSearchparam_grid = &#123;&#x27;n_epochs&#x27;: [5, 10], &#x27;lr_all&#x27;: [0.002, 0.005], &#x27;reg_all&#x27;: [0.4, 0.6]&#125;###################################### 指定了三个值，迭代次数，学习率，正则化的强度# 做其8种组合。#####################################grid_search = GridSearch(SVD, param_grid, measures=[&#x27;RMSE&#x27;, &#x27;FCP&#x27;]) # SVD 矩阵分解！data = Dataset.load_builtin(&#x27;ml-100k&#x27;)data.split(n_folds=3)grid_search.evaluate(data) ------------ Parameters combination 1 of 8 params: &#123;&#39;lr_all&#39;: 0.002, &#39;n_epochs&#39;: 5, &#39;reg_all&#39;: 0.4&#125; ------------ Mean RMSE: 0.9972 Mean FCP : 0.6843 ------------ ------------ Parameters combination 2 of 8 params: &#123;&#39;lr_all&#39;: 0.005, &#39;n_epochs&#39;: 5, &#39;reg_all&#39;: 0.4&#125; ------------ Mean RMSE: 0.9734 Mean FCP : 0.6946 ------------ ------------ Parameters combination 3 of 8 params: &#123;&#39;lr_all&#39;: 0.002, &#39;n_epochs&#39;: 10, &#39;reg_all&#39;: 0.4&#125; ------------ Mean RMSE: 0.9777 Mean FCP : 0.6926 ------------ ------------ Parameters combination 4 of 8 params: &#123;&#39;lr_all&#39;: 0.005, &#39;n_epochs&#39;: 10, &#39;reg_all&#39;: 0.4&#125; ------------ Mean RMSE: 0.9635 Mean FCP : 0.6987 ------------ ------------ Parameters combination 5 of 8 params: &#123;&#39;lr_all&#39;: 0.002, &#39;n_epochs&#39;: 5, &#39;reg_all&#39;: 0.6&#125; ------------ Mean RMSE: 1.0029 Mean FCP : 0.6875 ------------ ------------ Parameters combination 6 of 8 params: &#123;&#39;lr_all&#39;: 0.005, &#39;n_epochs&#39;: 5, &#39;reg_all&#39;: 0.6&#125; ------------ Mean RMSE: 0.9820 Mean FCP : 0.6953 ------------ ------------ Parameters combination 7 of 8 params: &#123;&#39;lr_all&#39;: 0.002, &#39;n_epochs&#39;: 10, &#39;reg_all&#39;: 0.6&#125; ------------ Mean RMSE: 0.9860 Mean FCP : 0.6943 ------------ ------------ Parameters combination 8 of 8 params: &#123;&#39;lr_all&#39;: 0.005, &#39;n_epochs&#39;: 10, &#39;reg_all&#39;: 0.6&#125; ------------ Mean RMSE: 0.9733 Mean FCP : 0.6991 ------------ GridSearch会帮我们存一些函数值 1234567891011121314# best RMSE scoreprint(grid_search.best_score[&#x27;RMSE&#x27;])# combination of parameters that gave the best RMSE scoreprint(grid_search.best_params[&#x27;RMSE&#x27;])# best FCP scoreprint(grid_search.best_score[&#x27;FCP&#x27;])# combination of parameters that gave the best FCP scoreprint(grid_search.best_params[&#x27;FCP&#x27;]) 0.963501988854 &#123;&#39;lr_all&#39;: 0.005, &#39;n_epochs&#39;: 10, &#39;reg_all&#39;: 0.4&#125; 0.699084153002 &#123;&#39;lr_all&#39;: 0.005, &#39;n_epochs&#39;: 10, &#39;reg_all&#39;: 0.6&#125; 1234567import pandas as pd results_df = pd.DataFrame.from_dict(grid_search.cv_results) ###################################### pd.DataFrame.from_dict可以传字典进行数据查看#####################################results_df FCP RMSE lr_all n_epochs params scores 0 0.684266 0.997160 0.002 5 {'lr_all': 0.002, 'n_epochs': 5, 'reg_all': 0.4} {'RMSE': 0.997160189649, 'FCP': 0.684266412476} 1 0.694552 0.973383 0.005 5 {'lr_all': 0.005, 'n_epochs': 5, 'reg_all': 0.4} {'RMSE': 0.973383132387, 'FCP': 0.694551932996} 2 0.692616 0.977697 0.002 10 {'lr_all': 0.002, 'n_epochs': 10, 'reg_all': 0.4} {'RMSE': 0.977696629511, 'FCP': 0.692615513155} 3 0.698722 0.963502 0.005 10 {'lr_all': 0.005, 'n_epochs': 10, 'reg_all': 0.4} {'RMSE': 0.963501988854, 'FCP': 0.698721750945} 4 0.687482 1.002855 0.002 5 {'lr_all': 0.002, 'n_epochs': 5, 'reg_all': 0.6} {'RMSE': 1.00285516237, 'FCP': 0.687481665759} 5 0.695337 0.982047 0.005 5 {'lr_all': 0.005, 'n_epochs': 5, 'reg_all': 0.6} {'RMSE': 0.98204676013, 'FCP': 0.695337489535} 6 0.694338 0.985981 0.002 10 {'lr_all': 0.002, 'n_epochs': 10, 'reg_all': 0.6} {'RMSE': 0.985980855401, 'FCP': 0.694337564062} 7 0.699084 0.973282 0.005 10 {'lr_all': 0.005, 'n_epochs': 10, 'reg_all': 0.6} {'RMSE': 0.973281870802, 'FCP': 0.699084153002} 模型搭建出来了，用它来推荐东西！1234567891011121314151617181920212223242526272829303132from __future__ import (absolute_import, division, print_function, unicode_literals)import osimport iofrom surprise import KNNBaselinefrom surprise import Datasetdef read_item_names(): &quot;&quot;&quot;把电影的名字做成了id的映射&quot;&quot;&quot; file_name = (&#x27;./ml-100k/u.item&#x27;) rid_to_name = &#123;&#125; name_to_rid = &#123;&#125; with io.open(file_name, &#x27;r&#x27;, encoding=&#x27;ISO-8859-1&#x27;) as f: for line in f: line = line.split(&#x27;|&#x27;) rid_to_name[line[0]] = line[1] name_to_rid[line[1]] = line[0] return rid_to_name, name_to_rid# 1.导入数据data = Dataset.load_builtin(&#x27;ml-100k&#x27;)# 2.数据是一行的，将其转换成矩阵（稀疏的）trainset = data.build_full_trainset()# 3.指定相似度的方法——此处用了皮尔孙，指定了基于物品的相似度。sim_options = &#123;&#x27;name&#x27;: &#x27;pearson_baseline&#x27;, &#x27;user_based&#x27;: False&#125;algo = KNNBaseline(sim_options=sim_options)algo.train(trainset) Estimating biases using als... Computing the pearson_baseline similarity matrix... Done computing similarity matrix. 123456rid_to_name, name_to_rid = read_item_names()toy_story_raw_id = name_to_rid[&#x27;Now and Then (1995)&#x27;]# 直接传电影名字不行，因为传的是id，先对其进行id的转换。toy_story_raw_id # 在数据的id &#39;1053&#39; 12toy_story_inner_id = algo.trainset.to_inner_iid(toy_story_raw_id)toy_story_inner_id # 在实际计算的(矩阵的)id 961 123toy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10)# 找出最接近的10个电影idtoy_story_neighbors [291, 82, 366, 528, 179, 101, 556, 310, 431, 543] 123456789toy_story_neighbors = (algo.trainset.to_raw_iid(inner_id) for inner_id in toy_story_neighbors)toy_story_neighbors = (rid_to_name[rid] for rid in toy_story_neighbors)print()print(&#x27;The 10 nearest neighbors of Toy Story are:&#x27;)for movie in toy_story_neighbors: print(movie) The 10 nearest neighbors of Toy Story are: While You Were Sleeping (1995) Batman (1989) Dave (1993) Mrs. Doubtfire (1993) Groundhog Day (1993) Raiders of the Lost Ark (1981) Maverick (1994) French Kiss (1995) Stand by Me (1986) Net, The (1995)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"EM算法应用——GMM聚类","slug":"EM算法应用——GMM聚类","date":"2021-05-18T16:55:06.000Z","updated":"2021-05-18T17:02:03.180Z","comments":true,"path":"20210519/EM算法应用——GMM聚类.html","link":"","permalink":"https://xxren8218.github.io/20210519/EM%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94GMM%E8%81%9A%E7%B1%BB.html","excerpt":"","text":"GMM实例 桥东西两个地方摆放了共享单车。统计使用共享单车的数量。 123import pandas as pddata = pd.read_csv (&#x27;Fremont.csv&#x27;, index_col=&#x27;Date&#x27;, parse_dates=True)data.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Fremont Bridge West Sidewalk Fremont Bridge East Sidewalk Date 2012-10-03 00:00:00 4.0 9.0 2012-10-03 01:00:00 4.0 6.0 2012-10-03 02:00:00 1.0 1.0 2012-10-03 03:00:00 2.0 3.0 2012-10-03 04:00:00 6.0 1.0 Visualizing the Dataset12%matplotlib inlinedata.plot(); 可以看到数据太密集，因为是以小时为单位创建的， 数据重采样，按周进行计算 12%matplotlib inlinedata.resample(&#x27;w&#x27;).sum().plot(); # 时间序列化 可以看到东西边差别不大。采用滑动窗口进行可视化——当前365天数据的总和 1data.resample(&#x27;D&#x27;).sum().rolling(365).sum().plot(); 123import matplotlib.pyplot as pltdata.groupby(data.index.time).mean().plot();plt.xticks(rotation=45) (array([ 0., 10000., 20000., 30000., 40000., 50000., 60000., 70000., 80000., 90000.]), &lt;a list of 10 Text xticklabel objects&gt;) 蓝线和绿线满足不同分布规则。 隐变量有两个。桥东和桥西。 做GMM模型 12345# pivot tabledata.columns =[&#x27;West&#x27;, &#x27;East&#x27;]data [&#x27;Total&#x27;] =data[&#x27;West&#x27;]+data[&#x27;East&#x27;]pivoted = data.pivot_table(&#x27;Total&#x27;, index=data.index.time, columns=data.index.date) # 做一个透视表pivoted.iloc[:5,:5] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } 2012-10-03 2012-10-04 2012-10-05 2012-10-06 2012-10-07 00:00:00 13.0 18.0 11.0 15.0 11.0 01:00:00 10.0 3.0 8.0 15.0 17.0 02:00:00 2.0 9.0 7.0 9.0 3.0 03:00:00 5.0 3.0 4.0 3.0 6.0 04:00:00 7.0 8.0 9.0 5.0 3.0 12pivoted.plot(legend=False, alpha =0.01);plt.xticks(rotation=45) (array([ 0., 10000., 20000., 30000., 40000., 50000., 60000., 70000., 80000., 90000.]), &lt;a list of 10 Text xticklabel objects&gt;) 可以看到有两种分布，能否用GMM进行聚类任务，然后分开呢？ 1pivoted.shape (24, 1763) 样本数24，特征数1763不合适 12X = pivoted.fillna(0).T.valuesX.shape (1763, 24) 然后拿它GMM进行聚类的任务 拿到了1763个数据，以及它每天24小时的数据，我们想做个聚类，看他是东还是西。 为了方便演示，将图像画在2D空间——进行PCA的降维 123from sklearn.decomposition import PCAX2 = PCA(2).fit_transform(X)X2.shape (1763, 2) 1plt.scatter(X2[:,0],X2[:,1]) &lt;matplotlib.collections.PathCollection at 0x1b92c86a630&gt; PCA降维以后数据特征不存在具体物理意义、但是空间意义还在 然后进行聚类分析： GMM聚类与KMEANS有些类似，因为kmeans需要知道K点个数。而此处需要知道隐变量的个数。 12345from sklearn.mixture import GaussianMixturegmm =GaussianMixture(2)gmm.fit(X)labels = gmm.predict_proba(X) # 得到的是概率值，即每一个样本属于（第一个类别【分布】的可能性，第二个类别【分布】的可能性）labels array([[ 1.00000000e+000, 4.27200703e-157], [ 1.00000000e+000, 2.97606604e-125], [ 1.00000000e+000, 1.00461289e-101], ..., [ 2.50152395e-030, 1.00000000e+000], [ 2.49747986e-025, 1.00000000e+000], [ 1.00000000e+000, 1.79108465e-301]]) 12labels = gmm.predict(X) # 得到的是概类别的标签labels array([0, 0, 0, ..., 1, 1, 0], dtype=int64) 12plt.scatter(X2[:,0],X2[:,1], c=labels, cmap=&#x27;rainbow&#x27;) #plt.colorbar() &lt;matplotlib.collections.PathCollection at 0x1b92cdd0f98&gt; 1234567fig, ax = plt.subplots(1, 2, figsize =(14, 6)) pivoted.T[labels == 0].T.plot(legend =False, alpha =0.1, ax=ax[0]) # 进行数据的还原，显示数据的样子pivoted.T[labels == 1].T.plot(legend =False, alpha =0.1, ax=ax[1])ax[0].set_title (&#x27;Purple Cluster&#x27;)ax[1].set_title (&#x27;Red Cluster&#x27;) &lt;matplotlib.text.Text at 0x1b932208b38&gt; 来看看GMM和KMEANS的差异吧！ 123from sklearn.datasets.samples_generator import make_blobsX, y_true = make_blobs(n_samples=800, centers=4, random_state=11)plt.scatter(X[:, 0], X[:, 1]); 12345678from sklearn.cluster import KMeanskmeans = KMeans(n_clusters=4)kmeans.fit(X)y_kmeans = kmeans.predict(X)plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap=&#x27;viridis&#x27;)centers = kmeans.cluster_centers_ 1234from sklearn.mixture import GaussianMixturegmm = GaussianMixture(n_components=4).fit(X)labels = gmm.predict(X)plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap=&#x27;viridis&#x27;); GMM和KMEANS在规则数据上基本一样！ 12345678rng = np.random.RandomState(13)X_stretched = np.dot(X, rng.randn(2, 2))kmeans = KMeans(n_clusters=4, random_state=1)kmeans.fit(X_stretched)y_kmeans = kmeans.predict(X_stretched)plt.scatter(X_stretched[:, 0], X_stretched[:, 1], c=y_kmeans, s=50, cmap=&#x27;viridis&#x27;)centers = kmeans.cluster_centers_ 1234gmm = GaussianMixture(n_components=4) gmm.fit(X_stretched)y_gmm = gmm.predict(X_stretched)plt.scatter(X_stretched[:, 0], X_stretched[:, 1], c=y_gmm, s=50, cmap=&#x27;viridis&#x27;) &lt;matplotlib.collections.PathCollection at 0x1b932008208&gt; GMM比KMEANS好！ 所以，如果数据内部有多个类别，且服从不同的分布，用GMM比KMEANS好","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"力扣（一）：两数之和","slug":"力扣（一）：两数之和","date":"2021-05-18T10:00:40.000Z","updated":"2021-05-18T10:01:48.267Z","comments":true,"path":"20210518/力扣（一）：两数之和.html","link":"","permalink":"https://xxren8218.github.io/20210518/%E5%8A%9B%E6%89%A3%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C.html","excerpt":"","text":"1.题目给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。 你可以按任意顺序返回答案。 2.思路1方法一：暴力枚举思路及算法 最容易想到的方法是枚举数组中的每一个数 x，寻找数组中是否存在 target - x。 当我们使用遍历整个数组的方式寻找 target - x 时，需要注意到每一个位于 x 之前的元素都已经和 x 匹配过，因此不需要再进行匹配。而每一个元素不能被使用两次，所以我们只需要在 x 后面的元素中寻找 target - x。 3.代码123456789class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: n = len(nums) for i in range(n): for j in range(i + 1, n): if nums[i] + nums[j] == target: return [i, j] return [] 思路2方法二：哈希表思路及算法 注意到方法一的时间复杂度较高的原因是寻找 target - x 的时间复杂度过高。因此，我们需要一种更优秀的方法，能够快速寻找数组中是否存在目标元素。如果存在，我们需要找出它的索引。 使用哈希表，可以将寻找 target - x 的时间复杂度降低到从 O(N) 降低到 O(1)。 这样我们创建一个哈希表，对于每一个 x，我们首先查询哈希表中是否存在 target - x，然后将 x 插入到哈希表中，即可保证不会让 x 和自己匹配。 代码12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: hashtable = dict() for i, num in enumerate(nums): if target - num in hashtable: return [hashtable[target - num], i] hashtable[nums[i]] = i return []","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"哈希表","slug":"哈希表","permalink":"https://xxren8218.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"}]},{"title":"剑指Offer（二十）：包含min函数的栈","slug":"剑指Offer（二十）：包含min函数的栈","date":"2021-05-17T10:05:38.000Z","updated":"2021-05-17T10:08:06.276Z","comments":true,"path":"20210517/剑指Offer（二十）：包含min函数的栈.html","link":"","permalink":"https://xxren8218.github.io/20210517/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89%EF%BC%9A%E5%8C%85%E5%90%ABmin%E5%87%BD%E6%95%B0%E7%9A%84%E6%A0%88.html","excerpt":"","text":"1.题目定义栈的数据结构，请在类型中实现一个能够得到栈最小元素的min函数。 2.思路使用两个stack，一个为数据栈，另一个为辅助栈。数据栈用于存储所有数据，辅助栈用于存储最小值。 若加入的元素比辅助栈的栈顶元素小，则再辅助栈增加该元素，否则，增加辅助栈的栈顶元素。 Data = [1,2,-2,3,-4,6]Min = [1,1,-2,-2,-4,-4] 在Data出栈的时候，同时将Data和Min出栈即可。 获得栈顶元素的时候：直接返回数据栈的栈顶元素。 栈最小元素：直接返回辅助栈的栈顶元素。 3.代码123456789101112131415161718192021222324252627282930class Solution: def __init__(self): self.Data = [] self.Min = [] def push(self, node): # write code here self.Data.append(node) if self.Min: if self.Min[-1] &gt; node: self.Min.append(node) else: self.Min.append(self.Min[-1]) #保证Data出栈的时候Min也是出栈的，且最小数字不会改变。 else: self.Min.append(node) def pop(self): # write code here if self.Data == []: return None self.Min.pop() return self.Data.pop() def top(self): # write code here if self.Data == []: return None return self.Data[-1] def min(self): # write code here if self.Min == []: return None return self.Min[-1] 还有一种思路——使用单调栈。入栈的时候：首先往空的数据栈里压入数字3，显然现在3是最小值，我们也把最小值压入辅助栈。接下来往数据栈里压入数字4。由于4大于之前的最小值，因此我们只要入数据栈，不压入辅助栈。 出栈的时候：当数据栈和辅助栈的栈顶元素相同的时候，辅助栈的栈顶元素出栈。否则，数据栈的栈顶元素出栈。 Data = [1,2,-2,3,-4,6]Min = [1,-2,-4] 这个与上个不同的是：Data出栈时候需要判断是否为Min的栈顶元素,若是，则需要同时出栈。上面的方法不需要判断，只需要同时出栈 获得栈顶元素的时候：直接返回数据栈的栈顶元素。 栈最小元素：直接返回辅助栈的栈顶元素。 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class MinStack(object): def __init__(self): &quot;&quot;&quot; initialize your data structure here. &quot;&quot;&quot; self.Data = [] self.Min = [] def push(self, x): &quot;&quot;&quot; :type x: int :rtype: None &quot;&quot;&quot; self.Data.append(x) if self.Min: if self.Min[-1] &gt;= x: self.Min.append(x) else: self.Min.append(x) def pop(self): &quot;&quot;&quot; :rtype: None &quot;&quot;&quot; if self.Data == []: return None if self.Data[-1] == self.Min[-1]: self.Min.pop() self.Data.pop() def top(self): &quot;&quot;&quot; :rtype: int &quot;&quot;&quot; if self.Data == []:return None return self.Data[-1] def min(self): &quot;&quot;&quot; :rtype: int &quot;&quot;&quot; if self.Min == []:return None return self.Min[-1]# Your MinStack object will be instantiated and called as such:# obj = MinStack()# obj.push(x)# obj.pop()# param_3 = obj.top()# param_4 = obj.min()","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"辅助栈","slug":"辅助栈","permalink":"https://xxren8218.github.io/tags/%E8%BE%85%E5%8A%A9%E6%A0%88/"},{"name":"栈","slug":"栈","permalink":"https://xxren8218.github.io/tags/%E6%A0%88/"},{"name":"单调栈","slug":"单调栈","permalink":"https://xxren8218.github.io/tags/%E5%8D%95%E8%B0%83%E6%A0%88/"}]},{"title":"降维算法——PCA(主成分分析)","slug":"降维算法——PCA-主成分分析","date":"2021-05-15T06:31:02.000Z","updated":"2021-05-15T06:39:58.519Z","comments":true,"path":"20210515/降维算法——PCA-主成分分析.html","link":"","permalink":"https://xxren8218.github.io/20210515/%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94PCA-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90.html","excerpt":"","text":"鸢尾花数据集的降维——PCA PCA的降维是不依赖标签（分类的），而是依赖样本的方差（协方差） 1234import numpy as npimport pandas as pddf = pd.read_csv(&#x27;iris.data&#x27;)df.head() 5.1 3.5 1.4 0.2 Iris-setosa 0 4.9 3.0 1.4 0.2 Iris-setosa 1 4.7 3.2 1.3 0.2 Iris-setosa 2 4.6 3.1 1.5 0.2 Iris-setosa 3 5.0 3.6 1.4 0.2 Iris-setosa 4 5.4 3.9 1.7 0.4 Iris-setosa 12df.columns=[&#x27;sepal_len&#x27;, &#x27;sepal_wid&#x27;, &#x27;petal_len&#x27;, &#x27;petal_wid&#x27;, &#x27;class&#x27;]df.head() sepal_len sepal_wid petal_len petal_wid class 0 4.9 3.0 1.4 0.2 Iris-setosa 1 4.7 3.2 1.3 0.2 Iris-setosa 2 4.6 3.1 1.5 0.2 Iris-setosa 3 5.0 3.6 1.4 0.2 Iris-setosa 4 5.4 3.9 1.7 0.4 Iris-setosa 1234# split data table into data X and class labels yX = df.ix[:,0:4].valuesy = df.ix[:,4].values 数据的可视化 1234567891011121314151617181920212223242526from matplotlib import pyplot as pltimport mathlabel_dict = &#123;1: &#x27;Iris-Setosa&#x27;, 2: &#x27;Iris-Versicolor&#x27;, 3: &#x27;Iris-Virgnica&#x27;&#125;feature_dict = &#123;0: &#x27;sepal length [cm]&#x27;, 1: &#x27;sepal width [cm]&#x27;, 2: &#x27;petal length [cm]&#x27;, 3: &#x27;petal width [cm]&#x27;&#125;plt.figure(figsize=(8, 6))for cnt in range(4): plt.subplot(2, 2, cnt+1) for lab in (&#x27;Iris-setosa&#x27;, &#x27;Iris-versicolor&#x27;, &#x27;Iris-virginica&#x27;): plt.hist(X[y==lab, cnt], label=lab, bins=10, alpha=0.3,) plt.xlabel(feature_dict[cnt]) plt.legend(loc=&#x27;upper right&#x27;, fancybox=True, fontsize=8)plt.tight_layout()plt.show() 一般要先进行数据的标准化，归一化，或正态化的预处理 123from sklearn.preprocessing import StandardScalerX_std = StandardScaler().fit_transform(X)print (X_std) [[-1.1483555 -0.11805969 -1.35396443 -1.32506301] [-1.3905423 0.34485856 -1.41098555 -1.32506301] [-1.51163569 0.11339944 -1.29694332 -1.32506301] [-1.02726211 1.27069504 -1.35396443 -1.32506301] [-0.54288852 1.9650724 -1.18290109 -1.0614657 ] [-1.51163569 0.8077768 -1.35396443 -1.19326436] [-1.02726211 0.8077768 -1.29694332 -1.32506301] [-1.75382249 -0.34951881 -1.35396443 -1.32506301] [-1.1483555 0.11339944 -1.29694332 -1.45686167] [-0.54288852 1.50215416 -1.29694332 -1.32506301] [-1.2694489 0.8077768 -1.23992221 -1.32506301] [-1.2694489 -0.11805969 -1.35396443 -1.45686167] [-1.87491588 -0.11805969 -1.52502777 -1.45686167] [-0.05851493 2.19653152 -1.46800666 -1.32506301] ... [-0.54288852 1.9650724 -1.41098555 -1.0614657 ] [-0.90616871 1.03923592 -1.35396443 -1.19326436] [-0.17960833 1.73361328 -1.18290109 -1.19326436] [-0.90616871 1.73361328 -1.29694332 -1.19326436] [ 1.15241904 0.34485856 1.21198569 1.4427088 ] [ 1.03132564 0.57631768 1.09794346 1.70630611] [ 1.03132564 -0.11805969 0.81283789 1.4427088 ] [ 0.54695205 -1.27535529 0.69879566 0.91551417] [ 0.78913885 -0.11805969 0.81283789 1.04731282] [ 0.42585866 0.8077768 0.92688012 1.4427088 ] [ 0.06257847 -0.11805969 0.75581678 0.78371551]] 手撕协方差 123mean_vec = np.mean(X_std, axis=0)cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)print(&#x27;Covariance matrix \\n%s&#x27; %cov_mat) Covariance matrix [[ 1.00675676 -0.10448539 0.87716999 0.82249094] [-0.10448539 1.00675676 -0.41802325 -0.35310295] [ 0.87716999 -0.41802325 1.00675676 0.96881642] [ 0.82249094 -0.35310295 0.96881642 1.00675676]] 直接可以在numpy中调用cov（）,协方差函数 1print(&#x27;NumPy covariance matrix: \\n%s&#x27; %np.cov(X_std.T)) NumPy covariance matrix: [[ 1.00675676 -0.10448539 0.87716999 0.82249094] [-0.10448539 1.00675676 -0.41802325 -0.35310295] [ 0.87716999 -0.41802325 1.00675676 0.96881642] [ 0.82249094 -0.35310295 0.96881642 1.00675676]] 可以看到对角线上的都为1，因为自身与自身的协方差肯定是1 标准化以后是1。——相关系数。 没有标准化的数据，仅仅是方差。 计算协方差矩阵的特征值和特征向量123456cov_mat = np.cov(X_std.T)eig_vals, eig_vecs = np.linalg.eig(cov_mat)print(&#x27;Eigenvectors \\n%s&#x27; %eig_vecs)print(&#x27;\\nEigenvalues \\n%s&#x27; %eig_vals) Eigenvectors [[ 0.52308496 -0.36956962 -0.72154279 0.26301409] [-0.25956935 -0.92681168 0.2411952 -0.12437342] [ 0.58184289 -0.01912775 0.13962963 -0.80099722] [ 0.56609604 -0.06381646 0.63380158 0.52321917]] Eigenvalues [ 2.92442837 0.93215233 0.14946373 0.02098259] 特征值的大小代表特征向量的重要程度 下面的代码将其进行排序 1234567891011# Make a list of (eigenvalue, eigenvector) tupleseig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]print (eig_pairs)print (&#x27;----------&#x27;)# Sort the (eigenvalue, eigenvector) tuples from high to loweig_pairs.sort(key=lambda x: x[0], reverse=True)# Visually confirm that the list is correctly sorted by decreasing eigenvaluesprint(&#x27;Eigenvalues in descending order:&#x27;)for i in eig_pairs: print(i[0]) [(2.9244283691111144, array([ 0.52308496, -0.25956935, 0.58184289, 0.56609604])), (0.93215233025350641, array([-0.36956962, -0.92681168, -0.01912775, -0.06381646])), (0.14946373489813314, array([-0.72154279, 0.2411952 , 0.13962963, 0.63380158])), (0.020982592764270606, array([ 0.26301409, -0.12437342, -0.80099722, 0.52321917]))] ---------- Eigenvalues in descending order: 2.92442836911 0.932152330254 0.149463734898 0.0209825927643 12345tot = sum(eig_vals)var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]print (var_exp)cum_var_exp = np.cumsum(var_exp) # 来验证最后的和是否为100% 以及维度的选择——cumsum()的解释在下面。cum_var_exp [72.620033326920336, 23.147406858644135, 3.7115155645845164, 0.52104424985101538] array([ 72.62003333, 95.76744019, 99.47895575, 100. ]) cumsum()函数的解释： 为什么要用它呢？ 可以判断到底降维到几维。（设置阈值。比如百分之95%，那么降到2维即可） 1234a = np.array([1,2,3,4])print (a)print (&#x27;-----------&#x27;)print (np.cumsum(a)) [1 2 3 4] ----------- [ 1 3 6 10] 作图展示特征值所占的百分比 123456789101112plt.figure(figsize=(6, 4))plt.bar(range(4), var_exp, alpha=0.5, align=&#x27;center&#x27;, label=&#x27;individual explained variance&#x27;)plt.step(range(4), cum_var_exp, where=&#x27;mid&#x27;, # 阶梯图。where，表示在哪里开始跳跃。 label=&#x27;cumulative explained variance&#x27;)plt.ylabel(&#x27;Explained variance ratio&#x27;)plt.xlabel(&#x27;Principal components&#x27;)plt.legend(loc=&#x27;best&#x27;)plt.tight_layout() # 最自动调整图的大小。使其填满图像区域。plt.show() 确定w的维度 拿前两个特征向量。——基变换:w.T * x0 = x1 1234matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))print(&#x27;Matrix W:\\n&#x27;, matrix_w) Matrix W: [[ 0.52308496 -0.36956962] [-0.25956935 -0.92681168] [ 0.58184289 -0.01912775] [ 0.56609604 -0.06381646]] 12Y = X_std.dot(matrix_w)Y array([[-2.10795032, 0.64427554], [-2.38797131, 0.30583307], [-2.32487909, 0.56292316], [-2.40508635, -0.687591 ], [-2.08320351, -1.53025171], [-2.4636848 , -0.08795413], [-2.25174963, -0.25964365], [-2.3645813 , 1.08255676], [-2.20946338, 0.43707676], [-2.17862017, -1.08221046], [-2.34525657, -0.17122946], ... [ 2.00701161, -0.60663655], [ 1.89319854, -0.68227708], [ 1.13831104, 0.70171953], [ 2.03519535, -0.86076914], [ 1.99464025, -1.04517619], [ 1.85977129, -0.37934387], [ 1.54200377, 0.90808604], [ 1.50925493, -0.26460621], [ 1.3690965 , -1.01583909], [ 0.94680339, 0.02182097]]) 123456789101112plt.figure(figsize=(6, 4))for lab, col in zip((&#x27;Iris-setosa&#x27;, &#x27;Iris-versicolor&#x27;, &#x27;Iris-virginica&#x27;), (&#x27;blue&#x27;, &#x27;red&#x27;, &#x27;green&#x27;)): plt.scatter(X[y==lab, 0], X[y==lab, 1], label=lab, c=col)plt.xlabel(&#x27;sepal_len&#x27;)plt.ylabel(&#x27;sepal_wid&#x27;)plt.legend(loc=&#x27;best&#x27;)plt.tight_layout()plt.show() 123456789101112plt.figure(figsize=(6, 4))for lab, col in zip((&#x27;Iris-setosa&#x27;, &#x27;Iris-versicolor&#x27;, &#x27;Iris-virginica&#x27;), (&#x27;blue&#x27;, &#x27;red&#x27;, &#x27;green&#x27;)): plt.scatter(Y[y==lab, 0], Y[y==lab, 1], label=lab, c=col)plt.xlabel(&#x27;Principal Component 1&#x27;)plt.ylabel(&#x27;Principal Component 2&#x27;)plt.legend(loc=&#x27;lower center&#x27;)plt.tight_layout()plt.show() PCA使得结果更容易分类 降维后数据的意义不存在了","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"降维算法——LDA(线性判别分析)","slug":"降维算法——LDA-线性判别分析","date":"2021-05-15T06:16:53.000Z","updated":"2021-05-15T06:29:10.304Z","comments":true,"path":"20210515/降维算法——LDA-线性判别分析.html","link":"","permalink":"https://xxren8218.github.io/20210515/%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94LDA-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90.html","excerpt":"","text":"鸢尾花的数据集进行降维 有四个维度，萼片的长度。萼片的宽度，花瓣的长度，花瓣的宽度——将其降低维度为2维 数据集大概有150条 数据集没有列名，给其进行指定，使用zip()函数，将其进行对应 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253feature_dict = &#123;i:label for i,label in zip( range(4), (&#x27;sepal length in cm&#x27;, &#x27;sepal width in cm&#x27;, &#x27;petal length in cm&#x27;, # 用到了字典推导式！ &#x27;petal width in cm&#x27;, ))&#125;########################################## feature_dict = &#123;0: &#x27;sepal length in cm&#x27;, # 1: &#x27;sepal width in cm&#x27;, # 2: &#x27;petal length in cm&#x27;, # 3: &#x27;petal width in cm&#x27;&#125;#########################################import pandas as pddf = pd.io.parsers.read_csv( filepath_or_buffer=&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&#x27;, header=None, sep=&#x27;,&#x27;, )# 数据加列名df.columns = [l for i,l in sorted(feature_dict.items())] + [&#x27;class label&#x27;]&quot;&quot;&quot;sorted()得到的是一个列表，需要接收，而list.sort()原地排序。O(nlogn)若排序的是字典，会将字典的键值，放入元祖中，则可以通过 i,j 进行取元祖里的值。#######################################z = [i for i in sorted(feature_dict.items())]# [(0, &#x27;sepal length in cm&#x27;), (1, &#x27;sepal width in cm&#x27;), (2, &#x27;petal length in cm&#x27;), (3, &#x27;petal width in cm&#x27;)]##############################################################################x = [l for i,l in sorted(feature_dict.items())]# [&#x27;sepal length in cm&#x27;, &#x27;sepal width in cm&#x27;, &#x27;petal length in cm&#x27;, &#x27;petal width in cm&#x27;]#######################################&quot;&quot;&quot;df.dropna(how=&quot;all&quot;, inplace=True) # to drop the empty line at file-end&quot;&quot;&quot;#######################################dropna()方法，能够找到DataFrame类型数据的空值（缺失值），将空值所在的行/列删除后，将新的DataFrame作为返回值返回。#######################################axis：轴。0或&#x27;index&#x27;，表示按行删除；1或&#x27;columns&#x27;，表示按列删除。how：筛选方式。‘any’，表示该行/列只要有一个以上的空值，就删除该行/列； ‘all’，表示该行/列全部都为空值，就删除该行/列。thresh：非空元素最低数量。int型，默认为None。如果该行/列中，非空元素数量小于这个值，就删除该行/列inplace：是否原地替换。布尔值，默认为False。如果为True，则在原DataFrame上进行操作，返回值为None&quot;&quot;&quot;df.tail() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } sepal length in cm sepal width in cm petal length in cm petal width in cm class label 145 6.7 3.0 5.2 2.3 Iris-virginica 146 6.3 2.5 5.0 1.9 Iris-virginica 147 6.5 3.0 5.2 2.0 Iris-virginica 148 6.2 3.4 5.4 2.3 Iris-virginica 149 5.9 3.0 5.1 1.8 Iris-virginica 可以看到y数据的（lebal）是字符串，不好。可以将其转化成数字。 使用sklearn.preprocessing的LabelEncoder模块实现 12345678910from sklearn.preprocessing import LabelEncoderX = df[[&#x27;sepal length in cm&#x27;,&#x27;sepal width in cm&#x27;,&#x27;petal length in cm&#x27;,&#x27;petal width in cm&#x27;]].valuesy = df[&#x27;class label&#x27;].valuesenc = LabelEncoder()label_encoder = enc.fit(y) # 与之前不同的是，它在y上进行 fit !y = label_encoder.transform(y) + 1 # 默认从零开始，可以改为从一开始，# label_dict = &#123;1: &#x27;Setosa&#x27;, 2: &#x27;Versicolor&#x27;, 3:&#x27;Virginica&#x27;&#125; 分别求三种鸢尾花数据在不同特征维度上的均值向量 mi 12345678import numpy as npnp.set_printoptions(precision=4)# 输出时，小数点后面四位，若没有四位的话，不输出。（0不输出！）mean_vectors = []for cl in range(1,4): mean_vectors.append(np.mean(X[y==cl], axis=0)) print(&#x27;Mean Vector class %s: %s\\n&#x27; %(cl, mean_vectors[cl-1])) Mean Vector class 1: [ 5.006 3.418 1.464 0.244] Mean Vector class 2: [ 5.936 2.77 4.26 1.326] Mean Vector class 3: [ 6.588 2.974 5.552 2.026] 计算两个 4×4 维矩阵：类内散布矩阵和类间散布矩阵 12345678S_W = np.zeros((4,4)) # 每个类有4个特征值。即协方差矩阵——收藏的！for cl,mv in zip(range(1,4), mean_vectors): # 3个类 class_sc_mat = np.zeros((4,4)) # scatter matrix for every class 由协方差决定其是4x4的！ for row in X[y == cl]: # 选取第 c1 类的 X row, mv = row.reshape(4,1), mv.reshape(4,1) # make column vectors 将矩阵组成 4x4 的形式！ class_sc_mat += (row-mv).dot((row-mv).T) S_W += class_sc_mat # sum class scatter matricesprint(&#x27;within-class Scatter Matrix:\\n&#x27;, S_W) within-class Scatter Matrix: [[ 38.9562 13.683 24.614 5.6556] [ 13.683 17.035 8.12 4.9132] [ 24.614 8.12 27.22 6.2536] [ 5.6556 4.9132 6.2536 6.1756]] 类间的散布矩阵进行简化。不是m1-m2了，而是与全局均值的作比较 123456789101112131415161718192021222324252627282930overall_mean = np.mean(X, axis=0)&quot;&quot;&quot;zip()和enumerate()类似，但是有所不同。zip()可以跟多对象，########################names = [&#x27;张三&#x27;,&#x27;李四&#x27;,&#x27;王五&#x27;]sexs = [&#x27;boy&#x27;,&#x27;girl&#x27;,&#x27;boy&#x27;]scores = [86, 92]for name, sex, score in zip(names,sexs,scores): print(&#x27;&#123;&#125;: &#123;&#125;, &#123;&#125;&#x27;.format(name, sex, score))out[]:张三: boy, 86李四: girl, 92########################enumerate()不可以。&quot;&quot;&quot;S_B = np.zeros((4,4))for i,mean_vec in enumerate(mean_vectors): n = X[y==i+1,:].shape[0] # 获得数据的个数 mean_vec = mean_vec.reshape(4,1) # make column vector overall_mean = overall_mean.reshape(4,1) # make column vector S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)print(&#x27;between-class Scatter Matrix:\\n&#x27;, S_B) between-class Scatter Matrix: [[ 63.2121 -19.534 165.1647 71.3631] [ -19.534 10.9776 -56.0552 -22.4924] [ 165.1647 -56.0552 436.6437 186.9081] [ 71.3631 -22.4924 186.9081 80.6041]] np.linalg.inv()可以求逆，np.linalg.eig()可以求特征值，特征向量 123456789eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))for i in range(len(eig_vals)): eigvec_sc = eig_vecs[:,i].reshape(4,1) &quot;&quot;&quot; 返回的v是归一化后的特征向量（length为1）。特征向量v[:,i]对应特征值w[i]。 &quot;&quot;&quot; print(&#x27;\\nEigenvector &#123;&#125;: \\n&#123;&#125;&#x27;.format(i+1, eigvec_sc.real)) # 取实部 print(&#x27;Eigenvalue &#123;:&#125;: &#123;:.2e&#125;&#x27;.format(i+1, eig_vals[i].real)) Eigenvector 1: [[ 0.2049] [ 0.3871] [-0.5465] [-0.7138]] Eigenvalue 1: 3.23e+01 Eigenvector 2: [[-0.009 ] [-0.589 ] [ 0.2543] [-0.767 ]] Eigenvalue 2: 2.78e-01 Eigenvector 3: [[-0.7113] [ 0.0353] [-0.0267] [ 0.7015]] Eigenvalue 3: -5.76e-15 Eigenvector 4: [[ 0.422 ] [-0.4364] [-0.4851] [ 0.6294]] Eigenvalue 4: 7.80e-15 特征值与特征向量： 特征向量：表示映射方向 特征值：特征向量的重要程度 假设我们投影到的低维空间的维度为d，(n-&gt;d维的转换)对应的基向量为：W = (w1,w2,w3…wd)——收藏的！ 将x = W.T * x ((d,n) x (n,1)=(d,1)) 这里是先求整体的四个映射方向，再进行筛选，选择两个，特征值大的。(具体看收藏的多分类LDA),然后进行降维。 对本征值进行排序 1234567891011121314151617# Make a list of (eigenvalue, eigenvector) tupleseig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]# Sort the (eigenvalue, eigenvector) tuples from high to loweig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)&quot;&quot;&quot;key = lambda k:k[0]的意义：若可迭代对象是单个数字等，则可以直接排序。若是元祖，字典等，则需要指定根据什么进行排序了。即k[0]的第零个元素进行排序。&quot;&quot;&quot;# Visually confirm that the list is correctly sorted by decreasing eigenvaluesprint(&#x27;Eigenvalues in decreasing order:\\n&#x27;)for i in eig_pairs: print(i[0]) Eigenvalues in decreasing order: 32.2719577997 0.27756686384 7.7995841654e-15 5.76433252705e-15 1234567891011121314151617181920212223print(&#x27;Variance explained:\\n&#x27;)eigv_sum = sum(eig_vals)for i,j in enumerate(eig_pairs): print(&#x27;eigenvalue &#123;0:&#125;: &#123;1:.2%&#125;&#x27;.format(i+1, (j[0]/eigv_sum).real)) &quot;&quot;&quot; ############################################### print &#x27;hello &#123;&#125; i am &#123;&#125;&#x27;.format(&#x27;Kevin&#x27;,&#x27;Tom&#x27;) # hello Kevin i am Tom ############################################### ############################################### print &#x27;&#123;0&#125; i am &#123;1&#125; . my name is &#123;0&#125;&#x27;.format(&#x27;Kevin&#x27;,&#x27;Tom&#x27;) # hello Kevin i am Tom . my name is Kevin ############################################### ############################################### &#123;0:&#125;冒号后面可以跟需要的操作，如保留两位小数 ############################################### &quot;&quot;&quot; Variance explained: eigenvalue 1: 99.15% eigenvalue 2: 0.85% eigenvalue 3: 0.00% eigenvalue 4: 0.00% 选择前两维特征 x ((1,n)-&gt;(1,d)) x1 = x0 * W ((1,n) x (n,d)) =&gt; 所以W为(n,d)，此题为(4,2) 理解意思即可，此题x=(1,n) 1234567W = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))&quot;&quot;&quot;np.vstack():在竖直方向上堆叠np.hstack():在水平方向上平铺&quot;&quot;&quot;print(&#x27;Matrix W:\\n&#x27;, W.real) Matrix W: [[ 0.2049 -0.009 ] [ 0.3871 -0.589 ] [-0.5465 0.2543] [-0.7138 -0.767 ]] 进行数据的降维12X_lda = X.dot(W)assert X_lda.shape == (150,2), &quot;The matrix is not 150x2 dimensional.&quot; 1234567891011121314151617181920212223242526272829303132333435363738from matplotlib import pyplot as pltdef plot_step_lda(): ax = plt.subplot(111) for label,marker,color in zip( range(1,4),(&#x27;^&#x27;, &#x27;s&#x27;, &#x27;o&#x27;),(&#x27;blue&#x27;, &#x27;red&#x27;, &#x27;green&#x27;)): plt.scatter(x=X_lda[:,0].real[y == label], y=X_lda[:,1].real[y == label], marker=marker, color=color, alpha=0.5, label=label_dict[label] ) plt.xlabel(&#x27;LD1&#x27;) plt.ylabel(&#x27;LD2&#x27;) leg = plt.legend(loc=&#x27;upper right&#x27;, fancybox=True) leg.get_frame().set_alpha(0.5) plt.title(&#x27;LDA: Iris projection onto the first 2 linear discriminants&#x27;) # hide axis ticks plt.tick_params(axis=&quot;both&quot;, which=&quot;both&quot;, bottom=&quot;off&quot;, top=&quot;off&quot;, labelbottom=&quot;on&quot;, left=&quot;off&quot;, right=&quot;off&quot;, labelleft=&quot;on&quot;) # remove axis spines ax.spines[&quot;top&quot;].set_visible(False) ax.spines[&quot;right&quot;].set_visible(False) ax.spines[&quot;bottom&quot;].set_visible(False) ax.spines[&quot;left&quot;].set_visible(False) plt.grid() plt.tight_layout plt.show()plot_step_lda() sklearn 有LDA模块可以调用。12345from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA# LDAsklearn_lda = LDA(n_components=2) # 降低成2DX_lda_sklearn = sklearn_lda.fit_transform(X, y) 12345from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA# LDAsklearn_lda = LDA(n_components=2)X_lda_sklearn = sklearn_lda.fit_transform(X, y) 12345678910111213141516171819202122232425262728293031323334def plot_scikit_lda(X, title): ax = plt.subplot(111) for label,marker,color in zip( range(1,4),(&#x27;^&#x27;, &#x27;s&#x27;, &#x27;o&#x27;),(&#x27;blue&#x27;, &#x27;red&#x27;, &#x27;green&#x27;)): plt.scatter(x=X[:,0][y == label], y=X[:,1][y == label] * -1, # flip the figure marker=marker, color=color, alpha=0.5, label=label_dict[label]) plt.xlabel(&#x27;LD1&#x27;) plt.ylabel(&#x27;LD2&#x27;) leg = plt.legend(loc=&#x27;upper right&#x27;, fancybox=True) leg.get_frame().set_alpha(0.5) plt.title(title) # hide axis ticks plt.tick_params(axis=&quot;both&quot;, which=&quot;both&quot;, bottom=&quot;off&quot;, top=&quot;off&quot;, labelbottom=&quot;on&quot;, left=&quot;off&quot;, right=&quot;off&quot;, labelleft=&quot;on&quot;) # remove axis spines ax.spines[&quot;top&quot;].set_visible(False) ax.spines[&quot;right&quot;].set_visible(False) ax.spines[&quot;bottom&quot;].set_visible(False) ax.spines[&quot;left&quot;].set_visible(False) plt.grid() plt.tight_layout plt.show() 12plot_step_lda()plot_scikit_lda(X_lda_sklearn, title=&#x27;Default LDA via scikit-learn&#x27;) 可以看出来sklearn和自己做的效果是差不多的","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"聚类基础——KMEANS & DBSCAN","slug":"聚类基础——KMEANS-DBSCAN","date":"2021-05-13T10:05:21.000Z","updated":"2021-05-13T10:34:24.724Z","comments":true,"path":"20210513/聚类基础——KMEANS-DBSCAN.html","link":"","permalink":"https://xxren8218.github.io/20210513/%E8%81%9A%E7%B1%BB%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94KMEANS-DBSCAN.html","excerpt":"","text":"聚类——啤酒分类1234# beer datasetimport pandas as pdbeer = pd.read_csv(&#x27;data.txt&#x27;, sep=&#x27; &#x27;)beer .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name calories sodium alcohol cost 0 Budweiser 144 15 4.7 0.43 1 Schlitz 151 19 4.9 0.43 2 Lowenbrau 157 15 0.9 0.48 3 Kronenbourg 170 7 5.2 0.73 4 Heineken 152 11 5.0 0.77 5 Old_Milwaukee 145 23 4.6 0.28 6 Augsberger 175 24 5.5 0.40 7 Srohs_Bohemian_Style 149 27 4.7 0.42 8 Miller_Lite 99 10 4.3 0.43 9 Budweiser_Light 113 8 3.7 0.40 10 Coors 140 18 4.6 0.44 11 Coors_Light 102 15 4.1 0.46 12 Michelob_Light 135 11 4.2 0.50 13 Becks 150 19 4.7 0.76 14 Kirin 149 6 5.0 0.79 15 Pabst_Extra_Light 68 15 2.3 0.38 16 Hamms 139 19 4.4 0.43 17 Heilemans_Old_Style 144 24 4.9 0.43 18 Olympia_Goled_Light 72 6 2.9 0.46 19 Schlitz_Light 97 7 4.2 0.47 1X = beer[[&quot;calories&quot;,&quot;sodium&quot;,&quot;alcohol&quot;,&quot;cost&quot;]] K-means clustering1234from sklearn.cluster import KMeanskm = KMeans(n_clusters=3).fit(X) # n_cluster就是聚集成几个簇km2 = KMeans(n_clusters=2).fit(X) 1km.labels_ # 调用函数直接返回结果了！ array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 0, 0, 2, 1]) 123beer[&#x27;cluster&#x27;] = km.labels_beer[&#x27;cluster2&#x27;] = km2.labels_beer.sort_values(&#x27;cluster&#x27;) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name calories sodium alcohol cost cluster cluster2 0 Budweiser 144 15 4.7 0.43 0 1 1 Schlitz 151 19 4.9 0.43 0 1 2 Lowenbrau 157 15 0.9 0.48 0 1 3 Kronenbourg 170 7 5.2 0.73 0 1 4 Heineken 152 11 5.0 0.77 0 1 5 Old_Milwaukee 145 23 4.6 0.28 0 1 6 Augsberger 175 24 5.5 0.40 0 1 7 Srohs_Bohemian_Style 149 27 4.7 0.42 0 1 17 Heilemans_Old_Style 144 24 4.9 0.43 0 1 16 Hamms 139 19 4.4 0.43 0 1 10 Coors 140 18 4.6 0.44 0 1 14 Kirin 149 6 5.0 0.79 0 1 12 Michelob_Light 135 11 4.2 0.50 0 1 13 Becks 150 19 4.7 0.76 0 1 9 Budweiser_Light 113 8 3.7 0.40 1 0 8 Miller_Lite 99 10 4.3 0.43 1 0 11 Coors_Light 102 15 4.1 0.46 1 0 19 Schlitz_Light 97 7 4.2 0.47 1 0 15 Pabst_Extra_Light 68 15 2.3 0.38 2 0 18 Olympia_Goled_Light 72 6 2.9 0.46 2 0 123456from pandas.tools.plotting import scatter_matrix # 导入scatter_matrix可以画多福图形。%matplotlib inlinecluster_centers = km.cluster_centers_cluster_centers_2 = km2.cluster_centers_ 1beer.groupby(&quot;cluster&quot;).mean() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } calories sodium alcohol cost cluster2 cluster 0 150.00 17.0 4.521429 0.520714 1 1 102.75 10.0 4.075000 0.440000 0 2 70.00 10.5 2.600000 0.420000 0 1beer.groupby(&quot;cluster2&quot;).mean() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } calories sodium alcohol cost cluster cluster2 0 91.833333 10.166667 3.583333 0.433333 1.333333 1 150.000000 17.000000 4.521429 0.520714 0.000000 12centers = beer.groupby(&quot;cluster&quot;).mean().reset_index() # 中心点，为后面作图做准备！ # reset_index(默认drop=False)，表示获取新的索引，并保留原来索引 123%matplotlib inlineimport matplotlib.pyplot as pltplt.rcParams[&#x27;font.size&#x27;] = 14 # rcParams 可以设置图形整体的字体大小等。 12import numpy as npcolors = np.array([&#x27;red&#x27;, &#x27;green&#x27;, &#x27;blue&#x27;, &#x27;yellow&#x27;]) 先看其中两个特征的分布情况 123456plt.scatter(beer[&quot;calories&quot;], beer[&quot;alcohol&quot;],c=colors[beer[&quot;cluster&quot;]])plt.scatter(centers.calories, centers.alcohol, linewidths=3, marker=&#x27;+&#x27;, s=300, c=&#x27;black&#x27;) # centers.calories获取质心plt.xlabel(&quot;Calories&quot;)plt.ylabel(&quot;Alcohol&quot;) &lt;matplotlib.text.Text at 0x18a25af4ac8&gt; 再看其中各个维度特征的分布 scatter_matrix 数据是多维的，要么PCA，要么这种方式进行可视化 条形图代表自身的特征的分布情况 12scatter_matrix(beer[[&quot;calories&quot;,&quot;sodium&quot;,&quot;alcohol&quot;,&quot;cost&quot;]],s=100, alpha=1, c=colors[beer[&quot;cluster&quot;]], figsize=(10,10))plt.suptitle(&quot;With 3 centroids initialized&quot;) C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: FutureWarning: 'pandas.tools.plotting.scatter_matrix' is deprecated, import 'pandas.plotting.scatter_matrix' instead. if __name__ == '__main__': &lt;matplotlib.text.Text at 0x18a25b67e80&gt; 12scatter_matrix(beer[[&quot;calories&quot;,&quot;sodium&quot;,&quot;alcohol&quot;,&quot;cost&quot;]],s=100, alpha=1, c=colors[beer[&quot;cluster2&quot;]], figsize=(10,10))plt.suptitle(&quot;With 2 centroids initialized&quot;) C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: FutureWarning: 'pandas.tools.plotting.scatter_matrix' is deprecated, import 'pandas.plotting.scatter_matrix' instead. if __name__ == '__main__': &lt;matplotlib.text.Text at 0x18a2613c710&gt; Scaled data sklearn进行数据标准化 1234from sklearn.preprocessing import StandardScalerscaler = StandardScaler()X_scaled = scaler.fit_transform(X)X_scaled array([[ 0.38791334, 0.00779468, 0.43380786, -0.45682969], [ 0.6250656 , 0.63136906, 0.62241997, -0.45682969], [ 0.82833896, 0.00779468, -3.14982226, -0.10269815], [ 1.26876459, -1.23935408, 0.90533814, 1.66795955], [ 0.65894449, -0.6157797 , 0.71672602, 1.95126478], [ 0.42179223, 1.25494344, 0.3395018 , -1.5192243 ], [ 1.43815906, 1.41083704, 1.1882563 , -0.66930861], [ 0.55730781, 1.87851782, 0.43380786, -0.52765599], [-1.1366369 , -0.7716733 , 0.05658363, -0.45682969], [-0.66233238, -1.08346049, -0.5092527 , -0.66930861], [ 0.25239776, 0.47547547, 0.3395018 , -0.38600338], [-1.03500022, 0.00779468, -0.13202848, -0.24435076], [ 0.08300329, -0.6157797 , -0.03772242, 0.03895447], [ 0.59118671, 0.63136906, 0.43380786, 1.88043848], [ 0.55730781, -1.39524768, 0.71672602, 2.0929174 ], [-2.18688263, 0.00779468, -1.82953748, -0.81096123], [ 0.21851887, 0.63136906, 0.15088969, -0.45682969], [ 0.38791334, 1.41083704, 0.62241997, -0.45682969], [-2.05136705, -1.39524768, -1.26370115, -0.24435076], [-1.20439469, -1.23935408, -0.03772242, -0.17352445]]) 1km = KMeans(n_clusters=3).fit(X_scaled) 12beer[&quot;scaled_cluster&quot;] = km.labels_beer.sort_values(&quot;scaled_cluster&quot;) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name calories sodium alcohol cost cluster cluster2 scaled_cluster 0 Budweiser 144 15 4.7 0.43 0 1 0 1 Schlitz 151 19 4.9 0.43 0 1 0 17 Heilemans_Old_Style 144 24 4.9 0.43 0 1 0 16 Hamms 139 19 4.4 0.43 0 1 0 5 Old_Milwaukee 145 23 4.6 0.28 0 1 0 6 Augsberger 175 24 5.5 0.40 0 1 0 7 Srohs_Bohemian_Style 149 27 4.7 0.42 0 1 0 10 Coors 140 18 4.6 0.44 0 1 0 15 Pabst_Extra_Light 68 15 2.3 0.38 2 0 1 12 Michelob_Light 135 11 4.2 0.50 0 1 1 11 Coors_Light 102 15 4.1 0.46 1 0 1 9 Budweiser_Light 113 8 3.7 0.40 1 0 1 8 Miller_Lite 99 10 4.3 0.43 1 0 1 2 Lowenbrau 157 15 0.9 0.48 0 1 1 18 Olympia_Goled_Light 72 6 2.9 0.46 2 0 1 19 Schlitz_Light 97 7 4.2 0.47 1 0 1 13 Becks 150 19 4.7 0.76 0 1 2 14 Kirin 149 6 5.0 0.79 0 1 2 4 Heineken 152 11 5.0 0.77 0 1 2 3 Kronenbourg 170 7 5.2 0.73 0 1 2 What are the “characteristics” of each cluster? 1beer.groupby(&quot;scaled_cluster&quot;).mean() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } calories sodium alcohol cost cluster cluster2 scaled_cluster 0 148.375 21.125 4.7875 0.4075 0.0 1.00 1 105.375 10.875 3.3250 0.4475 1.0 0.25 2 155.250 10.750 4.9750 0.7625 0.0 1.00 1pd.scatter_matrix(X, c=colors[beer.scaled_cluster], alpha=1, figsize=(10,10), s=100) C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: FutureWarning: pandas.scatter_matrix is deprecated. Use pandas.plotting.scatter_matrix instead if __name__ == '__main__': array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A279F8F28&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A282989B0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27B5E2E8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27B94F60&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27BE41D0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27C19F28&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27C61F60&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27C71C88&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27CF1860&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27D3B7B8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27D7C5C0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27DC6F98&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27E02748&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27E4FEB8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27E8D588&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000018A27ED47B8&gt;]], dtype=object) 聚类评估：轮廓系数（Silhouette Coefficient ）（常用） 计算样本i到同簇其他样本的平均距离ai。ai 越小，说明样本i越应该被聚类到该簇。将ai 称为样本i的簇内不相似度。越小越好。 计算样本i到其他某簇Cj 的所有样本的平均距离bij，称为样本i与簇Cj 的不相似度。定义为样本i的簇间不相似度：bi =min{bi1, bi2, …, bik}。越大越好。 si接近1，则说明样本i聚类合理 si接近-1，则说明样本i更应该分类到另外的簇 若si 近似为0，则说明样本i在两个簇的边界上。 使用sklearn模块的metrics进行聚类评估 1234from sklearn import metricsscore_scaled = metrics.silhouette_score(X,beer.scaled_cluster) # 做归一化的结果score = metrics.silhouette_score(X,beer.cluster) # 不做归一化的结果print(score_scaled, score) 0.179780680894 0.673177504646 做归一化的结果反而低了。 做归一化不一定会得到好结果。 尝试计算不同的k值对结果的影响。1234567scores = []for k in range(2,20): labels = KMeans(n_clusters=k).fit(X).labels_ score = metrics.silhouette_score(X, labels) scores.append(score)scores [0.69176560340794857, 0.67317750464557957, 0.58570407211277953, 0.42254873351720201, 0.4559182167013377, 0.43776116697963124, 0.38946337473125997, 0.39746405172426014, 0.33061511213823314, 0.34131096180393328, 0.34597752371272478, 0.31221439248428434, 0.30707782144770296, 0.31834561839139497, 0.28495140011748982, 0.23498077333071996, 0.15880910174962809, 0.084230513801511767] 123plt.plot(list(range(2,20)), scores)plt.xlabel(&quot;Number of Clusters Initialized&quot;)plt.ylabel(&quot;Sihouette Score&quot;) &lt;matplotlib.text.Text at 0x18a288239e8&gt; 可以看出K=2的时候比较合适 DBSCAN clustering 在不规则的数据集上比较强大，简单数据集可能还不如kmeans. 12from sklearn.cluster import DBSCANdb = DBSCAN(eps=10, min_samples=2).fit(X) # eps半径，min_samples指密度 1labels = db.labels_ 12beer[&#x27;cluster_db&#x27;] = labelsbeer.sort_values(&#x27;cluster_db&#x27;) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name calories sodium alcohol cost cluster cluster2 scaled_cluster cluster_db 9 Budweiser_Light 113 8 3.7 0.40 1 0 1 -1 3 Kronenbourg 170 7 5.2 0.73 0 1 2 -1 6 Augsberger 175 24 5.5 0.40 0 1 0 -1 17 Heilemans_Old_Style 144 24 4.9 0.43 0 1 0 0 16 Hamms 139 19 4.4 0.43 0 1 0 0 14 Kirin 149 6 5.0 0.79 0 1 2 0 13 Becks 150 19 4.7 0.76 0 1 2 0 12 Michelob_Light 135 11 4.2 0.50 0 1 1 0 10 Coors 140 18 4.6 0.44 0 1 0 0 0 Budweiser 144 15 4.7 0.43 0 1 0 0 7 Srohs_Bohemian_Style 149 27 4.7 0.42 0 1 0 0 5 Old_Milwaukee 145 23 4.6 0.28 0 1 0 0 4 Heineken 152 11 5.0 0.77 0 1 2 0 2 Lowenbrau 157 15 0.9 0.48 0 1 1 0 1 Schlitz 151 19 4.9 0.43 0 1 0 0 8 Miller_Lite 99 10 4.3 0.43 1 0 1 1 11 Coors_Light 102 15 4.1 0.46 1 0 1 1 19 Schlitz_Light 97 7 4.2 0.47 1 0 1 1 15 Pabst_Extra_Light 68 15 2.3 0.38 2 0 1 2 18 Olympia_Goled_Light 72 6 2.9 0.46 2 0 1 2 1beer.groupby(&#x27;cluster_db&#x27;).mean() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } calories sodium alcohol cost cluster cluster2 scaled_cluster cluster_db -1 152.666667 13.000000 4.800000 0.510000 0.333333 0.666667 1.000000 0 146.250000 17.250000 4.383333 0.513333 0.000000 1.000000 0.666667 1 99.333333 10.666667 4.200000 0.453333 1.000000 0.000000 1.000000 2 70.000000 10.500000 2.600000 0.420000 2.000000 0.000000 1.000000 1pd.scatter_matrix(X, c=colors[beer.cluster_db], figsize=(10,10), s=100) C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: FutureWarning: pandas.scatter_matrix is deprecated. Use pandas.plotting.scatter_matrix instead if __name__ == '__main__': array([[, , , ], [, , , ], [, , , ], [, , , ]], dtype=object) DBSCAN的后续过程一样，可以用轮廓系数进行评估。for循环eps和min_samples。也可以做数据增强。（看哪个数据的维度对结果有比较好的影响）","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"剑指Offer（六十七）：剪绳子","slug":"剑指Offer（六十七）：剪绳子","date":"2021-05-12T08:49:55.000Z","updated":"2021-05-19T16:53:47.241Z","comments":true,"path":"20210512/剑指Offer（六十七）：剪绳子.html","link":"","permalink":"https://xxren8218.github.io/20210512/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AD%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9A%E5%89%AA%E7%BB%B3%E5%AD%90.html","excerpt":"","text":"1.题目 给你一根长度为n的绳子，请把绳子剪成整数长的m段（m、n都是整数，n>1并且m>1，m","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数学","slug":"数学","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E5%AD%A6/"}]},{"title":"剑指Offer（六十三）：数据流中的中位数","slug":"剑指Offer（六十三）：数据流中的中位数","date":"2021-05-12T08:38:58.000Z","updated":"2021-05-15T13:52:18.331Z","comments":true,"path":"20210512/剑指Offer（六十三）：数据流中的中位数.html","link":"","permalink":"https://xxren8218.github.io/20210512/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AD%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0.html","excerpt":"","text":"1.题目 如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。 2.思路 可以建立一个列表来append所给的数据 然后将列表排序，计算列表的长度。只需要区分奇数和偶数情况即可。 3.代码1234567891011121314151617181920# -*- coding:utf-8 -*-class Solution: def __init__(self): self.s = [] def Insert(self, num): # write code here self.s.append(num) def GetMedian(self): # write code here self.s = sorted(self.s) n = len(self.s) if n == 0: return 0 if n % 2 != 0: # 奇数 return self.s[n/2] else: # 偶数 return (self.s[n/2]+self.s[n/2-1])/2.0 # 注意如果这里写2的话，得到的是向下取整的整数，所以需要写成浮点型。 上述的代码虽然可以，但是在力扣上执行时间长。提出另一种思路——最大堆，最小堆。 如果我们可以对数据进行排序，分成小的一半和大的一半。找小的最大值和大的最小值即可。这个不就是最小、最大堆嘛！ 最大堆（找出最小的k个元素） 最小堆（找出最大的k个元素） 如图所示过程。用最大堆存最小的k个元素，最小堆存最大k个元素。我们就可以用O（1）的复杂度找到值max和min了 在数据存放的时候我们可以这样，当最小堆和最大堆的尺寸相等时，存放进入最大堆maxheap，不等时，存放进最小堆minheap。 但是这存在一个问题，若将a1存放进入minheap的时候，若a1的值比maxheap的一些数字还要小，则不满足我们之前的假设——maxheap的数字比minheap的数字小；若将a2的存放到maxheap的时候，若a2比minheap的某些值还要大，那么也不符合我们的假定；那么我们可以这样做； 将a1先插入到maxheap中，再将maxheap的队顶元素插入到minheap，这样就保证了maxheap始终为最小的元素。 同理，将a2先放入到minheap中，再将minheap的队顶元素放入maxheap中 python没有最大堆，只需要把最小堆取反。 heappushpop()相当于先推后弹出，这意味着堆大小可能会在进程中发生变化. heapreplace()相当于先弹出，然后推送，附加的限制是保证堆大小在这个过程中不会改变。 代码12345678910111213141516171819202122232425262728293031323334353637import heapqclass MedianFinder(object): def __init__(self): &quot;&quot;&quot; initialize your data structure here. &quot;&quot;&quot; self.maxheap = [] self.minheap = [] def addNum(self, num): &quot;&quot;&quot; :type num: int :rtype: None &quot;&quot;&quot; # 为了使得数据平均分配到两个堆，当两个堆的尺寸一样时，将新增加的元素放到minheap中 # 具体做法分两步：1.现将新元素放到maxheap中，2.再将maxheap的堆顶元素放入minheap # python没有最大堆，可以用最小堆取反实现 if len(self.maxheap) == len(self.minheap): heapq.heappush(self.minheap,-heapq.heappushpop(self.maxheap,-num)) else: heapq.heappush(self.maxheap,-heapq.heappushpop(self.minheap,num)) def findMedian(self): &quot;&quot;&quot; :rtype: float &quot;&quot;&quot; if len(self.maxheap) == len(self.minheap): return (-self.maxheap[0]+self.minheap[0])/2.0 else: return self.minheap[0]# Your MedianFinder object will be instantiated and called as such:# obj = MedianFinder()# obj.addNum(num)# param_2 = obj.findMedian()","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"https://xxren8218.github.io/tags/%E6%8E%92%E5%BA%8F/"},{"name":"最小堆","slug":"最小堆","permalink":"https://xxren8218.github.io/tags/%E6%9C%80%E5%B0%8F%E5%A0%86/"}]},{"title":"剑指Offer（六十四）：滑动窗口的最大值","slug":"剑指Offer（六十四）：滑动窗口的最大值","date":"2021-05-12T08:25:59.000Z","updated":"2021-05-17T10:13:35.853Z","comments":true,"path":"20210512/剑指Offer（六十四）：滑动窗口的最大值.html","link":"","permalink":"https://xxren8218.github.io/20210512/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AD%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9A%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC.html","excerpt":"","text":"1.题目 给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}。 2.思路—暴力解法 创建一个列表res来存储最终结果 创建一个临时列表temp来存储目前窗口内的数字 python有内置的max()函数可以找出列表中的最大数字。将其放入结果列表中即可。 while循环，直到窗口的右边界到达给定列表长度时，停止。 ## 3.代码 123456789101112131415# -*- coding:utf-8 -*-class Solution: def maxInWindows(self, num, size): # write code here if not num or size &lt;= 0: return [] res = [] start = 0 end = start + size while end &lt;= len(num): temp_list = num[start:end] res.append(max(temp_list)) # 其实可以和上面一步的合并在一起。 start += 1 end += 1 return res - max的时间复杂度为O(n) 对于每个滑动窗口，我们可以使用 O(k)的时间遍历其中的每一个元素，找出其中的最大值。对于长度为 n 的数组 nums 而言，窗口的数量为 n-k+1，因此该算法的时间复杂度为 O((n-k+1)k)=O(nk) 会超出时间限制，因此我们需要进行一些优化。——需要将取最大值的操作进行降低时间复杂度。 在力扣上提交时，超时，时间复杂度为O(nk),想办法对其进行改进。对于实现栈min函数的题目使用 单调栈 实现了随意入栈、出栈情况下的 O(1)时间获取 “栈内最小值” 。本题同理，不同点在于 “出栈操作” 删除的是 “列表尾部元素” ，而 “窗口滑动” 删除的是 “列表首部元素” ——为单调递减队列。 ** 理解了栈的min函数，再理解队的最大函数，再写这道题容易些。 队的最大函数的理解 入队顺序为： [5]-&gt;[5,-1]-&gt;[5,-1,3]-&gt;[5,-1,3,6]-&gt;[5,-1,3,6,2]队的最大函数时的辅助队列： [5]-&gt;[5,-1]-&gt;[5,3] -&gt;[6] -&gt;[6,2]最大值为： [5]-&gt;[5] -&gt;[5] -&gt;[6] -&gt;[6] 出队顺序为：(先进先出)。 [5,-1,3,6,2]-&gt;[-1,3,6,2]-&gt;[3,6,2]-&gt;[6,2]-&gt;[6]队的最大函数时的辅助队列： [6,2] -&gt;[6,2] -&gt;[6,2] -&gt;[6,2]-&gt;[2] (若出队和辅助队的元素相同-删除)最大值为： [6] -&gt;[6] -&gt;[6] -&gt;[6] -&gt;[2] 还有一点不同的是：在未形成窗口时是不需要“出队操作”的 未形成窗口时：相当于一直为进队的操作。并保障其单调. 形成窗口后：相当于进队和出队操作同时进行。需要用到双端队列。——能使得leftpop为O(1) 在向右移动一次后，相当于最右边值入队，使新队列满足单调。 如果在移动一次后的最左边-1值等于队列的最大的值，则将其删除 代码12345678910111213141516171819202122232425262728293031323334from collections import dequeclass Solution(object): def maxSlidingWindow(self, nums, k): &quot;&quot;&quot; :type nums: List[int] :type k: int :rtype: List[int] &quot;&quot;&quot; res = [] # 当窗口未形成的时候：将nums的元素加入双端队列中，并保证其单调（增减都可，只不过后面处理一个头一个尾而已，我这里取递减） n = len(nums) if n == 0 or k &gt; n: return [] dq = deque() for i in range(k): while dq and dq[-1] &lt; nums[i]: dq.pop() dq.append(nums[i]) # 此时窗口形成。因为加了k个数字到dq了 res.append(dq[0]) # 窗口形成以后相当于队列的入队和出队同时进行了。需要进行两步 # 1.移动以后的窗口的最左边值若等于dp[0]，则dp[0]需删除 # 2.入队的元素与dp保证递减 for i in range(k,n): if nums[i-k] == dq[0]: dq.popleft() while dq and dq[-1] &lt; nums[i]: dq.pop() dq.append(nums[i]) res.append(dq[0]) return res","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"双指针","slug":"双指针","permalink":"https://xxren8218.github.io/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"双端队列","slug":"双端队列","permalink":"https://xxren8218.github.io/tags/%E5%8F%8C%E7%AB%AF%E9%98%9F%E5%88%97/"},{"name":"单调队列","slug":"单调队列","permalink":"https://xxren8218.github.io/tags/%E5%8D%95%E8%B0%83%E9%98%9F%E5%88%97/"},{"name":"辅助队列","slug":"辅助队列","permalink":"https://xxren8218.github.io/tags/%E8%BE%85%E5%8A%A9%E9%98%9F%E5%88%97/"}]},{"title":"贝叶斯实战-新闻分类","slug":"贝叶斯实战-新闻分类","date":"2021-05-11T06:25:35.000Z","updated":"2021-05-11T10:39:40.681Z","comments":true,"path":"20210511/贝叶斯实战-新闻分类.html","link":"","permalink":"https://xxren8218.github.io/20210511/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9E%E6%88%98-%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB.html","excerpt":"","text":"文本分析——新闻分类123import pandas as pdimport jieba# pip install jieba 数据来源：http://www.sogou.com/labs/resource/ca.php123df_news = pd.read_table(&#x27;./data/val.txt&#x27;,names=[&#x27;category&#x27;,&#x27;theme&#x27;,&#x27;URL&#x27;,&#x27;content&#x27;],encoding=&#x27;utf-8&#x27;) # 涉及中文，用encodingdf_news = df_news.dropna() # 缺失值直接drop掉df_news.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } category theme URL content 0 汽车 新辉腾 ４．２ Ｖ８ ４座加长Ｉｎｄｉｖｉｄｕａｌ版２０１１款 最新报价 http://auto.data.people.com.cn/model_15782/ 经销商 电话 试驾／订车Ｕ憬杭州滨江区江陵路１７８０号４００８－１１２２３３转５８６４＃保常... 1 汽车 ９１８ Ｓｐｙｄｅｒ概念车 http://auto.data.people.com.cn/prdview_165423.... 呼叫热线 ４００８－１００－３００ 服务邮箱 ｋｆ＠ｐｅｏｐｌｅｄａｉｌｙ．ｃｏｍ．ｃｎ 2 汽车 日内瓦亮相 ＭＩＮＩ性能版／概念车－１．６Ｔ引擎 http://auto.data.people.com.cn/news/story_5249... ＭＩＮＩ品牌在二月曾经公布了最新的ＭＩＮＩ新概念车Ｃｌｕｂｖａｎ效果图，不过现在在日内瓦车展... 3 汽车 清仓大甩卖一汽夏利Ｎ５威志Ｖ２低至３．３９万 http://auto.data.people.com.cn/news/story_6144... 清仓大甩卖！一汽夏利Ｎ５、威志Ｖ２低至３．３９万＝日，启新中国一汽强势推出一汽夏利Ｎ５、威志... 4 汽车 大众敞篷家族新成员 高尔夫敞篷版实拍 http://auto.data.people.com.cn/news/story_5686... 在今年３月的日内瓦车展上，我们见到了高尔夫家族的新成员，高尔夫敞篷版，这款全新敞篷车受到了众... 1df_news.shape (5000, 4) 分词：使用结吧分词器 结巴分词器需要转换成list的格式 12content = df_news.content.values.tolist()print (content[1000]) 阿里巴巴集团昨日宣布，将在集团管理层面设立首席数据官岗位（Ｃｈｉｅｆ Ｄａｔａ Ｏｆｆｉｃｅｒ），阿里巴巴Ｂ２Ｂ公司ＣＥＯ陆兆禧将会出任上述职务，向集团ＣＥＯ马云直接汇报。＞菹ぃ和６月初的首席风险官职务任命相同，首席数据官亦为阿里巴巴集团在完成与雅虎股权谈判，推进“ｏｎｅ ｃｏｍｐａｎｙ”目标后，在集团决策层面新增的管理岗位。０⒗锛团昨日表示，“变成一家真正意义上的数据公司”已是战略共识。记者刘夏 12345content_S = []for line in content: current_segment = jieba.lcut(line) # lcut可以进行分词 if len(current_segment) &gt; 1 and current_segment != &#x27;\\r\\n&#x27;: # \\n换行符,\\r回车符号 content_S.append(current_segment) 1content_S[1000] [&#39;阿里巴巴&#39;, &#39;集团&#39;, &#39;昨日&#39;, &#39;宣布&#39;, &#39;，&#39;, &#39;将&#39;, &#39;在&#39;, &#39;集团&#39;, &#39;管理&#39;, &#39;层面&#39;, &#39;设立&#39;, &#39;首席&#39;, &#39;数据&#39;, &#39;官&#39;, &#39;岗位&#39;, &#39;（&#39;, &#39;Ｃ&#39;, &#39;ｈ&#39;, &#39;ｉ&#39;, &#39;ｅ&#39;, &#39;ｆ&#39;, &#39;\\u3000&#39;, &#39;Ｄ&#39;, &#39;ａ&#39;, &#39;ｔ&#39;, &#39;ａ&#39;, &#39;\\u3000&#39;, &#39;Ｏ&#39;, &#39;ｆ&#39;, &#39;ｆ&#39;, &#39;ｉ&#39;, &#39;ｃ&#39;, &#39;ｅ&#39;, &#39;ｒ&#39;, &#39;）&#39;, &#39;，&#39;, &#39;阿里巴巴&#39;, &#39;Ｂ&#39;, &#39;２&#39;, &#39;Ｂ&#39;, &#39;公司&#39;, &#39;Ｃ&#39;, &#39;Ｅ&#39;, &#39;Ｏ&#39;, &#39;陆兆禧&#39;, &#39;将&#39;, &#39;会&#39;, &#39;出任&#39;, &#39;上述&#39;, &#39;职务&#39;, &#39;，&#39;, &#39;向&#39;, &#39;集团&#39;, &#39;Ｃ&#39;, &#39;Ｅ&#39;, &#39;Ｏ&#39;, &#39;马云&#39;, &#39;直接&#39;, &#39;汇报&#39;, &#39;。&#39;, &#39;＞&#39;, &#39;菹&#39;, &#39;ぃ&#39;, &#39;和&#39;, &#39;６&#39;, &#39;月初&#39;, &#39;的&#39;, &#39;首席&#39;, &#39;风险&#39;, &#39;官&#39;, &#39;职务&#39;, &#39;任命&#39;, &#39;相同&#39;, &#39;，&#39;, &#39;首席&#39;, &#39;数据&#39;, &#39;官亦为&#39;, &#39;阿里巴巴&#39;, &#39;集团&#39;, &#39;在&#39;, &#39;完成&#39;, &#39;与&#39;, &#39;雅虎&#39;, &#39;股权&#39;, &#39;谈判&#39;, &#39;，&#39;, &#39;推进&#39;, &#39;“&#39;, &#39;ｏ&#39;, &#39;ｎ&#39;, &#39;ｅ&#39;, &#39;\\u3000&#39;, &#39;ｃ&#39;, &#39;ｏ&#39;, &#39;ｍ&#39;, &#39;ｐ&#39;, &#39;ａ&#39;, &#39;ｎ&#39;, &#39;ｙ&#39;, &#39;”&#39;, &#39;目标&#39;, &#39;后&#39;, &#39;，&#39;, &#39;在&#39;, &#39;集团&#39;, &#39;决策&#39;, &#39;层面&#39;, &#39;新增&#39;, &#39;的&#39;, &#39;管理&#39;, &#39;岗位&#39;, &#39;。&#39;, &#39;０&#39;, &#39;⒗&#39;, &#39;锛&#39;, &#39;团&#39;, &#39;昨日&#39;, &#39;表示&#39;, &#39;，&#39;, &#39;“&#39;, &#39;变成&#39;, &#39;一家&#39;, &#39;真正&#39;, &#39;意义&#39;, &#39;上&#39;, &#39;的&#39;, &#39;数据&#39;, &#39;公司&#39;, &#39;”&#39;, &#39;已&#39;, &#39;是&#39;, &#39;战略&#39;, &#39;共识&#39;, &#39;。&#39;, &#39;记者&#39;, &#39;刘夏&#39;] 12df_content=pd.DataFrame(&#123;&#x27;content_S&#x27;:content_S&#125;) # DataFrame可以使用键值对的方式来获得下面的数据形式。df_content.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } content_S 0 [经销商, , 电话, , 试驾, ／, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, ... 1 [呼叫, 热线, , ４, ０, ０, ８, －, １, ０, ０, －, ３, ０, ０... 2 [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 在, 二月, 曾经, 公布, 了, 最新, 的, Ｍ, Ｉ... 3 [清仓, 大, 甩卖, ！, 一汽, 夏利, Ｎ, ５, 、, 威志, Ｖ, ２, 低至, ... 4 [在, 今年, ３, 月, 的, 日内瓦, 车展, 上, ，, 我们, 见到, 了, 高尔夫... 进行数据的清洗，去掉停用词。—停用词表，可以网上下载。12stopwords=pd.read_csv(&quot;stopwords.txt&quot;,index_col=False,sep=&quot;\\t&quot;,quoting=3,names=[&#x27;stopword&#x27;], encoding=&#x27;utf-8&#x27;)stopwords.head(20) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } stopword 0 ! 1 \" 2 # 3 $ 4 % 5 &amp; 6 ' 7 ( 8 ) 9 * 10 + 11 , 12 - 13 -- 14 . 15 .. 16 ... 17 ...... 18 ................... 19 ./ 12345678910111213141516171819202122def drop_stopwords(contents,stopwords): contents_clean = [] all_words = [] for line in contents: line_clean = [] for word in line: if word in stopwords: continue line_clean.append(word) all_words.append(str(word)) # 一会做一个词云。有现成的库，可以轻松实现出来。 contents_clean.append(line_clean) return contents_clean,all_words # print (contents_clean) contents = df_content.content_S.values.tolist() stopwords = stopwords.stopword.values.tolist()contents_clean,all_words = drop_stopwords(contents,stopwords)# df_content.content_S.isin(stopwords.stopword)# df_content=df_content[~df_content.content_S.isin(stopwords.stopword)]# df_content.head() 12df_content=pd.DataFrame(&#123;&#x27;contents_clean&#x27;:contents_clean&#125;)df_content.head() # 也可以在停用词里面增加字母，将字母去掉。 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } contents_clean 0 [经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ... 1 [呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,... 2 [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念... 3 [清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ... 4 [日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,... 12df_all_words=pd.DataFrame(&#123;&#x27;all_words&#x27;:all_words&#125;)df_all_words.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } all_words 0 经销商 1 电话 2 试驾 3 订车 4 Ｕ 123words_count=df_all_words.groupby(by=[&#x27;all_words&#x27;])[&#x27;all_words&#x27;].agg(&#123;&quot;count&quot;:numpy.size&#125;) # 先分组再agg求和。words_count=words_count.reset_index().sort_values(by=[&quot;count&quot;],ascending=False) # 按值进行排序words_count.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } all_words count 4077 中 5199 4209 中国 3115 88255 说 3055 104747 Ｓ 2646 1373 万 2390 词云的展示 ——在github上面直接搜wordcloud就行！ 12345678910from wordcloud import WordCloud # 导入词云import matplotlib.pyplot as plt%matplotlib inlineimport matplotlibmatplotlib.rcParams[&#x27;figure.figsize&#x27;] = (10.0, 5.0)wordcloud=WordCloud(font_path=&quot;./data/simhei.ttf&quot;,background_color=&quot;white&quot;,max_font_size=80)word_frequence = &#123;x[0]:x[1] for x in words_count.head(100).values&#125; # 画前100个词wordcloud=wordcloud.fit_words(word_frequence)plt.imshow(wordcloud) &lt;matplotlib.image.AxesImage at 0x186064c64e0&gt; TF-IDF ：提取关键词12345import jieba.analyse # jieba和sklearn都可以进行词频的提取index = 2400print (df_news[&#x27;content&#x27;][index])content_S_str = &quot;&quot;.join(content_S[index]) # 将分词完的数据拿出来！print (&quot; &quot;.join(jieba.analyse.extract_tags(content_S_str, topK=5, withWeight=False))) # jieba里面有提取关键词的模块 法国ＶＳ西班牙、里贝里ＶＳ哈维，北京时间６月２４日凌晨一场的大战举世瞩目，而这场胜利不仅仅关乎两支顶级强队的命运，同时也是他们背后的球衣赞助商耐克和阿迪达斯之间的一次角逐。Ｔ谌胙”窘炫分薇的１６支球队之中，阿迪达斯和耐克的势力范围也是几乎旗鼓相当：其中有５家球衣由耐克提供，而阿迪达斯则赞助了６家，此外茵宝有３家，而剩下的两家则由彪马赞助。而当比赛进行到现在，率先挺进四强的两支球队分别被耐克支持的葡萄牙和阿迪达斯支持的德国占据，而由于最后一场１／４决赛是茵宝（英格兰）和彪马（意大利）的对决，这也意味着明天凌晨西班牙同法国这场阿迪达斯和耐克在１／４决赛的唯一一次直接交手将直接决定两家体育巨头在此次欧洲杯上的胜负。８据评估，在２０１２年足球商品的销售额能总共超过４０亿欧元，而单单是不足一个月的欧洲杯就有高达５亿的销售额，也就是说在欧洲杯期间将有７００万件球衣被抢购一空。根据市场评估，两大巨头阿迪达斯和耐克的市场占有率也是并驾齐驱，其中前者占据３８％，而后者占据３６％。体育权利顾问奥利弗－米歇尔在接受《队报》采访时说：“欧洲杯是耐克通过法国翻身的一个绝佳机会！”Ｃ仔尔接着谈到两大赞助商的经营策略：“竞技体育的成功会燃起球衣购买的热情，不过即便是水平相当，不同国家之间的欧洲杯效应却存在不同。在德国就很出色，大约１／４的德国人通过电视观看了比赛，而在西班牙效果则差很多，由于民族主义高涨的加泰罗尼亚地区只关注巴萨和巴萨的球衣，他们对西班牙国家队根本没什么兴趣。”因此尽管西班牙接连拿下欧洲杯和世界杯，但是阿迪达斯只为西班牙足协支付每年２６００万的赞助费＃相比之下尽管最近两届大赛表现糟糕法国足协将从耐克手中每年可以得到４０００万欧元。米歇尔解释道：“法国创纪录的４０００万欧元赞助费得益于阿迪达斯和耐克竞逐未来１５年欧洲市场的竞争。耐克需要笼络一个大国来打赢这场欧洲大陆的战争，而尽管德国拿到的赞助费并不太高，但是他们却显然牢牢掌握在民族品牌阿迪达斯手中。从长期投资来看，耐克给法国的赞助并不算过高。” 耐克 阿迪达斯 欧洲杯 球衣 西班牙 LDA ：主题模型格式要求（很重要）：list（分好词的语料） of list（不同的文章）形式，分词好的的整个语料 123from gensim import corpora, models, similaritiesimport gensim# http://radimrehurek.com/gensim/ C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial warnings.warn(&quot;detected Windows; aliasing chunkize to chunkize_serial&quot;) 123# 做映射，相当于词袋dictionary = corpora.Dictionary(contents_clean)corpus = [dictionary.doc2bow(sentence) for sentence in contents_clean] 12lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20) # 类似Kmeans自己指定K值。无监督 # 自定义主题数目 12# 一号分类结果print (lda.print_topic(1, topn=5)) 0.007*&quot;中&quot; + 0.006*&quot;说&quot; + 0.004*&quot;观众&quot; + 0.002*&quot;赛区&quot; + 0.002*&quot;岁&quot; 12for topic in lda.print_topics(num_topics=20, num_words=5): print (topic[1]) 0.007*&quot;女人&quot; + 0.006*&quot;男人&quot; + 0.006*&quot;Ｍ&quot; + 0.004*&quot;Ｓ&quot; + 0.004*&quot;说&quot; 0.004*&quot;中&quot; + 0.004*&quot;训练&quot; + 0.003*&quot;说&quot; + 0.003*&quot;学校&quot; + 0.002*&quot;研究生&quot; 0.006*&quot;戏&quot; + 0.006*&quot;导演&quot; + 0.005*&quot;该剧&quot; + 0.004*&quot;中&quot; + 0.004*&quot;演员&quot; 0.007*&quot;中&quot; + 0.006*&quot;说&quot; + 0.004*&quot;观众&quot; + 0.002*&quot;赛区&quot; + 0.002*&quot;岁&quot; 0.004*&quot;万&quot; + 0.003*&quot;号&quot; + 0.003*&quot;中&quot; + 0.002*&quot;Ｓ&quot; + 0.002*&quot;Ｒ&quot; 0.014*&quot;电影&quot; + 0.009*&quot;导演&quot; + 0.007*&quot;影片&quot; + 0.006*&quot;中国&quot; + 0.005*&quot;中&quot; 0.006*&quot;中&quot; + 0.005*&quot;比赛&quot; + 0.004*&quot;说&quot; + 0.003*&quot;撒&quot; + 0.002*&quot;时间&quot; 0.006*&quot;赛季&quot; + 0.005*&quot;中&quot; + 0.003*&quot;联赛&quot; + 0.003*&quot;中国&quot; + 0.002*&quot;航母&quot; 0.005*&quot;李小璐&quot; + 0.004*&quot;中&quot; + 0.002*&quot;贾乃亮&quot; + 0.002*&quot;Ｗ&quot; + 0.002*&quot;皮肤&quot; 0.004*&quot;万&quot; + 0.003*&quot;号&quot; + 0.003*&quot;Ｖ&quot; + 0.003*&quot;Ｔ&quot; + 0.003*&quot;刘涛&quot; 0.021*&quot;男人&quot; + 0.008*&quot;女人&quot; + 0.007*&quot;考生&quot; + 0.004*&quot;说&quot; + 0.003*&quot;中&quot; 0.005*&quot;中&quot; + 0.005*&quot;食物&quot; + 0.004*&quot;ｉ&quot; + 0.004*&quot;ａ&quot; + 0.004*&quot;吃&quot; 0.006*&quot;中&quot; + 0.004*&quot;电影&quot; + 0.004*&quot;说&quot; + 0.002*&quot;中国&quot; + 0.002*&quot;高考&quot; 0.007*&quot;中&quot; + 0.006*&quot;孩子&quot; + 0.004*&quot;说&quot; + 0.003*&quot;教育&quot; + 0.003*&quot;中国&quot; 0.005*&quot;中&quot; + 0.005*&quot;节目&quot; + 0.004*&quot;说&quot; + 0.004*&quot;表演&quot; + 0.003*&quot;岁&quot; 0.007*&quot;电视剧&quot; + 0.004*&quot;中&quot; + 0.003*&quot;说&quot; + 0.003*&quot;飞行&quot; + 0.002*&quot;飞机&quot; 0.007*&quot;中&quot; + 0.006*&quot;球队&quot; + 0.005*&quot;选手&quot; + 0.004*&quot;观众&quot; + 0.004*&quot;ｉ&quot; 0.005*&quot;中&quot; + 0.005*&quot;天籁&quot; + 0.004*&quot;产品&quot; + 0.004*&quot;肌肤&quot; + 0.003*&quot;职场&quot; 0.008*&quot;中国&quot; + 0.008*&quot;饰演&quot; + 0.007*&quot;中&quot; + 0.004*&quot;说&quot; + 0.004*&quot;节目&quot; 0.021*&quot;ｅ&quot; + 0.021*&quot;ａ&quot; + 0.016*&quot;ｏ&quot; + 0.013*&quot;ｉ&quot; + 0.013*&quot;ｎ&quot; 贝叶斯算法进行新闻的分类12df_train=pd.DataFrame(&#123;&#x27;contents_clean&#x27;:contents_clean,&#x27;label&#x27;:df_news[&#x27;category&#x27;]&#125;)df_train.tail() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } contents_clean label 4995 [天气, 炎热, 补水, 变得, 美国, 跑步, 世界, 杂志, 报道, 喝水, 身体, 补... 时尚 4996 [不想, 说, 话, 刺激, 说, 做, 只能, 走, 离开, 伤心地, 想起, 一句, 话... 时尚 4997 [岁, 刘晓庆, 最新, 嫩照, Ｏ, 衷, 诘, 牧跸, 庆, 看不出, 岁, 秒杀, 刘... 时尚 4998 [导语, 做, 爸爸, 一种, 幸福, 无论是, 领养, 亲生, 更何况, 影视剧, 中, ... 时尚 4999 [全球, 最美, 女人, 合成图, 国, 整形外科, 教授, 李承哲, 国际, 学术, 杂志... 时尚 1df_train.label.unique() array([&#39;汽车&#39;, &#39;财经&#39;, &#39;科技&#39;, &#39;健康&#39;, &#39;体育&#39;, &#39;教育&#39;, &#39;文化&#39;, &#39;军事&#39;, &#39;娱乐&#39;, &#39;时尚&#39;], dtype=object) pandas 容易可以很容易进行打标签 123label_mapping = &#123;&quot;汽车&quot;: 1, &quot;财经&quot;: 2, &quot;科技&quot;: 3, &quot;健康&quot;: 4, &quot;体育&quot;:5, &quot;教育&quot;: 6,&quot;文化&quot;: 7,&quot;军事&quot;: 8,&quot;娱乐&quot;: 9,&quot;时尚&quot;: 0&#125;df_train[&#x27;label&#x27;] = df_train[&#x27;label&#x27;].map(label_mapping) # label的替换df_train.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } contents_clean label 0 [经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ... 1 1 [呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,... 1 2 [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念... 1 3 [清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ... 1 4 [日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,... 1 123from sklearn.model_selection import train_test_splitx_train, x_test, y_train, y_test = train_test_split(df_train[&#x27;contents_clean&#x27;].values, df_train[&#x27;label&#x27;].values, random_state=1) 12# x_train = x_train.flatten()x_train[0][1] &#39;上海&#39; 12345678words = [] for line_index in range(len(x_train)): try: # x_train[line_index][word_index] = str(x_train[line_index][word_index]) words.append(&#x27; &#x27;.join(x_train[line_index])) # python的list往字符串转化 except: print (line_index,word_index)words[0] # 格式转换，不是list of list格式（下文的CoountVector的格式） '中新网 上海 日电 于俊 父亲节 网络 吃 一顿 电影 快餐 微 电影 爸 对不起 我爱你 定于 本月 父亲节 当天 各大 视频 网站 首映 葜 谱 鞣 剑 保慈 障蚣 钦 呓 樯 埽 ⒌ 缬 埃 ǎ 停 椋 悖 颍 铩 妫 椋 恚 称 微型 电影 新 媒体 平台 播放 状态 短时 休闲 状态 观看 完整 策划 系统 制作 体系 支持 显示 较完整 故事情节 电影 微 超短 放映 微 周期 制作 天 数周 微 规模 投资 人民币 几千 数万元 每部 内容 融合 幽默 搞怪 时尚 潮流 人文 言情 公益 教育 商业 定制 主题 单独 成篇 系列 成剧 唇 开播 微 电影 爸 对不起 我爱你 讲述 一对 父子 观念 缺少 沟通 导致 关系 父亲 传统 固执 钟情 传统 生活 方式 儿子 新派 音乐 达 习惯 晚出 早 生活 性格 张扬 叛逆 两种 截然不同 生活 方式 理念 差异 一场 父子 间 拉开序幕 子 失手 打破 父亲 心爱 物品 父亲 赶出 家门 剧情 演绎 父亲节 妹妹 哥哥 化解 父亲 这场 矛盾 映逋坏 嚼 斫 狻 ⒍ 粤 ⒌ 桨容 争执 退让 传统 尴尬 父子 尴尬 情 男人 表达 心中 那份 感恩 一杯 滤挂 咖啡 父亲节 变得 温馨 镁 缬 缮 虾 Ｎ 逄 煳 幕 传播 迪欧 咖啡 联合 出品 出品人 希望 观摩 扪心自问 父亲节 父亲 记得 父亲 生日 哪一天 父亲 爱喝 跨出 家门 那一刻 感觉 一颗 颤动 心 操劳 天下 儿女 父亲节 大声 喊出 父亲 家人 爱 完' 1print (len(words)) 3750 CountVectorizer的简要介绍↓——将词转换成向量 12345678910from sklearn.feature_extraction.text import CountVectorizertexts=[&quot;dog cat fish&quot;,&quot;dog cat cat&quot;,&quot;fish bird&quot;, &#x27;bird&#x27;] # 四篇文章，注意其格式，不是list of list格式cv = CountVectorizer()cv_fit=cv.fit_transform(texts)print(cv.get_feature_names()) # 获得语料库中不重复的词print(cv_fit.toarray())print(cv_fit.toarray().sum(axis=0)) [&#39;bird&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;fish&#39;] [[0 1 1 1] [0 2 1 0] [1 0 0 1] [1 0 0 0]] [2 3 2 2] ngram_range=(1,4)可以将特征值进行组合，让向量更复杂，一般2就可以 12345678910from sklearn.feature_extraction.text import CountVectorizertexts=[&quot;dog cat fish&quot;,&quot;dog cat cat&quot;,&quot;fish bird&quot;, &#x27;bird&#x27;]cv = CountVectorizer(ngram_range=(1,4)) # 词可以组合。可以让向量更复杂！cv_fit=cv.fit_transform(texts)print(cv.get_feature_names())print(cv_fit.toarray())print(cv_fit.toarray().sum(axis=0)) [&#39;bird&#39;, &#39;cat&#39;, &#39;cat cat&#39;, &#39;cat fish&#39;, &#39;dog&#39;, &#39;dog cat&#39;, &#39;dog cat cat&#39;, &#39;dog cat fish&#39;, &#39;fish&#39;, &#39;fish bird&#39;] [[0 1 0 1 1 1 0 1 1 0] [0 2 1 0 1 1 1 0 0 0] [1 0 0 0 0 0 0 0 1 1] [1 0 0 0 0 0 0 0 0 0]] [2 3 1 1 2 2 1 1 2 1] 数据准备好后可以开始进行操作了 12345from sklearn.feature_extraction.text import CountVectorizervec = CountVectorizer(analyzer=&#x27;word&#x27;, max_features=4000, lowercase = False)vec.fit(words) # fit,以某种规则传化 进行贝叶斯的计算 123from sklearn.naive_bayes import MultinomialNBclassifier = MultinomialNB()classifier.fit(vec.transform(words), y_train) # transform,规则训练好，进行转换为向量。为特征 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) 12345678test_words = []for line_index in range(len(x_test)): try: #x_train[line_index][word_index] = str(x_train[line_index][word_index]) test_words.append(&#x27; &#x27;.join(x_test[line_index])) except: print (line_index,word_index)test_words[0] '国家 公务员 考试 申论 应用文 类 试题 实质 一道 集 概括 分析 提出 解决问题 一体 综合性 试题 说 一道 客观 凝练 申发 论述 文章 题目 分析 历年 国考 申论 真题 公文 类 试题 类型 多样 包括 公文 类 事务性 文书 类 题材 从题 干 作答 材料 内容 整合 分析 无需 太 创造性 发挥 纵观 历年 申论 真题 作答 应用文 类 试题 文种 格式 作出 特别 重在 内容 考查 行文 格式 考生 平常心 面对 应用文 类 试题 准确 把握 作答 领会 内在 含义 把握 题材 主旨 材料 结构 轻松 应对 应用文 类 试题 Ｒ 弧 ⒆ 钒 盐 展文 写作 原则 Ｔ 材料 中来 应用文 类 试题 材料 总体 把握 客观 考生 材料 中来 材料 中 把握 材料 准确 理解 题材 主旨 Ｔ 政府 角度 作答 应用文 类 试题 更应 注重 政府 角度 观点 政府 角度 出发 原则 表述 观点 提出 解决 之策 考生 作答 站 政府 人员 角度 看待 提出 解决问题 Ｔ 文体 结构 形式 考查 重点 文体 结构 大部分 评分 关键点 解答 方法 薄 ⒆ ス 丶 词 明 方向 作答 题目 题干 作答 作答 方向 作答 角度 关键 向导 考生 仔细阅读 题干 作答 抓住 关键词 作答 方向 相关 要点 整理 作答 思路 年国考 地市级 真 题为 例 潦惺姓 府 宣传 推进 近海 水域 污染 整治 工作 请 给定 资料 市政府 工作人员 身份 草拟 一份 宣传 纲要 Ｒ 求 保对 宣传 内容 要点 提纲挈领 陈述 玻 体现 政府 精神 全市 各界 关心 支持 污染 整治 工作 通俗易懂 超过 字 肮 丶 词 近海 水域 污染 整治 工作 市政府 工作人员 身份 宣传 纲要 提纲挈领 陈述 体现 政府 精神 全市 各界 关心 支持 污染 整治 工作 通俗易懂 提示 归结 作答 要点 包括 污染 情况 原因 解决 对策 作答 思路 情况 原因 对策 意义 逻辑 顺序 安排 文章 结构 病 ⒋ 缶殖 龇 ⅲ 明 结构 解答 应用文 类 试题 考生 材料 整体 出发 大局 出发 高屋建瓴 把握 材料 主题 思想 事件 起因 解决 对策 阅读文章 构建 文章 结构 直至 快速 解答 场 ⒗ 硭 乘悸 罚明 逻辑 应用文 类 试题 严密 逻辑思维 情况 原因 对策 意义 考生 作答 先 弄清楚 解答 思路 统筹安排 脉络 清晰 逻辑 表达 内容 表述 础 把握 明 详略 考生 仔细阅读 分析 揣摩 应用文 类 试题 内容 答题 时要 详略 得当 主次 分明 安排 内容 增加 文章 层次感 阅卷 老师 阅卷 时能 明白 清晰 一目了然 玻埃 保蹦旯 考 考试 申论 试卷 分为 省级 地市级 两套 试卷 能力 大有 省级 申论 试题 考生 宏观 角度看 注重 深度 广度 考生 深谋远虑 地市级 试题 考生 微观 视角 观察 侧重 考查 解决 能力 考生 贯彻执行 作答 区别对待' 1classifier.score(vec.transform(test_words), y_test) 0.80400000000000005 TF-IDF的到导入计算1234from sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(analyzer=&#x27;word&#x27;, max_features=4000, lowercase = False)vectorizer.fit(words) TfidfVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;, dtype=&lt;class &#39;numpy.int64&#39;&gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;, lowercase=False, max_df=1.0, max_features=4000, min_df=1, ngram_range=(1, 1), norm=&#39;l2&#39;, preprocessor=None, smooth_idf=True, stop_words=None, strip_accents=None, sublinear_tf=False, token_pattern=&#39;(?u)\\\\b\\\\w\\\\w+\\\\b&#39;, tokenizer=None, use_idf=True, vocabulary=None) 123from sklearn.naive_bayes import MultinomialNBclassifier = MultinomialNB()classifier.fit(vectorizer.transform(words), y_train) MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) 1classifier.score(vectorizer.transform(test_words), y_test) 0.81520000000000004","categories":[{"name":"机器学习基础实战","slug":"机器学习基础实战","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"机器学习基础实战","slug":"机器学习基础实战","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/"}]},{"title":"剑指Offer（五十四）：字符流中第一个不重复的字符","slug":"剑指Offer（五十四）：字符流中第一个不重复的字符","date":"2021-05-11T06:17:23.000Z","updated":"2021-05-11T06:21:39.150Z","comments":true,"path":"20210511/剑指Offer（五十四）：字符流中第一个不重复的字符.html","link":"","permalink":"https://xxren8218.github.io/20210511/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%94%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%AD%97%E7%AC%A6%E6%B5%81%E4%B8%AD%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%B8%8D%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AD%97%E7%AC%A6.html","excerpt":"","text":"1.题目请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符&quot;go&quot;时， 第一个只出现一次的字符是&quot;g&quot;。当从该字符流中读出前六个字符“google&quot;时，第一个只出现一次的字符是&quot;l&quot;。 如果当前字符流没有存在出现一次的字符，返回#字符。 2.思路可以创建一个列表进行字符的记录。然后按顺序进行遍历 列表有count方法可以进行计数。 3.代码1234567891011121314# -*- coding:utf-8 -*-class Solution: # 返回对应char def __init__(self): self.s = [] def Insert(self, char): self.s.append(char) def FirstAppearingOnce(self): # write code here for i in self.s: if self.s.count(i) == 1: return i return &#x27;#&#x27;","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"其他","slug":"其他","permalink":"https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"}]},{"title":"剑指Offer（四十八）：不用加减乘除的加法","slug":"剑指Offer（四十八）：不用加减乘除的加法","date":"2021-05-11T06:12:44.000Z","updated":"2021-05-17T10:10:35.662Z","comments":true,"path":"20210511/剑指Offer（四十八）：不用加减乘除的加法.html","link":"","permalink":"https://xxren8218.github.io/20210511/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E7%94%A8%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E7%9A%84%E5%8A%A0%E6%B3%95.html","excerpt":"","text":"1.题目写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 2.思路首先看十进制是如何做的： 5+7=12， 可以使用三步走： 第一步：相加各位的值，不算进位，得到2。 第二步：计算进位值，得到10. 如果这一步的进位值为0，那么第一步得到的值就是最终结果。 第三步：重复上述两步，只是相加的值变成上述两步的得到的结果2和10，得到12。 同样我们可以 三步走的方式计算二进制值相加： 5-101，7-111 第一步：相加各位的值，不算进位，得到010，二进制每位相加就相当于各位做异或操作，101^111。 第二步：计算进位值，得到1010，相当于各位做与操作得到101，再向左移一位得到1010，(101&amp;111)&lt;&lt;1。 第三步：重复上述两步， 各位相加 010^1010=1000，进位值为100=(010&amp;1010)&lt;&lt;1。 继续重复上述两步：1000^100 = 1100，进位值为0，跳出循环，1100为最终结果。 3.代码1234567891011121314151617181920212223# -*- coding:utf-8 -*-class Solution: def Add(self, num1, num2): # write code here # python2.7的int取值范围为：-2147483648至2147483647 # 将数字 n &amp; 0xffffffff，得到32位二进制。python的二进制不一样。可以用 ~(num1 ^ 0xffffffff)将负数还原存储形式。。 MAX = 0x7fffffff mask = 0xffffffff while num2 != 0: num1, num2 = (num1 ^ num2), ((num1 &amp; num2) &lt;&lt; 1) num1 = num1 &amp; mask num2 = num2 &amp; mask return num1 if num1 &lt;= MAX else ~(num1 ^ mask)## 解释```pythonprint(hex(1)) # = 0x1 补码print(hex(-1)) # = -0x1 负号 + 原码 （ Python 特色，Java 会直接输出补码）print(hex(1 &amp; 0xffffffff)) # = 0x1 正数补码print(hex(-1 &amp; 0xffffffff)) # = 0xffffffff 负数补码print(-1 &amp; 0xffffffff) # = 4294967295 （ Python 将其认为正数）可以用 ~(num1 ^ mask)将负数还原。 ```","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"位运算","slug":"位运算","permalink":"https://xxren8218.github.io/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"}]},{"title":"剑指Offer（四十七）：求1+2+3+…+n","slug":"剑指Offer（四十七）：求1-2-3-…-n","date":"2021-05-11T06:08:14.000Z","updated":"2021-05-18T09:58:37.919Z","comments":true,"path":"20210511/剑指Offer（四十七）：求1-2-3-…-n.html","link":"","permalink":"https://xxren8218.github.io/20210511/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9A%E6%B1%821-2-3-%E2%80%A6-n.html","excerpt":"","text":"1.题目求1+2+3+...+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 2.思路这是一道超级无敌送分题，使用递归即可。 3.代码123456789# -*- coding:utf-8 -*-class Solution: def Sum_Solution(self, n): # write code here # 递归终止条件 if n = 1: return 1 n += self.Sum_Solution(n-1) return n 上述代码不对，含有if判断语句. 可以用逻辑运算符的短路效应 常见的逻辑运算符有三种，即 “与 &amp; ”，“或 || ”，“非 ! ” ；而其有重要的短路效应，如下所示：if(A &amp;&amp; B) // 若 A 为 false ，则 B 的判断不会执行（即短路），直接判定 A &amp;&amp; B 为 false if(A || B) // 若 A 为 true ，则 B 的判断不会执行（即短路），直接判定 A || B 为 true 本题需要实现 “当 n = 0 时终止递归” 的需求，可通过短路效应实现。 n &amp;&amp; sumNums(n - 1) // 当 n = 0 时不成立 ，此时 “短路” ，终止后续递归 代码12345678class Solution: def __init__(self): self.res = 0 def sumNums(self, n: int) -&gt; int: n &gt; 1 and self.sumNums(n - 1) # 用逻辑运算符来避免递归条件if的使用 self.res += n return self.res","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"递归","slug":"递归","permalink":"https://xxren8218.github.io/tags/%E9%80%92%E5%BD%92/"}]},{"title":"剑指Offer（四十六）：孩子们的游戏（圆圈中最后剩下的数）","slug":"剑指Offer（四十六）：孩子们的游戏（圆圈中最后剩下的数）","date":"2021-05-10T12:16:29.000Z","updated":"2021-05-18T09:50:10.479Z","comments":true,"path":"20210510/剑指Offer（四十六）：孩子们的游戏（圆圈中最后剩下的数）.html","link":"","permalink":"https://xxren8218.github.io/20210510/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9A%E5%AD%A9%E5%AD%90%E4%BB%AC%E7%9A%84%E6%B8%B8%E6%88%8F%EF%BC%88%E5%9C%86%E5%9C%88%E4%B8%AD%E6%9C%80%E5%90%8E%E5%89%A9%E4%B8%8B%E7%9A%84%E6%95%B0%EF%BC%89.html","excerpt":"","text":"1.题目每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。 然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始, 继续0...m-1报数....这样下去....直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。 请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1) 2.思路可以按照题目进行分析: 先建立一个孩子的列表[0,1,…n-1]共n个孩子 pop列表的元素（报数到m-1的人出去,这个人记为index），终止条件为列表中只剩一个元素 然后在循环内嵌套循环来找是第几个人pop出列表。 需要一个计数变量count。因为每次进入这个循环的时候，已经有一个人出去了。所以count初始化为0 count和index同时加一。若index大于人数了,那么需要将其归零 —— 因为是个环，即从第一个人又开始。 注意pop完一个元素（人）后，需要判断是否目前的index是否超出人数，若超出，则需要从第一个同学开始计数。返回列表第一个元素即可。 3.代码1234567891011121314151617181920212223242526# -*- coding:utf-8 -*-class Solution: def LastRemaining_Solution(self, n, m): # write code here if not n: return -1 res = [i for i in range(n)] index = 0 # 用来pop()人数 while len(res) &gt; 1: count = 0 # 用来判断达到count时,谁来pop while count &lt; m-1: count += 1 index += 1 # 来判断越界 if index == len(res): index = 0 res.pop(index) # 弹出后也需要看目前的index有没有越界。 # 如n个人[0,1,...,n-1]，共n个数字。弹出了n-1, # 那么此时为[0,1,2,...n-1]共n-1个数字，那么这个index-&gt;n-1，不存在，应为第一个index-&gt;0 if index == len(res): index = 0 return res[0] 发现在牛客可以执行，但是到力扣超时，采用力扣的数学方法对其进行求解。原理如图所示： 代码12345678910111213class Solution(object): def lastRemaining(self, n, m): &quot;&quot;&quot; :type n: int :type m: int :rtype: int &quot;&quot;&quot; if not n: return None f = 0 for i in range(2,n+1): f = (f + m) % i return f","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"双指针","slug":"双指针","permalink":"https://xxren8218.github.io/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"数学","slug":"数学","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E5%AD%A6/"}]},{"title":"剑指Offer（四十五）：扑克牌顺子","slug":"剑指Offer（四十五）：扑克牌顺子","date":"2021-05-10T12:01:26.000Z","updated":"2021-05-18T09:51:17.509Z","comments":true,"path":"20210510/剑指Offer（四十五）：扑克牌顺子.html","link":"","permalink":"https://xxren8218.github.io/20210510/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9A%E6%89%91%E5%85%8B%E7%89%8C%E9%A1%BA%E5%AD%90.html","excerpt":"","text":"1.题目LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张)...他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子, 如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子.....LL不高兴了,他想了想,决定大\\小 王可以看成任何数字,并且A看作1, J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们 LL的运气如何。为了方便起见,你可以认为大小王是0。 2.思路这道题可以用另外一种思路解决： 无非两种特殊情况需要考虑： 1.因为有四个鬼，所以最多有四个0，遇到0直接跳过 2.若含有重复的数字，那肯定不能成为顺子。可以用python的set()得以解决。 3.要组成顺子，这几张牌的max(max-min)只能是5-1=4（1，2，3，4，5） 若有零，则肯定更小于4了。 3.代码123456789101112131415# -*- coding:utf-8 -*-class Solution: def IsContinuous(self, numbers): # write code here repeat = set() for i in numbers: if i == 0:continue if i not in repeat: repeat.add(i) # 注意为add()方法。 else: return False if max(repeat) - min(repeat) &lt;= 4: return True else: return False","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"其他","slug":"其他","permalink":"https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"}]},{"title":"剑指Offer（四十二）：和为S的两个数字","slug":"剑指Offer（四十二）：和为S的两个数字","date":"2021-05-08T10:27:38.000Z","updated":"2021-05-18T10:02:43.253Z","comments":true,"path":"20210508/剑指Offer（四十二）：和为S的两个数字.html","link":"","permalink":"https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%92%8C%E4%B8%BAS%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97.html","excerpt":"","text":"1.题目输入一个递增排序的数组和一个数字S，在数组中查找两个数，是的他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。 输出描述： 对应每个测试案例，输出两个数，小的先输出。 2.思路对于一个数组，我们可以定义两个指针，一个从左往右遍历（left），另一个从右往左遍历（right）。 首先，我们比较第一个数字和最后一个数字的和cursum与给定数字tsum， 如果cursum &lt; tsum，那么我们就要加大输入值，所以，left向右移动一位，重复之前的计算； 如果cursum &gt; tsum，那么我们就要减小输入值，所以，right向左移动一位，重复之前的计算； 如果相等，那么这两个数字就是我们要找的数字，直接输出即可。 这么做的好处是，也保证了乘积最小。 3.代码123456789101112131415161718# -*- coding:utf-8 -*-class Solution: def FindNumbersWithSum(self, array, tsum): # write code here res = [] if len(array) &lt;= 1: return res left, right = 0, len(array)-1 while left &lt; right: if array[left] + array[right] == tsum: res.append(array[left]) res.append(array[right]) return res elif array[left] + array[right] &lt; tsum: left += 1 else: right -= 1 return res 可以进行扩展，找出和为目标函数，且乘积最小的组合可以扩展，不给你排序。找出和为目标函数，且乘积最小的组合。如何做？- 先排序，再同样的套路。","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"双指针","slug":"双指针","permalink":"https://xxren8218.github.io/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"排序","slug":"排序","permalink":"https://xxren8218.github.io/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"剑指Offer（四十一）：和为S的连续正数序列","slug":"剑指Offer（四十一）：和为S的连续正数序列","date":"2021-05-08T10:23:27.000Z","updated":"2021-05-19T16:52:30.026Z","comments":true,"path":"20210508/剑指Offer（四十一）：和为S的连续正数序列.html","link":"","permalink":"https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E5%92%8C%E4%B8%BAS%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97.html","excerpt":"","text":"1.题目小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Good Luck! 输出描述： 输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序。 2.思路设定两个指针，一个指向第一个数，一个指向最后一个数，在此之前需要设定第一个数和最后一个数的值，由于是正数序列，所以可以把第一个数设为1，最后一个数为2（因为是要求是连续正数序列，最后不可能和第一个数重合）。 下一步就是不断改变第一个数和最后一个数的值，如果从第一个数到最后一个数的和刚好是要求的和，那么把所有的数都添加到一个序列中；如果大于要求的和，则说明从第一个数到最后一个数之间的范围太大，因此减小范围，需要把第一个数的值加1，同时把当前和减去原来的第一个数的值； 如果小于要求的和，说明范围太小，因此把最后一个数加1，同时把当前的和加上改变之后的最后一个数的值。这样，不断修改第一个数和最后一个数的值，就能确定所有连续正数序列的和等于S的序列了。 注意：初中的求和公式应该记得吧，首项加尾项的和乘以个数除以2，即sum = (a + b) * n / 2。 3.代码12345678910111213141516171819202122# -*- coding:utf-8 -*-class Solution: def FindContinuousSequence(self, tsum): # write code here res = [] left, right = 1, 2 while left &lt; right: # 求连续整数的和(a+b)*n/2 cursum = (left + right)*(right-left+1)/2 if cursum == tsum: tmp = [] for i in range(left, right+1): tmp.append(i) res.append(tmp) # 接着后移 right += 1 elif cursum &lt; tsum: right += 1 else: left += 1 return res","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"双指针","slug":"双指针","permalink":"https://xxren8218.github.io/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"滑动窗口","slug":"滑动窗口","permalink":"https://xxren8218.github.io/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"}]},{"title":"剑指Offer（三十三）：丑数","slug":"剑指Offer（三十三）：丑数","date":"2021-05-08T10:02:57.000Z","updated":"2021-05-24T10:57:58.561Z","comments":true,"path":"20210508/剑指Offer（三十三）：丑数.html","link":"","permalink":"https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9A%E4%B8%91%E6%95%B0.html","excerpt":"","text":"1.题目把只包含因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 2.思路丑数其实可以看成是是下图的形式。即2、3、5的乘积， 然后找出相乘后最小的数字放入列表(数组)a中即可。 可以设立三个指针。p2,p3,p5 = 0 如下图所示。 对于n=1 a[0],他们的值都是1.从a[2]开始(n=2)。如下图 最小的数是a[p2]x2,则将其放入数组a中，将p2指针后移一位。 接下来 n=3 时候,以此类推。。。 当 n = 6时，两个都是6，如下图，则需要移动两个指针。 3.代码12345678910111213141516# -*- coding:utf-8 -*-class Solution: def GetUglyNumber_Solution(self, index): # write code here if index == 0: return 0 a = [1] p2 = p3 = p5 = 0 for i in range(2,index+1): newNum = min(a[p2]*2, a[p3]*3, a[p5]*5) a.append(newNum) # 此处保证了，数值相等时都向后移动一位。 if newNum == a[p2]*2:p2 += 1 if newNum == a[p3]*3:p3 += 1 if newNum == a[p5]*5:p5 += 1 return a[-1]","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数学","slug":"数学","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E5%AD%A6/"},{"name":"多指针","slug":"多指针","permalink":"https://xxren8218.github.io/tags/%E5%A4%9A%E6%8C%87%E9%92%88/"}]},{"title":"剑指Offer（三十一）：整数中1出现的次数（从1到n整数中1出现的次数）","slug":"剑指Offer（三十一）：整数中1出现的次数（从1到n整数中1出现的次数）","date":"2021-05-08T09:36:53.000Z","updated":"2021-05-19T16:54:23.442Z","comments":true,"path":"20210508/剑指Offer（三十一）：整数中1出现的次数（从1到n整数中1出现的次数）.html","link":"","permalink":"https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%88%E4%BB%8E1%E5%88%B0n%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%89.html","excerpt":"","text":"1.题目输入一个整数n，求从1到n这n个整数的十进制表示中1出现的次数。例如输入12，从1到12这些整数中包含1的数字有1，10，11和12，1一共出现了5次。 2.思路“1”出现次数实际上就是各个位置上1出现的次数的求和，以3101592为例。 我们以cur表示目前所在的位数,例如在百位 base = 100, cur = n//base%10 a表示左侧的数字，b表示右侧的位数. a = n//base//10, b = n%base 如下图所示 接下来只需要考虑三种情况即可： 1.cur &gt; 1: 共有(a+1)xbase 2.cur == 1: 共有axbase+b+1 3.cur &lt; 1: 共有axbase 最后求和即可。 如下图所示 3.代码12345678910111213141516# -*- coding:utf-8 -*-class Solution: def NumberOf1Between1AndN_Solution(self, n): # write code here base = 1 res = 0 while base &lt;= n: a = n//base b = n%base cur = a%10 a //= 10 if cur &gt; 1:res += (a+1)*base elif cur == 1:res += (a*base+b+1) else:res += a*base base *= 10 return res","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数学","slug":"数学","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E5%AD%A6/"}]},{"title":"剑指Offer（二十九）：最小的K个数","slug":"剑指Offer（二十九）：最小的K个数","date":"2021-05-08T09:28:55.000Z","updated":"2021-05-15T12:28:02.747Z","comments":true,"path":"20210508/剑指Offer（二十九）：最小的K个数.html","link":"","permalink":"https://xxren8218.github.io/20210508/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B9%9D%EF%BC%89%EF%BC%9A%E6%9C%80%E5%B0%8F%E7%9A%84K%E4%B8%AA%E6%95%B0.html","excerpt":"","text":"1.题目输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4。 2.思路可以通过python的排序对给定的数组进行排序，然后取值即可。 3.代码12345678# -*- coding:utf-8 -*-class Solution: def GetLeastNumbers_Solution(self, tinput, k): # write code here if k==0 or k&gt;len(tinput): return [] tinput.sort() # list.sort()默认是升序排序。 return tinput[:k] 解析：sorted()的时间复杂度为O(nlogn)力扣的解题思路： 最大堆(最小的k个元素) 最小堆(最大的k个元素) 先存列表的前k个元素 原地转换为最大堆 heapq.heapify(),时间复杂度O(n) 循环取剩余的元素，若比最大堆的队顶元素小，则将最大堆的堆顶元素弹出，将此元素放入。heapq.heapreplace(heap,-num) 它从堆中弹出最小的元素，再压入一个新元素。 python没有最大堆，只有最小堆。我们可以对其取反就可以得到解决。 代码：123456789101112131415class Solution(object): def getLeastNumbers(self, arr, k): &quot;&quot;&quot; :type arr: List[int] :type k: int :rtype: List[int] &quot;&quot;&quot; if k == 0: return [] heap = [-x for x in arr[:k]] heapq.heapify(heap) for num in arr[k:]: if -num &gt; heap[0]: heapq.heapreplace(heap, -num) return [-x for x in heap]","categories":[{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"https://xxren8218.github.io/tags/%E6%8E%92%E5%BA%8F/"},{"name":"最小堆","slug":"最小堆","permalink":"https://xxren8218.github.io/tags/%E6%9C%80%E5%B0%8F%E5%A0%86/"}]},{"title":"贝叶斯基础","slug":"贝叶斯基础","date":"2021-04-29T15:52:24.000Z","updated":"2021-04-29T15:54:44.328Z","comments":true,"path":"20210429/贝叶斯基础.html","link":"","permalink":"https://xxren8218.github.io/20210429/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9F%BA%E7%A1%80.html","excerpt":"","text":"贝叶斯拼写检查器1234567891011121314151617181920212223242526272829import re, collections def words(text): return re.findall(&#x27;[a-z]+&#x27;, text.lower()) def train(features): model = collections.defaultdict(lambda: 1) for f in features: model[f] += 1 return model NWORDS = train(words(open(&#x27;big.txt&#x27;).read())) alphabet = &#x27;abcdefghijklmnopqrstuvwxyz&#x27; def edits1(word): n = len(word) return set([word[0:i]+word[i+1:] for i in range(n)] + # deletion [word[0:i]+word[i+1]+word[i]+word[i+2:] for i in range(n-1)] + # transposition [word[0:i]+c+word[i+1:] for i in range(n) for c in alphabet] + # alteration [word[0:i]+c+word[i:] for i in range(n+1) for c in alphabet]) # insertion def known_edits2(word): return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS) def known(words): return set(w for w in words if w in NWORDS) def correct(word): candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word] return max(candidates, key=lambda w: NWORDS[w]) 12# appl #appla #learw #tess #morwcorrect(&#x27;tess&#x27;) &#39;less&#39; 求解：argmaxc P(c|w) -&gt; argmaxc P(w|c) P(c) / P(w) P(c), 先验概率 文章中出现一个正确拼写词 c 的概率, 也就是说, 在英语文章中, c 出现的概率有多大 P(w|c), 在用户想键入 c 的情况下敲成 w 的概率. 因为这个是代表用户会以多大的概率把 c 敲错成 w argmaxc, 用来枚举所有可能的 c 并且选取概率最大的 123456789101112# 把语料中的单词全部抽取出来, 转成小写, 并且去除单词中间的特殊符号def words(text): return re.findall(&#x27;[a-z]+&#x27;, text.lower()) def train(features): # 匿名函数：新出现的词，将其出现次数设置为1，符合实际，不然上面的公式可得，概率为0.太过绝对。另外出现一次就会加一 model = collections.defaultdict(lambda: 1) for f in features: model[f] += 1 return model NWORDS = train(words(open(&#x27;big.txt&#x27;).read())) 要是遇到我们从来没有过见过的新词怎么办. 假如说一个词拼写完全正确, 但是语料库中没有包含这个词, 从而这个词也永远不会出现在训练集中. 于是, 我们就要返回出现这个词的概率是0. 这个情况不太妙, 因为概率为0这个代表了这个事件绝对不可能发生, 而在我们的概率模型中, 我们期望用一个很小的概率来代表这种情况. lambda: 1 相当于先求先验概率1NWORDS defaultdict(&lt;function __main__.train.&lt;locals&gt;.&lt;lambda&gt;&gt;, &#123;&#39;the&#39;: 80031, &#39;project&#39;: 289, &#39;gutenberg&#39;: 264, &#39;ebook&#39;: 88, &#39;of&#39;: 40026, &#39;adventures&#39;: 18, &#39;sherlock&#39;: 102, &#39;holmes&#39;: 468, &#39;by&#39;: 6739, &#39;sir&#39;: 178, &#39;arthur&#39;: 35, &#39;conan&#39;: 5, &#39;doyle&#39;: 6, &#39;in&#39;: 22048, &#39;our&#39;: 1067, &#39;series&#39;: 129, &#39;copyright&#39;: 70, &#39;laws&#39;: 234, &#39;are&#39;: 3631, &#39;changing&#39;: 45, &#39;all&#39;: 4145, &#39;over&#39;: 1283, &#39;world&#39;: 363, &#39;be&#39;: 6156, &#39;sure&#39;: 124, ...&#125;) 编辑距离:两个词之间的编辑距离定义为使用了几次插入(在词中插入一个单字母), 删除(删除一个单字母), 交换(交换相邻两个字母), 替换(把一个字母换成另一个)的操作从一个词变到另一个词. 1234567# 返回所有与单词 w 编辑距离为 1 的集合.一次操作，能从A到Bdef edits1(word): n = len(word) return set([word[0:i]+word[i+1:] for i in range(n)] + # deletion [word[0:i]+word[i+1]+word[i]+word[i+2:] for i in range(n-1)] + # transposition [word[0:i]+c+word[i+1:] for i in range(n) for c in alphabet] + # alteration [word[0:i]+c+word[i:] for i in range(n+1) for c in alphabet]) # insertion 与 something 编辑距离为2的单词居然达到了 114,324 个 优化:在这些编辑距离小于2的词中间, 只把那些正确的词作为候选词,只能返回 3 个单词: ‘smoothing’, ‘something’ 和 ‘soothing’ 1234# 返回所有与单词 w 编辑距离为 2 的集合# 在这些编辑距离小于2的词中间, 只把那些正确的词作为候选词def edits2(word): return set(e2 for e1 in edits1(word) for e2 in edits1(e1)) P(w|c)的计算 正常来说把一个元音拼成另一个的概率要大于辅音 (因为人常常把 hello 打成 hallo 这样); 把单词的第一个字母拼错的概率会相对小, 等等.但是为了简单起见, 选择了一个简单的方法: 编辑距离为1的正确单词比编辑距离为2的优先级高, 而编辑距离为0的正确单词优先级比编辑距离为1的高. 123456def known(words): return set(w for w in words if w in NWORDS)# 如果known(set)非空, candidate 就会选取这个集合, 而不继续计算后面的def correct(word): candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word] # 优先级，前面的能得到结果，不会算后面的。 return max(candidates, key=lambda w: NWORDS[w])","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"决策树基础","slug":"决策树","date":"2021-04-29T15:45:59.000Z","updated":"2021-04-29T15:50:48.301Z","comments":true,"path":"20210429/决策树.html","link":"","permalink":"https://xxren8218.github.io/20210429/%E5%86%B3%E7%AD%96%E6%A0%91.html","excerpt":"","text":"决策树12345%matplotlib inlineimport matplotlib.pyplot as pltimport pandas as pd 123from sklearn.datasets.california_housing import fetch_california_housing # 内置的数据集housing = fetch_california_housing()print(housing.DESCR) downloading Cal. housing from http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz to C:\\Users\\user\\scikit_learn_data California housing dataset. The original database is available from StatLib http://lib.stat.cmu.edu/ The data contains 20,640 observations on 9 variables. This dataset contains the average house value as target variable and the following input variables (features): average income, housing average age, average rooms, average bedrooms, population, average occupation, latitude, and longitude in that order. References ---------- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297. 1housing.data.shape # 有两万多个数据，每个数据有8个特征 (20640, 8) 1housing.data[0] array([ 8.3252 , 41. , 6.98412698, 1.02380952, 322. , 2.55555556, 37.88 , -122.23 ]) sklearn运行决策树只需要两步 1.实例化树模型，传参数(下面有部分API文档说明)——此处最大深度为2 2.用实例化的对象.fit（数据的X，数据的y）就可以了 123from sklearn import tree # 导入需要指定的树模块dtr = tree.DecisionTreeRegressor(max_depth = 2)dtr.fit(housing.data[:, [6, 7]], housing.target) # 只选择两个数据来用（试验而已）。——经度和纬度 DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&#39;best&#39;) 会自动输出一些默认的参数。12345678910# 要可视化显示 首先需要安装 graphviz http://www.graphviz.org/Download..phpdot_data = \\ tree.export_graphviz( dtr, # 构造出来的决策树对象传入第一个参数 out_file = None, feature_names = housing.feature_names[6:8], # 用哪个特征，传哪个特征（含左不含右），其余不需要改了。 filled = True, impurity = False, rounded = True ) 执行完以后会生成一个.dat的文件 需要一个库来对其进行显示 123456# pip install pydotplusimport pydotplusgraph = pydotplus.graph_from_dot_data(dot_data)graph.get_nodes()[7].set_fillcolor(&quot;#FFF2DD&quot;)from IPython.display import ImageImage(graph.create_png()) 可以用下列的命令将构造好的图保存起来 1graph.write_png(&quot;dtr_white_background.png&quot;) True 数据的切分 1234567from sklearn.model_selection import train_test_splitdata_train, data_test, target_train, target_test = \\ train_test_split(housing.data, housing.target, test_size = 0.1, random_state = 42)dtr = tree.DecisionTreeRegressor(random_state = 42) # （切分时的随机性）为了方面复现，使得每一次随机的结果都是一致的。dtr.fit(data_train, target_train)dtr.score(data_test, target_test) 0.637318351331017 1234from sklearn.ensemble import RandomForestRegressorrfr = RandomForestRegressor( random_state = 42)rfr.fit(data_train, target_train)rfr.score(data_test, target_test) 0.79086492280964926 树模型参数:（为了防止过拟合） 1.衡量标准 gini系数 or entropy（熵） 2.splitter best or random 前者是在所有特征中找最好的切分点 后者是在部分特征中（数据量大的时候）——特征多的时候，从头到尾遍历特征比较耗时（best默认），随机选择几个特征（random）,很少去改，因为很少遇到几百个特征。 3.max_features None（所有），log2，sqrt，N 特征小于50的时候一般使用所有的 4.max_depth 数据少或者特征少的时候可以不管这个值，如果模型样本量多，特征也多的情况下，可以尝试限制下（没有定值，可以交叉验证得到最优值） 5.min_samples_split 如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 6.min_samples_leaf 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝，如果样本量不大，不需要管这个值，大些如10W可是尝试下5 7.min_weight_fraction_leaf 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。 8.max_leaf_nodes 通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到。 9.class_weight 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。 10.min_impurity_split 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点 。 n_estimators:要建立树的个数 GridSearchCV 模块相当于循环遍历，方便对参数进行选择。 实例化GridSearchCV后需要传的参数(算法，参数的候选值，VC交叉验证的次数)，一般候选的值写成字典的形式 12345from sklearn.grid_search import GridSearchCVtree_param_grid = &#123; &#x27;min_samples_split&#x27;: list((3,6,9)),&#x27;n_estimators&#x27;:list((10,50,100))&#125;grid = GridSearchCV(RandomForestRegressor(),param_grid=tree_param_grid, cv=5)grid.fit(data_train, target_train)grid.grid_scores_, grid.best_params_, grid.best_score_ ([mean: 0.78405, std: 0.00505, params: &#123;&#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 10&#125;, mean: 0.80529, std: 0.00448, params: &#123;&#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 50&#125;, mean: 0.80673, std: 0.00433, params: &#123;&#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 100&#125;, mean: 0.79016, std: 0.00124, params: &#123;&#39;min_samples_split&#39;: 6, &#39;n_estimators&#39;: 10&#125;, mean: 0.80496, std: 0.00491, params: &#123;&#39;min_samples_split&#39;: 6, &#39;n_estimators&#39;: 50&#125;, mean: 0.80671, std: 0.00408, params: &#123;&#39;min_samples_split&#39;: 6, &#39;n_estimators&#39;: 100&#125;, mean: 0.78747, std: 0.00341, params: &#123;&#39;min_samples_split&#39;: 9, &#39;n_estimators&#39;: 10&#125;, mean: 0.80481, std: 0.00322, params: &#123;&#39;min_samples_split&#39;: 9, &#39;n_estimators&#39;: 50&#125;, mean: 0.80603, std: 0.00437, params: &#123;&#39;min_samples_split&#39;: 9, &#39;n_estimators&#39;: 100&#125;], &#123;&#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 100&#125;, 0.8067250881273065) 123rfr = RandomForestRegressor( min_samples_split=3,n_estimators = 100,random_state = 42)rfr.fit(data_train, target_train)rfr.score(data_test, target_test) 0.80908290496531576 1pd.Series(rfr.feature_importances_, index = housing.feature_names).sort_values(ascending = False) MedInc 0.524257 AveOccup 0.137947 Latitude 0.090622 Longitude 0.089414 HouseAge 0.053970 AveRooms 0.044443 Population 0.030263 AveBedrms 0.029084 dtype: float64","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"逻辑回归实战--信用卡诈骗检测","slug":"逻辑回归基础实战","date":"2021-04-17T17:26:48.000Z","updated":"2021-04-17T18:53:58.904Z","comments":true,"path":"20210418/逻辑回归基础实战.html","link":"","permalink":"https://xxren8218.github.io/20210418/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98.html","excerpt":"","text":"信用卡诈骗预测——二分类的问题12345import pandas as pdimport matplotlib.pyplot as pltimport numpy as np%matplotlib inline 先来看看数据长什么样子吧12data = pd.read_csv(&quot;creditcard.csv&quot;)data.head() Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class 0 0.0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 ... -0.018307 0.277838 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 149.62 0 1 0.0 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 0.085102 -0.255425 ... -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 2.69 0 2 1.0 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 ... 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 378.66 0 3 1.0 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 0.377436 -1.387024 ... -0.108300 0.005274 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 123.50 0 4 2.0 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 -0.270533 0.817739 ... -0.009431 0.798278 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 69.99 0 5 rows × 31 columns 先来看看正负样本的分布情况吧！12345count_classes = pd.value_counts(data[&#x27;Class&#x27;], sort = True).sort_index() # values_counts可以根据值进行计数。sort_index()按行索引排序count_classes.plot(kind = &#x27;bar&#x27;) # 除了plt，pd也可以做一些简单的图plt.title(&quot;Fraud class histogram&quot;)plt.xlabel(&quot;Class&quot;)plt.ylabel(&quot;Frequency&quot;) &lt;matplotlib.text.Text at 0x216366d8860&gt; 样本数据极度不均匀，应该怎么做？ 下采样——对于不均衡的数据，让 1 和 0 的数据一样少 过采样——对于不均衡的数据，生成一些数据，让生成的数据与 0 样本一样多 Amount数据分布不均衡，为了保证特征之间的分布是差不多的。——即保证数据的重要性一样。 标准化 归一化 可以使用sklearn的预处理模块进行标准化 123456from sklearn.preprocessing import StandardScalerdata[&#x27;normAmount&#x27;] = StandardScaler().fit_transform(data[&#x27;Amount&#x27;].reshape(-1, 1)) # reshape(-1,1) # -1表示让python给它行数data = data.drop([&#x27;Time&#x27;,&#x27;Amount&#x27;],axis=1) # 有了新特征后，将没用的特征去掉。data.head() V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ... V21 V22 V23 V24 V25 V26 V27 V28 Class normAmount 0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 0.090794 ... -0.018307 0.277838 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 0 0.244964 1 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 0.085102 -0.255425 -0.166974 ... -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 0 -0.342475 2 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 0.207643 ... 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 0 1.160686 3 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 0.377436 -1.387024 -0.054952 ... -0.108300 0.005274 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 0 0.140534 4 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 -0.270533 0.817739 0.753074 ... -0.009431 0.798278 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 0 -0.073403 5 rows × 30 columns 先来进行下采样吧！123456789101112131415161718192021222324252627X = data.ix[:, data.columns != &#x27;Class&#x27;] # loc[标签] iloc[索引数字] 取值 ，ix[都可以] y = data.ix[:, data.columns == &#x27;Class&#x27;]# Number of data points in the minority classnumber_records_fraud = len(data[data.Class == 1]) # 计算异常样本的数目——采用bool索引的方式进行fraud_indices = np.array(data[data.Class == 1].index) # 通过.index函数拿出来异常样本的索引# Picking the indices of the normal classesnormal_indices = data[data.Class == 0].index # 拿出来所有正常样本的index，为了下面的随机选择。# Out of the indices we picked, randomly select &quot;x&quot; number (number_records_fraud)random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)# np.random.choice(样本，选择数目)进行随机选择random_normal_indices = np.array(random_normal_indices) # 将拿出来的索引转化为np.array的类型# Appending the 2 indices ##合并两个样本的index进行合并under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])# Under sample dataset ## 经过下采样以后拿到的数据under_sample_data = data.iloc[under_sample_indices,:]X_undersample = under_sample_data.ix[:, under_sample_data.columns != &#x27;Class&#x27;] # 下采样的数据分成两部分y_undersample = under_sample_data.ix[:, under_sample_data.columns == &#x27;Class&#x27;]# Showing ratioprint(&quot;Percentage of normal transactions: &quot;, len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))print(&quot;Percentage of fraud transactions: &quot;, len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))print(&quot;Total number of transactions in resampled data: &quot;, len(under_sample_data)) Percentage of normal transactions: 0.5 Percentage of fraud transactions: 0.5 Total number of transactions in resampled data: 984 下采样的数据少了，会出现什么问题呢？ 后面说。先来进行交叉验证的数据的切分。——交叉验证，说到底是为了选参！12345678910111213141516171819from sklearn.cross_validation import train_test_split # sklearn有交叉验证的工具，能镜像数据的划分。# from sklearn.model_selection import train_test_split# Whole dataset #【1】对原始的数据进行切分——（为了使用它的测试集进行测试）X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0) ## 注意顺序！ 数据洗牌print(&quot;Number transactions train dataset: &quot;, len(X_train))print(&quot;Number transactions test dataset: &quot;, len(X_test))print(&quot;Total number of transactions: &quot;, len(X_train)+len(X_test))# Undersampled dataset # 【2】 对下采样的数据进行切分。（下采样的测试集小，不具备原始数据的分布规则。）X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample ,y_undersample ,test_size = 0.3 ,random_state = 0)print(&quot;&quot;)print(&quot;Number transactions train dataset: &quot;, len(X_train_undersample))print(&quot;Number transactions test dataset: &quot;, len(X_test_undersample))print(&quot;Total number of transactions: &quot;, len(X_train_undersample)+len(X_test_undersample)) C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. &quot;This module will be removed in 0.20.&quot;, DeprecationWarning) Number transactions train dataset: 199364 Number transactions test dataset: 85443 Total number of transactions: 284807 Number transactions train dataset: 688 Number transactions test dataset: 296 Total number of transactions: 984 现在数据切分完了，已经有数据了，可以进行建模了！——逻辑回归模型可以容易建立（如用sklearn），但是模型评估标准咋样呢？——精度骗人! 样本数目不均衡时，类偏移现象。100个人，99个正常(0)，1个得癌症(1)。那如果我的模型是y = 0,我的准确率是 99 %,但是检测不出来一个患有癌症的人。——所以希望我们的模型查的全一点。 查准率与查全率（召回率）1234#Recall = TP/(TP+FN) # 我判断得癌症的人/实际得癌症的人from sklearn.linear_model import LogisticRegressionfrom sklearn.cross_validation import KFold, cross_val_score # KFold 做几倍的交叉验证， cross_val_score交叉验证的结果from sklearn.metrics import confusion_matrix,recall_score,classification_report 交叉验证 正则化惩罚项——解决高偏差（欠拟合，数据误差大）&amp; 高方差（过拟合，泛化能力差） L1 惩罚项 L2 惩罚项 直接使用sklearn的逻辑回归库进行拟合 先实例化一个逻辑回归对象（传入正则化参数C，惩罚方式即可） 然后进行fit拟合 sklearn会返回预测结果 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def printing_Kfold_scores(x_train_data,y_train_data): fold = KFold(5,shuffle=False) # 对（训练的）测试集的五倍交叉验证。 ### 返回值是一个列表是[[train1,test1],[train2,test2],...] # Different C parameters # C越大惩罚力度越大，即heta的权重就越小。可以用交叉验证来检测到底等于多少好。 c_param_range = [0.01,0.1,1,10,100] results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = [&#x27;C_parameter&#x27;,&#x27;Mean recall score&#x27;]) results_table[&#x27;C_parameter&#x27;] = c_param_range # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1] ## k-fold会分成两个索引的列表：train_indices = indices[0], test_indices = indices[1] j = 0 &quot;&quot;&quot;来看哪个C好&quot;&quot;&quot; for c_param in c_param_range: print(&#x27;-------------------------------------------&#x27;) print(&#x27;C parameter: &#x27;, c_param) print(&#x27;-------------------------------------------&#x27;) print(&#x27;&#x27;) recall_accs = [] &quot;&quot;&quot;来进行交叉验证，1 3 训练 2验证，1 2 训练，3验证...&quot;&quot;&quot; for iteration, indices in enumerate(fold,start=1): ## 一般情况下，如果要对一个列表或者数组既要遍历索引又要遍历元素时，可以用enumerate # Call the logistic regression model with a certain C parameter lr = LogisticRegression(C = c_param, penalty = &#x27;l1&#x27;) ## C正则化，惩罚方式 L1惩罚。 # Use the training data to fit the model. In this case, we use the portion of the fold to train the model # with indices[0]. We then predict on the portion assigned as the &#x27;test cross validation&#x27; with indices[1] # 进行数据的拟合 lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel()) # Predict values using the test indices in the training data ## 比如在C = 0.01情况下，效果咋样 y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values) # Calculate the recall score and append it to a list for recall scores representing the current c_parameter recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample) ## 召回率库自己生成 recall_score(实际值，预测值) recall_accs.append(recall_acc) print(&#x27;Iteration &#x27;, iteration,&#x27;: recall score = &#x27;, recall_acc) # The mean value of those recall scores is the metric we want to save and get hold of. results_table.ix[j,&#x27;Mean recall score&#x27;] = np.mean(recall_accs) j += 1 print(&#x27;&#x27;) print(&#x27;Mean recall score &#x27;, np.mean(recall_accs)) print(&#x27;&#x27;) best_c = results_table.loc[results_table[&#x27;Mean recall score&#x27;].idxmax()][&#x27;C_parameter&#x27;] # 定义能取到最大值得索引位置， # Finally, we can check which C parameter is the best amongst the chosen. print(&#x27;*********************************************************************************&#x27;) print(&#x27;Best model to choose from cross validation is with C parameter = &#x27;, best_c) print(&#x27;*********************************************************************************&#x27;) return best_c 1best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.958904109589 Iteration 2 : recall score = 0.917808219178 Iteration 3 : recall score = 1.0 Iteration 4 : recall score = 0.972972972973 Iteration 5 : recall score = 0.954545454545 Mean recall score 0.960846151257 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.835616438356 Iteration 2 : recall score = 0.86301369863 Iteration 3 : recall score = 0.915254237288 Iteration 4 : recall score = 0.932432432432 Iteration 5 : recall score = 0.878787878788 Mean recall score 0.885020937099 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.835616438356 Iteration 2 : recall score = 0.86301369863 Iteration 3 : recall score = 0.966101694915 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.893939393939 Mean recall score 0.900923434357 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.849315068493 Iteration 2 : recall score = 0.86301369863 Iteration 3 : recall score = 0.966101694915 Iteration 4 : recall score = 0.959459459459 Iteration 5 : recall score = 0.893939393939 Mean recall score 0.906365863087 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.86301369863 Iteration 2 : recall score = 0.86301369863 Iteration 3 : recall score = 0.966101694915 Iteration 4 : recall score = 0.959459459459 Iteration 5 : recall score = 0.893939393939 Mean recall score 0.909105589115 ********************************************************************************* Best model to choose from cross validation is with C parameter = 0.01 ********************************************************************************* 12345678910111213141516171819202122def plot_confusion_matrix(cm, classes, title=&#x27;Confusion matrix&#x27;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. &quot;&quot;&quot; plt.imshow(cm, interpolation=&#x27;nearest&#x27;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=0) plt.yticks(tick_marks, classes) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#x27;True label&#x27;) plt.xlabel(&#x27;Predicted label&#x27;) 123456789101112131415161718import itertoolslr = LogisticRegression(C = best_c, penalty = &#x27;l1&#x27;)lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred_undersample = lr.predict(X_test_undersample.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)np.set_printoptions(precision=2)print(&quot;Recall metric in the testing dataset: &quot;, cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title=&#x27;Confusion matrix&#x27;)plt.show() Recall metric in the testing dataset: 0.931972789116 从下采样的测试集，可以看到召回率约为(137+10)/137≈90%1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = &#x27;l1&#x27;)lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred = lr.predict(X_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test,y_pred)np.set_printoptions(precision=2)print(&quot;Recall metric in the testing dataset: &quot;, cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title=&#x27;Confusion matrix&#x27;)plt.show() Recall metric in the testing dataset: 0.918367346939 可以看到下采样的预测应用到整体样本的测试集，虽然召回率约为90%，但有8581个误杀值，不是我们所希望的。对于原始数据集进行验证。会得到什么结果呢？——不进行上(下)采样1best_c = printing_Kfold_scores(X_train,y_train) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.492537313433 Iteration 2 : recall score = 0.602739726027 Iteration 3 : recall score = 0.683333333333 Iteration 4 : recall score = 0.569230769231 Iteration 5 : recall score = 0.45 Mean recall score 0.559568228405 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.567164179104 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.683333333333 Iteration 4 : recall score = 0.584615384615 Iteration 5 : recall score = 0.525 Mean recall score 0.595310250644 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.716666666667 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.5625 Mean recall score 0.612645688837 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.733333333333 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.575 Mean recall score 0.61847902217 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.733333333333 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.575 Mean recall score 0.61847902217 ********************************************************************************* Best model to choose from cross validation is with C parameter = 10.0 ********************************************************************************* 1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = &#x27;l1&#x27;)lr.fit(X_train,y_train.values.ravel())y_pred_undersample = lr.predict(X_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test,y_pred_undersample)np.set_printoptions(precision=2)print(&quot;Recall metric in the testing dataset: &quot;, cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title=&#x27;Confusion matrix&#x27;)plt.show() Recall metric in the testing dataset: 0.619047619048 可以看到拿原始数据进行预测得到的召回率是比较低的。接下来看看不同的阈值对模型的影响——划分正负样本的标准不是默认的0.5了123456789101112131415161718192021222324252627lr = LogisticRegression(C = 0.01, penalty = &#x27;l1&#x27;)lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred_undersample_proba = lr.predict_proba(X_test_undersample.values) ## 之前拿的是predict()现在是另外一个函数了 # 之前预测是类别的值，现在预测是概率值thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]plt.figure(figsize=(10,10))j = 1for i in thresholds: y_test_predictions_high_recall = y_pred_undersample_proba[:,1] &gt; i ## 这是关键，拿到概率后直接拿它与设定阈值比较 plt.subplot(3,3,j) j += 1 # Compute confusion matrix cnf_matrix = confusion_matrix(y_test_undersample,y_test_predictions_high_recall) np.set_printoptions(precision=2) print(&quot;Recall metric in the testing dataset: &quot;, cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])) # Plot non-normalized confusion matrix class_names = [0,1] plot_confusion_matrix(cnf_matrix , classes=class_names , title=&#x27;Threshold &gt;= %s&#x27;%i) Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 0.986394557823 Recall metric in the testing dataset: 0.931972789116 Recall metric in the testing dataset: 0.884353741497 Recall metric in the testing dataset: 0.836734693878 Recall metric in the testing dataset: 0.748299319728 Recall metric in the testing dataset: 0.571428571429 可以看到随着阈值的上升，误杀值减小，但是召回率也是减小了。——实际建模时，应该根据实际情况来选择阈值！看完下采样的分析，我们来看看上采样的结果吧！ 上采样需要额外的数据，这里我们采用 SMOTE 方法来生成少数样本的数据。 12345import pandas as pdfrom imblearn.over_sampling import SMOTE # 需要安装 imblearn 库from sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import confusion_matrixfrom sklearn.model_selection import train_test_split 12345678credit_cards=pd.read_csv(&#x27;creditcard.csv&#x27;)columns=credit_cards.columns# The labels are in the last column (&#x27;Class&#x27;). Simply remove it to obtain features columnsfeatures_columns=columns.delete(len(columns)-1)features=credit_cards[features_columns]labels=credit_cards[&#x27;Class&#x27;] 1234features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=0) 12oversampler=SMOTE(random_state=0) # 每次生辰的随机数一样。os_features,os_labels=oversampler.fit_sample(features_train,labels_train) # 注意传入的是训练的x和y的值。没有测试部分的 1len(os_labels[os_labels==1]) # 自动会进行平衡。1:1平衡 227454 123os_features = pd.DataFrame(os_features)os_labels = pd.DataFrame(os_labels)best_c = printing_Kfold_scores(os_features,os_labels) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.968861347792 Iteration 4 : recall score = 0.957595541926 Iteration 5 : recall score = 0.958430881173 Mean recall score 0.933989438728 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970410534469 Iteration 4 : recall score = 0.959980655302 Iteration 5 : recall score = 0.960178498807 Mean recall score 0.935125822266 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970454796946 Iteration 4 : recall score = 0.96014552489 Iteration 5 : recall score = 0.960596168431 Mean recall score 0.935251182603 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.97065397809 Iteration 4 : recall score = 0.960343368396 Iteration 5 : recall score = 0.960530220596 Mean recall score 0.935317397966 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970543321899 Iteration 4 : recall score = 0.960211472725 Iteration 5 : recall score = 0.960903924995 Mean recall score 0.935343628474 ********************************************************************************* Best model to choose from cross validation is with C parameter = 100.0 ********************************************************************************* 1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = &#x27;l1&#x27;)lr.fit(os_features,os_labels.values.ravel())y_pred = lr.predict(features_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(labels_test,y_pred)np.set_printoptions(precision=2)print(&quot;Recall metric in the testing dataset: &quot;, cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title=&#x27;Confusion matrix&#x27;)plt.show() Recall metric in the testing dataset: 0.90099009901 召回率还可以，误杀率降下来——模型的精度变高。(56344+91)/(569344+91+517+10)总之，能用数据生成方式尽量用，上采样的结果更好！案例流程总结： 1. 数据的观察。 1.1 数据浮动情况： 归一化 标准化 1.2 数据分布均匀情况： 下采样 上采样 1.3 此处的案例的特征是处理过的，纯净的特征，不需要额外处理。很多时候需要特种工程处理特征数据—后面讲 2. 对于不同的模型有不同的参数，需要自己进行选择。 比如逻辑回归的正则化参数C的选择(解决过拟合【高方差】和欠拟合【高偏差】)。 采用交叉验证的方式来确定参数C（交叉验证多次来确定C的合适大小） 3. 混淆矩阵，召回率——解决类偏移问题 预测模型为 y=1 ,准确率达到90%这类问题。 4. 不同的阈值（评判分类的不概率标准） 对结果有一定的影响。如此题。阈值越大，误杀率越高，召回率降低。——实际建模的时候，根据需要来确定。","categories":[{"name":"机器学习基础实战","slug":"机器学习基础实战","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"机器学习基础实战","slug":"机器学习基础实战","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/"}]},{"title":"逻辑回归基础","slug":"逻辑回归基础","date":"2021-04-17T17:13:29.000Z","updated":"2021-04-29T15:50:02.753Z","comments":true,"path":"20210418/逻辑回归基础.html","link":"","permalink":"https://xxren8218.github.io/20210418/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80.html","excerpt":"","text":"Logistic RegressionThe data我们将建立一个逻辑回归模型来预测一个学生是否被大学录取。假设你是一个大学系的管理员，你想根据两次考试的结果来决定每个申请人的录取机会。你有以前的申请人的历史数据，你可以用它作为逻辑回归的训练集。对于每一个培训例子，你有两个考试的申请人的分数和录取决定。为了做到这一点，我们将建立一个分类模型，根据考试成绩估计入学概率。 12345# 三大件 # 可以直接在你的python console里面生成图像。不需要plt.show()就可进行展示import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline 1234import ospath = &#x27;data&#x27; + os.sep + &#x27;LogiReg_data.txt&#x27; # 为了让代码在不同的平台上都能运行，路径应该写&#x27;\\&#x27;还是&#x27;/&#x27;无所谓。pdData = pd.read_csv(path, header=None, names=[&#x27;Exam 1&#x27;, &#x27;Exam 2&#x27;, &#x27;Admitted&#x27;]) # header = None自己制定列名pdData.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Exam 1 Exam 2 Admitted 0 34.623660 78.024693 0 1 30.286711 43.894998 0 2 35.847409 72.902198 0 3 60.182599 86.308552 1 4 79.032736 75.344376 1 1pdData.shape # 看数据的维度。 (100, 3) 12345678910positive = pdData[pdData[&#x27;Admitted&#x27;] == 1] # returns the subset of rows such Admitted = 1, i.e. the set of *positive* examplesnegative = pdData[pdData[&#x27;Admitted&#x27;] == 0] # returns the subset of rows such Admitted = 0, i.e. the set of *negative* examplesfig, ax = plt.subplots(figsize=(10,5))ax.scatter(positive[&#x27;Exam 1&#x27;], positive[&#x27;Exam 2&#x27;], s=30, c=&#x27;b&#x27;, marker=&#x27;o&#x27;, label=&#x27;Admitted&#x27;)ax.scatter(negative[&#x27;Exam 1&#x27;], negative[&#x27;Exam 2&#x27;], s=30, c=&#x27;r&#x27;, marker=&#x27;x&#x27;, label=&#x27;Not Admitted&#x27;)ax.legend() # legend（）有一个loc参数，用于控制图例的位置。 比如 plot.legend(loc=2) , 这个位置就是4象项中的第二象项，也就是左上角。 loc可以为1,2,3,4 这四个数字。 # 如果把那句legend() 的语句去掉，那么图形上的图例也就会消失了。ax.set_xlabel(&#x27;Exam 1 Score&#x27;)ax.set_ylabel(&#x27;Exam 2 Score&#x27;) Text(0,0.5,&#39;Exam 2 Score&#39;) The logistic regression目标：建立分类器—即决策边界（求解出三个参数 $\\theta_0 \\theta_1 \\theta_2 $） 设定阈值，根据阈值判断录取结果—就是分类的概率判断，一般为 0.5 要完成的模块 sigmoid : 映射到概率的函数 model : 返回预测结果值 cost : 根据参数计算损失 gradient : 计算每个参数的梯度方向 descent : 进行参数更新 accuracy: 计算精度 sigmoid 函数 g(z) = \\frac{1}{1+e^{-z}}12def sigmoid(z): return 1 / (1 + np.exp(-z)) # np.exp(-z)表示e的多少次幂 123nums = np.arange(-10, 10, step=1) #creates a vector containing 20 equally spaced values from -10 to 10fig, ax = plt.subplots(figsize=(12,4))ax.plot(nums, sigmoid(nums), &#x27;r&#x27;) [&lt;matplotlib.lines.Line2D at 0x244554b2b70&gt;] Sigmoid $g:\\mathbb{R} \\to [0,1]$ $g(0)=0.5$ $g(- \\infty)=0$ $g(+ \\infty)=1$ model 完成预测函数 $h_\\theta(x)$1234def model(X, theta): return sigmoid(np.dot(X, theta.T)) # np.dot是矩阵的乘法,也可以用 @ # 求出的 model是 m行1列.m--样本数目： \\begin{array}{ccc} \\begin{pmatrix}\\theta_{0} & \\theta_{1} & \\theta_{2}\\end{pmatrix} & \\times & \\begin{pmatrix}1\\\\ x_{1}\\\\ x_{2} \\end{pmatrix}\\end{array}=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}12345678910111213pdData.insert(0, &#x27;Ones&#x27;, 1) # 插入零的一列，列指标为 Ones# set X (training data) and y (target variable)orig_data = pdData.as_matrix() # 习惯性的操作。很多时候取得的数据是DataFrame的形式(直接转换csv格式的数据以后)，这个时候要记得转换成数组cols = orig_data.shape[1] # 看数据有几列。X = orig_data[:,0:cols-1]y = orig_data[:,cols-1:cols]# convert to numpy arrays and initalize the parameter array theta#X = np.matrix(X.values)#y = np.matrix(data.iloc[:,3:4].values) #np.array(y.values)theta = np.zeros([1, 3]) # 参数theta 一般先构造出来，用zero来占位，构造1行3列的数据。即三个theta参数 （1,3）[1,4]都可以。 来看看数据的样子吧 1X[:5] # 前 5 行的数据 array([[ 1. , 34.62365962, 78.02469282], [ 1. , 30.28671077, 43.89499752], [ 1. , 35.84740877, 72.90219803], [ 1. , 60.18259939, 86.3085521 ], [ 1. , 79.03273605, 75.34437644]]) 1y[:5] array([[0.], [0.], [0.], [1.], [1.]]) 1theta array([[0., 0., 0.]]) 1X.shape, y.shape, theta.shape ((100, 3), (100, 1), (1, 3)) 损失函数（代价函数）将对数似然函数去负号 D(h_\\theta(x), y) = -y\\log(h_\\theta(x)) - (1-y)\\log(1-h_\\theta(x))求平均损失 J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m} D(h_\\theta(x_i), y_i)1234def cost(X, y, theta): left = np.multiply(-y, np.log(model(X, theta))) # np.multiply对数据完成的乘的操作 right = np.multiply(1 - y, np.log(1 - model(X, theta))) return np.sum(left - right) / (len(X)) # np.sum完成对数据的加和 1cost(X, y, theta) 0.6931471805599453 计算梯度 — 最难的部分 \\frac{\\partial J}{\\partial \\theta_j}=-\\frac{1}{m}\\sum_{i=1}^n (y_i - h_\\theta (x_i))x_{ij}12345678def gradient(X, y, theta): grad = np.zeros(theta.shape) # 梯度计算需要考虑 theta 的个数（维度） error = (model(X, theta)- y).ravel() # 把负号提取到里面了，revel()将数据降为1维！(1,m) for j in range(len(theta.ravel())): # theta降低为 1 维度，[1,2,3,4]这样,就可以求theta的个数了，按列进行遍历 term = np.multiply(error, X[:,j]) # 矩阵的乘法，取第j列。 (1,m)@(m,1) grad[0, j] = np.sum(term) / len(X) # 每一个梯度j算一个值。取[0, j] return grad Gradient descent比较3种不同梯度下降方法—批量、随机、小批量 123456789STOP_ITER = 0 # 根据迭代次数停止STOP_COST = 1 # 根据损失值目标函数的变化停止STOP_GRAD = 2 # 根据梯度的变化（很小）停止def stopCriterion(type, value, threshold): &quot;&quot;&quot;设定三种不同的停止策略&quot;&quot;&quot; if type == STOP_ITER: return value &gt; threshold elif type == STOP_COST: return abs(value[-1]-value[-2]) &lt; threshold # abs()返回绝对值 elif type == STOP_GRAD: return np.linalg.norm(value) &lt; threshold # np.linalg.norm默认是 2 范数--平方和开根号。 12345678import numpy.random# 洗牌，将数据随机化，泛化能力变强def shuffleData(data): np.random.shuffle(data) # np.random.shuffle()将数据进行洗牌。 cols = data.shape[1] X = data[:, 0:cols-1] y = data[:, cols-1:] return X, y 123456789101112131415161718192021222324252627282930import timedef descent(data, theta, batchSize, stopType, thresh, alpha): &quot;&quot;&quot;梯度下降求解&quot;&quot;&quot; # 参数的初始化，第一次计算各个值。 init_time = time.time() i = 0 # 迭代次数 k = 0 # batch X, y = shuffleData(data) grad = np.zeros(theta.shape) # 计算的梯度 costs = [cost(X, y, theta)] # 损失值 while True: grad = gradient(X[k:k+batchSize], y[k:k+batchSize], theta) k += batchSize # 取batch个数据，每次取batchSize个数据进行计算。 if k &gt;= n: k = 0 X, y = shuffleData(data) # 重新洗牌 theta = theta - alpha*grad # 参数更新 costs.append(cost(X, y, theta)) # 计算新的损失 i += 1 if stopType == STOP_ITER: value = i elif stopType == STOP_COST: value = costs elif stopType == STOP_GRAD: value = grad if stopCriterion(stopType, value, thresh): break return theta, i-1, costs, grad, time.time() - init_time 123456789101112131415161718192021def runExpe(data, theta, batchSize, stopType, thresh, alpha): #import pdb; pdb.set_trace(); theta, iter, costs, grad, dur = descent(data, theta, batchSize, stopType, thresh, alpha) name = &quot;Original&quot; if (data[:,1]&gt;2).sum() &gt; 1 else &quot;Scaled&quot; # 归一化时的区分 name += &quot; data - learning rate: &#123;&#125; - &quot;.format(alpha) if batchSize==n: strDescType = &quot;Gradient&quot; elif batchSize==1: strDescType = &quot;Stochastic&quot; else: strDescType = &quot;Mini-batch (&#123;&#125;)&quot;.format(batchSize) name += strDescType + &quot; descent - Stop: &quot; if stopType == STOP_ITER: strStop = &quot;&#123;&#125; iterations&quot;.format(thresh) elif stopType == STOP_COST: strStop = &quot;costs change &lt; &#123;&#125;&quot;.format(thresh) else: strStop = &quot;gradient norm &lt; &#123;&#125;&quot;.format(thresh) name += strStop print (&quot;***&#123;&#125;\\nTheta: &#123;&#125; - Iter: &#123;&#125; - Last cost: &#123;:03.2f&#125; - Duration: &#123;:03.2f&#125;s&quot;.format( name, theta, iter, costs[-1], dur)) fig, ax = plt.subplots(figsize=(12,4)) ax.plot(np.arange(len(costs)), costs, &#x27;r&#x27;) ax.set_xlabel(&#x27;Iterations&#x27;) ax.set_ylabel(&#x27;Cost&#x27;) ax.set_title(name.upper() + &#x27; - Error vs. Iteration&#x27;) return theta 不同的停止策略设定迭代次数123#选择的梯度下降方法是基于所有样本的n=100runExpe(orig_data, theta, n, STOP_ITER, thresh=5000, alpha=0.000001) ***Original data - learning rate: 1e-06 - Gradient descent - Stop: 5000 iterations Theta: [[-0.00027127 0.00705232 0.00376711]] - Iter: 5000 - Last cost: 0.63 - Duration: 0.82s array([[-0.00027127, 0.00705232, 0.00376711]]) 根据损失值停止设定阈值 1E-6, 差不多需要110 000次迭代 1runExpe(orig_data, theta, n, STOP_COST, thresh=0.000001, alpha=0.001) ***Original data - learning rate: 0.001 - Gradient descent - Stop: costs change &lt; 1e-06 Theta: [[-5.13364014 0.04771429 0.04072397]] - Iter: 109901 - Last cost: 0.38 - Duration: 17.97s array([[-5.13364014, 0.04771429, 0.04072397]]) 根据梯度变化停止设定阈值 0.05,差不多需要40 000次迭代 1runExpe(orig_data, theta, n, STOP_GRAD, thresh=0.05, alpha=0.001) ***Original data - learning rate: 0.001 - Gradient descent - Stop: gradient norm &lt; 0.05 Theta: [[-2.37033409 0.02721692 0.01899456]] - Iter: 40045 - Last cost: 0.49 - Duration: 6.87s array([[-2.37033409, 0.02721692, 0.01899456]]) 对比不同的梯度下降方法Stochastic descent1runExpe(orig_data, theta, 1, STOP_ITER, thresh=5000, alpha=0.001) ***Original data - learning rate: 0.001 - Stochastic descent - Stop: 5000 iterations Theta: [[-0.39253059 0.04095984 -0.07371051]] - Iter: 5000 - Last cost: 1.84 - Duration: 0.27s array([[-0.39253059, 0.04095984, -0.07371051]]) 有点爆炸。。。很不稳定,再来试试把学习率调小一些 1runExpe(orig_data, theta, 1, STOP_ITER, thresh=15000, alpha=0.000002) ***Original data - learning rate: 2e-06 - Stochastic descent - Stop: 15000 iterations Theta: [[-0.00202238 0.00995606 0.00088035]] - Iter: 15000 - Last cost: 0.63 - Duration: 0.77s array([[-0.00202238, 0.00995606, 0.00088035]]) 速度快，但稳定性差，需要很小的学习率 Mini-batch descent1runExpe(orig_data, theta, 16, STOP_ITER, thresh=15000, alpha=0.001) ***Original data - learning rate: 0.001 - Mini-batch (16) descent - Stop: 15000 iterations Theta: [[-1.0364887 0.02542788 0.00549476]] - Iter: 15000 - Last cost: 0.57 - Duration: 1.04s array([[-1.0364887 , 0.02542788, 0.00549476]]) 浮动仍然比较大，我们来尝试下对数据进行标准化将数据按其属性(按列进行)减去其均值，然后除以其方差。最后得到的结果是，对每个属性/每列来说所有数据都聚集在0附近，方差值为1 123456from sklearn import preprocessing as ppscaled_data = orig_data.copy()scaled_data[:, 1:3] = pp.scale(orig_data[:, 1:3])runExpe(scaled_data, theta, n, STOP_ITER, thresh=5000, alpha=0.001) ***Scaled data - learning rate: 0.001 - Gradient descent - Stop: 5000 iterations Theta: [[0.3080807 0.86494967 0.77367651]] - Iter: 5000 - Last cost: 0.38 - Duration: 0.88s array([[0.3080807 , 0.86494967, 0.77367651]]) 它好多了！原始数据，只能达到达到0.61，而我们得到了0.38个在这里！所以对数据做预处理是非常重要的 1runExpe(scaled_data, theta, n, STOP_GRAD, thresh=0.02, alpha=0.001) ***Scaled data - learning rate: 0.001 - Gradient descent - Stop: gradient norm &lt; 0.02 Theta: [[1.0707921 2.63030842 2.41079787]] - Iter: 59422 - Last cost: 0.22 - Duration: 10.67s array([[1.0707921 , 2.63030842, 2.41079787]]) 更多的迭代次数会使得损失下降的更多！ 1theta = runExpe(scaled_data, theta, 1, STOP_GRAD, thresh=0.002/5, alpha=0.001) ***Scaled data - learning rate: 0.001 - Stochastic descent - Stop: gradient norm &lt; 0.0004 Theta: [[1.14814786 2.79253048 2.56596963]] - Iter: 72591 - Last cost: 0.22 - Duration: 4.82s 随机梯度下降更快，但是我们需要迭代的次数也需要更多，所以还是用batch的比较合适！！！ 1runExpe(scaled_data, theta, 16, STOP_GRAD, thresh=0.002*2, alpha=0.001) ***Scaled data - learning rate: 0.001 - Mini-batch (16) descent - Stop: gradient norm &lt; 0.004 Theta: [[1.14982001 2.79586036 2.56934533]] - Iter: 307 - Last cost: 0.22 - Duration: 0.03s array([[1.14982001, 2.79586036, 2.56934533]]) 精度123# 设定阈值def predict(X, theta): return [1 if x &gt;= 0.5 else 0 for x in model(X, theta)] 123456789101112scaled_X = scaled_data[:, :3]y = scaled_data[:, 3]predictions = predict(scaled_X, theta)correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)] &quot;&quot;&quot; zip(itertion1, iteration2) A[1,2,3] B[4,5,6] zip[A,B] = [(1,4),(2,5),(3,6)] 可用list()进行转换&quot;&quot;&quot;accuracy = (sum(map(int, correct)) / len(correct)) # map (func, iterations)print (&#x27;accuracy = &#123;&#125;%&#x27;.format(accuracy*100)) accuracy = 89.0% 逻辑回归到这里就结束了！","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"机器学习数学基础-线性代数","slug":"机器学习数学基础-线性代数","date":"2021-04-04T17:11:14.000Z","updated":"2021-04-17T18:10:24.541Z","comments":true,"path":"20210405/机器学习数学基础-线性代数.html","link":"","permalink":"https://xxren8218.github.io/20210405/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0.html","excerpt":"","text":"线性代数复习和参考1. 基础概念和符号线性代数提供了一种紧凑地表示和操作线性方程组的方法。 例如，以下方程组： 4x_1 − 5x_2 = −13 −2x_1 + 3x_2 = 9这是两个方程和两个变量，正如你从高中代数中所知，你可以找到 $x_1$ 和 $x_2$ 的唯一解（除非方程以某种方式退化，例如，如果第二个方程只是第一个的倍数，但在上面的情况下，实际上只有一个唯一解）。 在矩阵表示法中，我们可以更紧凑地表达： Ax= b \\text { with } A=\\left[\\begin{array}{cc}{4} & {-5} \\\\ {-2} & {3}\\end{array}\\right], b=\\left[\\begin{array}{c}{-13} \\\\ {9}\\end{array}\\right]我们可以看到，这种形式的线性方程有许多优点（比如明显地节省空间）。 1.1 基本符号我们使用以下符号： $A \\in \\mathbb{R}^{m \\times n}$，表示 $A$ 为由实数组成具有$m$行和$n$列的矩阵。 $x \\in \\mathbb{R}^{ n}$，表示具有$n$个元素的向量。 通常，向量$x$将表示列向量: 即，具有$n$行和$1$列的矩阵。 如果我们想要明确地表示行向量: 具有 $1$ 行和$n$列的矩阵 - 我们通常写$x^T$（这里$x^T$$x$的转置）。 $x_i$表示向量$x$的第$i$个元素 x=\\left[\\begin{array}{c}{x_{1}} \\\\ {x_{2}} \\\\ {\\vdots} \\\\ {x_{n}}\\end{array}\\right] 我们使用符号 $a_{ij}$（或$A_{ij}$,$A_{i,j}$等）来表示第 $i$ 行和第$j$列中的 $A$ 的元素： A=\\left[\\begin{array}{cccc}{a_{11}} & {a_{12}} & {\\cdots} & {a_{1 n}} \\\\ {a_{21}} & {a_{22}} & {\\cdots} & {a_{2 n}} \\\\ {\\vdots} & {\\vdots} & {\\ddots} & {\\vdots} \\\\ {a_{m 1}} & {a_{m 2}} & {\\cdots} & {a_{m n}}\\end{array}\\right] 我们用$a^j$或者$A_{:,j}$表示矩阵$A$的第$j$列： A=\\left[\\begin{array}{llll}{ |} & { |} & {} & { |} \\\\ {a^{1}} & {a^{2}} & {\\cdots} & {a^{n}} \\\\ { |} & { |} & {} & { |}\\end{array}\\right] 我们用$a^T_i$或者$A_{i,:}$表示矩阵$A$的第$i$行： A=\\left[\\begin{array}{c}{-a_{1}^{T}-} \\\\ {-a_{2}^{T}-} \\\\ {\\vdots} \\\\ {-a_{m}^{T}-}\\end{array}\\right] 在许多情况下，将矩阵视为列向量或行向量的集合非常重要且方便。 通常，在向量而不是标量上操作在数学上（和概念上）更清晰。只要明确定义了符号，用于矩阵的列或行的表示方式并没有通用约定。 2.矩阵乘法两个矩阵相乘，其中 $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$ ，则： C = AB \\in \\mathbb{R}^{m \\times p}其中： C_{i j}=\\sum_{k=1}^{n} A_{i k} B_{k j}请注意，为了使矩阵乘积存在，$A$中的列数必须等于$B$中的行数。有很多方法可以查看矩阵乘法，我们将从检查一些特殊情况开始。 2.1 向量-向量乘法给定两个向量$x, y \\in \\mathbb{R}^{n}$,$x^T y$通常称为向量内积或者点积，结果是个实数。 x^{T} y \\in \\mathbb{R}=\\left[\\begin{array}{llll}{x_{1}} & {x_{2}} & {\\cdots} & {x_{n}}\\end{array}\\right]\\left[\\begin{array}{c}{y_{1}} \\\\ {y_{2}} \\\\ {\\vdots} \\\\ {y_{n}}\\end{array}\\right]=\\sum_{i=1}^{n} x_{i} y_{i}注意：$x^T y = y^Tx$ 始终成立。 给定向量 $x \\in \\mathbb{R}^{m}$, $y \\in \\mathbb{R}^{n}$ (他们的维度是否相同都没关系)，$xy^T \\in \\mathbb{R}^{m \\times n}$叫做向量外积 , 当 $(xy^T)_{ij} = x_iy_j$ 的时候，它是一个矩阵。 x y^{T} \\in \\mathbb{R}^{m \\times n}=\\left[\\begin{array}{c}{x_{1}} \\\\ {x_{2}} \\\\ {\\vdots} \\\\ {x_{m}}\\end{array}\\right]\\left[\\begin{array}{llll}{y_{1}} & {y_{2}} & {\\cdots} & {y_{n}}\\end{array}\\right]=\\left[\\begin{array}{cccc}{x_{1} y_{1}} & {x_{1} y_{2}} & {\\cdots} & {x_{1} y_{n}} \\\\ {x_{2} y_{1}} & {x_{2} y_{2}} & {\\cdots} & {x_{2} y_{n}} \\\\ {\\vdots} & {\\vdots} & {\\ddots} & {\\vdots} \\\\ {x_{m} y_{1}} & {x_{m} y_{2}} & {\\cdots} & {x_{m} y_{n}}\\end{array}\\right]举一个外积如何使用的一个例子：让$1\\in R^{n}$表示一个$n$维向量，其元素都等于1，此外，考虑矩阵$A \\in R^{m \\times n}$，其列全部等于某个向量 $x \\in R^{m}$。 我们可以使用外积紧凑地表示矩阵 $A$: A=\\left[\\begin{array}{llll}{ |} & { |} & {} & { |} \\\\ {x} & {x} & {\\cdots} & {x} \\\\ { |} & { |} & {} & { |}\\end{array}\\right]=\\left[\\begin{array}{cccc}{x_{1}} & {x_{1}} & {\\cdots} & {x_{1}} \\\\ {x_{2}} & {x_{2}} & {\\cdots} & {x_{2}} \\\\ {\\vdots} & {\\vdots} & {\\ddots} & {\\vdots} \\\\ {x_{m}} & {x_{m}} & {\\cdots} & {x_{m}}\\end{array}\\right]=\\left[\\begin{array}{c}{x_{1}} \\\\ {x_{2}} \\\\ {\\vdots} \\\\ {x_{m}}\\end{array}\\right]\\left[\\begin{array}{lll}{1} & {1} & {\\cdots} & {1}\\end{array}\\right]=x \\mathbf{1}^{T}2.2 矩阵-向量乘法给定矩阵 $A \\in \\mathbb{R}^{m \\times n}$，向量 $x \\in \\mathbb{R}^{n}$ , 它们的积是一个向量 $y = Ax \\in R^{m}$。 有几种方法可以查看矩阵向量乘法，我们将依次查看它们中的每一种。 如果我们按行写$A$，那么我们可以表示$Ax$为： y=A x=\\left[\\begin{array}{ccc}{-} & {a_{1}^{T}} & {-} \\\\ {-} & {a_{2}^{T}} & {-} \\\\ {} & {\\vdots} & {} \\\\ {-} & {a_{m}^{T}} & {-}\\end{array}\\right] x=\\left[\\begin{array}{c}{a_{1}^{T} x} \\\\ {a_{2}^{T} x} \\\\ {\\vdots} \\\\ {a_{m}^{T} x}\\end{array}\\right]换句话说，第$i$个$y$是$A$的第$i$行和$x$的内积，即：$y_i = y_{i}=a_{i}^{T} x$。 同样的， 可以把 $A$ 写成列的方式，则公式如下： y=A x=\\left[\\begin{array}{cccc}{ |} & { |} & {} & { |} \\\\ {a^{1}} & {a^{2}} & {\\cdots} & {a^{n}} \\\\ { |} & { |} & {} & { |}\\end{array}\\right]\\left[\\begin{array}{c}{x_{1}} \\\\ {x_{2}} \\\\ {\\vdots} \\\\ {x_{n}}\\end{array}\\right]=\\left[\\begin{array}{c}{ } \\\\ {a^{1}{ } \\\\ }\\end{array}\\right] x_{1}+\\left[\\begin{array}{c}{ } \\\\ {a^{2}{ } \\\\ }\\end{array}\\right] x_{2}+{\\cdots} +\\left[\\begin{array}{c}{ } \\\\ {a^{n}{ } \\\\ }\\end{array}\\right] x_{n}换句话说，$y$是$A$的列的线性组合，其中线性组合的系数由$x$的元素给出。 到目前为止，我们一直在右侧乘以列向量，但也可以在左侧乘以行向量。 这是写的，$y^T = x^TA$ 表示$A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{m}$，$y \\in \\mathbb{R}^{n}$。 和以前一样，我们可以用两种可行的方式表达$y^T$，这取决于我们是否根据行或列表达$A$. 第一种情况，我们把$A$用列表示： y^{T}=x^{T} A=x^{T}\\left[\\begin{array}{cccc}{ |} & { |} & {} & { |} \\\\ {a^{1}} & {a^{2}} & {\\cdots} & {a^{n}} \\\\ { |} & { |} & {} & { |}\\end{array}\\right]=\\left[\\begin{array}{cccc}{x^{T} a^{1}} & {x^{T} a^{2}} & {\\dots} & {x^{T} a^{n}}\\end{array}\\right]这表明$y^T$的第$i$个元素等于$x$和$A$的第$i$列的内积。 最后，根据行表示$A$，我们得到了向量-矩阵乘积的最终表示: y^T=x^TA =\\left[\\begin{array}{llll}{x_{1}} & {x_{2}} & {\\cdots} & {x_{n}}\\end{array}\\right]\\left[\\begin{array}{c}{-a_{1}^{T}-} \\\\ {-a_{2}^{T}-} \\\\ {\\vdots} \\\\ {-a_{m}^{T}-}\\end{array}\\right] =x_{1}\\left[-a_{1}^{T}-\\right]+x_{2}\\left[-a_{2}^{T}-\\right]+\\ldots+x_{n}\\left[-a_{n}^{T}-\\right]所以我们看到$y^T$是$A$的行的线性组合，其中线性组合的系数由$x$的元素给出。 2.3 矩阵-矩阵乘法有了这些知识，我们现在可以看看四种不同的（形式不同，但结果是相同的）矩阵-矩阵乘法：也就是本节开头所定义的$C=AB$的乘法。 首先，我们可以将矩阵 - 矩阵乘法视为一组向量-向量乘积。 从定义中可以得出：最明显的观点是$C $的$( i，j )$元素等于$A$的第$i$行和$B$的的$j$列的内积。如下面的公式所示： C=A B=\\left[\\begin{array}{cc}{-} & {a_{1}^{T}} &{-} \\\\ {-} & {a_{2}^{T}} &{-} \\\\ {} & {\\vdots} \\\\ {-} & {a_{m}^{T}} &{-} \\end{array}\\right]\\left[\\begin{array}{cccc}{ |} & { |} & {} & { |} \\\\ {b_{1}} & {b_{2}} & {\\cdots} & {b_{p}} \\\\ { |} & { |} & {} & { |}\\end{array}\\right]=\\left[\\begin{array}{cccc}{a_{1}^{T} b_{1}} & {a_{1}^{T} b_{2}} & {\\cdots} & {a_{1}^{T} b_{p}} \\\\ {a_{2}^{T} b_{1}} & {a_{2}^{T} b_{2}} & {\\cdots} & {a_{2}^{T} b_{p}} \\\\ {\\vdots} & {\\vdots} & {\\ddots} & {\\vdots} \\\\ {a_{m}^{T} b_{1}} & {a_{m}^{T} b_{2}} & {\\cdots} & {a_{m}^{T} b_{p}}\\end{array}\\right]这里的$ A \\in \\mathbb{R}^{m\\times n}$ ，$B \\in \\mathbb{R}^{n \\times p}$， $a_i \\in \\mathbb{R}^n$ ，$b^j \\in \\mathbb{R}^{n \\times p}$， 这里的$ A \\in \\mathbb{R}^ {m \\times n}，$ $B \\in \\mathbb{R}^ {n \\times p} $， $a_i \\in \\mathbb{R} ^ n $，$ b ^ j \\in \\mathbb{R} ^ {n \\times p} $，所以它们可以计算内积。 我们用通常用行表示$ A $而用列表示$B$。或者，我们可以用列表示$ A$，用行表示$B $，这时$AB$是求外积的和。公式如下： C=A B=\\left[\\begin{array}{cccc}{ |} & { |} & {} & { |} \\\\ {a_{1}} & {a_{2}} & {\\cdots} & {a_{n}} \\\\ { |} & { |} & {} & { |}\\end{array}\\right]\\left[\\begin{array}{c}{-}& {b_{1}^{T}}&{-} \\\\ {-}& {b_{2}^{T}}&{-} \\\\ {\\vdots} \\\\{-}& {b_{n}^{T}}&{-}\\end{array}\\right]=\\sum_{i=1}^{n} a_{i} b_{i}^{T}换句话说，$AB$等于所有的$A$的第$i$列和$B$第$i$行的外积的和。因此，在这种情况下， $a_i \\in \\mathbb{R}^ m $和$b_i \\in \\mathbb{R}^p$， 外积$a^ib_i^T$的维度是$m×p$，与$C$的维度一致。 其次，我们还可以将矩阵 - 矩阵乘法视为一组矩阵向量积。如果我们把$B$用列表示，我们可以将$C$的列视为$A$和$B$的列的矩阵向量积。公式如下： C=A B=A\\left[\\begin{array}{cccc}{ |} & { |} & {} & { |} \\\\ {b_{1}} & {b_{2}} & {\\cdots} & {b_{p}} \\\\ { |} & { |} & {} & { |}\\end{array}\\right]=\\left[\\begin{array}{cccc}{ |} & { |} & {} & { |} \\\\ {A b_{1}} & {A b_{2}} & {\\cdots} & {A b_{p}} \\\\ { |} & { |} & {} & { |}\\end{array}\\right]这里$C$的第$i$列由矩阵向量乘积给出，右边的向量为$c_i = Ab_i$。 这些矩阵向量乘积可以使用前一小节中给出的两个观点来解释。最后，我们有类似的观点，我们用行表示$A$，$C$的行作为$A$和$C$行之间的矩阵向量积。公式如下： C=A B=\\left[\\begin{array}{ccc}{-} & {a_{1}^{T}} & {-} \\\\ {-} & {a_{2}^{T}} & {-} \\\\ {} & {\\vdots} & {} \\\\ {-} & {a_{m}^{T}} & {-}\\end{array}\\right] B=\\left[\\begin{array}{c} {-} & {a_{1}^{T} B} & {-}\\\\ {-} & {a_{2}^{T} B} & {-} \\\\ {\\vdots} \\\\ {-} & {a_{m}^{T} B}& {-}\\end{array}\\right]这里第$i$行的$C$由左边的向量的矩阵向量乘积给出：$c_i^T = a_i^T B$ 将矩阵乘法剖析到如此大的程度似乎有点过分，特别是当所有这些观点都紧跟在我们在本节开头给出的初始定义（在一行数学中）之后。 这些不同方法的直接优势在于它们允许您在向量的级别/单位而不是标量上进行操作。 为了完全理解线性代数而不会迷失在复杂的索引操作中，关键是要用尽可能多的概念进行操作。 实际上所有的线性代数都处理某种矩阵乘法，花一些时间对这里提出的观点进行直观的理解是非常必要的。 除此之外，了解一些更高级别的矩阵乘法的基本属性是很有必要的： 矩阵乘法结合律: $(AB)C = A(BC)$ 矩阵乘法分配律: $A(B + C) = AB + AC$ 矩阵乘法通常不是可交换的; 也就是说，通常$AB \\ne BA$。 （例如，假设$ A \\in \\mathbb{R}^ {m \\times n}，$ $B \\in \\mathbb{R}^ {n \\times p} $，如果$m$和$q$不相等，矩阵乘积$BA$甚至不存在！） 如果您不熟悉这些属性，请花点时间自己验证它们。 例如，为了检查矩阵乘法的相关性，假设$A \\in \\mathbb{R}^ {m \\times n}，$ $B \\in \\mathbb{R}^ {n \\times p} $，$C \\in \\mathbb{R}^ {p \\times q}$。 注意$AB \\in \\mathbb{R}^ {m \\times p}$，所以$(AB)C \\in \\mathbb{R}^ {m \\times q}$。 类似地，$BC \\in \\mathbb{R}^ {n \\times q}$，所以$A(BC) \\in \\mathbb{R}^ {m \\times q}$。 因此，所得矩阵的维度一致。 为了表明矩阵乘法是相关的，足以检查$(AB)C $的第$(i,j)$个元素是否等于$A(BC)$的第$(i,j)$个元素。 我们可以使用矩阵乘法的定义直接验证这一点： \\begin{aligned}((A B) C)_{i j} &=\\sum_{k=1}^{p}(A B)_{i k} C_{k j}=\\sum_{k=1}^{p}\\left(\\sum_{l=1}^{n} A_{i l} B_{l k}\\right) C_{k j} \\\\ &=\\sum_{k=1}^{p}\\left(\\sum_{l=1}^{n} A_{i l} B_{l k} C_{k j}\\right)=\\sum_{l=1}^{n}\\left(\\sum_{k=1}^{p} A_{i l} B_{l k} C_{k j}\\right) \\\\ &=\\sum_{l=1}^{n} A_{i l}\\left(\\sum_{k=1}^{p} B_{l k} C_{k j}\\right)=\\sum_{l=1}^{n} A_{i l}(B C)_{l j}=(A(B C))_{i j} \\end{aligned}3 运算和属性在本节中，我们介绍矩阵和向量的几种运算和属性。 希望能够为您复习大量此类内容，这些笔记可以作为这些主题的参考。 3.1 单位矩阵和对角矩阵单位矩阵,$I \\in \\mathbb{R}^{n \\times n} $，它是一个方阵，对角线的元素是1，其余元素都是0： I_{i j}=\\left\\{\\begin{array}{ll}{1} & {i=j} \\\\ {0} & {i \\neq j}\\end{array}\\right.对于所有$A \\in \\mathbb{R}^ {m \\times n}$，有： AI = A = IA注意，在某种意义上，单位矩阵的表示法是不明确的，因为它没有指定$I$的维数。通常，$I$的维数是从上下文推断出来的，以便使矩阵乘法成为可能。 例如，在上面的等式中，$AI = A$中的I是$n\\times n$矩阵，而$A = IA$中的$I$是$m\\times m$矩阵。 对角矩阵是一种这样的矩阵：对角线之外的元素全为0。对角阵通常表示为：$D= diag(d_1, d_2, . . . , d_n)$，其中： D_{i j}=\\left\\{\\begin{array}{ll}{d_{i}} & {i=j} \\\\ {0} & {i \\neq j}\\end{array}\\right.很明显：单位矩阵$ I = diag(1, 1, . . . , 1)$。 3.2 转置矩阵的转置是指翻转矩阵的行和列。 给定一个矩阵： $A \\in \\mathbb{R}^ {m \\times n}$, 它的转置为$n \\times m$的矩阵$A^T \\in \\mathbb{R}^ {n \\times m}$ ，其中的元素为： (A^T)_{ij} = A_{ji}事实上，我们在描述行向量时已经使用了转置，因为列向量的转置自然是行向量。 转置的以下属性很容易验证： $(A^T )^T = A$ $ (AB)^T = B^T A^T$ $(A + B)^T = A^T + B^T$ 3.3 对称矩阵如果$A = A^T$，则矩阵$A \\in \\mathbb{R}^ {n \\times n}$是对称矩阵。 如果$ A = - A^T$，它是反对称的。 很容易证明，对于任何矩阵$A \\in \\mathbb{R}^ {n \\times n}$，矩阵$A + A^ T$是对称的，矩阵$A -A^T$是反对称的。 由此得出，任何方矩阵$A \\in \\mathbb{R}^ {n \\times n}$可以表示为对称矩阵和反对称矩阵的和，所以： A=\\frac{1}{2}(A+A^T)+\\frac{1}{2}(A-A^T)上面公式的右边的第一个矩阵是对称矩阵，而第二个矩阵是反对称矩阵。 事实证明，对称矩阵在实践中用到很多，它们有很多很好的属性，我们很快就会看到它们。通常将大小为$n$的所有对称矩阵的集合表示为$\\mathbb{S}^n$，因此$A \\in \\mathbb{S}^n$意味着$A$是对称的$n\\times n$矩阵; 3.4 矩阵的迹方矩阵$A \\in \\mathbb{R}^ {n \\times n}$的迹，表示为$\\operatorname{tr} (A)$（或者只是$\\operatorname{tr} A$，如果括号显然是隐含的），是矩阵中对角元素的总和： \\operatorname{tr} A=\\sum_{i=1}^{n} A_{i i}如CS229讲义中所述，迹具有以下属性（如下所示）： 对于矩阵$A \\in \\mathbb{R}^ {n \\times n}$，则：$\\operatorname{tr}A =\\operatorname{tr}A^T$ 对于矩阵$A,B \\in \\mathbb{R}^ {n \\times n}$，则：$\\operatorname{tr}(A + B) = \\operatorname{tr}A + \\operatorname{tr}B$ 对于矩阵$A \\in \\mathbb{R}^ {n \\times n}$，$ t \\in \\mathbb{R}$，则：$\\operatorname{tr}(tA) = t\\operatorname{tr}A$. 对于矩阵 $A$, $B$，$AB$ 为方阵, 则：$\\operatorname{tr}AB = \\operatorname{tr}BA$ 对于矩阵 $A$, $B$, $C$, $ABC$为方阵, 则：$\\operatorname{tr}ABC = \\operatorname{tr}BCA=\\operatorname{tr}CAB$, 同理，更多矩阵的积也是有这个性质。 作为如何证明这些属性的示例，我们将考虑上面给出的第四个属性。 假设$A \\in \\mathbb{R}^ {m \\times n}$和$B \\in \\mathbb{R}^ {n \\times m}$（因此$AB \\in \\mathbb{R}^ {m \\times m}$是方阵）。 观察到$BA \\in \\mathbb{R}^ {n \\times n}$也是一个方阵，因此对它们进行迹的运算是有意义的。 要证明$\\operatorname{tr}AB = \\operatorname{tr}BA$，请注意： \\begin{aligned} \\operatorname{tr} A B &=\\sum_{i=1}^{m}(A B)_{i i}=\\sum_{i=1}^{m}\\left(\\sum_{j=1}^{n} A_{i j} B_{j i}\\right) \\\\ &=\\sum_{i=1}^{m} \\sum_{j=1}^{n} A_{i j} B_{j i}=\\sum_{j=1}^{n} \\sum_{i=1}^{m} B_{j i} A_{i j} \\\\ &=\\sum_{j=1}^{n}\\left(\\sum_{i=1}^{m} B_{j i} A_{i j}\\right)=\\sum_{j=1}^{n}(B A)_{j j}=\\operatorname{tr} B A \\end{aligned}这里，第一个和最后两个等式使用迹运算符和矩阵乘法的定义，重点在第四个等式，使用标量乘法的可交换性来反转每个乘积中的项的顺序，以及标量加法的可交换性和相关性，以便重新排列求和的顺序。 3.5 范数向量的范数$|x|$是非正式度量的向量的“长度” 。 例如，我们有常用的欧几里德或$\\ell_{2}$范数， \\|x\\|_{2}=\\sqrt{\\sum_{i=1}^{n} x_{i}^{2}}注意：$|x|_{2}^{2}=x^{T} x$ 更正式地，范数是满足4个属性的函数（$f : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$）： 对于所有的 $x \\in \\mathbb{R}^ {n}$, $f(x) \\geq 0 $(非负). 当且仅当$x = 0$ 时，$f(x) = 0$ (明确性). 对于所有$x \\in \\mathbb{R}^ {n}$,$t\\in \\mathbb{R}$，则 $f(tx) = \\left| t \\right|f(x)$ (正齐次性). 对于所有 $x,y \\in \\mathbb{R}^ {n}$, $f(x + y) \\leq f(x) + f(y)$ (三角不等式) 其他范数的例子是$\\ell_1$范数: \\|x\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}\\right|和$\\ell_{\\infty }$范数： \\|x\\|_{\\infty}=\\max _{i}\\left|x_{i}\\right|事实上，到目前为止所提出的所有三个范数都是$\\ell_p$范数族的例子，它们由实数$p \\geq 1$参数化，并定义为： \\|x\\|_{p}=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}也可以为矩阵定义范数，例如Frobenius范数: \\|A\\|_{F}=\\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} A_{i j}^{2}}=\\sqrt{\\operatorname{tr}\\left(A^{T} A\\right)}许多其他更多的范数，但它们超出了这个复习材料的范围。 3.6 线性相关性和秩一组向量${x_1,x_2, \\cdots x_n} \\in \\mathbb{R}$， 如果没有向量可以表示为其余向量的线性组合，则称称该向量是线性无相关的。 相反，如果属于该组的一个向量可以表示为其余向量的线性组合，则称该向量是线性相关的。 也就是说，如果： x_{n}=\\sum_{i=1}^{n-1} \\alpha_{i} x_{i}对于某些标量值$\\alpha_1,\\cdots \\alpha_n-1 \\in \\mathbb{R}$，要么向量$x_1,x_2, \\cdots x_n$是线性相关的; 否则，向量是线性无关的。 例如，向量： x_{1}=\\left[\\begin{array}{l}{1} \\\\ {2} \\\\ {3}\\end{array}\\right] \\quad x_{2}=\\left[\\begin{array}{c}{4} \\\\ {1} \\\\ {5}\\end{array}\\right] \\quad x_{3}=\\left[\\begin{array}{c}{2} \\\\ {-3} \\\\ {-1}\\end{array}\\right]是线性相关的，因为：$x_3=-2x_1+x_2$。 矩阵$A \\in \\mathbb{R}^{m \\times n}$的列秩是构成线性无关集合的$A$的最大列子集的大小。 由于术语的多样性，这通常简称为$A$的线性无关列的数量。同样，行秩是构成线性无关集合的$A$的最大行数。 对于任何矩阵$A \\in \\mathbb{R}^{m \\times n}$，事实证明$A$的列秩等于$A$的行秩（尽管我们不会证明这一点），因此两个量统称为$A$的秩，用 $\\text{rank}(A)$表示。 以下是秩的一些基本属性： 对于 $A \\in \\mathbb{R}^{m \\times n}$，$\\text{rank}(A) \\leq min(m, n)$，如果$ \\text(A) = \\text{min} (m, n)$，则： $A$ 被称作满秩。 对于 $A \\in \\mathbb{R}^{m \\times n}$， $\\text{rank}(A) = \\text{rank}(A^T)$ 对于 $A \\in \\mathbb{R}^{m \\times n}$,$B \\in \\mathbb{R}^{n \\times p}$ ,$\\text{rank}(AB) \\leq \\text{min} ( \\text{rank}(A), \\text{rank}(B))$ 对于 $A,B \\in \\mathbb{R}^{m \\times n}$，$\\text{rank}(A + B) \\leq \\text{rank}(A) + \\text{rank}(B)$ 3.7 方阵的逆方阵$A \\in \\mathbb{R}^{n \\times n}$的倒数表示为$A^{-1}$，并且是这样的独特矩阵: A^{-1}A=I=AA^{-1}请注意，并非所有矩阵都具有逆。 例如，非方形矩阵根据定义没有逆。 然而，对于一些方形矩阵$A$，可能仍然存在$A^{-1}$可能不存在的情况。 特别是，如果$A^{-1}$存在，我们说$A$是可逆的或非奇异的，否则就是不可逆或奇异的。为了使方阵A具有逆$A^{-1}$，则$A$必须是满秩。 我们很快就会发现，除了满秩之外，还有许多其它的充分必要条件。以下是逆的属性; 假设$A,B \\in \\mathbb{R}^{n \\times n}$，而且是非奇异的： $(A^{-1})^{-1} = A$ $(AB)^{-1} = B^{-1}A^{-1}$ $(A^{-1})^{T} =(A^{T})^{-1} $因此，该矩阵通常表示为$A^{-T}$。作为如何使用逆的示例，考虑线性方程组，$Ax = b$，其中$A \\in \\mathbb{R}^{n \\times n}$，$x,b\\in \\mathbb{R}$， 如果$A$是非奇异的（即可逆的），那么$x = A^{-1}b$。 （如果$A \\in \\mathbb{R}^{m \\times n}$不是方阵，这公式还有用吗？） 3.8 正交阵如果 $x^Ty=0$，则两个向量$x,y\\in \\mathbb{R}^{n}$ 是正交的。如果$|x|_2=1$，则向量$x\\in \\mathbb{R}^{n}$ 被归一化。如果一个方阵$U\\in \\mathbb{R}^{n \\times n}$的所有列彼此正交并被归一化（这些列然后被称为正交），则方阵$U$是正交阵（注意在讨论向量时的意义不一样）。 它可以从正交性和正态性的定义中得出: U^ TU = I = U U^T换句话说，正交矩阵的逆是其转置。 注意，如果$U$不是方阵 :即，$U\\in \\mathbb{R}^{m \\times n}$，$n &lt;m$ ，但其列仍然是正交的，则$U^TU = I$，但是$UU^T \\neq I$。我们通常只使用术语”正交”来描述先前的情况 ，其中$U$是方阵。正交矩阵的另一个好的特性是在具有正交矩阵的向量上操作不会改变其欧几里德范数，即: \\|U x\\|_{2}=\\|x\\|_{2}对于任何 $x\\in \\mathbb{R}$ , $U\\in \\mathbb{R}^{n}$是正交的。 3.9 矩阵的值域和零空间一组向量$\\{x_{1}, \\ldots x_{n}\\}$是可以表示为$\\{x_{1}, \\ldots x_{n}\\}$的线性组合的所有向量的集合。 即： \\operatorname{span}\\left(\\left\\{x_{1}, \\ldots x_{n}\\right\\}\\right)=\\left\\{v : v=\\sum_{i=1}^{n} \\alpha_{i} x_{i}, \\quad \\alpha_{i} \\in \\mathbb{R}\\right\\}可以证明，如果$\\{x_{1}, \\ldots x_{n}\\}$是一组$n$个线性无关的向量，其中每个$x_i \\in \\mathbb{R}^{n}$，则$\\text{span}(\\{x_{1}, \\ldots x_{n}\\})=\\mathbb{R}^{n}$。 换句话说，任何向量$v\\in \\mathbb{R}^{n}$都可以写成$x_1$到$x_n$的线性组合。 向量$y\\in \\mathbb{R}^{m}$投影到$\\{x_{1}, \\ldots x_{n}\\}$（这里我们假设$x_i \\in \\mathbb{R}^{m}$）得到向量$v \\in \\operatorname{span}(\\{x_{1}, \\ldots, x_{n}\\})$，由欧几里德范数$|v - y|_2$可以得知，这样$v$尽可能接近$y$。 我们将投影表示为$\\operatorname{Proj}\\left(y ;\\left\\{x_{1}, \\ldots x_{n}\\right\\}\\right)$，并且可以将其正式定义为: \\operatorname{Proj}\\left(y ;\\left\\{x_{1}, \\ldots x_{n}\\right\\}\\right)=\\operatorname{argmin}_{v \\in \\operatorname{span}\\left(\\left\\{x_{1}, \\ldots, x_{n}\\right\\}\\right)}\\|y-v\\|_{2}矩阵$A\\in \\mathbb{R}^{m \\times n}$的值域（有时也称为列空间），表示为$\\mathcal{R}(A)$，是$A$列的跨度。换句话说， \\mathcal{R}(A)=\\left\\{v \\in \\mathbb{R}^{m} : v=A x, x \\in \\mathbb{R}^{n}\\right\\}做一些技术性的假设（即$A$是满秩且$n &lt;m$），向量$y \\in \\mathbb{R}^{m}$到$A$的范围的投影由下式给出: \\operatorname{Proj}(y ; A)=\\operatorname{argmin}_{v \\in \\mathcal{R}(A)}\\|v-y\\|_{2}=A\\left(A^{T} A\\right)^{-1} A^{T} y这个最后的方程应该看起来非常熟悉，因为它几乎与我们在课程中（我们将很快再次得出）得到的公式：用于参数的最小二乘估计一样。 看一下投影的定义，显而易见，这实际上是我们在最小二乘问题中最小化的目标（除了范数的平方这里有点不一样，这不会影响找到最优解），所以这些问题自然是非常相关的。 当$A$只包含一列时，$a \\in \\mathbb{R}^{m}$，这给出了向量投影到一条线上的特殊情况： \\operatorname{Proj}(y ; a)=\\frac{a a^{T}}{a^{T} a} y一个矩阵$A\\in \\mathbb{R}^{m \\times n}$的零空间 $\\mathcal{N}(A)$ 是所有乘以$A$时等于0向量的集合，即： \\mathcal{N}(A)=\\left\\{x \\in \\mathbb{R}^{n} : A x=0\\right\\}注意，$\\mathcal{R}(A)$中的向量的大小为$m$，而 $\\mathcal{N}(A)$ 中的向量的大小为$n$，因此$\\mathcal{R}(A^T)$和 $\\mathcal{N}(A)$ 中的向量的大小均为$\\mathbb{R}^{n}$。 事实上，还有很多例子。 证明： \\left\\{w : w=u+v, u \\in \\mathcal{R}\\left(A^{T}\\right), v \\in \\mathcal{N}(A)\\right\\}=\\mathbb{R}^{n} \\text { and } \\mathcal{R}\\left(A^{T}\\right) \\cap \\mathcal{N}(A)=\\{\\mathbf{0}\\}换句话说，$\\mathcal{R}(A^T)$和 $\\mathcal{N}(A)$ 是不相交的子集，它们一起跨越$\\mathbb{R}^{n}$的整个空间。 这种类型的集合称为正交补，我们用$\\mathcal{R}(A^T)= \\mathcal{N}(A)^{\\perp}$表示。 3.10 行列式一个方阵$A \\in \\mathbb{R}^{n \\times n}$的行列式是函数$\\text {det}$：$\\mathbb{R}^{n \\times n} \\rightarrow \\mathbb{R}^{n} $，并且表示为$\\left| A \\right|$。 或者$\\text{det} A$（有点像迹运算符，我们通常省略括号）。 从代数的角度来说，我们可以写出一个关于$A$行列式的显式公式。 因此，我们首先提供行列式的几何解释，然后探讨它的一些特定的代数性质。 给定一个矩阵： \\left[\\begin{array}{cccc}{-} & {a_{1}^{T}} & {-} \\\\ {-} & {a_{2}^{T}} & {-} \\\\ {} & {\\vdots} & {} \\\\ {-} & {a_{n}^{T}} & {-}\\end{array}\\right]考虑通过采用$A$行向量$a_{1}, \\ldots a_{n}\\in \\mathbb{R}^{n}$的所有可能线性组合形成的点$S \\subset \\mathbb{R}^{n}$的集合，其中线性组合的系数都在0和1之间; 也就是说，集合$S$是$\\text{span}(\\{a_{1}, \\ldots a_{n}\\})$受到系数$a_{1}, \\ldots a_{n}$的限制的线性组合，$\\alpha_1, \\cdots ,\\alpha_n$满足$0 \\leq \\alpha_{i} \\leq 1, i=1, \\ldots, n$。从形式上看， S=\\left\\{v \\in \\mathbb{R}^{n} : v=\\sum_{i=1}^{n} \\alpha_{i} a_{i} \\text { where } 0 \\leq \\alpha_{i} \\leq 1, i=1, \\ldots, n\\right\\}事实证明，$A$的行列式的绝对值是对集合$S$的“体积”的度量。 比方说：一个$2 \\times2$的矩阵(4)： A=\\left[\\begin{array}{ll}{1} & {3} \\\\ {3} & {2}\\end{array}\\right]它的矩阵的行是： a_{1}=\\left[\\begin{array}{l}{1} \\\\ {3}\\end{array}\\right] \\quad a_{2}=\\left[\\begin{array}{l}{3} \\\\ {2}\\end{array}\\right]对应于这些行对应的集合$S$如图1所示。对于二维矩阵，$S$通常具有平行四边形的形状。 在我们的例子中，行列式的值是$\\left| A \\right| = -7$（可以使用本节后面显示的公式计算），因此平行四边形的面积为7。（请自己验证！） 在三维中，集合$S$对应于一个称为平行六面体的对象（一个有倾斜边的三维框，这样每个面都有一个平行四边形）。行定义$S$的$3×3$矩阵S的行列式的绝对值给出了平行六面体的三维体积。在更高的维度中，集合$S$是一个称为$n$维平行切的对象。 图1：（4）中给出的$2×2$矩阵$A$的行列式的图示。 这里，$a_1$和$a_2$是对应于$A$行的向量，并且集合$S$对应于阴影区域（即，平行四边形）。 这个行列式的绝对值，$\\left| \\text{det} A \\right| = 7$，即平行四边形的面积。 在代数上，行列式满足以下三个属性（所有其他属性都遵循这些属性，包括通用公式）： 恒等式的行列式为1, $\\left| I \\right|= 1$（几何上，单位超立方体的体积为1）。 给定一个矩阵 $A \\in \\mathbb{R}^{n \\times n}$, 如果我们将$A$中的一行乘上一个标量$t \\in \\mathbb{R}$，那么新矩阵的行列式是$t\\left| A \\right|$ \\left|\\left[\\begin{array}{ccc}{-} & {t a_{1}^{T}} & {-} \\\\ {-} & {a_{2}^{T}} & {-} \\\\ {} & {\\vdots} & {} \\\\ {} & {a_{m}^{T}} & {-}\\end{array}\\right]\\right|=t|A|几何上，将集合$S$的一个边乘以系数$t$，体积也会增加一个系数$t$。 如果我们交换任意两行在$a_i^T$和$a_j^T$，那么新矩阵的行列式是$-\\left| A \\right|$，例如： \\left|\\left[\\begin{array}{ccc}{-} & {a_{2}^{T}} & {-} \\\\ {-} & {a_{1}^{T}} & {-} \\\\ {} & {\\vdots} & {} \\\\ {-} & {a_{m}^{T}} & {-}\\end{array}\\right]\\right|=-|A|你一定很奇怪，满足上述三个属性的函数的存在并不多。事实上，这样的函数确实存在，而且是唯一的（我们在这里不再证明了）。 从上述三个属性中得出的几个属性包括： 对于 $A \\in \\mathbb{R}^{n \\times n}$, $\\left| A \\right| = \\left| A^T \\right|$ 对于 $A,B \\in \\mathbb{R}^{n \\times n}$, $\\left| AB \\right|= \\left| A \\right|\\left| B \\right|$ 对于 $A \\in \\mathbb{R}^{n \\times n}$, 有且只有当$A$是奇异的（比如不可逆） ，则：$\\left| A \\right|= 0$ 对于 $A \\in \\mathbb{R}^{n \\times n}$ 同时，$A$为非奇异的，则：$\\left| A ^{−1}\\right| = 1/\\left| A \\right|$ 在给出行列式的一般定义之前，我们定义，对于$A \\in \\mathbb{R}^{n \\times n}$，$A_{\\backslash i, \\backslash j}\\in \\mathbb{R}^{(n-1) \\times (n-1)}$是由于删除第$i$行和第$j$列而产生的矩阵。 行列式的一般（递归）公式是： \\begin{aligned}|A| &=\\sum_{i=1}^{n}(-1)^{i+j} a_{i j}\\left|A_{\\backslash i, \\backslash j}\\right| \\quad(\\text { for any } j \\in 1, \\ldots, n) \\\\ &=\\sum_{j=1}^{n}(-1)^{i+j} a_{i j}\\left|A_{\\backslash i, \\backslash j}\\right| \\quad(\\text { for any } i \\in 1, \\ldots, n) \\end{aligned}对于 $A \\in \\mathbb{R}^{1 \\times 1}$，初始情况为$\\left| A \\right|= a_{11}$。如果我们把这个公式完全展开为 $A \\in \\mathbb{R}^{n \\times n}$，就等于$n!$（$n$阶乘）不同的项。因此，对于大于$3×3$的矩阵，我们几乎没有明确地写出完整的行列式方程。然而，$3×3$大小的矩阵的行列式方程是相当常见的，建议好好地了解它们： \\left|\\left[a_{11}\\right]\\right|=a_{11} \\left|\\left[\\begin{array}{ll}{a_{11}} & {a_{12}} \\\\ {a_{21}} & {a_{22}}\\end{array}\\right]\\right|=a_{11} a_{22}-a_{12} a_{21} \\left|\\left[\\begin{array}{l}{a_{11}} & {a_{12}} & {a_{13}} \\\\ {a_{21}} & {a_{22}} & {a_{23}} \\\\ {a_{31}} & {a_{32}} & {a_{33}}\\end{array}\\right]\\right|=\\quad \\begin{array}{c}{a_{11} a_{22} a_{33}+a_{12} a_{23} a_{31}+a_{13} a_{21} a_{32}} \\\\\\quad \\quad {-a_{11} a_{23} a_{32}-a_{12} a_{21} a_{33}-a_{13} a_{22} a_{31}} \\\\ {}\\end{array}矩阵$A \\in \\mathbb{R}^{n \\times n}$的经典伴随矩阵（通常称为伴随矩阵）表示为$\\operatorname{adj}(A)$，并定义为： \\operatorname{adj}(A) \\in \\mathbb{R}^{n \\times n}, \\quad(\\operatorname{adj}(A))_{i j}=(-1)^{i+j}\\left|A_{\\backslash j, \\backslash i}\\right|（注意索引$A_{\\backslash j, \\backslash i}$中的变化）。可以看出，对于任何非奇异$A \\in \\mathbb{R}^{n \\times n}$， A^{-1}=\\frac{1}{|A|} \\operatorname{adj}(A)虽然这是一个很好的“显式”的逆矩阵公式，但我们应该注意，从数字上讲，有很多更有效的方法来计算逆矩阵。 3.11 二次型和半正定矩阵给定方矩阵$A \\in \\mathbb{R}^{n \\times n}$和向量$x \\in \\mathbb{R}^{n}$，标量值$x^T Ax$被称为二次型。 写得清楚些，我们可以看到： x^{T} A x=\\sum_{i=1}^{n} x_{i}(A x)_{i}=\\sum_{i=1}^{n} x_{i}\\left(\\sum_{j=1}^{n} A_{i j} x_{j}\\right)=\\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{i j} x_{i} x_{j}注意： x^{T} A x=\\left(x^{T} A x\\right)^{T}=x^{T} A^{T} x=x^{T}\\left(\\frac{1}{2} A+\\frac{1}{2} A^{T}\\right) x第一个等号的是因为是标量的转置与自身相等，而第二个等号是因为是我们平均两个本身相等的量。 由此，我们可以得出结论，只有$A$的对称部分有助于形成二次型。 出于这个原因，我们经常隐含地假设以二次型出现的矩阵是对称阵。我们给出以下定义： 对于所有非零向量$x \\in \\mathbb{R}^n$，$x^TAx&gt;0$，对称阵$A \\in \\mathbb{S}^n$为正定（positive definite,PD）。这通常表示为$A\\succ0$（或$A&gt;0$），并且通常将所有正定矩阵的集合表示为$\\mathbb{S}_{++}^n$。 对于所有向量$x^TAx\\geq 0$，对称矩阵$A \\in \\mathbb{S}^n$是半正定(positive semidefinite ,PSD)。 这写为（或$A \\succeq 0$仅$A≥0$），并且所有半正定矩阵的集合通常表示为$\\mathbb{S}_+^n$。 同样，对称矩阵$A \\in \\mathbb{S}^n$是负定（negative definite,ND），如果对于所有非零$x \\in \\mathbb{R}^n$，则$x^TAx &lt;0$表示为$A\\prec0$（或$A &lt;0$）。 类似地，对称矩阵$A \\in \\mathbb{S}^n$是半负定(negative semidefinite,NSD），如果对于所有$x \\in \\mathbb{R}^n$，则$x^TAx \\leq 0$表示为$A\\preceq 0$（或$A≤0$）。 最后，对称矩阵$A \\in \\mathbb{S}^n$是不定的，如果它既不是正半定也不是负半定，即，如果存在$x_1,x_2 \\in \\mathbb{R}^n$，那么$x_1^TAx_1&gt;0$且$x_2^TAx_2&lt;0$。 很明显，如果$A$是正定的，那么$−A$是负定的，反之亦然。同样，如果$A$是半正定的，那么$−A$是是半负定的，反之亦然。如果果$A$是不定的，那么$−A$是也是不定的。 正定矩阵和负定矩阵的一个重要性质是它们总是满秩，因此是可逆的。为了了解这是为什么，假设某个矩阵$A \\in \\mathbb{S}^n$不是满秩。然后，假设$A$的第$j$列可以表示为其他$n-1$列的线性组合： a_{j}=\\sum_{i \\neq j} x_{i} a_{i}对于某些$x_1,\\cdots x_{j-1},x_{j + 1} ,\\cdots ,x_n\\in \\mathbb{R}$。设$x_j = -1$，则： Ax=\\sum_{i \\neq j} x_{i} a_{i}=0但这意味着对于某些非零向量$x$，$x^T Ax = 0$，因此$A$必须既不是正定也不是负定。如果$A$是正定或负定，则必须是满秩。最后，有一种类型的正定矩阵经常出现，因此值得特别提及。 给定矩阵$A \\in \\mathbb{R}^{m \\times n}$（不一定是对称或偶数平方），矩阵$G = A^T A$（有时称为Gram矩阵）总是半正定的。 此外，如果$m\\geq n$（同时为了方便起见，我们假设$A$是满秩），则$G = A^T A$是正定的。 3.12 特征值和特征向量给定一个方阵$A \\in\\mathbb{R}^{n\\times n}$，我们认为在以下条件下，$\\lambda \\in\\mathbb{C}$是$A$的特征值，$x\\in\\mathbb{C}^n$是相应的特征向量： Ax=\\lambda x,x \\ne 0直观地说，这个定义意味着将$A$乘以向量$x$会得到一个新的向量，该向量指向与$x$相同的方向，但按系数$\\lambda$缩放。值得注意的是，对于任何特征向量$x\\in\\mathbb{C}^n$和标量$t\\in\\mathbb{C}$，$A(cx)=cAx=c\\lambda x=\\lambda(cx)$，$cx$也是一个特征向量。因此，当我们讨论与$\\lambda$相关的特征向量时，我们通常假设特征向量被标准化为长度为1（这仍然会造成一些歧义，因为$x$和$−x$都是特征向量，但我们必须接受这一点）。 我们可以重写上面的等式来说明$(\\lambda,x)$是$A$的特征值和特征向量的组合： (\\lambda I-A)x=0,x \\ne 0但是$(\\lambda I-A)x=0$只有当$(\\lambda I-A)$有一个非空零空间时，同时$(\\lambda I-A)$是奇异的，$x$才具有非零解，即： |(\\lambda I-A)|=0现在，我们可以使用行列式的先前定义将表达式$|(\\lambda I-A)|$扩展为$\\lambda$中的（非常大的）多项式，其中，$\\lambda$的度为$n$。它通常被称为矩阵$A$的特征多项式。 然后我们找到这个特征多项式的$n$（可能是复数）根，并用$\\lambda_1,\\cdots,\\lambda_n$表示。这些都是矩阵$A$的特征值，但我们注意到它们可能不明显。为了找到特征值$\\lambda_i$对应的特征向量，我们只需解线性方程$(\\lambda I-A)x=0$，因为$(\\lambda I-A)$是奇异的，所以保证有一个非零解（但也可能有多个或无穷多个解）。 应该注意的是，这不是实际用于数值计算特征值和特征向量的方法（记住行列式的完全展开式有$n!$项），这是一个数学上的争议。 以下是特征值和特征向量的属性（所有假设在$A \\in\\mathbb{R}^{n\\times n}$具有特征值$\\lambda_1,\\cdots,\\lambda_n$的前提下）： $A$的迹等于其特征值之和 \\operatorname{tr} A=\\sum_{i=1}^{n} \\lambda_{i} $A$的行列式等于其特征值的乘积 |A|=\\prod_{i=1}^{n} \\lambda_{i} $A$的秩等于$A$的非零特征值的个数 假设$A$非奇异，其特征值为$\\lambda$和特征向量为$x$。那么$1/\\lambda$是具有相关特征向量$x$的$A^{-1}$的特征值，即$A^{-1}x=(1/\\lambda)x$。（要证明这一点，取特征向量方程，$Ax=\\lambda x$，两边都左乘$A^{-1}$） 对角阵的特征值$d=diag(d_1，\\cdots,d_n)$实际上就是对角元素$d_1，\\cdots,d_n$ 3.13 对称矩阵的特征值和特征向量通常情况下，一般的方阵的特征值和特征向量的结构可以很细微地表示出来。值得庆幸的是，在机器学习的大多数场景下，处理对称实矩阵就足够了，其处理的对称实矩阵的特征值和特征向量具有显着的特性。 在本节中，我们假设$A$是实对称矩阵, 具有以下属性： $A$的所有特征值都是实数。 我们用用$\\lambda_1,\\cdots,\\lambda_n$表示。 存在一组特征向量$u_1，\\cdots u_n$，对于所有$i$，$u_i$是具有特征值$\\lambda_{i}$和$b$的特征向量。$u_1，\\cdots u_n$是单位向量并且彼此正交。 设$U$是包含$u_i$作为列的正交矩阵： U=\\left[\\begin{array}{cccc}{ |} & { |} & {} & { |} \\\\ {u_{1}} & {u_{2}} & {\\cdots} & {u_{n}} \\\\ { |} & { |} & {} & { |}\\end{array}\\right]设$\\Lambda= diag(\\lambda_1,\\cdots,\\lambda_n)$是包含$\\lambda_1,\\cdots,\\lambda_n$作为对角线上的元素的对角矩阵。 使用2.3节的方程（2）中的矩阵 - 矩阵向量乘法的方法，我们可以验证： A U=\\left[\\begin{array}{cccc}{ |} & { |} & {} & { |} \\\\ {A u_{1}} & {A u_{2}} & {\\cdots} & {A u_{n}} \\\\ { |} & { |} & {} & { |}\\end{array}\\right]=\\left[\\begin{array}{ccc}{ |} & { |} & { |} & { |}\\\\ {\\lambda_{1} u_{1}} & {\\lambda_{2} u_{2}} & {\\cdots} & {\\lambda_{n} u_{n}} \\\\ { |} & { |} & {|} & { |}\\end{array}\\right]=U \\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{n}\\right)=U \\Lambda考虑到正交矩阵$U$满足$UU^T=I$，利用上面的方程，我们得到： A=AUU^T=U\\Lambda U^T这种$A$的新的表示形式为$U\\Lambda U^T$，通常称为矩阵$A$的对角化。术语对角化是这样来的：通过这种表示，我们通常可以有效地将对称矩阵$A$视为对角矩阵 , 这更容易理解。关于由特征向量$U$定义的基础， 我们将通过几个例子详细说明。 背景知识：代表另一个基的向量。 任何正交矩阵$U=\\left[\\begin{array}{cccc}{ |} &amp; { |} &amp; {} &amp; { |} \\ {u_{1}} &amp; {u_{2}} &amp; {\\cdots} &amp; {u_{n}} \\ { |} &amp; { |} &amp; {} &amp; { |}\\end{array}\\right]$定义了一个新的属于$\\mathbb {R}^{n}$的基（坐标系），意义如下：对于任何向量$x \\in\\mathbb{R}^{n}$都可以表示为$u_1，\\cdots u_n$的线性组合，其系数为$x_1,\\cdots x_n$： x=\\hat x_1u_1+\\cdots +\\cdots \\hat x_nu_n=U\\hat x在第二个等式中，我们使用矩阵和向量相乘的方法。 实际上，这种$\\hat x$是唯一存在的: x=U \\hat{x} \\Leftrightarrow U^{T} x=\\hat{x}换句话说，向量$\\hat x=U^Tx$可以作为向量$x$的另一种表示，与$U$定义的基有关。 “对角化”矩阵向量乘法。 通过上面的设置，我们将看到左乘矩阵$A$可以被视为左乘以对角矩阵关于特征向量的基。 假设$x$是一个向量，$\\hat x$表示$U$的基。设$z=Ax$为矩阵向量积。现在让我们计算关于$U$的基$z$：然后，再利用$UU^T=U^T=I$和方程$A=AUU^T=U\\Lambda U^T$，我们得到： \\hat{z}=U^{T} z=U^{T} A x=U^{T} U \\Lambda U^{T} x=\\Lambda \\hat{x}=\\left[\\begin{array}{c}{\\lambda_{1} \\hat{x}_{1}} \\\\ {\\lambda_{2} \\hat{x}_{2}} \\\\ {\\vdots} \\\\ {\\lambda_{n} \\hat{x}_{n}}\\end{array}\\right]我们可以看到，原始空间中的左乘矩阵$A$等于左乘对角矩阵$\\Lambda$相对于新的基，即仅将每个坐标缩放相应的特征值。在新的基上，矩阵多次相乘也变得简单多了。例如，假设$q=AAAx$。根据$A$的元素导出$q$的分析形式，使用原始的基可能是一场噩梦，但使用新的基就容易多了： \\hat{q}=U^{T} q=U^{T} AAA x=U^{T} U \\Lambda U^{T} U \\Lambda U^{T} U \\Lambda U^{T} x=\\Lambda^{3} \\hat{x}=\\left[\\begin{array}{c}{\\lambda_{1}^{3} \\hat{x}_{1}} \\\\ {\\lambda_{2}^{3} \\hat{x}_{2}} \\\\ {\\vdots} \\\\ {\\lambda_{n}^{3} \\hat{x}_{n}}\\end{array}\\right]“对角化”二次型。作为直接的推论，二次型$x^TAx$也可以在新的基上简化。 x^{T} A x=x^{T} U \\Lambda U^{T} x=\\hat{x} \\Lambda \\hat{x}=\\sum_{i=1}^{n} \\lambda_{i} \\hat{x}_{i}^{2}(回想一下，在旧的表示法中，$x^{T} A x=\\sum_{i=1, j=1}^{n} x_{i} x_{j} A_{i j}$涉及一个$n^2$项的和，而不是上面等式中的$n$项。)利用这个观点，我们还可以证明矩阵$A$的正定性完全取决于其特征值的符号： 如果所有的$\\lambda_i&gt;0$，则矩阵$A$正定的，因为对于任意的$\\hat x \\ne 0$,$x^{T} A x=\\sum_{i=1}^{n} \\lambda_{i} \\hat{x}_{i}^{2}&gt;0$ 如果所有的$\\lambda_i\\geq 0$，则矩阵$A$是为正半定，因为对于任意的$\\hat x $,$x^{T} A x=\\sum_{i=1}^{n} \\lambda_{i} \\hat{x}_{i}^{2} \\geq 0$ 同样，如果所有$\\lambda_i&lt;0$或$\\lambda_i\\leq 0$，则矩阵$A$分别为负定或半负定。 最后，如果$A$同时具有正特征值和负特征值，比如λ$\\lambda_i&gt;0$和$\\lambda_j0$ ,我们让$\\hat x$满足$\\hat x_i=1$和$\\hat x_k=0$，同时所有的$k\\ne i$，那么$x^{T} A x=\\sum_{i=1}^{n} \\lambda_{i} \\hat{x}_{i}^{2}&lt;0$ 特征值和特征向量经常出现的应用是最大化矩阵的某些函数。特别是对于矩阵$A \\in \\mathbb{S}^{n}$，考虑以下最大化问题： \\max _{x \\in \\mathbb{R}^{n}} \\ x^{T} A x=\\sum_{i=1}^{n} \\lambda_{i} \\hat{x}_{i}^{2} \\quad \\text { subject to }\\|x\\|_{2}^{2}=1也就是说，我们要找到（范数1）的向量，它使二次型最大化。假设特征值的阶数为$\\lambda_1 \\geq \\lambda _2 \\geq \\cdots \\lambda_n$，此优化问题的最优值为$\\lambda_1$，且与$\\lambda_1$对应的任何特征向量$u_1$都是最大值之一。（如果$\\lambda_1 &gt; \\lambda_2$，那么有一个与特征值$\\lambda_1$对应的唯一特征向量，它是上面那个优化问题的唯一最大值。）我们可以通过使用对角化技术来证明这一点：注意，通过公式$|U x|_{2}=|x|_{2}$推出$|x|_{2}=|\\hat{x}|_{2}$，并利用公式： $x^{T} A x=x^{T} U \\Lambda U^{T} x=\\hat{x} \\Lambda \\hat{x}=\\sum_{i=1}^{n} \\lambda_{i} \\hat{x}_{i}^{2}$，我们可以将上面那个优化问题改写为： \\max _{\\hat{x} \\in \\mathbb{R}^{n}}\\ \\hat{x}^{T} \\Lambda \\hat{x}=\\sum_{i=1}^{n} \\lambda_{i} \\hat{x}_{i}^{2} \\quad \\text { subject to }\\|\\hat{x}\\|_{2}^{2}=1然后，我们得到目标的上界为$\\lambda_1$： \\hat{x}^{T} \\Lambda \\hat{x}=\\sum_{i=1}^{n} \\lambda_{i} \\hat{x}_{i}^{2} \\leq \\sum_{i=1}^{n} \\lambda_{1} \\hat{x}_{i}^{2}=\\lambda_{1}此外，设置$\\hat{x}=\\left[\\begin{array}{c}{1} \\ {0} \\ {\\vdots} \\ {0}\\end{array}\\right]$可让上述等式成立，这与设置$x=u_1$相对应。 4.矩阵微积分虽然前面章节中的主题通常包含在线性代数的标准课程中，但似乎很少涉及（我们将广泛使用）的一个主题是微积分扩展到向量设置展。尽管我们使用的所有实际微积分都是相对微不足道的，但是符号通常会使事情看起来比实际困难得多。 在本节中，我们将介绍矩阵微积分的一些基本定义，并提供一些示例。 4.1 梯度假设$f: \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}$是将维度为$m \\times n$的矩阵$A\\in \\mathbb{R}^{m \\times n}$作为输入并返回实数值的函数。 然后$f$的梯度（相对于$A\\in \\mathbb{R}^{m \\times n}$）是偏导数矩阵，定义如下： \\nabla_{A} f(A) \\in \\mathbb{R}^{m \\times n}=\\left[\\begin{array}{cccc}{\\frac{\\partial f(A)}{\\partial A_{11}}} & {\\frac{\\partial f(A)}{\\partial A_{12}}} & {\\cdots} & {\\frac{\\partial f(A)}{\\partial A_{1n}}} \\\\ {\\frac{\\partial f(A)}{\\partial A_{21}}} & {\\frac{\\partial f(A)}{\\partial A_{22}}} & {\\cdots} & {\\frac{\\partial f(A)}{\\partial A_{2 n}}} \\\\ {\\vdots} & {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial f(A)}{\\partial A_{m 1}}} & {\\frac{\\partial f(A)}{\\partial A_{m 2}}} & {\\cdots} & {\\frac{\\partial f(A)}{\\partial A_{m n}}}\\end{array}\\right]即，$m \\times n$矩阵: \\left(\\nabla_{A} f(A)\\right)_{i j}=\\frac{\\partial f(A)}{\\partial A_{i j}}请注意，$\\nabla_{A} f(A) $的维度始终与$A$的维度相同。特殊情况，如果$A$只是向量$A\\in \\mathbb{R}^{n}$，则 \\nabla_{x} f(x)=\\left[\\begin{array}{c}{\\frac{\\partial f(x)}{\\partial x_{1}}} \\\\ {\\frac{\\partial f(x)}{\\partial x_{2}}} \\\\ {\\vdots} \\\\ {\\frac{\\partial f(x)}{\\partial x_{n}}}\\end{array}\\right]重要的是要记住，只有当函数是实值时，即如果函数返回标量值，才定义函数的梯度。例如，$A\\in \\mathbb{R}^{m \\times n}$相对于$x$，我们不能取$Ax$的梯度，因为这个量是向量值。它直接从偏导数的等价性质得出： $\\nabla_{x}(f(x)+g(x))=\\nabla_{x} f(x)+\\nabla_{x} g(x)$ 对于$t \\in \\mathbb{R}$ ，$\\nabla_{x}(t f(x))=t \\nabla_{x} f(x)$ 原则上，梯度是偏导数对多变量函数的自然延伸。然而，在实践中，由于符号的原因，使用梯度有时是很困难的。例如，假设$A\\in \\mathbb{R}^{m \\times n}$是一个固定系数矩阵，假设$b\\in \\mathbb{R}^{m}$是一个固定系数向量。设$f: \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}$为$f(z)=z^Tz$定义的函数，因此$\\nabla_{z}f(z)=2z$。但现在考虑表达式， \\nabla f(Ax)该表达式应该如何解释？ 至少有两种可能性：1.在第一个解释中，回想起$\\nabla_{z}f(z)=2z$。 在这里，我们将$\\nabla f(Ax)$解释为评估点$Ax$处的梯度，因此: \\nabla f(A x)=2(A x)=2 A x \\in \\mathbb{R}^{m}2.在第二种解释中，我们将数量$f(Ax)$视为输入变量$x$的函数。 更正式地说，设$g(x) =f(Ax)$。 然后在这个解释中: \\nabla f(A x)=\\nabla_{x} g(x) \\in \\mathbb{R}^{n}在这里，我们可以看到这两种解释确实不同。 一种解释产生$m$维向量作为结果，而另一种解释产生$n$维向量作为结果！ 我们怎么解决这个问题？ 这里，关键是要明确我们要区分的变量。在第一种情况下，我们将函数$f$与其参数$z$进行区分，然后替换参数$Ax$。在第二种情况下，我们将复合函数$g(x)=f(Ax)$直接与$x$进行微分。 我们将第一种情况表示为$\\nabla zf(Ax)$，第二种情况表示为$\\nabla xf(Ax)$。 保持符号清晰是非常重要的，以后完成课程作业时候你就会发现。 4.2 黑塞矩阵假设$f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$是一个函数，它接受$\\mathbb{R}^{n}$中的向量并返回实数。那么关于$x$的黑塞矩阵（也有翻译作海森矩阵），写做：$\\nabla_x ^2 f(A x)$，或者简单地说，$H$是$n \\times n$矩阵的偏导数： \\nabla_{x}^{2} f(x) \\in \\mathbb{R}^{n \\times n}=\\left[\\begin{array}{cccc}{\\frac{\\partial^{2} f(x)}{\\partial x_{1}^{2}}} & {\\frac{\\partial^{2} f(x)}{\\partial x_{1} \\partial x_{2}}} & {\\cdots} & {\\frac{\\partial^{2} f(x)}{\\partial x_{1} \\partial x_{n}}} \\\\ {\\frac{\\partial^{2} f(x)}{\\partial x_{2} \\partial x_{1}}} & {\\frac{\\partial^{2} f(x)}{\\partial x_{2}^{2}}} & {\\cdots} & {\\frac{\\partial^{2} f(x)}{\\partial x_{2} \\partial x_{n}}} \\\\ {\\vdots} & {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial^{2} f(x)}{\\partial x_{n} \\partial x_{1}}} & {\\frac{\\partial^{2} f(x)}{\\partial x_{n} \\partial x_{2}}} & {\\cdots} & {\\frac{\\partial^{2} f(x)}{\\partial x_{n}^{2}}}\\end{array}\\right]换句话说，$\\nabla_{x}^{2} f(x) \\in \\mathbb{R}^{n \\times n}$，其： \\left(\\nabla_{x}^{2} f(x)\\right)_{i j}=\\frac{\\partial^{2} f(x)}{\\partial x_{i} \\partial x_{j}}注意：黑塞矩阵通常是对称阵： \\frac{\\partial^{2} f(x)}{\\partial x_{i} \\partial x_{j}}=\\frac{\\partial^{2} f(x)}{\\partial x_{j} \\partial x_{i}}与梯度相似，只有当$f(x)$为实值时才定义黑塞矩阵。 很自然地认为梯度与向量函数的一阶导数的相似，而黑塞矩阵与二阶导数的相似（我们使用的符号也暗示了这种关系）。 这种直觉通常是正确的，但需要记住以下几个注意事项。首先，对于一个变量$f: \\mathbb{R} \\rightarrow \\mathbb{R}$的实值函数，它的基本定义：二阶导数是一阶导数的导数，即： \\frac{\\partial^{2} f(x)}{\\partial x^{2}}=\\frac{\\partial}{\\partial x} \\frac{\\partial}{\\partial x} f(x)然而，对于向量的函数，函数的梯度是一个向量，我们不能取向量的梯度，即: \\nabla_{x} \\nabla_{x} f(x)=\\nabla_{x}\\left[\\begin{array}{c}{\\frac{\\partial f(x)}{\\partial x_{1}}} \\\\ {\\frac{\\partial f(x)}{\\partial x_{2}}} \\\\ {\\vdots} \\\\ {\\frac{\\partial f(x)}{\\partial x_{n}}}\\end{array}\\right]上面这个表达式没有意义。 因此，黑塞矩阵不是梯度的梯度。 然而，下面这种情况却这几乎是正确的：如果我们看一下梯度$\\left(\\nabla_{x} f(x)\\right)_{i}=\\partial f(x) / \\partial x_{i}$的第$i$个元素，并取关于于$x$的梯度我们得到： \\nabla_{x} \\frac{\\partial f(x)}{\\partial x_{i}}=\\left[\\begin{array}{c}{\\frac{\\partial^{2} f(x)}{\\partial x_{i} \\partial x_{1}}} \\\\ {\\frac{\\partial^{2} f(x)}{\\partial x_{2} \\partial x_{2}}} \\\\ {\\vdots} \\\\ {\\frac{\\partial f(x)}{\\partial x_{i} \\partial x_{n}}}\\end{array}\\right]这是黑塞矩阵第$i$行（列）,所以： \\nabla_{x}^{2} f(x)=\\left[\\nabla_{x}\\left(\\nabla_{x} f(x)\\right)_{1} \\quad \\nabla_{x}\\left(\\nabla_{x} f(x)\\right)_{2} \\quad \\cdots \\quad \\nabla_{x}\\left(\\nabla_{x} f(x)\\right)_{n}\\right]简单地说：我们可以说由于：$\\nabla_{x}^{2} f(x)=\\nabla_{x}\\left(\\nabla_{x} f(x)\\right)^{T}$，只要我们理解，这实际上是取$\\nabla_{x} f(x)$的每个元素的梯度，而不是整个向量的梯度。 最后，请注意，虽然我们可以对矩阵$A\\in \\mathbb{R}^{n}$取梯度，但对于这门课，我们只考虑对向量$x \\in \\mathbb{R}^{n}$取黑塞矩阵。这会方便很多（事实上，我们所做的任何计算都不要求我们找到关于矩阵的黑森方程），因为关于矩阵的黑塞方程就必须对矩阵所有元素求偏导数$\\partial^{2} f(A) /\\left(\\partial A_{i j} \\partial A_{k \\ell}\\right)$，将其表示为矩阵相当麻烦。 4.3 二次函数和线性函数的梯度和黑塞矩阵现在让我们尝试确定几个简单函数的梯度和黑塞矩阵。 应该注意的是，这里给出的所有梯度都是CS229讲义中给出的梯度的特殊情况。 对于$x \\in \\mathbb{R}^{n}$, 设$f(x)=b^Tx$ 的某些已知向量$b \\in \\mathbb{R}^{n}$ ，则： f(x)=\\sum_{i=1}^{n} b_{i} x_{i}所以： \\frac{\\partial f(x)}{\\partial x_{k}}=\\frac{\\partial}{\\partial x_{k}} \\sum_{i=1}^{n} b_{i} x_{i}=b_{k}由此我们可以很容易地看出$\\nabla_{x} b^{T} x=b$。 这应该与单变量微积分中的类似情况进行比较，其中$\\partial /(\\partial x) a x=a$。现在考虑$A\\in \\mathbb{S}^{n}$的二次函数$f(x)=x^TAx$。 记住这一点： f(x)=\\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{i j} x_{i} x_{j}为了取偏导数，我们将分别考虑包括$x_k$和$x_2^k$因子的项： \\begin{aligned} \\frac{\\partial f(x)}{\\partial x_{k}} &=\\frac{\\partial}{\\partial x_{k}} \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{i j} x_{i} x_{j} \\\\ &=\\frac{\\partial}{\\partial x_{k}}\\left[\\sum_{i \\neq k} \\sum_{j \\neq k} A_{i j} x_{i} x_{j}+\\sum_{i \\neq k} A_{i k} x_{i} x_{k}+\\sum_{j \\neq k} A_{k j} x_{k} x_{j}+A_{k k} x_{k}^{2}\\right] \\\\ &=\\sum_{i \\neq k} A_{i k} x_{i}+\\sum_{j \\neq k} A_{k j} x_{j}+2 A_{k k} x_{k} \\\\ &=\\sum_{i=1}^{n} A_{i k} x_{i}+\\sum_{j=1}^{n} A_{k j} x_{j}=2 \\sum_{i=1}^{n} A_{k i} x_{i} \\end{aligned}最后一个等式，是因为$A$是对称的（我们可以安全地假设，因为它以二次形式出现）。 注意，$\\nabla_{x} f(x)$的第$k$个元素是$A$和$x$的第$k$行的内积。 因此，$\\nabla_{x} x^{T} A x=2 A x$。 同样，这应该提醒你单变量微积分中的类似事实，即$\\partial /(\\partial x) a x^{2}=2 a x$。 最后，让我们来看看二次函数$f(x)=x^TAx$黑塞矩阵（显然，线性函数$b^Tx$的黑塞矩阵为零）。在这种情况下: \\frac{\\partial^{2} f(x)}{\\partial x_{k} \\partial x_{\\ell}}=\\frac{\\partial}{\\partial x_{k}}\\left[\\frac{\\partial f(x)}{\\partial x_{\\ell}}\\right]=\\frac{\\partial}{\\partial x_{k}}\\left[2 \\sum_{i=1}^{n} A_{\\ell i} x_{i}\\right]=2 A_{\\ell k}=2 A_{k \\ell}因此，应该很清楚$\\nabla_{x}^2 x^{T} A x=2 A$，这应该是完全可以理解的（同样类似于$\\partial^2 /(\\partial x^2) a x^{2}=2a$的单变量事实）。 简要概括起来： $\\nabla_{x} b^{T} x=b$ $\\nabla_{x} x^{T} A x=2 A x$ (如果$A$是对称阵) $\\nabla_{x}^2 x^{T} A x=2 A $ (如果$A$是对称阵) 4.4 最小二乘法让我们应用上一节中得到的方程来推导最小二乘方程。假设我们得到矩阵$A\\in \\mathbb{R}^{m \\times n}$（为了简单起见，我们假设$A$是满秩）和向量$b\\in \\mathbb{R}^{m}$，从而使$b \\notin \\mathcal{R}(A)$。在这种情况下，我们将无法找到向量$x\\in \\mathbb{R}^{n}$，由于$Ax = b$，因此我们想要找到一个向量$x$，使得$Ax$尽可能接近 $b$，用欧几里德范数的平方$|A x-b|_{2}^{2} $来衡量。 使用公式$|x|^{2}=x^Tx$，我们可以得到： \\begin{aligned}\\|A x-b\\|_{2}^{2} &=(A x-b)^{T}(A x-b) \\\\ &=x^{T} A^{T} A x-2 b^{T} A x+b^{T} b \\end{aligned}根据$x$的梯度，并利用上一节中推导的性质： \\begin{aligned} \\nabla_{x}\\left(x^{T} A^{T} A x-2 b^{T} A x+b^{T} b\\right) &=\\nabla_{x} x^{T} A^{T} A x-\\nabla_{x} 2 b^{T} A x+\\nabla_{x} b^{T} b \\\\ &=2 A^{T} A x-2 A^{T} b \\end{aligned}将最后一个表达式设置为零，然后解出$x$，得到了正规方程： x = (A^TA)^{-1}A^Tb这和我们在课堂上得到的相同。 4.5 行列式的梯度现在让我们考虑一种情况，我们找到一个函数相对于矩阵的梯度，也就是说，对于$A\\in \\mathbb{R}^{n \\times n}$，我们要找到$\\nabla_{A}|A|$。回想一下我们对行列式的讨论： |A|=\\sum_{i=1}^{n}(-1)^{i+j} A_{i j}\\left|A_{\\backslash i, \\backslash j}\\right| \\quad(\\text { for any } j \\in 1, \\ldots, n)所以： \\frac{\\partial}{\\partial A_{k \\ell}}|A|=\\frac{\\partial}{\\partial A_{k \\ell}} \\sum_{i=1}^{n}(-1)^{i+j} A_{i j}\\left|A_{\\backslash i, \\backslash j}\\right|=(-1)^{k+\\ell}\\left|A_{\\backslash k,\\backslash \\ell}\\right|=(\\operatorname{adj}(A))_{\\ell k}从这里可以知道，它直接从伴随矩阵的性质得出： \\nabla_{A}|A|=(\\operatorname{adj}(A))^{T}=|A| A^{-T}现在我们来考虑函数$f : \\mathbb{S}_{++}^{n} \\rightarrow \\mathbb{R}$，$f(A)=\\log |A|$。注意，我们必须将$f$的域限制为正定矩阵，因为这确保了$|A|&gt;0$，因此$|A|$的对数是实数。在这种情况下，我们可以使用链式法则（没什么奇怪的，只是单变量演算中的普通链式法则）来看看： \\frac{\\partial \\log |A|}{\\partial A_{i j}}=\\frac{\\partial \\log |A|}{\\partial|A|} \\frac{\\partial|A|}{\\partial A_{i j}}=\\frac{1}{|A|} \\frac{\\partial|A|}{\\partial A_{i j}}从这一点可以明显看出： \\nabla_{A} \\log |A|=\\frac{1}{|A|} \\nabla_{A}|A|=A^{-1}我们可以在最后一个表达式中删除转置，因为$A$是对称的。注意与单值情况的相似性，其中$\\partial /(\\partial x) \\log x=1 / x$。 4.6 特征值优化最后，我们使用矩阵演算以直接导致特征值/特征向量分析的方式求解优化问题。 考虑以下等式约束优化问题： \\max _{x \\in \\mathbb{R}^{n}} x^{T} A x \\quad \\text { subject to }\\|x\\|_{2}^{2}=1对于对称矩阵$A\\in \\mathbb{S}^{n}$。求解等式约束优化问题的标准方法是采用拉格朗日形式，一种包含等式约束的目标函数，在这种情况下，拉格朗日函数可由以下公式给出： \\mathcal{L}(x, \\lambda)=x^{T} A x-\\lambda x^{T} x其中，$\\lambda $被称为与等式约束关联的拉格朗日乘子。可以确定，要使$x^$成为问题的最佳点，拉格朗日的梯度必须在$x^$处为零（这不是唯一的条件，但它是必需的）。也就是说， \\nabla_{x} \\mathcal{L}(x, \\lambda)=\\nabla_{x}\\left(x^{T} A x-\\lambda x^{T} x\\right)=2 A^{T} x-2 \\lambda x=0请注意，这只是线性方程$Ax =\\lambda x$。 这表明假设$x^T x = 1$，可能最大化（或最小化）$x^T Ax$的唯一点是$A$的特征向量。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]},{"title":"函数进阶","slug":"函数进阶","date":"2021-03-05T12:29:09.000Z","updated":"2021-03-05T12:30:03.128Z","comments":true,"path":"20210305/函数进阶.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E5%87%BD%E6%95%B0%E8%BF%9B%E9%98%B6.html","excerpt":"","text":"函数进阶目标 函数参数和返回值的作用 函数的返回值 进阶 函数的参数 进阶 递归函数 01. 函数参数和返回值的作用函数根据 有没有参数 以及 有没有返回值，可以 相互组合，一共有 4 种 组合形式 无参数，无返回值 无参数，有返回值 有参数，无返回值 有参数，有返回值 定义函数时，是否接收参数，或者是否返回结果，是根据 实际的功能需求 来决定的！ 如果函数 内部处理的数据不确定，就可以将外界的数据以参数传递到函数内部 如果希望一个函数 执行完成后，向外界汇报执行结果，就可以增加函数的返回值 1.1 无参数，无返回值此类函数，不接收参数，也没有返回值，应用场景如下： 只是单纯地做一件事情，例如 显示菜单 在函数内部 针对全局变量进行操作，例如：新建名片，最终结果 记录在全局变量 中 注意： 如果全局变量的数据类型是一个 可变类型，在函数内部可以使用 方法 修改全局变量的内容 —— 变量的引用不会改变 在函数内部，使用赋值语句 才会 修改变量的引用 1.2 无参数，有返回值此类函数，不接收参数，但是有返回值，应用场景如下： 采集数据，例如 温度计，返回结果就是当前的温度，而不需要传递任何的参数 1.3 有参数，无返回值此类函数，接收参数，没有返回值，应用场景如下： 函数内部的代码保持不变，针对 不同的参数 处理 不同的数据 例如 名片管理系统 针对 找到的名片 做 修改、删除 操作 1.4 有参数，有返回值此类函数，接收参数，同时有返回值，应用场景如下： 函数内部的代码保持不变，针对 不同的参数 处理 不同的数据，并且 返回期望的处理结果 例如 名片管理系统 使用 字典默认值 和 提示信息 提示用户输入内容 如果输入，返回输入内容 如果没有输入，返回字典默认值 02. 函数的返回值 进阶 在程序开发中，有时候，会希望 一个函数执行结束后，告诉调用者一个结果，以便调用者针对具体的结果做后续的处理 返回值 是函数 完成工作后，最后 给调用者的 一个结果 在函数中使用 return 关键字可以返回结果 调用函数一方，可以 使用变量 来 接收 函数的返回结果 问题：一个函数执行后能否返回多个结果？ 示例 —— 温度和湿度测量 假设要开发一个函数能够同时返回当前的温度和湿度 先完成返回温度的功能如下： 1234567891011def measure(): &quot;&quot;&quot;返回当前的温度&quot;&quot;&quot; print(&quot;开始测量...&quot;) temp = 39 print(&quot;测量结束...&quot;) return tempresult = measure()print(result) 在利用 元组 在返回温度的同时，也能够返回 湿度 改造如下： 123456789def measure(): &quot;&quot;&quot;返回当前的温度&quot;&quot;&quot; print(&quot;开始测量...&quot;) temp = 39 wetness = 10 print(&quot;测量结束...&quot;) return (temp, wetness) 提示：如果一个函数返回的是元组，括号可以省略 技巧 在 Python 中，可以 将一个元组 使用 赋值语句 同时赋值给 多个变量 注意：变量的数量需要和元组中的元素数量保持一致 1result = temp, wetness = measure() 面试题 —— 交换两个数字题目要求 有两个整数变量 a = 6, b = 100 不使用其他变量，交换两个变量的值 解法 1 —— 使用其他变量1234# 解法 1 - 使用临时变量c = bb = aa = c 解法 2 —— 不使用临时变量1234# 解法 2 - 不使用临时变量a = a + bb = a - ba = a - b 解法 3 —— Python 专有，利用元组1a, b = b, a 03. 函数的参数 进阶3.1. 不可变和可变的参数 问题 1：在函数内部，针对参数使用 赋值语句，会不会影响调用函数时传递的 实参变量？ —— 不会！ 无论传递的参数是 可变 还是 不可变 只要 针对参数 使用 赋值语句，会在 函数内部 修改 局部变量的引用，不会影响到 外部变量的引用 1234567891011121314151617181920def demo(num, num_list): print(&quot;函数内部&quot;) # 赋值语句 num = 200 num_list = [1, 2, 3] print(num) print(num_list) print(&quot;函数代码完成&quot;)gl_num = 99gl_list = [4, 5, 6]demo(gl_num, gl_list)print(gl_num)print(gl_list) 问题 2：如果传递的参数是 可变类型，在函数内部，使用 方法 修改了数据的内容，同样会影响到外部的数据 12345678910def mutable(num_list): # num_list = [1, 2, 3] num_list.extend([1, 2, 3]) print(num_list)gl_list = [6, 7, 8]mutable(gl_list)print(gl_list) 面试题 —— += 在 python 中，列表变量调用 += 本质上是在执行列表变量的 extend 方法，不会修改变量的引用 123456789101112131415161718192021def demo(num, num_list): print(&quot;函数内部代码&quot;) # num = num + num num += num # num_list.extend(num_list) 由于是调用方法，所以不会修改变量的引用 # 函数执行结束后，外部数据同样会发生变化 num_list += num_list print(num) print(num_list) print(&quot;函数代码完成&quot;)gl_num = 9gl_list = [1, 2, 3]demo(gl_num, gl_list)print(gl_num)print(gl_list) 3.2 缺省参数 定义函数时，可以给 某个参数 指定一个默认值，具有默认值的参数就叫做 缺省参数 调用函数时，如果没有传入 缺省参数 的值，则在函数内部使用定义函数时指定的 参数默认值 函数的缺省参数，将常见的值设置为参数的缺省值，从而 简化函数的调用 例如：对列表排序的方法 123456789gl_num_list = [6, 3, 9]# 默认就是升序排序，因为这种应用需求更多gl_num_list.sort()print(gl_num_list)# 只有当需要降序排序时，才需要传递 `reverse` 参数gl_num_list.sort(reverse=True)print(gl_num_list) 指定函数的缺省参数 在参数后使用赋值语句，可以指定参数的缺省值 1234567def print_info(name, gender=True): gender_text = &quot;男生&quot; if not gender: gender_text = &quot;女生&quot; print(&quot;%s 是 %s&quot; % (name, gender_text)) 提示 缺省参数，需要使用 最常见的值 作为默认值！ 如果一个参数的值 不能确定，则不应该设置默认值，具体的数值在调用函数时，由外界传递！ 缺省参数的注意事项1) 缺省参数的定义位置 必须保证 带有默认值的缺省参数 在参数列表末尾 所以，以下定义是错误的！ 1def print_info(name, gender=True, title): 2) 调用带有多个缺省参数的函数 在 调用函数时，如果有 多个缺省参数，需要指定参数名，这样解释器才能够知道参数的对应关系！ 123456789101112131415161718192021def print_info(name, title=&quot;&quot;, gender=True): &quot;&quot;&quot; :param title: 职位 :param name: 班上同学的姓名 :param gender: True 男生 False 女生 &quot;&quot;&quot; gender_text = &quot;男生&quot; if not gender: gender_text = &quot;女生&quot; print(&quot;%s%s 是 %s&quot; % (title, name, gender_text))# 提示：在指定缺省参数的默认值时，应该使用最常见的值作为默认值！print_info(&quot;小明&quot;)print_info(&quot;老王&quot;, title=&quot;班长&quot;)print_info(&quot;小美&quot;, gender=False) 3.3 多值参数（知道）定义支持多值参数的函数 有时可能需要 一个函数 能够处理的参数 个数 是不确定的，这个时候，就可以使用 多值参数 python 中有 两种 多值参数： 参数名前增加 一个 * 可以接收 元组 参数名前增加 两个 * 可以接收 字典 一般在给多值参数命名时，习惯使用以下两个名字 *args —— 存放 元组 参数，前面有一个 * **kwargs —— 存放 字典 参数，前面有两个 * args 是 arguments 的缩写，有变量的含义 kw 是 keyword 的缩写，kwargs 可以记忆 键值对参数 123456789def demo(num, *args, **kwargs): print(num) print(args) print(kwargs)demo(1, 2, 3, 4, 5, name=&quot;小明&quot;, age=18, gender=True) 提示：多值参数 的应用会经常出现在网络上一些大牛开发的框架中，知道多值参数，有利于我们能够读懂大牛的代码 多值参数案例 —— 计算任意多个数字的和需求 定义一个函数 sum_numbers，可以接收的 任意多个整数 功能要求：将传递的 所有数字累加 并且返回累加结果 12345678910def sum_numbers(*args): num = 0 # 遍历 args 元组顺序求和 for n in args: num += n return numprint(sum_numbers(1, 2, 3)) 元组和字典的拆包（知道） 在调用带有多值参数的函数时，如果希望： 将一个 元组变量，直接传递给 args 将一个 字典变量，直接传递给 kwargs 就可以使用 拆包，简化参数的传递，拆包 的方式是： 在 元组变量前，增加 一个 * 在 字典变量前，增加 两个 * 1234567891011121314def demo(*args, **kwargs): print(args) print(kwargs)# 需要将一个元组变量/字典变量传递给函数对应的参数gl_nums = (1, 2, 3)gl_xiaoming = &#123;&quot;name&quot;: &quot;小明&quot;, &quot;age&quot;: 18&#125;# 会把 num_tuple 和 xiaoming 作为元组传递个 args# demo(gl_nums, gl_xiaoming)demo(*gl_nums, **gl_xiaoming) 04. 函数的递归 函数调用自身的 编程技巧 称为递归 4.1 递归函数的特点特点 一个函数 内部 调用自己 函数内部可以调用其他函数，当然在函数内部也可以调用自己 代码特点 函数内部的 代码 是相同的，只是针对 参数 不同，处理的结果不同 当 参数满足一个条件 时，函数不再执行 这个非常重要，通常被称为递归的出口，否则 会出现死循环！ 示例代码 123456789101112def sum_numbers(num): print(num) # 递归的出口很重要，否则会出现死循环 if num == 1: return sum_numbers(num - 1) sum_numbers(3) 4.2 递归案例 —— 计算数字累加需求 定义一个函数 sum_numbers 能够接收一个 num 的整数参数 计算 1 + 2 + … num 的结果 12345678910111213def sum_numbers(num): if num == 1: return 1 # 假设 sum_numbers 能够完成 num - 1 的累加 temp = sum_numbers(num - 1) # 函数内部的核心算法就是 两个数字的相加 return num + tempprint(sum_numbers(2)) 提示：递归是一个 编程技巧，初次接触递归会感觉有些吃力！在处理 不确定的循环条件时，格外的有用，例如：遍历整个文件目录的结构","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"变量进阶","slug":"变量进阶","date":"2021-03-05T12:26:02.000Z","updated":"2021-03-05T12:26:18.120Z","comments":true,"path":"20210305/变量进阶.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E5%8F%98%E9%87%8F%E8%BF%9B%E9%98%B6.html","excerpt":"","text":"变量进阶（理解）目标 变量的引用 可变和不可变类型 局部变量和全局变量 01. 变量的引用 变量 和 数据 都是保存在 内存 中的 在 Python 中 函数 的 参数传递 以及 返回值 都是靠 引用 传递的 1.1 引用的概念在 Python 中 变量 和 数据 是分开存储的 数据 保存在内存中的一个位置 变量 中保存着数据在内存中的地址 变量 中 记录数据的地址，就叫做 引用 使用 id() 函数可以查看变量中保存数据所在的 内存地址 注意：如果变量已经被定义，当给一个变量赋值的时候，本质上是 修改了数据的引用 变量 不再 对之前的数据引用 变量 改为 对新赋值的数据引用 1.2 变量引用 的示例在 Python 中，变量的名字类似于 便签纸 贴在 数据 上 定义一个整数变量 a，并且赋值为 1 代码 图示 a = 1 将变量 a 赋值为 2 代码 图示 a = 2 定义一个整数变量 b，并且将变量 a 的值赋值给 b 代码 图示 b = a 变量 b 是第 2 个贴在数字 2 上的标签 1.3 函数的参数和返回值的传递在 Python 中，函数的 实参/返回值 都是是靠 引用 来传递来的 1234567891011121314151617181920def test(num): print(&quot;-&quot; * 50) print(&quot;%d 在函数内的内存地址是 %x&quot; % (num, id(num))) result = 100 print(&quot;返回值 %d 在内存中的地址是 %x&quot; % (result, id(result))) print(&quot;-&quot; * 50) return resulta = 10print(&quot;调用函数前 内存地址是 %x&quot; % id(a))r = test(a)print(&quot;调用函数后 实参内存地址是 %x&quot; % id(a))print(&quot;调用函数后 返回值内存地址是 %x&quot; % id(r)) 02. 可变和不可变类型 不可变类型，内存中的数据不允许被修改： 数字类型 int, bool, float, complex, long(2.x) 字符串 str 元组 tuple 可变类型，内存中的数据可以被修改： 列表 list 字典 dict 1234a = 1a = &quot;hello&quot;a = [1, 2, 3]a = [3, 2, 1] 123456789101112131415161718192021demo_list = [1, 2, 3]print(&quot;定义列表后的内存地址 %d&quot; % id(demo_list))demo_list.append(999)demo_list.pop(0)demo_list.remove(2)demo_list[0] = 10print(&quot;修改数据后的内存地址 %d&quot; % id(demo_list))demo_dict = &#123;&quot;name&quot;: &quot;小明&quot;&#125;print(&quot;定义字典后的内存地址 %d&quot; % id(demo_dict))demo_dict[&quot;age&quot;] = 18demo_dict.pop(&quot;name&quot;)demo_dict[&quot;name&quot;] = &quot;老王&quot;print(&quot;修改数据后的内存地址 %d&quot; % id(demo_dict)) 注意：字典的 key 只能使用不可变类型的数据 注意 可变类型的数据变化，是通过 方法 来实现的 如果给一个可变类型的变量，赋值了一个新的数据，引用会修改 变量 不再 对之前的数据引用 变量 改为 对新赋值的数据引用 哈希 (hash) Python 中内置有一个名字叫做 hash(o) 的函数 接收一个 不可变类型 的数据作为 参数 返回 结果是一个 整数 哈希 是一种 算法，其作用就是提取数据的 特征码（指纹） 相同的内容 得到 相同的结果 不同的内容 得到 不同的结果 在 Python 中，设置字典的 键值对 时，会首先对 key 进行 hash 已决定如何在内存中保存字典的数据，以方便 后续 对字典的操作：增、删、改、查 键值对的 key 必须是不可变类型数据 键值对的 value 可以是任意类型的数据 03. 局部变量和全局变量 局部变量 是在 函数内部 定义的变量，只能在函数内部使用 全局变量 是在 函数外部定义 的变量（没有定义在某一个函数内），所有函数 内部 都可以使用这个变量 提示：在其他的开发语言中，大多 不推荐使用全局变量 —— 可变范围太大，导致程序不好维护！ 3.1 局部变量 局部变量 是在 函数内部 定义的变量，只能在函数内部使用 函数执行结束后，函数内部的局部变量，会被系统回收 不同的函数，可以定义相同的名字的局部变量，但是 彼此之间 不会产生影响 局部变量的作用 在函数内部使用，临时 保存 函数内部需要使用的数据 12345678910111213141516171819202122def demo1(): num = 10 print(num) num = 20 print(&quot;修改后 %d&quot; % num)def demo2(): num = 100 print(num)demo1()demo2()print(&quot;over&quot;) 局部变量的生命周期 所谓 生命周期 就是变量从 被创建 到 被系统回收 的过程 局部变量 在 函数执行时 才会被创建 函数执行结束后 局部变量 被系统回收 局部变量在生命周期 内，可以用来存储 函数内部临时使用到的数据 3.2 全局变量 全局变量 是在 函数外部定义 的变量，所有函数内部都可以使用这个变量 123456789101112131415161718# 定义一个全局变量num = 10def demo1(): print(num)def demo2(): print(num)demo1()demo2()print(&quot;over&quot;) 注意：函数执行时，需要处理变量时 会： 首先 查找 函数内部 是否存在 指定名称 的局部变量，如果有，直接使用 如果没有，查找 函数外部 是否存在 指定名称 的全局变量，如果有，直接使用 如果还没有，程序报错！ 1) 函数不能直接修改 全局变量的引用 全局变量 是在 函数外部定义 的变量（没有定义在某一个函数内），所有函数 内部 都可以使用这个变量 提示：在其他的开发语言中，大多 不推荐使用全局变量 —— 可变范围太大，导致程序不好维护！ 在函数内部，可以 通过全局变量的引用获取对应的数据 但是，不允许直接修改全局变量的引用 —— 使用赋值语句修改全局变量的值 12345678910111213141516171819202122num = 10def demo1(): print(&quot;demo1&quot; + &quot;-&quot; * 50) # 只是定义了一个局部变量，不会修改到全局变量，只是变量名相同而已 num = 100 print(num)def demo2(): print(&quot;demo2&quot; + &quot;-&quot; * 50) print(num)demo1()demo2()print(&quot;over&quot;) 注意：只是在函数内部定义了一个局部变量而已，只是变量名相同 —— 在函数内部不能直接修改全局变量的值 2) 在函数内部修改全局变量的值 如果在函数中需要修改全局变量，需要使用 global 进行声明 123456789101112131415161718192021222324num = 10def demo1(): print(&quot;demo1&quot; + &quot;-&quot; * 50) # global 关键字，告诉 Python 解释器 num 是一个全局变量 global num # 只是定义了一个局部变量，不会修改到全局变量，只是变量名相同而已 num = 100 print(num)def demo2(): print(&quot;demo2&quot; + &quot;-&quot; * 50) print(num)demo1()demo2()print(&quot;over&quot;) 3) 全局变量定义的位置 为了保证所有的函数都能够正确使用到全局变量，应该 将全局变量定义在其他函数的上方 123456789101112a = 10def demo(): print(&quot;%d&quot; % a) print(&quot;%d&quot; % b) print(&quot;%d&quot; % c)b = 20demo()c = 30 注意 由于全局变量 c，是在调用函数之后，才定义的，在执行函数时，变量还没有定义，所以程序会报错！ 代码结构示意图如下 4) 全局变量命名的建议 为了避免局部变量和全局变量出现混淆，在定义全局变量时，有些公司会有一些开发要求，例如： 全局变量名前应该增加 g_ 或者 gl_ 的前缀 提示：具体的要求格式，各公司要求可能会有些差异","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"综合应用--名片管理系统","slug":"综合应用-名片管理系统","date":"2021-03-05T12:20:08.000Z","updated":"2021-03-05T12:20:24.542Z","comments":true,"path":"20210305/综合应用-名片管理系统.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E7%BB%BC%E5%90%88%E5%BA%94%E7%94%A8-%E5%90%8D%E7%89%87%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.html","excerpt":"","text":"综合应用 —— 名片管理系统目标综合应用已经学习过的知识点： 变量 流程控制 函数 模块 开发 名片管理系统 系统需求 程序启动，显示名片管理系统欢迎界面，并显示功能菜单 123456789**************************************************欢迎使用【名片管理系统】V1.01. 新建名片2. 显示全部3. 查询名片0. 退出系统************************************************** 用户用数字选择不同的功能 根据功能选择，执行不同的功能 用户名片需要记录用户的 姓名、电话、QQ、邮件 如果查询到指定的名片，用户可以选择 修改 或者 删除 名片 步骤 框架搭建 新增名片 显示所有名片 查询名片 查询成功后修改、删除名片 让 Python 程序能够直接运行 01. 框架搭建目标 搭建名片管理系统 框架结构 准备文件，确定文件名，保证能够 在需要的位置 编写代码 编写 主运行循环，实现基本的 用户输入和判断 1.1 文件准备 新建 cards_main.py 保存 主程序功能代码 程序的入口 每一次启动名片管理系统都通过 main 这个文件启动 新建 cards_tools.py 保存 所有名片功能函数 将对名片的 新增、查询、修改、删除 等功能封装在不同的函数中 1.2 编写主运行循环 在 cards_main 中添加一个 无限循环 12345678910111213141516171819while True: # TODO(小明) 显示系统菜单 action = input(&quot;请选择操作功能：&quot;) print(&quot;您选择的操作是：%s&quot; % action) # 根据用户输入决定后续的操作 if action in [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]: pass elif action == &quot;0&quot;: print(&quot;欢迎再次使用【名片管理系统】&quot;) break else: print(&quot;输入错误，请重新输入&quot;) 字符串判断1if action in [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]: 1if action == &quot;1&quot; or action == &quot;2&quot; or action == &quot;3&quot;: 使用 in 针对 列表 判断，避免使用 or 拼接复杂的逻辑条件 没有使用 int 转换用户输入，可以避免 一旦用户输入的不是数字，导致程序运行出错 pass pass 就是一个空语句，不做任何事情，一般用做占位语句 是为了保持程序结构的完整性 无限循环 在开发软件时，如果 不希望程序执行后 立即退出 可以在程序中增加一个 无限循环 由用户来决定 退出程序的时机 TODO 注释 在 # 后跟上 TODO，用于标记需要去做的工作 1# TODO(作者/邮件) 显示系统菜单 1.3 在 cards_tools 中增加四个新函数1234567891011121314151617181920212223242526272829def show_menu(): &quot;&quot;&quot;显示菜单 &quot;&quot;&quot; passdef new_card(): &quot;&quot;&quot;新建名片 &quot;&quot;&quot; print(&quot;-&quot; * 50) print(&quot;功能：新建名片&quot;)def show_all(): &quot;&quot;&quot;显示全部 &quot;&quot;&quot; print(&quot;-&quot; * 50) print(&quot;功能：显示全部&quot;)def search_card(): &quot;&quot;&quot;搜索名片 &quot;&quot;&quot; print(&quot;-&quot; * 50) print(&quot;功能：搜索名片&quot;) 1.4 导入模块 在 cards_main.py 中使用 import 导入 cards_tools 模块 1import cards_tools 修改 while 循环的代码如下： 1234567891011121314151617181920212223242526272829import cards_toolswhile True: cards_tools.show_menu() action = input(&quot;请选择操作功能：&quot;) print(&quot;您选择的操作是：%s&quot; % action) # 根据用户输入决定后续的操作 if action in [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]: if action == &quot;1&quot;: cards_tools.new_card() elif action == &quot;2&quot;: cards_tools.show_all() elif action == &quot;3&quot;: cards_tools.search_card() elif action == &quot;0&quot;: print(&quot;欢迎再次使用【名片管理系统】&quot;) break else: print(&quot;输入错误，请重新输入：&quot;) 至此：cards_main 中的所有代码全部开发完毕！ 1.5 完成 show_menu 函数1234567891011121314def show_menu(): &quot;&quot;&quot;显示菜单 &quot;&quot;&quot; print(&quot;*&quot; * 50) print(&quot;欢迎使用【菜单管理系统】V1.0&quot;) print(&quot;&quot;) print(&quot;1. 新建名片&quot;) print(&quot;2. 显示全部&quot;) print(&quot;3. 查询名片&quot;) print(&quot;&quot;) print(&quot;0. 退出系统&quot;) print(&quot;*&quot; * 50) 02. 保存名片数据的结构程序就是用来处理数据的，而变量就是用来存储数据的 使用 字典 记录 每一张名片 的详细信息 使用 列表 统一记录所有的 名片字典 定义名片列表变量 在 cards_tools 文件的顶部增加一个 列表变量 12# 所有名片记录的列表card_list = [] 注意 所有名片相关操作，都需要使用这个列表，所以应该 定义在程序的顶部 程序刚运行时，没有数据，所以是 空列表 03. 新增名片3.1 功能分析 提示用户依次输入名片信息 将名片信息保存到一个字典 将字典添加到名片列表 提示名片添加完成 3.2 实现 new_card 方法 根据步骤实现代码 123456789101112131415161718192021222324252627def new_card(): &quot;&quot;&quot;新建名片 &quot;&quot;&quot; print(&quot;-&quot; * 50) print(&quot;功能：新建名片&quot;) # 1. 提示用户输入名片信息 name = input(&quot;请输入姓名：&quot;) phone = input(&quot;请输入电话：&quot;) qq = input(&quot;请输入 QQ 号码：&quot;) email = input(&quot;请输入邮箱：&quot;) # 2. 将用户信息保存到一个字典 card_dict = &#123;&quot;name&quot;: name, &quot;phone&quot;: phone, &quot;qq&quot;: qq, &quot;email&quot;: email&#125; # 3. 将用户字典添加到名片列表 card_list.append(card_dict) print(card_list) # 4. 提示添加成功信息 print(&quot;成功添加 %s 的名片&quot; % card_dict[&quot;name&quot;]) 技巧：在 PyCharm 中，可以使用 SHIFT + F6 统一修改变量名 04. 显示所有名片4.1 功能分析 循环遍历名片列表，顺序显示每一个字典的信息 4.2 基础代码实现1234567891011def show_all(): &quot;&quot;&quot;显示全部 &quot;&quot;&quot; print(&quot;-&quot; * 50) print(&quot;功能：显示全部&quot;) for card_dict in card_list: print(card_dict) 显示效果不好！ 4.3 增加标题和使用 \\t 显示123456789101112131415161718192021def show_all(): &quot;&quot;&quot;显示全部 &quot;&quot;&quot; print(&quot;-&quot; * 50) print(&quot;功能：显示全部&quot;) # 打印表头 for name in [&quot;姓名&quot;, &quot;电话&quot;, &quot;QQ&quot;, &quot;邮箱&quot;]: print(name, end=&quot;\\t\\t&quot;) print(&quot;&quot;) # 打印分隔线 print(&quot;=&quot; * 50) for card_dict in card_list: print(&quot;%s\\t\\t%s\\t\\t%s\\t\\t%s&quot; % (card_dict[&quot;name&quot;], card_dict[&quot;phone&quot;], card_dict[&quot;qq&quot;], card_dict[&quot;email&quot;])) 4.4 增加没有名片记录判断123456789101112def show_all(): &quot;&quot;&quot;显示全部 &quot;&quot;&quot; print(&quot;-&quot; * 50) print(&quot;功能：显示全部&quot;) # 1. 判断是否有名片记录 if len(card_list) == 0: print(&quot;提示：没有任何名片记录&quot;) return 注意 在函数中使用 return 表示返回 如果在 return 后没有跟任何内容，只是表示该函数执行到此就不再执行后续的代码 05. 查询名片5.1 功能分析 提示用户要搜索的姓名 根据用户输入的姓名遍历列表 搜索到指定的名片后，再执行后续的操作 5.2 代码实现 查询功能实现 1234567891011121314151617181920212223242526272829303132def search_card(): &quot;&quot;&quot;搜索名片 &quot;&quot;&quot; print(&quot;-&quot; * 50) print(&quot;功能：搜索名片&quot;) # 1. 提示要搜索的姓名 find_name = input(&quot;请输入要搜索的姓名：&quot;) # 2. 遍历字典 for card_dict in card_list: if card_dict[&quot;name&quot;] == find_name: print(&quot;姓名\\t\\t\\t电话\\t\\t\\tQQ\\t\\t\\t邮箱&quot;) print(&quot;-&quot; * 40) print(&quot;%s\\t\\t\\t%s\\t\\t\\t%s\\t\\t\\t%s&quot; % ( card_dict[&quot;name&quot;], card_dict[&quot;phone&quot;], card_dict[&quot;qq&quot;], card_dict[&quot;email&quot;])) print(&quot;-&quot; * 40) # TODO(小明) 针对找到的字典进行后续操作：修改/删除 break else: print(&quot;没有找到 %s&quot; % find_name) 增加名片操作函数：修改/删除/返回主菜单 12345678910111213141516def deal_card(find_dict): &quot;&quot;&quot;操作搜索到的名片字典 :param find_dict:找到的名片字典 &quot;&quot;&quot; print(find_dict) action_str = input(&quot;请选择要执行的操作 &quot; &quot;[1] 修改 [2] 删除 [0] 返回上级菜单&quot;) if action == &quot;1&quot;: print(&quot;修改&quot;) elif action == &quot;2&quot;: print(&quot;删除&quot;) 06. 修改和删除6.1 查询成功后删除名片 由于找到的字典记录已经在列表中保存 要删除名片记录，只需要把列表中对应的字典删除即可 1234elif action == &quot;2&quot;: card_list.remove(find_dict) print(&quot;删除成功&quot;) 6.2 修改名片 由于找到的字典记录已经在列表中保存 要修改名片记录，只需要把列表中对应的字典中每一个键值对的数据修改即可 123456789if action == &quot;1&quot;: find_dict[&quot;name&quot;] = input(&quot;请输入姓名：&quot;) find_dict[&quot;phone&quot;] = input(&quot;请输入电话：&quot;) find_dict[&quot;qq&quot;] = input(&quot;请输入QQ：&quot;) find_dict[&quot;email&quot;] = input(&quot;请输入邮件：&quot;) print(&quot;%s 的名片修改成功&quot; % find_dict[&quot;name&quot;]) 修改名片细化 如果用户在使用时，某些名片内容并不想修改，应该如何做呢？—— 既然系统提供的 input 函数不能满足需求，那么就新定义一个函数 input_card_info 对系统的 input 函数进行扩展 1234567891011121314151617181920def input_card_info(dict_value, tip_message): &quot;&quot;&quot;输入名片信息 :param dict_value: 字典原有值 :param tip_message: 输入提示信息 :return: 如果输入，返回输入内容，否则返回字典原有值 &quot;&quot;&quot; # 1. 提示用户输入内容 result_str = input(tip_message) # 2. 针对用户的输入进行判断，如果用户输入了内容，直接返回结果 if len(result_str) &gt; 0: return result_str # 3. 如果用户没有输入内容，返回 `字典中原有的值` else: return dict_value 07. LINUX 上的 Shebang 符号(#!) #!这个符号叫做 Shebang 或者 Sha-bang Shebang 通常在 Unix 系统脚本的中 第一行开头 使用 指明 执行这个脚本文件 的 解释程序 使用 Shebang 的步骤 使用 which 查询 python3 解释器所在路径 1$ which python3 修改要运行的 主 python 文件，在第一行增加以下内容 1#! /usr/bin/python3 修改 主 python 文件 的文件权限，增加执行权限 1$ chmod +x cards_main.py 在需要时执行程序即可 1./cards_main.py","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"高级变量类型","slug":"高级变量类型","date":"2021-03-05T12:17:20.000Z","updated":"2021-03-05T12:17:36.209Z","comments":true,"path":"20210305/高级变量类型.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E9%AB%98%E7%BA%A7%E5%8F%98%E9%87%8F%E7%B1%BB%E5%9E%8B.html","excerpt":"","text":"高级变量类型目标 列表 元组 字典 字符串 公共方法 变量高级 知识点回顾 Python 中数据类型可以分为 数字型 和 非数字型 数字型 整型 (int) 浮点型（float） 布尔型（bool） 真 True 非 0 数 —— 非零即真 假 False 0 复数型 (complex) 主要用于科学计算，例如：平面场问题、波动问题、电感电容等问题 非数字型 字符串 列表 元组 字典 在 Python 中，所有 非数字型变量 都支持以下特点： 都是一个 序列 sequence，也可以理解为 容器 取值 [] 遍历 for in 计算长度、最大/最小值、比较、删除 链接 + 和 重复 * 切片 01. 列表1.1 列表的定义 List（列表） 是 Python 中使用 最频繁 的数据类型，在其他语言中通常叫做 数组 专门用于存储 一串 信息 列表用 [] 定义，数据 之间使用 , 分隔 列表的 索引 从 0 开始 索引 就是数据在 列表 中的位置编号，索引 又可以被称为 下标 注意：从列表中取值时，如果 超出索引范围，程序会报错 1name_list = [&quot;zhangsan&quot;, &quot;lisi&quot;, &quot;wangwu&quot;] 1.2 列表常用操作 在 ipython3 中定义一个 列表，例如：name_list = [] 输入 name_list. 按下 TAB 键，ipython 会提示 列表 能够使用的 方法 如下： 1234In [1]: name_list.name_list.append name_list.count name_list.insert name_list.reversename_list.clear name_list.extend name_list.pop name_list.sortname_list.copy name_list.index name_list.remove 序号 分类 关键字 / 函数 / 方法 说明 1 增加 列表.insert(索引, 数据) 在指定位置插入数据 列表.append(数据) 在末尾追加数据 列表.extend(列表2) 将列表2 的数据追加到列表 2 修改 列表[索引] = 数据 修改指定索引的数据 3 删除 del 列表[索引] 删除指定索引的数据 列表.remove[数据] 删除第一个出现的指定数据 列表.pop 删除末尾数据 列表.pop(索引) 删除指定索引数据 列表.clear 清空列表 4 统计 len(列表) 列表长度 列表.count(数据) 数据在列表中出现的次数 5 排序 列表.sort() 升序排序 列表.sort(reverse=True) 降序排序 列表.reverse() 逆序、反转 del 关键字（科普） 使用 del 关键字(delete) 同样可以删除列表中元素 del 关键字本质上是用来 将一个变量从内存中删除的 如果使用 del 关键字将变量从内存中删除，后续的代码就不能再使用这个变量了 1del name_list[1] 在日常开发中，要从列表删除数据，建议 使用列表提供的方法 关键字、函数和方法（科普） 关键字 是 Python 内置的、具有特殊意义的标识符 123In [1]: import keywordIn [2]: print(keyword.kwlist)In [3]: print(len(keyword.kwlist)) 关键字后面不需要使用括号 函数 封装了独立功能，可以直接调用 1函数名(参数) 函数需要死记硬背 方法 和函数类似，同样是封装了独立的功能 方法 需要通过 对象 来调用，表示针对这个 对象 要做的操作 1对象.方法名(参数) 在变量后面输入 .，然后选择针对这个变量要执行的操作，记忆起来比函数要简单很多 1.3 循环遍历 遍历 就是 从头到尾 依次 从 列表 中获取数据 在 循环体内部 针对 每一个元素，执行相同的操作 在 Python 中为了提高列表的遍历效率，专门提供的 迭代 iteration 遍历 使用 for 就能够实现迭代遍历 123456# for 循环内部使用的变量 in 列表for name in name_list: 循环内部针对列表元素进行操作 print(name) 1.4 应用场景 尽管 Python 的 列表 中可以 存储不同类型的数据 但是在开发中，更多的应用场景是 列表 存储相同类型的数据 通过 迭代遍历，在循环体内部，针对列表中的每一项元素，执行相同的操作 02. 元组2.1 元组的定义 Tuple（元组）与列表类似，不同之处在于元组的 元素不能修改 元组 表示多个元素组成的序列 元组 在 Python 开发中，有特定的应用场景 用于存储 一串 信息，数据 之间使用 , 分隔 元组用 () 定义 元组的 索引 从 0 开始 索引 就是数据在 元组 中的位置编号 1info_tuple = (&quot;zhangsan&quot;, 18, 1.75) 创建空元组1info_tuple = () 元组中 只包含一个元素 时，需要 在元素后面添加逗号1info_tuple = (50, ) 2.2 元组常用操作 在 ipython3 中定义一个 元组，例如：info = () 输入 info. 按下 TAB 键，ipython 会提示 元组 能够使用的函数如下： 1info.count info.index 有关 元组 的 常用操作 可以参照上图练习 2.3 循环遍历 取值 就是从 元组 中获取存储在指定位置的数据 遍历 就是 从头到尾 依次 从 元组 中获取数据 123456# for 循环内部使用的变量 in 元组for item in info: 循环内部针对元组元素进行操作 print(item) 在 Python 中，可以使用 for 循环遍历所有非数字型类型的变量：列表、元组、字典 以及 字符串 提示：在实际开发中，除非 能够确认元组中的数据类型，否则针对元组的循环遍历需求并不是很多 2.4 应用场景 尽管可以使用 for in 遍历 元组 但是在开发中，更多的应用场景是： 函数的 参数 和 返回值，一个函数可以接收 任意多个参数，或者 一次返回多个数据 有关 函数的参数 和 返回值，在后续 函数高级 给大家介绍 格式字符串，格式化字符串后面的 () 本质上就是一个元组 让列表不可以被修改，以保护数据安全 1234info = (&quot;zhangsan&quot;, 18)print(&quot;%s 的年龄是 %d&quot; % info) 元组和列表之间的转换 使用 list 函数可以把元组转换成列表 1list(元组) 使用 tuple 函数可以把列表转换成元组 1tuple(列表) 03. 字典3.1 字典的定义 dictionary（字典） 是 除列表以外 Python 之中 最灵活 的数据类型 字典同样可以用来 存储多个数据 通常用于存储 描述一个 物体 的相关信息 和列表的区别 列表 是 有序 的对象集合 字典 是 无序 的对象集合 字典用 &#123;&#125; 定义 字典使用 键值对 存储数据，键值对之间使用 , 分隔 键 key 是索引 值 value 是数据 键 和 值 之间使用 : 分隔 键必须是唯一的 值 可以取任何数据类型，但 键 只能使用 字符串、数字或 元组 1234xiaoming = &#123;&quot;name&quot;: &quot;小明&quot;, &quot;age&quot;: 18, &quot;gender&quot;: True, &quot;height&quot;: 1.75&#125; 3.2 字典常用操作 在 ipython3 中定义一个 字典，例如：xiaoming = &#123;&#125; 输入 xiaoming. 按下 TAB 键，ipython 会提示 字典 能够使用的函数如下： 12345In [1]: xiaoming.xiaoming.clear xiaoming.items xiaoming.setdefaultxiaoming.copy xiaoming.keys xiaoming.updatexiaoming.fromkeys xiaoming.pop xiaoming.valuesxiaoming.get xiaoming.popitem 有关 字典 的 常用操作 可以参照上图练习 3.3 循环遍历 遍历 就是 依次 从 字典 中获取所有键值对 1234# for 循环内部使用的 `key 的变量` in 字典for k in xiaoming: print(&quot;%s: %s&quot; % (k, xiaoming[k])) 提示：在实际开发中，由于字典中每一个键值对保存数据的类型是不同的，所以针对字典的循环遍历需求并不是很多 3.4 应用场景 尽管可以使用 for in 遍历 字典 但是在开发中，更多的应用场景是： 使用 多个键值对，存储 描述一个 物体 的相关信息 —— 描述更复杂的数据信息 将 多个字典 放在 一个列表 中，再进行遍历，在循环体内部针对每一个字典进行 相同的处理 12345678card_list = [&#123;&quot;name&quot;: &quot;张三&quot;, &quot;qq&quot;: &quot;12345&quot;, &quot;phone&quot;: &quot;110&quot;&#125;, &#123;&quot;name&quot;: &quot;李四&quot;, &quot;qq&quot;: &quot;54321&quot;, &quot;phone&quot;: &quot;10086&quot;&#125; ] 04. 字符串4.1 字符串的定义 字符串 就是 一串字符，是编程语言中表示文本的数据类型 在 Python 中可以使用 一对双引号 &quot; 或者 一对单引号 &#39; 定义一个字符串 虽然可以使用 \\&quot; 或者 \\&#39; 做字符串的转义，但是在实际开发中： 如果字符串内部需要使用 &quot;，可以使用 &#39; 定义字符串 如果字符串内部需要使用 &#39;，可以使用 &quot; 定义字符串 可以使用 索引 获取一个字符串中 指定位置的字符，索引计数从 0 开始 也可以使用 for 循环遍历 字符串中每一个字符 大多数编程语言都是用 &quot; 来定义字符串 12345string = &quot;Hello Python&quot;for c in string: print(c) 4.2 字符串的常用操作 在 ipython3 中定义一个 字符串，例如：hello_str = &quot;&quot; 输入 hello_str. 按下 TAB 键，ipython 会提示 字符串 能够使用的 方法 如下： 12345678910111213141516In [1]: hello_str.hello_str.capitalize hello_str.isidentifier hello_str.rindexhello_str.casefold hello_str.islower hello_str.rjusthello_str.center hello_str.isnumeric hello_str.rpartitionhello_str.count hello_str.isprintable hello_str.rsplithello_str.encode hello_str.isspace hello_str.rstriphello_str.endswith hello_str.istitle hello_str.splithello_str.expandtabs hello_str.isupper hello_str.splitlineshello_str.find hello_str.join hello_str.startswithhello_str.format hello_str.ljust hello_str.striphello_str.format_map hello_str.lower hello_str.swapcasehello_str.index hello_str.lstrip hello_str.titlehello_str.isalnum hello_str.maketrans hello_str.translatehello_str.isalpha hello_str.partition hello_str.upperhello_str.isdecimal hello_str.replace hello_str.zfillhello_str.isdigit hello_str.rfind 提示：正是因为 python 内置提供的方法足够多，才使得在开发时，能够针对字符串进行更加灵活的操作！应对更多的开发需求！ 1) 判断类型 - 9 方法 说明 string.isspace() 如果 string 中只包含空格，则返回 True string.isalnum() 如果 string 至少有一个字符并且所有字符都是字母或数字则返回 True string.isalpha() 如果 string 至少有一个字符并且所有字符都是字母则返回 True string.isdecimal() 如果 string 只包含数字则返回 True，全角数字 string.isdigit() 如果 string 只包含数字则返回 True，全角数字、⑴、\\u00b2 string.isnumeric() 如果 string 只包含数字则返回 True，全角数字，汉字数字 string.istitle() 如果 string 是标题化的(每个单词的首字母大写)则返回 True string.islower() 如果 string 中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是小写，则返回 True string.isupper() 如果 string 中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是大写，则返回 True 2) 查找和替换 - 7 方法 说明 string.startswith(str) 检查字符串是否是以 str 开头，是则返回 True string.endswith(str) 检查字符串是否是以 str 结束，是则返回 True string.find(str, start=0, end=len(string)) 检测 str 是否包含在 string 中，如果 start 和 end 指定范围，则检查是否包含在指定范围内，如果是返回开始的索引值，否则返回 -1 string.rfind(str, start=0, end=len(string)) 类似于 find()，不过是从右边开始查找 string.index(str, start=0, end=len(string)) 跟 find() 方法类似，不过如果 str 不在 string 会报错 string.rindex(str, start=0, end=len(string)) 类似于 index()，不过是从右边开始 string.replace(old_str, new_str, num=string.count(old)) 把 string 中的 old_str 替换成 new_str，如果 num 指定，则替换不超过 num 次 3) 大小写转换 - 5 方法 说明 string.capitalize() 把字符串的第一个字符大写 string.title() 把字符串的每个单词首字母大写 string.lower() 转换 string 中所有大写字符为小写 string.upper() 转换 string 中的小写字母为大写 string.swapcase() 翻转 string 中的大小写 4) 文本对齐 - 3 方法 说明 string.ljust(width) 返回一个原字符串左对齐，并使用空格填充至长度 width 的新字符串 string.rjust(width) 返回一个原字符串右对齐，并使用空格填充至长度 width 的新字符串 string.center(width) 返回一个原字符串居中，并使用空格填充至长度 width 的新字符串 5) 去除空白字符 - 3 方法 说明 string.lstrip() 截掉 string 左边（开始）的空白字符 string.rstrip() 截掉 string 右边（末尾）的空白字符 string.strip() 截掉 string 左右两边的空白字符 6) 拆分和连接 - 5 方法 说明 string.partition(str) 把字符串 string 分成一个 3 元素的元组 (str前面, str, str后面) string.rpartition(str) 类似于 partition() 方法，不过是从右边开始查找 string.split(str=””, num) 以 str 为分隔符拆分 string，如果 num 有指定值，则仅分隔 num + 1 个子字符串，str 默认包含 ‘\\r’, ‘\\t’, ‘\\n’ 和空格 string.splitlines() 按照行(‘\\r’, ‘\\n’, ‘\\r\\n’)分隔，返回一个包含各行作为元素的列表 string.join(seq) 以 string 作为分隔符，将 seq 中所有的元素（的字符串表示）合并为一个新的字符串 4.3 字符串的切片 切片 方法适用于 字符串、列表、元组 切片 使用 索引值 来限定范围，从一个大的 字符串 中 切出 小的 字符串 列表 和 元组 都是 有序 的集合，都能够 通过索引值 获取到对应的数据 字典 是一个 无序 的集合，是使用 键值对 保存数据 1字符串[开始索引:结束索引:步长] 注意： 指定的区间属于 左闭右开 型 [开始索引, 结束索引) =&gt; 开始索引 &gt;= 范围 &lt; 结束索引 从 起始 位开始，到 结束位的前一位 结束（不包含结束位本身) 从头开始，开始索引 数字可以省略，冒号不能省略 到末尾结束，结束索引 数字可以省略，冒号不能省略 步长默认为 1，如果连续切片，数字和冒号都可以省略 索引的顺序和倒序 在 Python 中不仅支持 顺序索引，同时还支持 倒序索引 所谓倒序索引就是 从右向左 计算索引 最右边的索引值是 -1，依次递减 演练需求 截取从 2 ~ 5 位置 的字符串 截取从 2 ~ 末尾 的字符串 截取从 开始 ~ 5 位置 的字符串 截取完整的字符串 从开始位置，每隔一个字符截取字符串 从索引 1 开始，每隔一个取一个 截取从 2 ~ 末尾 - 1 的字符串 截取字符串末尾两个字符 字符串的逆序（面试题） 答案 123456789101112131415161718192021222324252627282930313233num_str = &quot;0123456789&quot;# 1. 截取从 2 ~ 5 位置 的字符串print(num_str[2:6])# 2. 截取从 2 ~ `末尾` 的字符串print(num_str[2:])# 3. 截取从 `开始` ~ 5 位置 的字符串print(num_str[:6])# 4. 截取完整的字符串print(num_str[:])# 5. 从开始位置，每隔一个字符截取字符串print(num_str[::2])# 6. 从索引 1 开始，每隔一个取一个print(num_str[1::2])# 倒序切片# -1 表示倒数第一个字符print(num_str[-1])# 7. 截取从 2 ~ `末尾 - 1` 的字符串print(num_str[2:-1])# 8. 截取字符串末尾两个字符print(num_str[-2:])# 9. 字符串的逆序（面试题）print(num_str[::-1]) 05. 公共方法5.1 Python 内置函数Python 包含了以下内置函数： 函数 描述 备注 len(item) 计算容器中元素个数 del(item) 删除变量 del 有两种方式 max(item) 返回容器中元素最大值 如果是字典，只针对 key 比较 min(item) 返回容器中元素最小值 如果是字典，只针对 key 比较 cmp(item1, item2) 比较两个值，-1 小于/0 相等/1 大于 Python 3.x 取消了 cmp 函数 注意 字符串 比较符合以下规则： “0” &lt; “A” &lt; “a” 5.2 切片 描述 Python 表达式 结果 支持的数据类型 切片 “0123456789”[::-2] “97531” 字符串、列表、元组 切片 使用 索引值 来限定范围，从一个大的 字符串 中 切出 小的 字符串 列表 和 元组 都是 有序 的集合，都能够 通过索引值 获取到对应的数据 字典 是一个 无序 的集合，是使用 键值对 保存数据 5.3 运算符 运算符 Python 表达式 结果 描述 支持的数据类型 + [1, 2] + [3, 4] [1, 2, 3, 4] 合并 字符串、列表、元组 * [“Hi!”] * 4 [‘Hi!’, ‘Hi!’, ‘Hi!’, ‘Hi!’] 重复 字符串、列表、元组 in 3 in (1, 2, 3) True 元素是否存在 字符串、列表、元组、字典 not in 4 not in (1, 2, 3) True 元素是否不存在 字符串、列表、元组、字典 &gt; &gt;= == &lt; &lt;= (1, 2, 3) &lt; (2, 2, 3) True 元素比较 字符串、列表、元组 注意 in 在对 字典 操作时，判断的是 字典的键 in 和 not in 被称为 成员运算符 成员运算符成员运算符用于 测试 序列中是否包含指定的 成员 运算符 描述 实例 in 如果在指定的序列中找到值返回 True，否则返回 False 3 in (1, 2, 3) 返回 True not in 如果在指定的序列中没有找到值返回 True，否则返回 False 3 not in (1, 2, 3) 返回 False 注意：在对 字典 操作时，判断的是 字典的键 5.4 完整的 for 循环语法 在 Python 中完整的 for 循环 的语法如下： 12345for 变量 in 集合: 循环体代码else: 没有通过 break 退出循环，循环结束后，会执行的代码 应用场景 在 迭代遍历 嵌套的数据类型时，例如 一个列表包含了多个字典 需求：要判断 某一个字典中 是否存在 指定的 值 如果 存在，提示并且退出循环 如果 不存在，在 循环整体结束 后，希望 得到一个统一的提示 12345678910111213141516171819202122232425262728293031students = [ &#123;&quot;name&quot;: &quot;阿土&quot;, &quot;age&quot;: 20, &quot;gender&quot;: True, &quot;height&quot;: 1.7, &quot;weight&quot;: 75.0&#125;, &#123;&quot;name&quot;: &quot;小美&quot;, &quot;age&quot;: 19, &quot;gender&quot;: False, &quot;height&quot;: 1.6, &quot;weight&quot;: 45.0&#125;,]find_name = &quot;阿土&quot;for stu_dict in students: print(stu_dict) # 判断当前遍历的字典中姓名是否为find_name if stu_dict[&quot;name&quot;] == find_name: print(&quot;找到了&quot;) # 如果已经找到，直接退出循环，就不需要再对后续的数据进行比较 breakelse: print(&quot;没有找到&quot;)print(&quot;循环结束&quot;)","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"函数基础","slug":"函数基础","date":"2021-03-05T12:10:58.000Z","updated":"2021-03-05T12:11:15.055Z","comments":true,"path":"20210305/函数基础.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E5%87%BD%E6%95%B0%E5%9F%BA%E7%A1%80.html","excerpt":"","text":"函数基础目标 函数的快速体验 函数的基本使用 函数的参数 函数的返回值 函数的嵌套调用 在模块中定义函数 01. 函数的快速体验1.1 快速体验 所谓函数，就是把 具有独立功能的代码块 组织为一个小模块，在需要的时候 调用 函数的使用包含两个步骤： 定义函数 —— 封装 独立的功能 调用函数 —— 享受 封装 的成果 函数的作用，在开发程序时，使用函数可以提高编写的效率以及代码的 重用 演练步骤 新建 04_函数 项目 复制之前完成的 乘法表 文件 修改文件，增加函数定义 multiple_table(): 新建另外一个文件，使用 import 导入并且调用函数 02. 函数基本使用2.1 函数的定义定义函数的格式如下： 1234def 函数名(): 函数封装的代码 …… def 是英文 define 的缩写 函数名称 应该能够表达 函数封装代码 的功能，方便后续的调用 函数名称 的命名应该 符合 标识符的命名规则 可以由 字母、下划线 和 数字 组成 不能以数字开头 不能与关键字重名 2.2 函数调用调用函数很简单的，通过 函数名() 即可完成对函数的调用 2.3 第一个函数演练需求 编写一个打招呼 say_hello 的函数，封装三行打招呼的代码 在函数下方调用打招呼的代码 12345678910111213141516name = &quot;小明&quot;# 解释器知道这里定义了一个函数def say_hello(): print(&quot;hello 1&quot;) print(&quot;hello 2&quot;) print(&quot;hello 3&quot;)print(name)# 只有在调用函数时，之前定义的函数才会被执行# 函数执行完成之后，会重新回到之前的程序中，继续执行后续的代码say_hello()print(name) 用 单步执行 F8 和 F7 观察以下代码的执行过程 定义好函数之后，只表示这个函数封装了一段代码而已 如果不主动调用函数，函数是不会主动执行的 思考 能否将 函数调用 放在 函数定义 的上方？ 不能！ 因为在 使用函数名 调用函数之前，必须要保证 Python 已经知道函数的存在 否则控制台会提示 NameError: name &#39;say_hello&#39; is not defined (名称错误：say_hello 这个名字没有被定义) 2.4 PyCharm 的调试工具 F8 Step Over 可以单步执行代码，会把函数调用看作是一行代码直接执行 F7 Step Into 可以单步执行代码，如果是函数，会进入函数内部 2.5 函数的文档注释 在开发中，如果希望给函数添加注释，应该在 定义函数 的下方，使用 连续的三对引号 在 连续的三对引号 之间编写对函数的说明文字 在 函数调用 位置，使用快捷键 CTRL + Q 可以查看函数的说明信息 注意：因为 函数体相对比较独立，函数定义的上方，应该和其他代码（包括注释）保留 两个空行 03. 函数的参数演练需求 开发一个 sum_2_num 的函数 函数能够实现 两个数字的求和 功能 演练代码如下： 12345678910def sum_2_num(): num1 = 10 num2 = 20 result = num1 + num2 print(&quot;%d + %d = %d&quot; % (num1, num2, result))sum_2_num() 思考一下存在什么问题 函数只能处理 固定数值 的相加 如何解决？ 如果能够把需要计算的数字，在调用函数时，传递到函数内部就好了！ 3.1 函数参数的使用 在函数名的后面的小括号内部填写 参数 多个参数之间使用 , 分隔 12345678def sum_2_num(num1, num2): result = num1 + num2 print(&quot;%d + %d = %d&quot; % (num1, num2, result))sum_2_num(50, 20) 3.2 参数的作用 函数，把 具有独立功能的代码块 组织为一个小模块，在需要的时候 调用 函数的参数，增加函数的 通用性，针对 相同的数据处理逻辑，能够 适应更多的数据 在函数 内部，把参数当做 变量 使用，进行需要的数据处理 函数调用时，按照函数定义的参数顺序，把 希望在函数内部处理的数据，通过参数 传递 3.3 形参和实参 形参：定义 函数时，小括号中的参数，是用来接收参数用的，在函数内部 作为变量使用 实参：调用 函数时，小括号中的参数，是用来把数据传递到 函数内部 用的 04. 函数的返回值 在程序开发中，有时候，会希望 一个函数执行结束后，告诉调用者一个结果，以便调用者针对具体的结果做后续的处理 返回值 是函数 完成工作后，最后 给调用者的 一个结果 在函数中使用 return 关键字可以返回结果 调用函数一方，可以 使用变量 来 接收 函数的返回结果 注意：return 表示返回，后续的代码都不会被执行 12345678910def sum_2_num(num1, num2): &quot;&quot;&quot;对两个数字的求和&quot;&quot;&quot; return num1 + num2# 调用函数，并使用 result 变量接收计算结果result = sum_2_num(10, 20)print(&quot;计算结果是 %d&quot; % result) 05. 函数的嵌套调用 一个函数里面 又调用 了 另外一个函数，这就是 函数嵌套调用 如果函数 test2 中，调用了另外一个函数 test1 那么执行到调用 test1 函数时，会先把函数 test1 中的任务都执行完 才会回到 test2 中调用函数 test1 的位置，继续执行后续的代码 123456789101112131415161718def test1(): print(&quot;*&quot; * 50) print(&quot;test 1&quot;) print(&quot;*&quot; * 50)def test2(): print(&quot;-&quot; * 50) print(&quot;test 2&quot;) test1() print(&quot;-&quot; * 50)test2() 函数嵌套的演练 —— 打印分隔线 体会一下工作中 需求是多变 的 需求 1 定义一个 print_line 函数能够打印 * 组成的 一条分隔线 1234def print_line(char): print(&quot;*&quot; * 50) 需求 2 定义一个函数能够打印 由任意字符组成 的分隔线 1234def print_line(char): print(char * 50) 需求 3 定义一个函数能够打印 任意重复次数 的分隔线 1234def print_line(char, times): print(char * times) 需求 4 定义一个函数能够打印 5 行 的分隔线，分隔线要求符合需求 3 提示：工作中针对需求的变化，应该冷静思考，不要轻易修改之前已经完成的，能够正常执行的函数！ 1234567891011121314def print_line(char, times): print(char * times)def print_lines(char, times): row = 0 while row &lt; 5: print_line(char, times) row += 1 06. 使用模块中的函数 模块是 Python 程序架构的一个核心概念 模块 就好比是 工具包，要想使用这个工具包中的工具，就需要 导入 import 这个模块 每一个以扩展名 py 结尾的 Python 源代码文件都是一个 模块 在模块中定义的 全局变量 、 函数 都是模块能够提供给外界直接使用的工具 6.1 第一个模块体验步骤 新建 hm_10_分隔线模块.py 复制 hm_09_打印多条分隔线.py 中的内容，最后一行 print 代码除外 增加一个字符串变量 1name = &quot;黑马程序员&quot; 新建 hm_10_体验模块.py 文件，并且编写以下代码： 1234import hm_10_分隔线模块hm_10_分隔线模块.print_line(&quot;-&quot;, 80)print(hm_10_分隔线模块.name) 体验小结 可以 在一个 Python 文件 中 定义 变量 或者 函数 然后在 另外一个文件中 使用 import 导入这个模块 导入之后，就可以使用 模块名.变量 / 模块名.函数 的方式，使用这个模块中定义的变量或者函数 模块可以让 曾经编写过的代码 方便的被 复用！ 6.2 模块名也是一个标识符 标示符可以由 字母、下划线 和 数字 组成 不能以数字开头 不能与关键字重名 注意：如果在给 Python 文件起名时，以数字开头 是无法在 PyCharm 中通过导入这个模块的 6.3 Pyc 文件（了解） C 是 compiled 编译过 的意思 操作步骤 浏览程序目录会发现一个 __pycache__ 的目录 目录下会有一个 hm_10_分隔线模块.cpython-35.pyc 文件，cpython-35 表示 Python 解释器的版本 这个 pyc 文件是由 Python 解释器将 模块的源码 转换为 字节码 Python 这样保存 字节码 是作为一种启动 速度的优化 字节码 Python 在解释源程序时是分成两个步骤的 首先处理源代码，编译 生成一个二进制 字节码 再对 字节码 进行处理，才会生成 CPU 能够识别的 机器码 有了模块的字节码文件之后，下一次运行程序时，如果在 上次保存字节码之后 没有修改过源代码，Python 将会加载 .pyc 文件并跳过编译这个步骤 当 Python 重编译时，它会自动检查源文件和字节码文件的时间戳 如果你又修改了源代码，下次程序运行时，字节码将自动重新创建 提示：有关模块以及模块的其他导入方式，后续课程还会逐渐展开！ 模块是 Python 程序架构的一个核心概念","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"循环","slug":"循环","date":"2021-03-05T12:09:45.000Z","updated":"2021-03-05T12:10:02.592Z","comments":true,"path":"20210305/循环.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E5%BE%AA%E7%8E%AF.html","excerpt":"","text":"循环目标 程序的三大流程 while 循环基本使用 break 和 continue while 循环嵌套 01. 程序的三大流程 在程序开发中，一共有三种流程方式： 顺序 —— 从上向下，顺序执行代码 分支 —— 根据条件判断，决定执行代码的 分支 循环 —— 让 特定代码 重复 执行 02. while 循环基本使用 循环的作用就是让 指定的代码 重复的执行 while 循环最常用的应用场景就是 让执行的代码 按照 指定的次数 重复 执行 需求 —— 打印 5 遍 Hello Python 思考 —— 如果要求打印 100 遍怎么办？ 2.1 while 语句基本语法123456789初始条件设置 —— 通常是重复执行的 计数器while 条件(判断 计数器 是否达到 目标次数): 条件满足时，做的事情1 条件满足时，做的事情2 条件满足时，做的事情3 ...(省略)... 处理条件(计数器 + 1) 注意： while 语句以及缩进部分是一个 完整的代码块 第一个 while 循环需求 打印 5 遍 Hello Python 123456789101112# 1. 定义重复次数计数器i &#x3D; 1# 2. 使用 while 判断条件while i &lt;&#x3D; 5: # 要重复执行的代码 print(&quot;Hello Python&quot;) # 处理计数器 i i &#x3D; i + 1print(&quot;循环结束后的 i &#x3D; %d&quot; % i) 注意：循环结束后，之前定义的计数器条件的数值是依旧存在的 死循环 由于程序员的原因，忘记 在循环内部 修改循环的判断条件，导致循环持续执行，程序无法终止！ 2.2 赋值运算符 在 Python 中，使用 = 可以给变量赋值 在算术运算时，为了简化代码的编写，Python 还提供了一系列的 与 算术运算符 对应的 赋值运算符 注意：赋值运算符中间不能使用空格 运算符 描述 实例 = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c += 加法赋值运算符 c += a 等效于 c = c + a -= 减法赋值运算符 c -= a 等效于 c = c - a *= 乘法赋值运算符 c = a 等效于 c = c a /= 除法赋值运算符 c /= a 等效于 c = c / a //= 取整除赋值运算符 c //= a 等效于 c = c // a %= 取 模 (余数)赋值运算符 c %= a 等效于 c = c % a **= 幂赋值运算符 c = a 等效于 c = c a 2.3 Python 中的计数方法常见的计数方法有两种，可以分别称为： 自然计数法（从 1 开始）—— 更符合人类的习惯 程序计数法（从 0 开始）—— 几乎所有的程序语言都选择从 0 开始计数 因此，大家在编写程序时，应该尽量养成习惯：除非需求的特殊要求，否则 循环 的计数都从 0 开始 2.4 循环计算 在程序开发中，通常会遇到 利用循环 重复计算 的需求 遇到这种需求，可以： 在 while 上方定义一个变量，用于 存放最终计算结果 在循环体内部，每次循环都用 最新的计算结果，更新 之前定义的变量 需求 计算 0 ~ 100 之间所有数字的累计求和结果 12345678910111213141516171819# 计算 0 ~ 100 之间所有数字的累计求和结果# 0. 定义最终结果的变量result = 0# 1. 定义一个整数的变量记录循环的次数i = 0# 2. 开始循环while i &lt;= 100: print(i) # 每一次循环，都让 result 这个变量和 i 这个计数器相加 result += i # 处理计数器 i += 1print(&quot;0~100之间的数字求和结果 = %d&quot; % result) 需求进阶 计算 0 ~ 100 之间 所有 偶数 的累计求和结果 开发步骤 编写循环 确认 要计算的数字 添加 结果 变量，在循环内部 处理计算结果 12345678910111213141516171819# 0. 最终结果result = 0# 1. 计数器i = 0# 2. 开始循环while i &lt;= 100: # 判断偶数 if i % 2 == 0: print(i) result += i # 处理计数器 i += 1print(&quot;0~100之间偶数求和结果 = %d&quot; % result) 03. break 和 continue break 和 continue 是专门在循环中使用的关键字 break 某一条件满足时，退出循环，不再执行后续重复的代码 continue 某一条件满足时，不执行后续重复的代码 break 和 continue 只针对 当前所在循环 有效 3.1 break 在循环过程中，如果 某一个条件满足后，不 再希望 循环继续执行，可以使用 break 退出循环 1234567891011121314i = 0while i &lt; 10: # break 某一条件满足时，退出循环，不再执行后续重复的代码 # i == 3 if i == 3: break print(i) i += 1print(&quot;over&quot;) break 只针对当前所在循环有效 3.2 continue 在循环过程中，如果 某一个条件满足后，不 希望 执行循环代码，但是又不希望退出循环，可以使用 continue 也就是：在整个循环中，只有某些条件，不需要执行循环代码，而其他条件都需要执行 1234567891011121314151617i = 0while i &lt; 10: # 当 i == 7 时，不希望执行需要重复执行的代码 if i == 7: # 在使用 continue 之前，同样应该修改计数器 # 否则会出现死循环 i += 1 continue # 重复执行的代码 print(i) i += 1 需要注意：使用 continue 时，条件处理部分的代码，需要特别注意，不小心会出现 死循环 continue 只针对当前所在循环有效 04. while 循环嵌套4.1 循环嵌套 while 嵌套就是：while 里面还有 while 123456789101112131415while 条件 1: 条件满足时，做的事情1 条件满足时，做的事情2 条件满足时，做的事情3 ...(省略)... while 条件 2: 条件满足时，做的事情1 条件满足时，做的事情2 条件满足时，做的事情3 ...(省略)... 处理条件 2 处理条件 1 4.2 循环嵌套演练 —— 九九乘法表第 1 步：用嵌套打印小星星需求 在控制台连续输出五行 *，每一行星号的数量依次递增 12345*************** 使用字符串 * 打印 123456789# 1. 定义一个计数器变量，从数字1开始，循环会比较方便row = 1while row &lt;= 5: print(&quot;*&quot; * row) row += 1 第 2 步：使用循环嵌套打印小星星知识点 对 print 函数的使用做一个增强 在默认情况下，print 函数输出内容之后，会自动在内容末尾增加换行 如果不希望末尾增加换行，可以在 print 函数输出内容的后面增加 , end=&quot;&quot; 其中 &quot;&quot; 中间可以指定 print 函数输出内容之后，继续希望显示的内容 语法格式如下： 12345# 向控制台输出内容结束之后，不会换行print(&quot;*&quot;, end=&quot;&quot;)# 单纯的换行print(&quot;&quot;) end=&quot;&quot; 表示向控制台输出内容结束之后，不会换行 假设 Python 没有提供 字符串的 * 操作 拼接字符串 需求 在控制台连续输出五行 *，每一行星号的数量依次递增 12345*************** 开发步骤 1&gt; 完成 5 行内容的简单输出 2&gt; 分析每行内部的 * 应该如何处理？ 每行显示的星星和当前所在的行数是一致的 嵌套一个小的循环，专门处理每一行中 列 的星星显示 123456789101112131415161718row = 1while row &lt;= 5: # 假设 python 没有提供字符串 * 操作 # 在循环内部，再增加一个循环，实现每一行的 星星 打印 col = 1 while col &lt;= row: print(&quot;*&quot;, end=&quot;&quot;) col += 1 # 每一行星号输出完成后，再增加一个换行 print(&quot;&quot;) row += 1 第 3 步： 九九乘法表需求 输出 九九乘法表，格式如下： 123456789101 * 1 &#x3D; 1 1 * 2 &#x3D; 2 2 * 2 &#x3D; 4 1 * 3 &#x3D; 3 2 * 3 &#x3D; 6 3 * 3 &#x3D; 9 1 * 4 &#x3D; 4 2 * 4 &#x3D; 8 3 * 4 &#x3D; 12 4 * 4 &#x3D; 16 1 * 5 &#x3D; 5 2 * 5 &#x3D; 10 3 * 5 &#x3D; 15 4 * 5 &#x3D; 20 5 * 5 &#x3D; 25 1 * 6 &#x3D; 6 2 * 6 &#x3D; 12 3 * 6 &#x3D; 18 4 * 6 &#x3D; 24 5 * 6 &#x3D; 30 6 * 6 &#x3D; 36 1 * 7 &#x3D; 7 2 * 7 &#x3D; 14 3 * 7 &#x3D; 21 4 * 7 &#x3D; 28 5 * 7 &#x3D; 35 6 * 7 &#x3D; 42 7 * 7 &#x3D; 49 1 * 8 &#x3D; 8 2 * 8 &#x3D; 16 3 * 8 &#x3D; 24 4 * 8 &#x3D; 32 5 * 8 &#x3D; 40 6 * 8 &#x3D; 48 7 * 8 &#x3D; 56 8 * 8 &#x3D; 64 1 * 9 &#x3D; 9 2 * 9 &#x3D; 18 3 * 9 &#x3D; 27 4 * 9 &#x3D; 36 5 * 9 &#x3D; 45 6 * 9 &#x3D; 54 7 * 9 &#x3D; 63 8 * 9 &#x3D; 72 9 * 9 &#x3D; 81 开发步骤 打印 9 行小星星 123456789********************************************* 将每一个 * 替换成对应的行与列相乘 123456789101112131415161718192021222324# 定义起始行row = 1# 最大打印 9 行while row &lt;= 9: # 定义起始列 col = 1 # 最大打印 row 列 while col &lt;= row: # end = &quot;&quot;，表示输出结束后，不换行 # &quot;\\t&quot; 可以在控制台输出一个制表符，协助在输出文本时对齐 print(&quot;%d * %d = %d&quot; % (col, row, row * col), end=&quot;\\t&quot;) # 列数 + 1 col += 1 # 一行打印完成的换行 print(&quot;&quot;) # 行数 + 1 row += 1 字符串中的转义字符 \\t 在控制台输出一个 制表符，协助在输出文本时 垂直方向 保持对齐 \\n 在控制台输出一个 换行符 制表符 的功能是在不使用表格的情况下在 垂直方向 按列对齐文本 转义字符 描述 \\\\\\\\ 反斜杠符号 \\\\’ 单引号 \\\\” 双引号 \\n 换行 \\t 横向制表符 \\r 回车","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"运算符","slug":"运算符","date":"2021-03-05T12:06:31.000Z","updated":"2021-03-05T12:06:46.905Z","comments":true,"path":"20210305/运算符.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E8%BF%90%E7%AE%97%E7%AC%A6.html","excerpt":"","text":"运算符目标 算数运算符 比较（关系）运算符 逻辑运算符 赋值运算符 运算符的优先级 数学符号表链接：https://zh.wikipedia.org/wiki/数学符号表 01. 算数运算符 是完成基本的算术运算使用的符号，用来处理四则运算 运算符 描述 实例 + 加 10 + 20 = 30 - 减 10 - 20 = -10 * 乘 10 * 20 = 200 / 除 10 / 20 = 0.5 // 取整除 返回除法的整数部分（商） 9 // 2 输出结果 4 % 取余数 返回除法的余数 9 % 2 = 1 ** 幂 又称次方、乘方，2 ** 3 = 8 在 Python 中 * 运算符还可以用于字符串，计算结果就是字符串重复指定次数的结果 12In [1]: &quot;-&quot; * 50Out[1]: &#x27;----------------------------------------&#x27; 02. 比较（关系）运算符 运算符 描述 == 检查两个操作数的值是否 相等，如果是，则条件成立，返回 True != 检查两个操作数的值是否 不相等，如果是，则条件成立，返回 True &gt; 检查左操作数的值是否 大于 右操作数的值，如果是，则条件成立，返回 True &lt; 检查左操作数的值是否 小于 右操作数的值，如果是，则条件成立，返回 True &gt;= 检查左操作数的值是否 大于或等于 右操作数的值，如果是，则条件成立，返回 True &lt;= 检查左操作数的值是否 小于或等于 右操作数的值，如果是，则条件成立，返回 True Python 2.x 中判断 不等于 还可以使用 &lt;&gt; 运算符 != 在 Python 2.x 中同样可以用来判断 不等于 03. 逻辑运算符 运算符 逻辑表达式 描述 and x and y 只有 x 和 y 的值都为 True，才会返回 True否则只要 x 或者 y 有一个值为 False，就返回 False or x or y 只要 x 或者 y 有一个值为 True，就返回 True只有 x 和 y 的值都为 False，才会返回 False not not x 如果 x 为 True，返回 False如果 x 为 False，返回 True 04. 赋值运算符 在 Python 中，使用 = 可以给变量赋值 在算术运算时，为了简化代码的编写，Python 还提供了一系列的 与 算术运算符 对应的 赋值运算符 注意：赋值运算符中间不能使用空格 运算符 描述 实例 = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c += 加法赋值运算符 c += a 等效于 c = c + a -= 减法赋值运算符 c -= a 等效于 c = c - a *= 乘法赋值运算符 c = a 等效于 c = c a /= 除法赋值运算符 c /= a 等效于 c = c / a //= 取整除赋值运算符 c //= a 等效于 c = c // a %= 取 模 (余数)赋值运算符 c %= a 等效于 c = c % a **= 幂赋值运算符 c = a 等效于 c = c a 05. 运算符的优先级 以下表格的算数优先级由高到最低顺序排列 运算符 描述 ** 幂 (最高优先级) * / % // 乘、除、取余数、取整除 + - 加法、减法 &lt;= &lt; &gt; &gt;= 比较运算符 == != 等于运算符 = %= /= //= -= += = *= 赋值运算符 not or and 逻辑运算符","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"判断（if）语句","slug":"判断（if）语句","date":"2021-03-05T11:59:15.000Z","updated":"2021-03-05T12:05:39.367Z","comments":true,"path":"20210305/判断（if）语句.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E5%88%A4%E6%96%AD%EF%BC%88if%EF%BC%89%E8%AF%AD%E5%8F%A5.html","excerpt":"","text":"判断（if）语句目标 开发中的应用场景 if 语句体验 if 语句进阶 综合应用 01. 开发中的应用场景生活中的判断几乎是无所不在的，我们每天都在做各种各样的选择，如果这样？如果那样？…… 程序中的判断 1234567891011121314if 今天发工资: 先还信用卡的钱 if 有剩余: 又可以happy了，O(∩_∩)O哈哈~ else: 噢，no。。。还的等30天else: 盼着发工资 判断的定义 如果 条件满足，才能做某件事情， 如果 条件不满足，就做另外一件事情，或者什么也不做 正是因为有了判断，才使得程序世界丰富多彩，充满变化！ 判断语句 又被称为 “分支语句”，正是因为有了判断，才让程序有了很多的分支 02. if 语句体验2.1 if 判断语句基本语法在 Python 中，if 语句 就是用来进行判断的，格式如下： 123if 要判断的条件: 条件成立时，要做的事情 …… 注意：代码的缩进为一个 tab 键，或者 4 个空格 —— 建议使用空格 在 Python 开发中，Tab 和空格不要混用！ 我们可以把整个 if 语句看成一个完整的代码块 2.2 判断语句演练 —— 判断年龄需求 定义一个整数变量记录年龄 判断是否满 18 岁 （&gt;=） 如果满 18 岁，允许进网吧嗨皮 12345678910# 1. 定义年龄变量age = 18# 2. 判断是否满 18 岁# if 语句以及缩进部分的代码是一个完整的代码块if age &gt;= 18: print(&quot;可以进网吧嗨皮……&quot;)# 3. 思考！- 无论条件是否满足都会执行print(&quot;这句代码什么时候执行?&quot;) 注意： if 语句以及缩进部分是一个 完整的代码块 2.3 else 处理条件不满足的情况思考 在使用 if 判断时，只能做到满足条件时要做的事情。那如果需要在 不满足条件的时候，做某些事情，该如何做呢？ 答案 else，格式如下： 123456if 要判断的条件: 条件成立时，要做的事情 ……else: 条件不成立时，要做的事情 …… 注意： if 和 else 语句以及各自的缩进部分共同是一个 完整的代码块 2.4 判断语句演练 —— 判断年龄改进需求 输入用户年龄 判断是否满 18 岁 （&gt;=） 如果满 18 岁，允许进网吧嗨皮 如果未满 18 岁，提示回家写作业 123456789101112# 1. 输入用户年龄age = int(input(&quot;今年多大了？&quot;))# 2. 判断是否满 18 岁# if 语句以及缩进部分的代码是一个完整的语法块if age &gt;= 18: print(&quot;可以进网吧嗨皮……&quot;)else: print(&quot;你还没长大，应该回家写作业！&quot;)# 3. 思考！- 无论条件是否满足都会执行print(&quot;这句代码什么时候执行?&quot;) 03. 逻辑运算 在程序开发中，通常 在判断条件时，会需要同时判断多个条件 只有多个条件都满足，才能够执行后续代码，这个时候需要使用到 逻辑运算符 逻辑运算符 可以把 多个条件 按照 逻辑 进行 连接，变成 更复杂的条件 Python 中的 逻辑运算符 包括：与 and／或 or／非 not 三种 3.1 and1条件1 and 条件2 与／并且 两个条件同时满足，返回 True 只要有一个不满足，就返回 False 条件 1 条件 2 结果 成立 成立 成立 成立 不成立 不成立 不成立 成立 不成立 不成立 不成立 不成立 3.2 or1条件1 or 条件2 或／或者 两个条件只要有一个满足，返回 True 两个条件都不满足，返回 False 条件 1 条件 2 结果 成立 成立 成立 成立 不成立 成立 不成立 成立 成立 不成立 不成立 不成立 3.3 not1not 条件 非／不是 条件 结果 成立 不成立 不成立 成立 逻辑运算演练 练习1: 定义一个整数变量 age，编写代码判断年龄是否正确 要求人的年龄在 0-120 之间 练习2: 定义两个整数变量 python_score、c_score，编写代码判断成绩 要求只要有一门成绩 &gt; 60 分就算合格 练习3: 定义一个布尔型变量 is_employee，编写代码判断是否是本公司员工 如果不是提示不允许入内 答案 1： 123456789# 练习1: 定义一个整数变量 age，编写代码判断年龄是否正确age = 100# 要求人的年龄在 0-120 之间if age &gt;= 0 and age &lt;= 120: print(&quot;年龄正确&quot;)else: print(&quot;年龄不正确&quot;) 答案 2： 123456789# 练习2: 定义两个整数变量 python_score、c_score，编写代码判断成绩python_score = 50c_score = 50# 要求只要有一门成绩 &gt; 60 分就算合格if python_score &gt; 60 or c_score &gt; 60: print(&quot;考试通过&quot;)else: print(&quot;再接再厉！&quot;) 答案 3： 123456# 练习3: 定义一个布尔型变量 `is_employee`，编写代码判断是否是本公司员工is_employee = True# 如果不是提示不允许入内if not is_employee: print(&quot;非公勿内&quot;) 04. if 语句进阶4.1 elif 在开发中，使用 if 可以 判断条件 使用 else 可以处理 条件不成立 的情况 但是，如果希望 再增加一些条件，条件不同，需要执行的代码也不同 时，就可以使用 elif 语法格式如下： 123456789101112if 条件1: 条件1满足执行的代码 ……elif 条件2: 条件2满足时，执行的代码 ……elif 条件3: 条件3满足时，执行的代码 ……else: 以上条件都不满足时，执行的代码 …… 对比逻辑运算符的代码 123if 条件1 and 条件2: 条件1满足 并且 条件2满足 执行的代码 …… 注意 elif 和 else 都必须和 if 联合使用，而不能单独使用 可以将 if、elif 和 else 以及各自缩进的代码，看成一个 完整的代码块 elif 演练 —— 女友的节日需求 定义 holiday_name 字符串变量记录节日名称 如果是 情人节 应该 买玫瑰／看电影 如果是 平安夜 应该 买苹果／吃大餐 如果是 生日 应该 买蛋糕 其他的日子每天都是节日啊…… 12345678910111213holiday_name &#x3D; &quot;平安夜&quot;if holiday_name &#x3D;&#x3D; &quot;情人节&quot;: print(&quot;买玫瑰&quot;) print(&quot;看电影&quot;)elif holiday_name &#x3D;&#x3D; &quot;平安夜&quot;: print(&quot;买苹果&quot;) print(&quot;吃大餐&quot;)elif holiday_name &#x3D;&#x3D; &quot;生日&quot;: print(&quot;买蛋糕&quot;)else: print(&quot;每天都是节日啊……&quot;) 4.2 if 的嵌套 elif 的应用场景是：同时 判断 多个条件，所有的条件是 平级 的 在开发中，使用 if 进行条件判断，如果希望 在条件成立的执行语句中 再 增加条件判断，就可以使用 if 的嵌套 if 的嵌套 的应用场景就是：在之前条件满足的前提下，再增加额外的判断 if 的嵌套 的语法格式，除了缩进之外 和之前的没有区别 语法格式如下： 12345678910111213141516if 条件 1: 条件 1 满足执行的代码 …… if 条件 1 基础上的条件 2: 条件 2 满足时，执行的代码 …… # 条件 2 不满足的处理 else: 条件 2 不满足时，执行的代码 # 条件 1 不满足的处理else: 条件1 不满足时，执行的代码 …… if 的嵌套 演练 —— 火车站安检需求 定义布尔型变量 has_ticket 表示是否有车票 定义整型变量 knife_length 表示刀的长度，单位：厘米 首先检查是否有车票，如果有，才允许进行 安检 安检时，需要检查刀的长度，判断是否超过 20 厘米 如果超过 20 厘米，提示刀的长度，不允许上车 如果不超过 20 厘米，安检通过 如果没有车票，不允许进门 12345678910111213141516171819202122# 定义布尔型变量 has_ticket 表示是否有车票has_ticket = True# 定义整数型变量 knife_length 表示刀的长度，单位：厘米knife_length = 20# 首先检查是否有车票，如果有，才允许进行 安检if has_ticket: print(&quot;有车票，可以开始安检...&quot;) # 安检时，需要检查刀的长度，判断是否超过 20 厘米 # 如果超过 20 厘米，提示刀的长度，不允许上车 if knife_length &gt;= 20: print(&quot;不允许携带 %d 厘米长的刀上车&quot; % knife_length) # 如果不超过 20 厘米，安检通过 else: print(&quot;安检通过，祝您旅途愉快……&quot;)# 如果没有车票，不允许进门else: print(&quot;大哥，您要先买票啊&quot;) 05. 综合应用 —— 石头剪刀布目标 强化 多个条件 的 逻辑运算 体会 import 导入模块（“工具包”）的使用 需求 从控制台输入要出的拳 —— 石头（1）／剪刀（2）／布（3） 电脑 随机 出拳 —— 先假定电脑只会出石头，完成整体代码功能 比较胜负 序号 规则 1 石头 胜 剪刀 2 剪刀 胜 布 3 布 胜 石头 5.1 基础代码实现 先 假定电脑就只会出石头，完成整体代码功能 12345678910111213141516171819# 从控制台输入要出的拳 —— 石头（1）／剪刀（2）／布（3）player = int(input(&quot;请出拳 石头（1）／剪刀（2）／布（3）：&quot;))# 电脑 随机 出拳 - 假定电脑永远出石头computer = 1# 比较胜负# 如果条件判断的内容太长，可以在最外侧的条件增加一对大括号# 再在每一个条件之间，使用回车，PyCharm 可以自动增加 8 个空格if ((player == 1 and computer == 2) or (player == 2 and computer == 3) or (player == 3 and computer == 1)): print(&quot;噢耶！！！电脑弱爆了！！！&quot;)elif player == computer: print(&quot;心有灵犀，再来一盘！&quot;)else: print(&quot;不行，我要和你决战到天亮！&quot;) 5.2 随机数的处理 在 Python 中，要使用随机数，首先需要导入 随机数 的 模块 —— “工具包” 1import random 导入模块后，可以直接在 模块名称 后面敲一个 . 然后按 Tab 键，会提示该模块中包含的所有函数 random.randint(a, b) ，返回 [a, b] 之间的整数，包含 a 和 b 例如： 123random.randint(12, 20) # 生成的随机数n: 12 &lt;= n &lt;= 20 random.randint(20, 20) # 结果永远是 20 random.randint(20, 10) # 该语句是错误的，下限必须小于上限","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"变量的命名","slug":"变量的命名","date":"2021-03-05T05:58:41.000Z","updated":"2021-03-05T05:59:00.568Z","comments":true,"path":"20210305/变量的命名.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E5%8F%98%E9%87%8F%E7%9A%84%E5%91%BD%E5%90%8D.html","excerpt":"","text":"变量的命名目标 标识符和关键字 变量的命名规则 0.1 标识符和关键字1.1 标识符 标示符就是程序员定义的 变量名、函数名 名字 需要有 见名知义 的效果，见下图： 标示符可以由 字母、下划线 和 数字 组成 不能以数字开头 不能与关键字重名 思考：下面的标示符哪些是正确的，哪些不正确为什么？ 123456789101112131415fromNo12from#12my_Booleanmy-BooleanObj22ndObjmyIntMy_tExt_testtest!32haha(da)ttjack_rosejack&amp;roseGUIG.U.I 1.2 关键字 关键字 就是在 Python 内部已经使用的标识符 关键字 具有特殊的功能和含义 开发者 不允许定义和关键字相同的名字的标示符 通过以下命令可以查看 Python 中的关键字 12In [1]: import keywordIn [2]: print(keyword.kwlist) 提示：关键字的学习及使用，会在后面的课程中不断介绍 import 关键字 可以导入一个 “工具包” 在 Python 中不同的工具包，提供有不同的工具 02. 变量的命名规则 命名规则 可以被视为一种 惯例，并无绝对与强制目的是为了 增加代码的识别和可读性 注意 Python 中的 标识符 是 区分大小写的 在定义变量时，为了保证代码格式，= 的左右应该各保留一个空格 在 Python 中，如果 变量名 需要由 二个 或 多个单词 组成时，可以按照以下方式命名 每个单词都使用小写字母 单词与单词之间使用 _下划线 连接 例如：first_name、last_name、qq_number、qq_password 驼峰命名法 当 变量名 是由二个或多个单词组成时，还可以利用驼峰命名法来命名 小驼峰式命名法 第一个单词以小写字母开始，后续单词的首字母大写 例如：firstName、lastName 大驼峰式命名法 每一个单词的首字母都采用大写字母 例如：FirstName、LastName、CamelCase","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"变量的基本使用","slug":"变量的基本使用","date":"2021-03-05T05:57:22.000Z","updated":"2021-03-05T05:58:06.303Z","comments":true,"path":"20210305/变量的基本使用.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E5%8F%98%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html","excerpt":"","text":"变量的基本使用 程序就是用来处理数据的，而变量就是用来存储数据的 目标 变量定义 变量的类型 变量的命名 01. 变量定义 在 Python 中，每个变量 在使用前都必须赋值，变量 赋值以后 该变量 才会被创建 等号（=）用来给变量赋值 = 左边是一个变量名 = 右边是存储在变量中的值 1变量名 = 值 变量定义之后，后续就可以直接使用了 1) 变量演练1 —— iPython12345678910111213# 定义 qq_number 的变量用来保存 qq 号码In [1]: qq_number = &quot;1234567&quot;# 输出 qq_number 中保存的内容In [2]: qq_numberOut[2]: &#x27;1234567&#x27;# 定义 qq_password 的变量用来保存 qq 密码In [3]: qq_password = &quot;123&quot;# 输出 qq_password 中保存的内容In [4]: qq_passwordOut[4]: &#x27;123&#x27; 使用交互式方式，如果要查看变量内容，直接输入变量名即可，不需要使用 print 函数 2) 变量演练 2 —— PyCharm123456789# 定义 qq 号码变量qq_number = &quot;1234567&quot;# 定义 qq 密码变量qq_password = &quot;123&quot;# 在程序中，如果要输出变量的内容，需要使用 print 函数print(qq_number)print(qq_password) 使用解释器执行，如果要输出变量的内容，必须要要使用 print 函数 3) 变量演练 3 —— 超市买苹果 可以用 其他变量的计算结果 来定义变量 变量定义之后，后续就可以直接使用了 需求 苹果的价格是 8.5 元/斤 买了 7.5 斤 苹果 计算付款金额 12345678910# 定义苹果价格变量price = 8.5# 定义购买重量weight = 7.5# 计算金额money = price * weightprint(money) 思考题 如果 只要买苹果，就返 5 块钱 请重新计算购买金额 123456789101112# 定义苹果价格变量price = 8.5# 定义购买重量weight = 7.5# 计算金额money = price * weight# 只要买苹果就返 5 元money = money - 5print(money) 提问 上述代码中，一共定义有几个变量？ 三个：price／weight／money money = money - 5 是在定义新的变量还是在使用变量？ 直接使用之前已经定义的变量 变量名 只有在 第一次出现 才是 定义变量 变量名 再次出现，不是定义变量，而是直接使用之前定义过的变量 在程序开发中，可以修改之前定义变量中保存的值吗？ 可以 变量中存储的值，就是可以 变 的 02. 变量的类型 在内存中创建一个变量，会包括： 变量的名称 变量保存的数据 变量存储数据的类型 变量的地址（标示） 2.1 变量类型的演练 —— 个人信息需求 定义变量保存小明的个人信息 姓名：小明 年龄：18 岁 性别：是男生 身高：1.75 米 体重：75.0 公斤 利用 单步调试 确认变量中保存数据的类型 提问 在演练中，一共有几种数据类型？ 4 种 str —— 字符串 bool —— 布尔（真假） int —— 整数 float —— 浮点数（小数） 在 Python 中定义变量时需要指定类型吗？ 不需要 Python 可以根据 = 等号右侧的值，自动推导出变量中存储数据的类型 2.2 变量的类型 在 Python 中定义变量是 不需要指定类型（在其他很多高级语言中都需要） 数据类型可以分为 数字型 和 非数字型 数字型 整型 (int) 浮点型（float） 布尔型（bool） 真 True 非 0 数 —— 非零即真 假 False 0 复数型 (complex) 主要用于科学计算，例如：平面场问题、波动问题、电感电容等问题 非数字型 字符串 列表 元组 字典 提示：在 Python 2.x 中，整数 根据保存数值的长度还分为： int（整数） long（长整数） 使用 type 函数可以查看一个变量的类型 1In [1]: type(name) 2.3 不同类型变量之间的计算1) 数字型变量 之间可以直接计算 在 Python 中，两个数字型变量是可以直接进行 算数运算的 如果变量是 bool 型，在计算时 True 对应的数字是 1 False 对应的数字是 0 演练步骤 定义整数 i = 10 定义浮点数 f = 10.5 定义布尔型 b = True 在 iPython 中，使用上述三个变量相互进行算术运算 2) 字符串变量 之间使用 + 拼接字符串 在 Python 中，字符串之间可以使用 + 拼接生成新的字符串 123456In [1]: first_name = &quot;三&quot;In [2]: last_name = &quot;张&quot;In [3]: first_name + last_nameOut[3]: &#x27;三张&#x27; 3) 字符串变量 可以和 整数 使用 * 重复拼接相同的字符串12In [1]: &quot;-&quot; * 50Out[1]: &#x27;--------------------------------------------------&#x27; 4) 数字型变量 和 字符串 之间 不能进行其他计算12345678In [1]: first_name = &quot;zhang&quot;In [2]: x = 10In [3]: x + first_name---------------------------------------------------------------------------TypeError: unsupported operand type(s) for +: &#x27;int&#x27; and &#x27;str&#x27;类型错误：`+` 不支持的操作类型：`int` 和 `str` 2.4 变量的输入 所谓 输入，就是 用代码 获取 用户通过 键盘 输入的信息 例如：去银行取钱，在 ATM 上输入密码 在 Python 中，如果要获取用户在 键盘 上的输入信息，需要使用到 input 函数 1) 关于函数 一个 提前准备好的功能(别人或者自己写的代码)，可以直接使用，而 不用关心内部的细节 目前已经学习过的函数 函数 说明 print(x) 将 x 输出到控制台 type(x) 查看 x 的变量类型 2) input 函数实现键盘输入 在 Python 中可以使用 input 函数从键盘等待用户的输入 用户输入的 任何内容 Python 都认为是一个 字符串 语法如下： 1字符串变量 = input(&quot;提示信息：&quot;) 3) 类型转换函数 函数 说明 int(x) 将 x 转换为一个整数 float(x) 将 x 转换到一个浮点数 4) 变量输入演练 —— 超市买苹果增强版需求 收银员输入 苹果的价格，单位：元／斤 收银员输入 用户购买苹果的重量，单位：斤 计算并且 输出 付款金额 演练方式 11234567891011121314151617# 1. 输入苹果单价price_str = input(&quot;请输入苹果价格：&quot;)# 2. 要求苹果重量weight_str = input(&quot;请输入苹果重量：&quot;)# 3. 计算金额# 1&gt; 将苹果单价转换成小数price = float(price_str)# 2&gt; 将苹果重量转换成小数weight = float(weight_str)# 3&gt; 计算付款金额money = price * weightprint(money) 提问 演练中，针对 价格 定义了几个变量？ 两个 price_str 记录用户输入的价格字符串 price 记录转换后的价格数值 思考 —— 如果开发中，需要用户通过控制台 输入 很多个 数字，针对每一个数字都要定义两个变量，方便吗？ 演练方式 2 —— 买苹果改进版 定义 一个 浮点变量 接收用户输入的同时，就使用 float 函数进行转换 1price = float(input(&quot;请输入价格:&quot;)) 改进后的好处： 节约空间，只需要为一个变量分配空间 起名字方便，不需要为中间变量起名字 改进后的“缺点”： 初学者需要知道，两个函数能够嵌套使用，稍微有一些难度 提示 如果输入的不是一个数字，程序执行时会出错，有关数据转换的高级话题，后续会讲！ 2.5 变量的格式化输出 苹果单价 9.00 元／斤，购买了 5.00 斤，需要支付 45.00 元 在 Python 中可以使用 print 函数将信息输出到控制台 如果希望输出文字信息的同时，一起输出 数据，就需要使用到 格式化操作符 % 被称为 格式化操作符，专门用于处理字符串中的格式 包含 % 的字符串，被称为 格式化字符串 % 和不同的 字符 连用，不同类型的数据 需要使用 不同的格式化字符 格式化字符 含义 %s 字符串 %d 有符号十进制整数，%06d 表示输出的整数显示位数，不足的地方使用 0 补全 %f 浮点数，%.2f 表示小数点后只显示两位 %% 输出 % 语法格式如下： 123print(&quot;格式化字符串&quot; % 变量1)print(&quot;格式化字符串&quot; % (变量1, 变量2...)) 格式化输出演练 —— 基本练习需求 定义字符串变量 name，输出 我的名字叫 小明，请多多关照！ 定义整数变量 student_no，输出 我的学号是 000001 定义小数 price、weight、money，输出 苹果单价 9.00 元／斤，购买了 5.00 斤，需要支付 45.00 元 定义一个小数 scale，输出 数据比例是 10.00% 1234print(&quot;我的名字叫 %s，请多多关照！&quot; % name)print(&quot;我的学号是 %06d&quot; % student_no)print(&quot;苹果单价 %.02f 元／斤，购买 %.02f 斤，需要支付 %.02f 元&quot; % (price, weight, money))print(&quot;数据比例是 %.02f%%&quot; % (scale * 100)) 课后练习 —— 个人名片需求 在控制台依次提示用户输入：姓名、公司、职位、电话、邮箱 按照以下格式输出： 12345678**************************************************公司名称姓名 (职位)电话：电话邮箱：邮箱************************************************** 实现代码如下： ```python“””在控制台依次提示用户输入：姓名、公司、职位、电话、电子邮箱“””name = input(“请输入姓名：”)company = input(“请输入公司：”)title = input(“请输入职位：”)phone = input(“请输入电话：”)email = input(“请输入邮箱：”) print(““ 50)print(company)print()print(“%s (%s)” % (name, title))print()print(“电话：%s” % phone)","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"程序执行原理","slug":"程序执行原理","date":"2021-03-05T05:56:15.000Z","updated":"2021-03-05T05:56:41.145Z","comments":true,"path":"20210305/程序执行原理.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86.html","excerpt":"","text":"程序执行原理（科普）目标 计算机中的 三大件 程序执行的原理 程序的作用 01. 计算机中的三大件计算机中包含有较多的硬件，但是一个程序要运行，有 三个 核心的硬件，分别是： CPU 中央处理器，是一块超大规模的集成电路 负责 处理数据／计算 内存 临时 存储数据（断电之后，数据会消失） 速度快 空间小（单位价格高） 硬盘 永久 存储数据 速度慢 空间大（单位价格低） CPU 内存 硬盘 思考题 计算机中哪一个硬件设备负责执行程序？ CPU 内存 的速度快还是 硬盘 的速度快？ 内存 我们的程序是安装在内存中的，还是安装在硬盘中的？ 硬盘 我买了一个内存条，有 500G 的空间！！！，这句话对吗？ 不对，内存条通常只有 4G / 8G / 16G / 32G 计算机关机之后，内存中的数据都会消失，这句话对吗？ 正确 02. 程序执行的原理 程序 运行之前，程序是 保存在硬盘 中的 当要运行一个程序时 操作系统会首先让 CPU 把程序复制到 内存 中 CPU 执行 内存 中的 程序代码 程序要执行，首先要被加载到内存 2.1 Python 程序执行原理 操作系统会首先让 CPU 把 Python 解释器 的程序复制到 内存 中 Python 解释器 根据语法规则，从上向下 让 CPU 翻译 Python 程序中的代码 CPU 负责执行翻译完成的代码 Python 的解释器有多大？ 执行以下终端命令可以查看 Python 解释器的大小 12345678# 1. 确认解释器所在位置$ which python# 2. 查看 python 文件大小(只是一个软链接)$ ls -lh /usr/bin/python# 3. 查看具体文件大小$ ls -lh /usr/bin/python2.7 提示：建立 软链接 的目的，是为了方便使用者不用记住使用的解释器是 哪一个具体版本 03. 程序的作用 程序就是 用来处理数据 的！ 新闻软件 提供的 新闻内容、评论…… 是数据 电商软件 提供的 商品信息、配送信息…… 是数据 运动类软件 提供的 运动数据…… 是数据 地图类软件 提供的 地图信息、定位信息、车辆信息…… 是数据 即时通讯软件 提供的 聊天信息、好友信息…… 是数据 …… 3.1 思考 QQ 程序的启动过程 QQ 在运行之前，是保存在 硬盘 中的 运行之后，QQ 程序就会被加载到 内存 中了 3.2 思考 QQ 程序的 登录 过程 读取用户输入的 QQ 号码 读取用户输入的 QQ 密码 将 QQ 号码 和 QQ 密码 发送给腾讯的服务器，等待服务器确认用户信息 思考 1 在 QQ 这个程序将 QQ 号码 和 QQ 密码 发送给服务器之前，是否需要先存储一下 QQ 号码 和 密码? 答案 肯定需要！—— 否则 QQ 这个程序就不知道把什么内容发送给服务器了！ 思考 2 QQ 这个程序把 QQ 号码 和 QQ 密码 保存在哪里？ 答案 保存在 内存 中，因为 QQ 程序自己就在内存中 思考 3 QQ 这个程序是怎么保存用户的 QQ 号码 和 QQ 密码 的？ 答案 在内存中为 QQ 号码 和 QQ 密码 各自分配一块空间 在 QQ 程序结束之前，这两块空间是由 QQ 程序负责管理的，其他任何程序都不允许使用 在 QQ 自己使用完成之前，这两块空间始终都只负责保存 QQ 号码 和 QQ 密码 使用一个 别名 标记 QQ 号码 和 QQ 密码 在内存中的位置 在程序内部，为 QQ 号码 和 QQ 密码 在内存中分配的空间就叫做 变量 程序就是用来处理数据的，而变量就是用来存储数据的","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"算数运算符","slug":"算数运算符","date":"2021-03-05T05:54:25.000Z","updated":"2021-03-05T05:54:58.023Z","comments":true,"path":"20210305/算数运算符.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6.html","excerpt":"","text":"算数运算符计算机，顾名思义就是负责进行 数学计算 并且 存储计算结果 的电子设备 目标 算术运算符的基本使用 01. 算数运算符 算数运算符是 运算符的一种 是完成基本的算术运算使用的符号，用来处理四则运算 运算符 描述 实例 + 加 10 + 20 = 30 - 减 10 - 20 = -10 * 乘 10 * 20 = 200 / 除 10 / 20 = 0.5 // 取整除 返回除法的整数部分（商） 9 // 2 输出结果 4 % 取余数 返回除法的余数 9 % 2 = 1 ** 幂 又称次方、乘方，2 ** 3 = 8 在 Python 中 * 运算符还可以用于字符串，计算结果就是字符串重复指定次数的结果 12In [1]: &quot;-&quot; * 50Out[1]: &#x27;----------------------------------------&#x27; 02. 算数运算符的优先级 和数学中的运算符的优先级一致，在 Python 中进行数学计算时，同样也是： 先乘除后加减 同级运算符是 从左至右 计算 可以使用 () 调整计算的优先级 以下表格的算数优先级由高到最低顺序排列 运算符 描述 ** 幂 (最高优先级) * / % // 乘、除、取余数、取整除 + - 加法、减法 例如： 2 + 3 * 5 = 17 (2 + 3) * 5 = 25 2 * 3 + 5 = 11 2 * (3 + 5) = 16","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"注释","slug":"注释","date":"2021-03-05T05:53:10.000Z","updated":"2021-03-05T05:53:31.647Z","comments":true,"path":"20210305/注释.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E6%B3%A8%E9%87%8A.html","excerpt":"","text":"注释目标 注释的作用 单行注释（行注释） 多行注释（块注释） 01. 注释的作用 使用用自己熟悉的语言，在程序中对某些代码进行标注说明，增强程序的可读性 02. 单行注释(行注释) 以 # 开头，# 右边的所有东西都被当做说明文字，而不是真正要执行的程序，只起到辅助说明作用 示例代码如下： 12# 这是第一个单行注释print(&quot;hello python&quot;) 为了保证代码的可读性，# 后面建议先添加一个空格，然后再编写相应的说明文字 在代码后面增加的单行注释 在程序开发时，同样可以使用 # 在代码的后面（旁边）增加说明性的文字 但是，需要注意的是，为了保证代码的可读性，注释和代码之间 至少要有 两个空格 示例代码如下： 1print(&quot;hello python&quot;) # 输出 `hello python` 03. 多行注释（块注释） 如果希望编写的 注释信息很多，一行无法显示，就可以使用多行注释 要在 Python 程序中使用多行注释，可以用 一对 连续的 三个 引号(单引号和双引号都可以) 示例代码如下： 123456&quot;&quot;&quot;这是一个多行注释在多行注释之间，可以写很多很多的内容……&quot;&quot;&quot; print(&quot;hello python&quot;) 什么时候需要使用注释？ 注释不是越多越好，对于一目了然的代码，不需要添加注释 对于 复杂的操作，应该在操作开始前写上若干行注释 对于 不是一目了然的代码，应在其行尾添加注释（为了提高可读性，注释应该至少离开代码 2 个空格） 绝不要描述代码，假设阅读代码的人比你更懂 Python，他只是不知道你的代码要做什么 在一些正规的开发团队，通常会有 代码审核 的惯例，就是一个团队中彼此阅读对方的代码 关于代码规范 Python 官方提供有一系列 PEP（Python Enhancement Proposals） 文档 其中第 8 篇文档专门针对 Python 的代码格式 给出了建议，也就是俗称的 PEP 8 文档地址：https://www.python.org/dev/peps/pep-0008/ 谷歌有对应的中文文档：http://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/python_style_rules/ 任何语言的程序员，编写出符合规范的代码，是开始程序生涯的第一步","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"多文件项目的演练","slug":"多文件项目的演练","date":"2021-03-05T05:49:51.000Z","updated":"2021-03-05T05:50:25.330Z","comments":true,"path":"20210305/多文件项目的演练.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E5%A4%9A%E6%96%87%E4%BB%B6%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%BC%94%E7%BB%83.html","excerpt":"","text":"多文件项目的演练 开发 项目 就是开发一个 专门解决一个复杂业务功能的软件 通常每 一个项目 就具有一个 独立专属的目录，用于保存 所有和项目相关的文件 一个项目通常会包含 很多源文件 目标 在项目中添加多个文件，并且设置文件的执行 多文件项目演练 在 01_Python基础 项目中新建一个 hm_02_第2个Python程序.py 在 hm_02_第2个Python程序.py 文件中添加一句 print(&quot;hello&quot;) 点击右键执行 hm_02_第2个Python程序.py 提示 在 PyCharm 中，要想让哪一个 Python 程序能够执行，必须首先通过 鼠标右键的方式执行 一下 对于初学者而言，在一个项目中设置多个程序可以执行，是非常方便的，可以方便对不同知识点的练习和测试 对于商业项目而言，通常在一个项目中，只有一个 可以直接执行的 Python 源程序","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"python的初始设置","slug":"python的初始设置","date":"2021-03-05T05:48:27.000Z","updated":"2021-03-05T05:49:01.701Z","comments":true,"path":"20210305/python的初始设置.html","link":"","permalink":"https://xxren8218.github.io/20210305/python%E7%9A%84%E5%88%9D%E5%A7%8B%E8%AE%BE%E7%BD%AE.html","excerpt":"","text":"PyCharm 的初始设置（知道）目标 恢复 PyCharm 的初始设置 第一次启动 PyCharm 新建一个 Python 项目 设置 PyCharm 的字体显示 PyCharm 的升级以及其他 PyCharm 的官方网站地址是：https://www.jetbrains.com/pycharm/ 01. 恢复 PyCharm 的初始设置PyCharm 的 配置信息 是保存在 用户家目录下 的 .PyCharmxxxx.x 目录下的，xxxx.x 表示当前使用的 PyCharm 的版本号 如果要恢复 PyCharm 的初始设置，可以按照以下步骤进行： 关闭正在运行的 PyCharm 在终端中执行以下终端命令，删除 PyCharm 的配置信息目录： 1$ rm -r ~/.PyCharm2016.3 重新启动 PyCharm 02. 第一次启动 PyCharm 导入配置信息 选择许可协议 配置初始界面 2.1 导入配置信息 在第一次启动 PyCharm 时，会首先提示用户是否导入 之前的配置信息 如果是第一次使用，直接点击 OK 按钮 2.2 选择许可协议 PyCharm 是一个付费软件，购买费用为 199$ / 年 或者 19.90$ ／ 月 不过 PyCharm 提供了对 学生和教师免费使用的版本 下载地址是：https://www.jetbrains.com/pycharm-edu/download/#section=linux 商业版本会提示输入注册信息，或者选择免费评估 2.3 PyCharm 的配置初始界面 在初始配置界面，可以通过 Editor colors and fonts 选择 编辑器的配色方案 2.4 欢迎界面 所有基础配置工作结束之后，就可以看到 PyCharm 的 欢迎界面了，通过 欢迎界面 就可以开始开发 Python 项目了 03. 新建/打开一个 Python 项目3.1 项目简介 开发 项目 就是开发一个 专门解决一个复杂业务功能的软件 通常每 一个项目 就具有一个 独立专属的目录，用于保存 所有和项目相关的文件 一个项目通常会包含 很多源文件 3.2 打开 Python 项目 直接点击 Open 按钮，然后浏览到之前保存 Python 文件的目录，既可以打开项目 打开之后，会在目录下新建一个 .idea 的目录，用于保存 项目相关的信息，例如：解释器版本、项目包含的文件等等 第一次打开项目，需要耐心等待 PyCharm 对项目进行初始设置 设置项目使用的解释器版本 打开的目录如果不是由 PyCharm 建立的项目目录，有的时候 使用的解释器版本是 Python 2.x 的，需要单独设置解释器的版本 通过 File / Settings… 可以打开设置窗口，如下图所示： 3.3 新建项目1) 命名规则 以后 项目名 前面都以 数字编号，随着知识点递增，编号递增 例如：01_Python 基础、02_分支、03_循环… 每个项目下的 文件名 都以 hm_xx_知识点 方式来命名 其中 xx 是演练文件的序号 注意 命名文件名时建议只使用 小写字母、数字 和 下划线 文件名不能以数字开始 通过 欢迎界面 或者菜单 File / New Project 可以新建项目 2) 演练步骤 新建 01_Python基础 项目，使用 Python 3.x 解释器 在项目下新建 hm_01_hello.py Python 文件 编写 print(&quot;Hello Python&quot;) 代码 04. 设置 PyCharm 的字体显示 05. PyCharm 的升级以及其他 PyCharm 提供了对 学生和教师免费使用的版本 教育版下载地址：https://www.jetbrains.com/pycharm-edu/download/#section=linux 专业版下载地址：https://www.jetbrains.com/pycharm/download/#section=linux 5.1 安装和启动步骤 执行以下终端命令，解压缩下载后的安装包 1$ tar -zxvf pycharm-professional-2017.1.3.tar.gz 将解压缩后的目录移动到 /opt 目录下，可以方便其他用户使用 /opt 目录用户存放给主机额外安装的软件 1$ sudo mv pycharm-2017.1.3/ /opt/ 切换工作目录 1$ cd /opt/pycharm-2017.1.3/bin 启动 PyCharm 1$ ./pycharm.sh 5.2 设置专业版启动图标 在专业版中，选择菜单 Tools / Create Desktop Entry… 可以设置任务栏启动图标 注意：设置图标时，需要勾选 Create the entry for all users 5.3 卸载之前版本的 PyCharm1) 程序安装 程序文件目录 将安装包解压缩，并且移动到 /opt 目录下 所有的相关文件都保存在解压缩的目录中 配置文件目录 启动 PyCharm 后，会在用户家目录下建立一个 .PyCharmxxx 的隐藏目录 保存 PyCharm 相关的配置信息 快捷方式文件 /usr/share/applications/jetbrains-pycharm.desktop 在 ubuntu 中，应用程序启动的快捷方式通常都保存在 /usr/share/applications 目录下 2) 程序卸载 要卸载 PyCharm 只需要做以下两步工作： 删除解压缩目录 1$ sudo rm -r /opt/pycharm-2016.3.1/ 删除家目录下用于保存配置信息的隐藏目录 1$ rm -r ~/.PyCharm2016.3/ 如果不再使用 PyCharm 还需要将 /usr/share/applications/ 下的 jetbrains-pycharm.desktop 删掉 5.4 教育版安装演练12345678# 1. 解压缩下载后的安装包$ tar -zxvf pycharm-edu-3.5.1.tar.gz# 2. 将解压缩后的目录移动到 `/opt` 目录下，可以方便其他用户使用$ sudo mv pycharm-edu-3.5.1/ /opt/# 3. 启动 `PyCharm`/opt/pycharm-edu-3.5.1/bin/pycharm.sh 后续课程都使用专业版本演练 设置启动图标 编辑快捷方式文件 1$ sudo gedit /usr/share/applications/jetbrains-pycharm.desktop 按照以下内容修改文件内容，需要注意指定正确的 pycharm 目录 12345678910[Desktop Entry]Version=1.0Type=ApplicationName=PyCharmIcon=/opt/pycharm-edu-3.5.1/bin/pycharm.pngExec=&quot;/opt/pycharm-edu-3.5.1/bin/pycharm.sh&quot; %fComment=The Drive to DevelopCategories=Development;IDE;Terminal=falseStartupWMClass=jetbrains-pycharm","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"第一个python程序","slug":"第一个python程序","date":"2021-03-05T05:46:26.000Z","updated":"2021-03-05T05:47:11.023Z","comments":true,"path":"20210305/第一个python程序.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E7%AC%AC%E4%B8%80%E4%B8%AApython%E7%A8%8B%E5%BA%8F.html","excerpt":"","text":"第一个 Python 程序目标 第一个 HelloPython 程序 Python 2.x 与 3.x 版本简介 执行 Python 程序的三种方式 解释器 —— python / python3 交互式 —— ipython 集成开发环境 —— PyCharm 01. 第一个 HelloPython 程序1.1 Python 源程序的基本概念 Python 源程序就是一个特殊格式的文本文件，可以使用任意文本编辑软件做 Python 的开发 Python 程序的 文件扩展名 通常都是 .py 1.2 演练步骤 在桌面下，新建 认识Python 目录 在 认识Python 目录下新建 01-HelloPython.py 文件 使用 gedit 编辑 01-HelloPython.py 并且输入以下内容： 12print(&quot;hello python&quot;)print(&quot;hello world&quot;) 在终端中输入以下命令执行 01-HelloPython.py 1$ python 01-HelloPython.py print 是 python 中我们学习的第一个 函数 print 函数的作用，可以把 “” 内部的内容，输出到屏幕上 1.3 演练扩展 —— 认识错误（BUG）关于错误 编写的程序不能正常执行，或者执行的结果不是我们期望的 俗称 BUG，是程序员在开发时非常常见的，初学者常见错误的原因包括： 手误 对已经学习过的知识理解还存在不足 对语言还有需要学习和提升的内容 在学习语言时，不仅要学会语言的语法，而且还要学会如何认识错误和解决错误的方法 每一个程序员都是在不断地修改错误中成长的 第一个演练中的常见错误 1&gt; 手误，例如使用 pirnt(&quot;Hello world&quot;) 123NameError: name &#x27;pirnt&#x27; is not defined名称错误：&#x27;pirnt&#x27; 名字没有定义 2&gt; 将多条 print 写在一行 123SyntaxError: invalid syntax语法错误：语法无效 每行代码负责完成一个动作 3&gt; 缩进错误 123IndentationError: unexpected indent缩进错误：不期望出现的缩进 Python 是一个格式非常严格的程序设计语言 目前而言，大家记住每行代码前面都不要增加空格 4&gt; python 2.x 默认不支持中文 目前市场上有两个 Python 的版本并存着，分别是 Python 2.x 和 Python 3.x Python 2.x 默认不支持中文，具体原因，等到介绍 字符编码 时给大家讲解 Python 2.x 的解释器名称是 python Python 3.x 的解释器名称是 python3 123456SyntaxError: Non-ASCII character &#x27;\\xe4&#x27; in file 01-HelloPython.py on line 3, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details语法错误： 在 01-HelloPython.py 中第 3 行出现了非 ASCII 字符 &#x27;\\xe4&#x27;，但是没有声明文件编码请访问 http://python.org/dev/peps/pep-0263/ 了解详细信息 ASCII 字符只包含 256 个字符，不支持中文 有关字符编码的问题，后续会讲 单词列表12345678910111213* error 错误* name 名字* defined 已经定义* syntax 语法* invalid 无效* Indentation 索引* unexpected 意外的，不期望的* character 字符* line 行* encoding 编码* declared 声明* details 细节，详细信息* ASCII 一种字符编码 02. Python 2.x 与 3​​.x 版本简介目前市场上有两个 Python 的版本并存着，分别是 Python 2.x 和 Python 3.x 新的 Python 程序建议使用 Python 3.0 版本的语法 Python 2.x 是 过去的版本 解释器名称是 python Python 3.x 是 现在和未来 主流的版本 解释器名称是 python3 相对于 Python 的早期版本，这是一个 较大的升级 为了不带入过多的累赘，Python 3.0 在设计的时候 没有考虑向下兼容 许多早期 Python 版本设计的程序都无法在 Python 3.0 上正常执行 Python 3.0 发布于 2008 年 到目前为止，Python 3.0 的稳定版本已经有很多年了 Python 3.3 发布于 2012 Python 3.4 发布于 2014 Python 3.5 发布于 2015 Python 3.6 发布于 2016 为了照顾现有的程序，官方提供了一个过渡版本 —— Python 2.6 基本使用了 Python 2.x 的语法和库 同时考虑了向 Python 3.0 的迁移，允许使用部分 Python 3.0 的语法与函数 2010 年中推出的 Python 2.7 被确定为 最后一个Python 2.x 版本 提示：如果开发时，无法立即使用 Python 3.0（还有极少的第三方库不支持 3.0 的语法），建议 先使用 Python 3.0 版本进行开发 然后使用 Python 2.6、Python 2.7 来执行，并且做一些兼容性的处理 03. 执行 Python 程序的三种方式3.1. 解释器 python / python3Python 的解释器12345# 使用 python 2.x 解释器$ python xxx.py# 使用 python 3.x 解释器$ python3 xxx.py 其他解释器（知道）Python 的解释器 如今有多个语言的实现，包括： CPython —— 官方版本的 C 语言实现 Jython —— 可以运行在 Java 平台 IronPython —— 可以运行在 .NET 和 Mono 平台 PyPy —— Python 实现的，支持 JIT 即时编译 3.2. 交互式运行 Python 程序 直接在终端中运行解释器，而不输入要执行的文件名 在 Python 的 Shell 中直接输入 Python 的代码，会立即看到程序执行结果 1) 交互式运行 Python 的优缺点优点 适合于学习/验证 Python 语法或者局部代码 缺点 代码不能保存 不适合运行太大的程序 2) 退出 官方的解释器1&gt; 直接输入 exit()1&gt;&gt;&gt; exit() 2&gt; 使用热键退出在 python 解释器中，按热键 ctrl + d 可以退出解释器 3) IPython IPython 中 的 “I” 代表 交互 interactive 特点 IPython 是一个 python 的 交互式 shell，比默认的 python shell 好用得多 支持自动补全 自动缩进 支持 bash shell 命令 内置了许多很有用的功能和函数 IPython 是基于 BSD 开源的 版本 Python 2.x 使用的解释器是 ipython Python 3.x 使用的解释器是 ipython3 要退出解释器可以有以下两种方式： 1&gt; 直接输入 exit1In [1]: exit 2&gt; 使用热键退出在 IPython 解释器中，按热键 ctrl + d，IPython 会询问是否退出解释器 IPython 的安装1$ sudo apt install ipython 3.3. Python 的 IDE —— PyCharm1） 集成开发环境（IDE）集成开发环境（IDE，Integrated Development Environment）—— 集成了开发软件需要的所有工具，一般包括以下工具： 图形用户界面 代码编辑器（支持 代码补全／自动缩进） 编译器／解释器 调试器（断点／单步执行） …… 2）PyCharm 介绍 PyCharm 是 Python 的一款非常优秀的集成开发环境 PyCharm 除了具有一般 IDE 所必备功能外，还可以在 Windows、Linux、macOS 下使用 PyCharm 适合开发大型项目 一个项目通常会包含 很多源文件 每个 源文件 的代码行数是有限的，通常在几百行之内 每个 源文件 各司其职，共同完成复杂的业务功能 3）PyCharm 快速体验 文件导航区域 能够 浏览／定位／打开 项目文件 文件编辑区域 能够 编辑 当前打开的文件 控制台区域 能够： 输出程序执行内容 跟踪调试代码的执行 右上角的 工具栏 能够 执行(SHIFT + F10) / 调试(SHIFT + F9) 代码 通过控制台上方的单步执行按钮(F8)，可以单步执行代码","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"认识puthon","slug":"认识puthon","date":"2021-03-05T05:45:12.000Z","updated":"2021-03-05T05:45:41.469Z","comments":true,"path":"20210305/认识puthon.html","link":"","permalink":"https://xxren8218.github.io/20210305/%E8%AE%A4%E8%AF%86puthon.html","excerpt":"","text":"认识 Python 人生苦短，我用 Python —— Life is short, you need Python 目标 Python 的起源 为什么要用 Python？ Python 的特点 Python 的优缺点 01. Python 的起源 Python 的创始人为吉多·范罗苏姆（Guido van Rossum） 1989 年的圣诞节期间，吉多·范罗苏姆为了在阿姆斯特丹打发时间，决心开发一个新的解释程序，作为 ABC 语言的一种继承（感觉下什么叫牛人） ABC 是由吉多参加设计的一种教学语言，就吉多本人看来，ABC 这种语言非常优美和强大，是专门为非专业程序员设计的。但是 ABC 语言并没有成功，究其原因，吉多认为是非开放造成的。吉多决心在 Python 中避免这一错误，并获取了非常好的效果 之所以选中 Python（蟒蛇） 作为程序的名字，是因为他是 BBC 电视剧——蒙提·派森的飞行马戏团（Monty Python’s Flying Circus）的爱好者 1991 年，第一个 Python 解释器 诞生，它是用 C 语言实现的，并能够调用 C 语言的库文件 1.1 解释器（科普）计算机不能直接理解任何除机器语言以外的语言，所以必须要把程序员所写的程序语言翻译成机器语言，计算机才能执行程序。将其他语言翻译成机器语言的工具，被称为编译器 编译器翻译的方式有两种：一个是编译，另外一个是解释。两种方式之间的区别在于翻译时间点的不同。当编译器以解释方式运行的时候，也称之为解释器 编译型语言：程序在执行之前需要一个专门的编译过程，把程序编译成为机器语言的文件，运行时不需要重新翻译，直接使用编译的结果就行了。程序执行效率高，依赖编译器，跨平台性差些。如 C、C++ 解释型语言：解释型语言编写的程序不进行预先编译，以文本方式存储程序代码，会将代码一句一句直接运行。在发布程序时，看起来省了道编译工序，但是在运行程序的时候，必须先解释再运行 编译型语言和解释型语言对比 速度 —— 编译型语言比解释型语言执行速度快 跨平台性 —— 解释型语言比编译型语言跨平台性好 1.2 Python 的设计目标1999 年，吉多·范罗苏姆向 DARPA 提交了一条名为 “Computer Programming for Everybody” 的资金申请，并在后来说明了他对 Python 的目标： 一门简单直观的语言并与主要竞争者一样强大 开源，以便任何人都可以为它做贡献 代码像纯英语那样容易理解 适用于短期开发的日常任务 这些想法中的基本都已经成为现实，Python 已经成为一门流行的编程语言 1.3 Python 的设计哲学 优雅 明确 简单 Python 开发者的哲学是：用一种方法，最好是只有一种方法来做一件事 如果面临多种选择，Python 开发者一般会拒绝花俏的语法，而选择明确没有或者很少有歧义的语法 在 Python 社区，吉多被称为“仁慈的独裁者” 02. 为什么选择 Python？ 代码量少 …… 同一样问题，用不同的语言解决，代码量差距还是很多的，一般情况下 Python 是 Java 的 1/5，所以说 人生苦短，我用 Python 03. Python 特点 Python 是完全面向对象的语言 函数、模块、数字、字符串都是对象，在 Python 中一切皆对象 完全支持继承、重载、多重继承 支持重载运算符，也支持泛型设计 Python 拥有一个强大的标准库，Python 语言的核心只包含 数字、字符串、列表、字典、文件 等常见类型和函数，而由 Python 标准库提供了 系统管理、网络通信、文本处理、数据库接口、图形系统、XML 处理 等额外的功能 Python 社区提供了大量的第三方模块，使用方式与标准库类似。它们的功能覆盖 科学计算、人工智能、机器学习、Web 开发、数据库接口、图形系统 多个领域 面向对象的思维方式 面向对象 是一种 思维方式，也是一门 程序设计技术 要解决一个问题前，首先考虑 由谁 来做，怎么做事情是 谁 的职责，最后把事情做好就行！ 对象 就是 谁 要解决复杂的问题，就可以找多个不同的对象，各司其职，共同实现，最终完成需求 04. Python 的优缺点4.1 优点 简单、易学 免费、开源 面向对象 丰富的库 可扩展性 如果需要一段关键代码运行得更快或者希望某些算法不公开，可以把这部分程序用 C 或 C++ 编写，然后在 Python 程序中使用它们 …… 4.2 缺点 运行速度 国内市场较小 中文资料匮乏","categories":[{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"}],"tags":[{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"}]},{"title":"vi-终端中的编辑器","slug":"vi-终端中的编辑器","date":"2021-03-05T05:33:14.000Z","updated":"2021-03-05T05:38:24.231Z","comments":true,"path":"20210305/vi-终端中的编辑器.html","link":"","permalink":"https://xxren8218.github.io/20210305/vi-%E7%BB%88%E7%AB%AF%E4%B8%AD%E7%9A%84%E7%BC%96%E8%BE%91%E5%99%A8.html","excerpt":"摘要：vim的笔记。","text":"摘要：vim的笔记。 目标 vi 简介 打开和新建文件 三种工作模式 常用命令 分屏命令 常用命令速查图 01. vi 简介1.1 学习 vi 的目的 在工作中，要对 服务器 上的文件进行 简单 的修改，可以使用 ssh 远程登录到服务器上，并且使用 vi 进行快速的编辑即可 常见需要修改的文件包括： 源程序 配置文件，例如 ssh 的配置文件 ~/.ssh/config 在没有图形界面的环境下，要编辑文件，vi 是最佳选择！ 每一个要使用 Linux 的程序员，都应该或多或少的学习一些 vi 的常用命令 1.2 vi 和 vim 在很多 Linux 发行版中，直接把 vi 做成 vim 的软连接 vi vi 是 Visual interface 的简称，是 Linux 中 最经典 的文本编辑器 vi 的核心设计思想 —— 让程序员的手指始终保持在键盘的核心区域，就能完成所有的编辑操作 vi 的特点： 没有图形界面 的 功能强大 的编辑器 只能是编辑 文本内容，不能对字体、段落进行排版 不支持鼠标操作 没有菜单 只有命令 vi 编辑器在 系统管理、服务器管理 编辑文件时，其功能永远不是图形界面的编辑器能比拟的 vimvim = vi improved vim 是从 vi 发展出来的一个文本编辑器，支持 代码补全、编译 及 错误跳转 等方便编程的功能特别丰富，在程序员中被广泛使用，被称为 编辑器之神 查询软连接命令（知道） 在很多 Linux 发行版中直接把 vi 做成 vim 的软连接 123456789101112# 查找 vi 的运行文件$ which vi$ ls -l /usr/bin/vi$ ls -l /etc/alternatives/vi$ ls -l /usr/bin/vim.basic# 查找 vim 的运行文件$ which vim$ ls -l /usr/bin/vim$ ls -l /etc/alternatives/vim$ ls -l /usr/bin/vim.basic 02. 打开和新建文件 在终端中输入 vi 在后面跟上文件名 即可 1$ vi 文件名 如果文件已经存在，会直接打开该文件 如果文件不存在，会新建一个文件 2.1 打开文件并且定位行 在日常工作中，有可能会遇到 打开一个文件，并定位到指定行 的情况 例如：在开发时，知道某一行代码有错误，可以 快速定位 到出错代码的位置 这个时候，可以使用以下命令打开文件 1$ vi 文件名 +行数 提示：如果只带上 + 而不指定行号，会直接定位到文件末尾 2.2 异常处理 如果 vi 异常退出，在磁盘上可能会保存有 交换文件 下次再使用 vi 编辑该文件时，会看到以下屏幕信息，按下字母 d 可以 删除交换文件 即可 提示：按下键盘时，注意关闭输入法 03. 三种工作模式 vi 有三种基本工作模式： 命令模式 打开文件首先进入命令模式，是使用 vi 的 入口 通过 命令 对文件进行常规的编辑操作，例如：定位、翻页、复制、粘贴、删除…… 在其他图形编辑器下，通过 快捷键 或者 鼠标 实现的操作，都在 命令模式 下实现 末行模式 —— 执行 保存、退出 等操作 要退出 vi 返回到控制台，需要在末行模式下输入命令 末行模式 是 vi 的 出口 编辑模式 —— 正常的编辑文字 提示：在 Touch Bar 的 Mac 电脑上 ，按 ESC 不方便，可以使用 CTRL + [ 替代 末行模式命令 命令 英文 功能 w write 保存 q quit 退出，如果没有保存，不允许退出 q! quit 强行退出，不保存退出 wq write &amp; quit 保存并退出 x 保存并退出 04. 常用命令命令线路图 重复次数 在命令模式下，先输入一个数字，再跟上一个命令，可以让该命令 重复执行指定次数 移动和选择（多练） vi 之所以快，关键在于 能够快速定位到要编辑的代码行 移动命令 能够 和 编辑操作 命令 组合使用 编辑操作 删除、复制、粘贴、替换、缩排 撤销和重复 查找替换 编辑 学习提示 vi 的命令较多，不要期望一下子全部记住，个别命令忘记了，只是会影响编辑速度而已 在使用 vi 命令时，注意 关闭中文输入法 4.1 移动（基本） 要熟练使用 vi，首先应该学会怎么在 命令模式 下样快速移动光标 编辑操作命令，能够和 移动命令 结合在一起使用 1) 上、下、左、右 命令 功能 手指 h 向左 食指 j 向下 食指 k 向上 中指 l 向右 无名指 2) 行内移动 命令 英文 功能 w word 向后移动一个单词 b back 向前移动一个单词 0 行首 ^ 行首，第一个不是空白字符的位置 $ 行尾 3) 行数移动 命令 英文 功能 gg go 文件顶部 G go 文件末尾 数字gg go 移动到 数字 对应行数 数字G go 移动到 数字 对应行数 :数字 移动到 数字 对应行数 4) 屏幕移动 命令 英文 功能 Ctrl + b back 向上翻页 Ctrl + f forward 向下翻页 H Head 屏幕顶部 M Middle 屏幕中间 L Low 屏幕底部 4.2 移动（程序）1) 段落移动 vi 中使用 空行 来区分段落 在程序开发时，通常 一段功能相关的代码会写在一起 —— 之间没有空行 命令 功能 { 上一段 } 下一段 2) 括号切换 在程序世界中，()、[]、&#123;&#125; 使用频率很高，而且 都是成对出现的 命令 功能 % 括号匹配及切换 3) 标记 在开发时，某一块代码可能需要稍后处理，例如：编辑、查看 此时先使用 m 增加一个标记，这样可以 在需要时快速地跳转回来 或者 执行其他编辑操作 标记名称 可以是 a~z 或者 A~Z 之间的任意 一个 字母 添加了标记的 行如果被删除，标记同时被删除 如果 在其他行添加了相同名称的标记，之前添加的标记也会被替换掉 命令 英文 功能 mx mark 添加标记 x，x 是 a~z 或者 A~Z 之间的任意一个字母 ‘x 直接定位到标记 x 所在位置 4.3 选中文本（可视模式） 学习 复制 命令前，应该先学会 怎么样选中 要复制的代码 在 vi 中要选择文本，需要先使用 Visual 命令切换到 可视模式 vi 中提供了 三种 可视模式，可以方便程序员选择 选中文本的方式 按 ESC 可以放弃选中，返回到 命令模式 命令 模式 功能 v 可视模式 从光标位置开始按照正常模式选择文本 V 可视行模式 选中光标经过的完整行 Ctrl + v 可视块模式 垂直方向选中文本 可视模式下，可以和 移动命令 连用，例如：ggVG 能够选中所有内容 4.4 撤销和恢复撤销 在学习编辑命令之前，先要知道怎样撤销之前一次 错误的 编辑动作！ 命令 英文 功能 u undo 撤销上次命令 CTRL + r redo 恢复撤销的命令 4.5 删除文本 命令 英文 功能 x cut 删除光标所在字符，或者选中文字 d(移动命令) delete 删除移动命令对应的内容 dd delete 删除光标所在行，可以 ndd 复制多行 D delete 删除至行尾 提示：如果使用 可视模式 已经选中了一段文本，那么无论使用 d 还是 x，都可以删除选中文本 删除命令可以和 移动命令 连用，以下是常见的组合命令： 123456* dw # 从光标位置删除到单词末尾* d0 # 从光标位置删除到一行的起始位置* d&#125; # 从光标位置删除到段落结尾* ndd # 从光标位置向下连续删除 n 行* d代码行G # 从光标所在行 删除到 指定代码行 之间的所有代码* d&#x27;a # 从光标所在行 删除到 标记a 之间的所有代码 4.6 复制、粘贴 vi 中提供有一个 被复制文本的缓冲区 复制 命令会将选中的文字保存在缓冲区 删除 命令删除的文字会被保存在缓冲区 在需要的位置，使用 粘贴 命令可以将缓冲区的文字插入到光标所在位置 命令 英文 功能 y(移动命令) copy 复制 yy copy 复制一行，可以 nyy 复制多行 p paste 粘贴 提示 命令 d、x 类似于图形界面的 剪切操作 —— CTRL + X 命令 y 类似于图形界面的 复制操作 —— CTRL + C 命令 p 类似于图形界面的 粘贴操作 —— CTRL + V vi 中的 文本缓冲区同样只有一个，如果后续做过 复制、剪切 操作，之前缓冲区中的内容会被替换 注意 vi 中的 文本缓冲区 和系统的 剪贴板 不是同一个 所以在其他软件中使用 CTRL + C 复制的内容，不能在 vi 中通过 P 命令粘贴 可以在 编辑模式 下使用 鼠标右键粘贴 4.7 替换 命令 英文 功能 工作模式 r replace 替换当前字符 命令模式 R replace 替换当前行光标后的字符 替换模式 R 命令可以进入 替换模式，替换完成后，按下 ESC 可以回到 命令模式 替换命令 的作用就是不用进入 编辑模式，对文件进行 轻量级的修改 4.8 缩排和重复执行 命令 功能 &gt;&gt; 向右增加缩进 &lt;&lt; 向左减少缩进 . 重复上次命令 缩排命令 在开发程序时，统一增加代码的缩进 比较有用！ 一次性 在选中代码前增加 4 个空格，就叫做 增加缩进 一次性 在选中代码前删除 4 个空格，就叫做 减少缩进 在 可视模式 下，缩排命令只需要使用 一个 &gt; 或者 &lt; 在程序中，缩进 通常用来表示代码的归属关系 前面空格越少，代码的级别越高 前面空格越多，代码的级别越低 4.9 查找常规查找 命令 功能 /str 查找 str 查找到指定内容之后，使用 Next 查找下一个出现的位置： n: 查找下一个 N: 查找上一个 如果不想看到高亮显示，可以随便查找一个文件中不存在的内容即可 单词快速匹配 命令 功能 * 向后查找当前光标所在单词 # 向前查找当前光标所在单词 在开发中，通过单词快速匹配，可以快速看到这个单词在其他什么位置使用过 4.10 查找并替换 在 vi 中查找和替换命令需要在 末行模式 下执行 记忆命令格式： 1:%s&#x2F;&#x2F;&#x2F;g 1) 全局替换 一次性替换文件中的 所有出现的旧文本 命令格式如下： 1:%s&#x2F;旧文本&#x2F;新文本&#x2F;g 2) 可视区域替换 先选中 要替换文字的 范围 命令格式如下： 1:s&#x2F;旧文本&#x2F;新文本&#x2F;g 3) 确认替换 如果把末尾的 g 改成 gc 在替换的时候，会有提示！推荐使用！ 1:%s&#x2F;旧文本&#x2F;新文本&#x2F;gc y - yes 替换 n - no 不替换 a - all 替换所有 q - quit 退出替换 l - last 最后一个，并把光标移动到行首 ^E 向下滚屏 ^Y 向上滚屏 4.11 插入命令 在 vi 中除了常用的 i 进入 编辑模式 外，还提供了以下命令同样可以进入编辑模式： 命令 英文 功能 常用 i insert 在当前字符前插入文本 常用 I insert 在行首插入文本 较常用 a append 在当前字符后添加文本 A append 在行末添加文本 较常用 o 在当前行后面插入一空行 常用 O 在当前行前面插入一空行 常用 演练 1 —— 编辑命令和数字连用 在开发中，可能会遇到连续输入 N 个同样的字符 在 Python 中有简单的方法，但是其他语言中通常需要自己输入 例如：********** 连续 10 个星号 要实现这个效果可以在 命令模式 下 输入 10，表示要重复 10 次 输入 i 进入 编辑模式 输入 * 也就是重复的文字 按下 ESC 返回到 命令模式，返回之后 vi 就会把第 2、3 两步的操作重复 10 次 提示：正常开发时，在 进入编辑模式之前，不要按数字 演练 2 —— 利用 可视块 给多行代码增加注释 在开发中，可能会遇到一次性给多行代码 增加注释 的情况 在 Python 中，要给代码增加注释，可以在代码前增加一个 # 要实现这个效果可以在 命令模式 下 移动到要添加注释的 第 1 行代码，按 ^ 来到行首 按 CTRL + v 进入 可视块 模式 使用 j 向下连续选中要添加的代码行 输入 I 进入 编辑模式，并在 行首插入，注意：一定要使用 I 输入 # 也就是注释符号 按下 ESC 返回到 命令模式，返回之后 vi 会在之前选中的每一行代码 前 插入 # 05. 分屏命令 属于 vi 的高级命令 —— 可以 同时编辑和查看多个文件 5.1 末行命令扩展末行命令 主要是针对文件进行操作的：保存、退出、保存&amp;退出、搜索&amp;替换、另存、新建、浏览文件 命令 英文 功能 :e . edit 会打开内置的文件浏览器，浏览要当前目录下的文件 :n 文件名 new 新建文件 :w 文件名 write 另存为，但是仍然编辑当前文件，并不会切换文件 提示：切换文件之前，必须保证当前这个文件已经被保存！ 已经学习过的 末行命令： 命令 英文 功能 :w write 保存 :q quit 退出，如果没有保存，不允许退出 :q! quit 强行退出，不保存退出 :wq write &amp; quit 保存并退出 :x 保存并退出 :%s///gc 确认搜索并替换 在实际开发中，可以使用 w 命令 阶段性的备份代码 5.2 分屏命令 使用 分屏命令，可以 同时编辑和查看多个文件 命令 英文 功能 :sp [文件名] split 横向增加分屏 :vsp [文件名] vertical split 纵向增加分屏 1) 切换分屏窗口 分屏窗口都是基于 CTRL + W 这个快捷键的，w 对应的英文单词是 window 命令 英文 功能 w window 切换到下一个窗口 r reverse 互换窗口 c close 关闭当前窗口，但是不能关闭最后一个窗口 q quit 退出当前窗口，如果是最后一个窗口，则关闭 vi o other 关闭其他窗口 2) 调整窗口大小 分屏窗口都是基于 CTRL + W 这个快捷键的，w 对应的英文单词是 window 命令 英文 功能 + 增加窗口高度 - 减少窗口高度 &gt; 增加窗口宽度 &lt; 减少窗口宽度 = 等分窗口大小 调整窗口宽高的命令可以和数字连用，例如：5 CTRL + W + 连续 5 次增加高度 06. 常用命令速查图 vimrc vimrc 是 vim 的配置文件，可以设置 vim 的配置，包括：热键、配色、语法高亮、插件 等 Linux 中 vimrc 有两个位置，家目录下的配置文件优先级更高 12/etc/vim/vimrc~/.vimrc 常用的插件有： 代码补全 代码折叠 搜索 Git 集成 …… 网上有很多高手已经配置好的针对 python 开发的 vimrc 文件，可以下载过来直接使用，或者等大家多 Linux 比较熟悉后，再行学习！","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xxren8218.github.io/categories/Linux/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://xxren8218.github.io/tags/vim/"}]}],"categories":[{"name":"贪心算法","slug":"贪心算法","permalink":"https://xxren8218.github.io/categories/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"},{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/categories/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"回溯法","slug":"回溯法","permalink":"https://xxren8218.github.io/categories/%E5%9B%9E%E6%BA%AF%E6%B3%95/"},{"name":"二叉树","slug":"二叉树","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"机器学习","slug":"机器学习","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据的lambda架构","slug":"机器学习/大数据的lambda架构","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/"},{"name":"推荐系统实战","slug":"机器学习/大数据的lambda架构/推荐系统实战","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84lambda%E6%9E%B6%E6%9E%84/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"},{"name":"传统算法","slug":"传统算法","permalink":"https://xxren8218.github.io/categories/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"},{"name":"二分查找","slug":"二分查找","permalink":"https://xxren8218.github.io/categories/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"},{"name":"机器学习基础实战","slug":"机器学习基础实战","permalink":"https://xxren8218.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/"},{"name":"python","slug":"python","permalink":"https://xxren8218.github.io/categories/python/"},{"name":"Linux","slug":"Linux","permalink":"https://xxren8218.github.io/categories/Linux/"}],"tags":[{"name":"推荐系统基础","slug":"推荐系统基础","permalink":"https://xxren8218.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"},{"name":"链表","slug":"链表","permalink":"https://xxren8218.github.io/tags/%E9%93%BE%E8%A1%A8/"},{"name":"快慢指针","slug":"快慢指针","permalink":"https://xxren8218.github.io/tags/%E5%BF%AB%E6%85%A2%E6%8C%87%E9%92%88/"},{"name":"辅助列表","slug":"辅助列表","permalink":"https://xxren8218.github.io/tags/%E8%BE%85%E5%8A%A9%E5%88%97%E8%A1%A8/"},{"name":"哈希表","slug":"哈希表","permalink":"https://xxren8218.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"多指针","slug":"多指针","permalink":"https://xxren8218.github.io/tags/%E5%A4%9A%E6%8C%87%E9%92%88/"},{"name":"递归","slug":"递归","permalink":"https://xxren8218.github.io/tags/%E9%80%92%E5%BD%92/"},{"name":"伪头节点","slug":"伪头节点","permalink":"https://xxren8218.github.io/tags/%E4%BC%AA%E5%A4%B4%E8%8A%82%E7%82%B9/"},{"name":"数组","slug":"数组","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"辅助栈","slug":"辅助栈","permalink":"https://xxren8218.github.io/tags/%E8%BE%85%E5%8A%A9%E6%A0%88/"},{"name":"二分法","slug":"二分法","permalink":"https://xxren8218.github.io/tags/%E4%BA%8C%E5%88%86%E6%B3%95/"},{"name":"归并排序","slug":"归并排序","permalink":"https://xxren8218.github.io/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/"},{"name":"机器学习基础","slug":"机器学习基础","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"},{"name":"字符串","slug":"字符串","permalink":"https://xxren8218.github.io/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"name":"排序","slug":"排序","permalink":"https://xxren8218.github.io/tags/%E6%8E%92%E5%BA%8F/"},{"name":"内置函数","slug":"内置函数","permalink":"https://xxren8218.github.io/tags/%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/"},{"name":"动态规划","slug":"动态规划","permalink":"https://xxren8218.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"辅助素组","slug":"辅助素组","permalink":"https://xxren8218.github.io/tags/%E8%BE%85%E5%8A%A9%E7%B4%A0%E7%BB%84/"},{"name":"双指针","slug":"双指针","permalink":"https://xxren8218.github.io/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"查找","slug":"查找","permalink":"https://xxren8218.github.io/tags/%E6%9F%A5%E6%89%BE/"},{"name":"位运算","slug":"位运算","permalink":"https://xxren8218.github.io/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"},{"name":"快速幂","slug":"快速幂","permalink":"https://xxren8218.github.io/tags/%E5%BF%AB%E9%80%9F%E5%B9%82/"},{"name":"栈","slug":"栈","permalink":"https://xxren8218.github.io/tags/%E6%A0%88/"},{"name":"单调栈","slug":"单调栈","permalink":"https://xxren8218.github.io/tags/%E5%8D%95%E8%B0%83%E6%A0%88/"},{"name":"数学","slug":"数学","permalink":"https://xxren8218.github.io/tags/%E6%95%B0%E5%AD%A6/"},{"name":"最小堆","slug":"最小堆","permalink":"https://xxren8218.github.io/tags/%E6%9C%80%E5%B0%8F%E5%A0%86/"},{"name":"双端队列","slug":"双端队列","permalink":"https://xxren8218.github.io/tags/%E5%8F%8C%E7%AB%AF%E9%98%9F%E5%88%97/"},{"name":"单调队列","slug":"单调队列","permalink":"https://xxren8218.github.io/tags/%E5%8D%95%E8%B0%83%E9%98%9F%E5%88%97/"},{"name":"辅助队列","slug":"辅助队列","permalink":"https://xxren8218.github.io/tags/%E8%BE%85%E5%8A%A9%E9%98%9F%E5%88%97/"},{"name":"机器学习基础实战","slug":"机器学习基础实战","permalink":"https://xxren8218.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/"},{"name":"其他","slug":"其他","permalink":"https://xxren8218.github.io/tags/%E5%85%B6%E4%BB%96/"},{"name":"滑动窗口","slug":"滑动窗口","permalink":"https://xxren8218.github.io/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"},{"name":"python基础","slug":"python基础","permalink":"https://xxren8218.github.io/tags/python%E5%9F%BA%E7%A1%80/"},{"name":"vim","slug":"vim","permalink":"https://xxren8218.github.io/tags/vim/"}]}